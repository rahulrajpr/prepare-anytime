{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2Pk36/o+0iSIvFiqpGzEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/functions/20_spark_sql_dataframe_writer_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark DataFrame Writer Methods**\n",
        "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html"
      ],
      "metadata": {
        "id": "IhjL60qlzcYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07RX8Llbl5d4",
        "outputId": "79598c8f-2fcf-4f1e-86f7-26886792932f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install Java and PySpark\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark -q\n",
        "\n",
        "# Set Java home\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the postgre driver\n",
        "\n",
        "!mkdir -p ~/jars\n",
        "!wget -P ~/jars https://jdbc.postgresql.org/download/postgresql-42.6.0.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqoVz4mMmd7-",
        "outputId": "49eb9ea4-1770-42b0-9da3-c9e53e4a31d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-04 06:37:06--  https://jdbc.postgresql.org/download/postgresql-42.6.0.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1081604 (1.0M) [application/java-archive]\n",
            "Saving to: ‘/root/jars/postgresql-42.6.0.jar’\n",
            "\n",
            "postgresql-42.6.0.j 100%[===================>]   1.03M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-11-04 06:37:06 (11.8 MB/s) - ‘/root/jars/postgresql-42.6.0.jar’ saved [1081604/1081604]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession\\\n",
        "            .builder\\\n",
        "            .appName('spark-dataframe')\\\n",
        "            .config(\"spark.jars\", \"/root/jars/postgresql-42.6.0.jar\")\\\n",
        "            .getOrCreate()"
      ],
      "metadata": {
        "id": "9e6Ld9mdmhao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark DataFrame Writer vs DataFrameWriterV2\n",
        "---\n",
        "\n",
        "#### 1. Core Architectural Foundation\n",
        "\n",
        "#### DataFrameWriter (V1)\n",
        "- Built on original **DataSource V1 API**\n",
        "- **Monolithic architecture**: Single, fixed execution path\n",
        "- Tightly coupled with Spark's legacy SQL engine\n",
        "- Designed for traditional HDFS and relational databases\n",
        "\n",
        "#### DataFrameWriterV2\n",
        "- Built on modern **DataSource V2 API**\n",
        "- **Pluggable architecture**: Modular, composable components\n",
        "- Framework for extensible connector development\n",
        "- Designed for cloud-native and modern data formats\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Fundamental Design Philosophy\n",
        "\n",
        "### V1 Approach: \"One Size Fits All\"\n",
        "- Fixed write execution pattern for all data sources\n",
        "- Limited customization points for connector developers\n",
        "- Batch and streaming treated as separate concerns\n",
        "- File-system oriented commit protocols\n",
        "\n",
        "#### V2 Approach: \"Extensible Framework\"\n",
        "- Customizable write execution per data source\n",
        "- Rich interfaces for connector-specific optimizations\n",
        "- Unified batch and streaming treatment\n",
        "- Transaction-aware commit protocols\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Critical Technical Limitations in V1\n",
        "\n",
        "#### Reliability Issues\n",
        "- Non-atomic commits on cloud object stores\n",
        "- No transaction boundaries for partial failures\n",
        "- Corruption risks during job failures\n",
        "- Limited recovery mechanisms\n",
        "\n",
        "#### Extensibility Constraints\n",
        "- Black box execution model\n",
        "- Difficult to implement custom data sources\n",
        "- Limited push-down capability for operations\n",
        "- Rigid interface contracts\n",
        "\n",
        "#### Operational Limitations\n",
        "- Coarse-grained overwrite behavior\n",
        "- Poor integration with catalog systems\n",
        "- Limited schema evolution support\n",
        "- Basic data distribution controls\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. V2 Architectural Solutions\n",
        "\n",
        "#### Transaction Management\n",
        "- Pluggable commit protocols\n",
        "- ACID transaction support\n",
        "- Atomic operation guarantees\n",
        "- Recovery and rollback capabilities\n",
        "\n",
        "#### Extensibility Framework\n",
        "- Clean interfaces for custom implementations\n",
        "- Operation push-down framework\n",
        "- Customizable write optimization\n",
        "- Unified batch and streaming APIs\n",
        "\n",
        "#### Data Management\n",
        "- Fine-grained data distribution controls\n",
        "- Advanced partitioning strategies\n",
        "- Integrated catalog management\n",
        "- Native schema evolution\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Key Differentiators\n",
        "\n",
        "#### Execution Model\n",
        "- **V1**: Fixed pipeline, runtime optimizations only\n",
        "- **V2**: Customizable pipeline, both planning and runtime optimizations\n",
        "\n",
        "#### Connector Development\n",
        "- **V1**: Complex, requires deep Spark internals knowledge\n",
        "- **V2**: Structured, well-defined interfaces and contracts\n",
        "\n",
        "#### Cloud Compatibility\n",
        "- **V1**: Adapted to cloud storage with limitations\n",
        "- **V2**: Designed for cloud-native operation from inception\n",
        "\n",
        "#### Data Ecosystem Integration\n",
        "- **V1**: Basic table format support\n",
        "- **V2**: Native integration with modern table formats (Iceberg, Delta, Hudi)\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. Evolution Context\n",
        "\n",
        "#### V1 Represents Spark's Origins\n",
        "- Born from academic and early internet scale\n",
        "- HDFS and traditional database focus\n",
        "- Batch processing primacy\n",
        "- Single data center deployment model\n",
        "\n",
        "#### V2 Represents Spark's Maturity\n",
        "- Cloud-native and hybrid cloud reality\n",
        "- Streaming and batch unification\n",
        "- Global scale deployment requirements\n",
        "- Diverse data ecosystem integration\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. Practical Implications\n",
        "\n",
        "#### For Data Engineers:\n",
        "- **V1**: Sufficient for basic ETL and analytics\n",
        "- **V2**: Necessary for production-grade, reliable pipelines\n",
        "\n",
        "#### For Platform Developers:\n",
        "- **V1**: Maintenance and compatibility focus\n",
        "- **V2**: Innovation and ecosystem expansion\n",
        "\n",
        "#### For Organizations:\n",
        "- **V1**: Legacy pipeline maintenance\n",
        "- **V2**: Future-proof data platform foundation\n",
        "\n",
        "---\n",
        "\n",
        "#### 8. Strategic Direction\n",
        "\n",
        "#### V1 Status\n",
        "- Maintenance mode\n",
        "- Critical bug fixes only\n",
        "- No new feature development\n",
        "- Gradual deprecation path\n",
        "\n",
        "#### V2 Status\n",
        "- Active development focus\n",
        "- New feature delivery\n",
        "- Ecosystem expansion\n",
        "- Performance optimization priority\n",
        "\n",
        "---\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "The transition from DataFrameWriter to DataFrameWriterV2 represents Spark's evolution from a monolithic data processing engine to a modular, extensible data platform framework. While V1 addressed the initial scale challenges of big data, V2 addresses the reliability, extensibility, and operational requirements of modern data platforms in cloud-native environments.\n",
        "\n",
        "V2 isn't merely an API version increment—it's a fundamental architectural shift that enables Spark to remain relevant in the evolving data ecosystem while maintaining backward compatibility for existing workloads."
      ],
      "metadata": {
        "id": "NVRbDAnZl7ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = 'https://raw.githubusercontent.com/rahulrajpr/prepare-anytime/refs/heads/main/sample-files/csv/sample.csv'\n",
        "!wget {csv_file_path}\n",
        "csv_local_path = '/content/sample.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYw2VtRImWuH",
        "outputId": "b07433a5-199b-4e01-beaf-b639f4cd70e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-04 06:37:16--  https://raw.githubusercontent.com/rahulrajpr/prepare-anytime/refs/heads/main/sample-files/csv/sample.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60302 (59K) [text/plain]\n",
            "Saving to: ‘sample.csv’\n",
            "\n",
            "\rsample.csv            0%[                    ]       0  --.-KB/s               \rsample.csv          100%[===================>]  58.89K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-11-04 06:37:16 (5.08 MB/s) - ‘sample.csv’ saved [60302/60302]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = spark.read\\\n",
        "                 .option('header','true')\\\n",
        "                 .option('inferSchema','true')\\\n",
        "                 .csv(csv_local_path)\n",
        "dataframe.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh-ANsH-0F0w",
        "outputId": "a81ddc4b-93d6-4759-8e42-897635ab4e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+-------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|Name                                                   |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
            "+-----------+--------+------+-------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|1          |0       |3     |Braund, Mr. Owen Harris                                |male  |22.0|1    |0    |A/5 21171       |7.25   |NULL |S       |\n",
            "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)    |female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
            "|3          |1       |3     |Heikkinen, Miss. Laina                                 |female|26.0|0    |0    |STON/O2. 3101282|7.925  |NULL |S       |\n",
            "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)           |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
            "|5          |0       |3     |Allen, Mr. William Henry                               |male  |35.0|0    |0    |373450          |8.05   |NULL |S       |\n",
            "|6          |0       |3     |Moran, Mr. James                                       |male  |NULL|0    |0    |330877          |8.4583 |NULL |Q       |\n",
            "|7          |0       |1     |McCarthy, Mr. Timothy J                                |male  |54.0|0    |0    |17463           |51.8625|E46  |S       |\n",
            "|8          |0       |3     |Palsson, Master. Gosta Leonard                         |male  |2.0 |3    |1    |349909          |21.075 |NULL |S       |\n",
            "|9          |1       |3     |Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      |female|27.0|0    |2    |347742          |11.1333|NULL |S       |\n",
            "|10         |1       |2     |Nasser, Mrs. Nicholas (Adele Achem)                    |female|14.0|1    |0    |237736          |30.0708|NULL |C       |\n",
            "|11         |1       |3     |Sandstrom, Miss. Marguerite Rut                        |female|4.0 |1    |1    |PP 9549         |16.7   |G6   |S       |\n",
            "|12         |1       |1     |Bonnell, Miss. Elizabeth                               |female|58.0|0    |0    |113783          |26.55  |C103 |S       |\n",
            "|13         |0       |3     |Saundercock, Mr. William Henry                         |male  |20.0|0    |0    |A/5. 2151       |8.05   |NULL |S       |\n",
            "|14         |0       |3     |Andersson, Mr. Anders Johan                            |male  |39.0|1    |5    |347082          |31.275 |NULL |S       |\n",
            "|15         |0       |3     |Vestrom, Miss. Hulda Amanda Adolfina                   |female|14.0|0    |0    |350406          |7.8542 |NULL |S       |\n",
            "|16         |1       |2     |Hewlett, Mrs. (Mary D Kingcome)                        |female|55.0|0    |0    |248706          |16.0   |NULL |S       |\n",
            "|17         |0       |3     |Rice, Master. Eugene                                   |male  |2.0 |4    |1    |382652          |29.125 |NULL |Q       |\n",
            "|18         |1       |2     |Williams, Mr. Charles Eugene                           |male  |NULL|0    |0    |244373          |13.0   |NULL |S       |\n",
            "|19         |0       |3     |Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)|female|31.0|1    |0    |345763          |18.0   |NULL |S       |\n",
            "|20         |1       |3     |Masselmani, Mrs. Fatima                                |female|NULL|0    |0    |2649            |7.225  |NULL |C       |\n",
            "+-----------+--------+------+-------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# csv\n",
        "\n",
        "!mkdir output\n",
        "\n",
        "csv_write_path = 'output/csv/dataframe_csv_overwrite.csv'\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('header','true')\\\n",
        "         .option('delimeter',',')\\\n",
        "         .option('dateFormat','yyyy-MM-dd')\\\n",
        "         .option('nullValue','NULL')\\\n",
        "         .option('compression','none')\\\n",
        "         .csv(csv_write_path, mode = 'overwrite')\n",
        "\n",
        "# -- with mode as a method\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('header','true')\\\n",
        "         .option('delimeter',',')\\\n",
        "         .option('dateFormat','yyyy-MM-dd')\\\n",
        "         .option('nullValue','NULL')\\\n",
        "         .option('compression','none')\\\n",
        "         .mode('overwrite')\\\n",
        "         .csv(csv_write_path)\n",
        "\n",
        "# -- with mode as a method  (append)\n",
        "\n",
        "csv_write_path = 'output/csv/dataframe_csv_append.csv'\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('header','true')\\\n",
        "         .option('delimeter',',')\\\n",
        "         .option('dateFormat','yyyy-MM-dd')\\\n",
        "         .option('nullValue','NULL')\\\n",
        "         .option('compression','none')\\\n",
        "         .mode('append')\\\n",
        "         .csv(csv_write_path)\n",
        "\n",
        "# -- lets see how the partitions works\n",
        "\n",
        "csv_write_path = 'output/csv/dataframe_csv_partitions.csv'\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('header','true')\\\n",
        "         .option('delimeter',',')\\\n",
        "         .option('dateFormat','yyyy-MM-dd')\\\n",
        "         .option('nullValue','NULL')\\\n",
        "         .option('compression','none')\\\n",
        "         .mode('overwrite')\\\n",
        "         .partitionBy('PClass','survived')\\\n",
        "         .csv(csv_write_path)"
      ],
      "metadata": {
        "id": "DAV7OkBl0X2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PySpark Write Modes\n",
        "\n",
        "##### Available Write Modes\n",
        "- **`overwrite`** - Completely replaces existing data\n",
        "- **`append`** - Adds new data to existing data\n",
        "- **`ignore`** - No operation if target exists\n",
        "- **`error` / `errorifexists`** - Throws error if target exists (default)\n",
        "\n",
        "---\n",
        "\n",
        "##### Detailed Comparison\n",
        "\n",
        "| Mode | Behavior | When Target Exists | When Target Doesn't Exist | Common Use Cases | Risk Level |\n",
        "|------|----------|-------------------|--------------------------|------------------|------------|\n",
        "| **`overwrite`** | Replaces entire dataset | Deletes all existing data, writes new data | Creates new data | Full refreshes, schema changes, complete replacements | High |\n",
        "| **`append`** | Adds to existing data | New data added to existing data | Creates new data | Incremental loads, event streams, daily batches | Medium |\n",
        "| **`ignore`** | No operation if exists | Silent skip, no action taken | Creates new data | Safe initialization, idempotent pipelines | Low |\n",
        "| **`errorifexists`** | Fails if target exists | Throws AnalysisException | Creates new data | Safety default, preventing accidents | Low |\n",
        "\n",
        "---\n",
        "\n",
        "##### Key Characteristics Summary\n",
        "\n",
        "##### Data Safety\n",
        "- **Safest**: `errorifexists`, `ignore`\n",
        "- **Moderate**: `append`\n",
        "- **Riskiest**: `overwrite`\n",
        "\n",
        "##### Performance Impact\n",
        "- **Fastest**: `ignore` (when skipping)\n",
        "- **Moderate**: `append`, `overwrite` (depends on data size)\n",
        "- **Slowest**: `overwrite` (for large datasets)\n",
        "\n",
        "##### Common Patterns\n",
        "- **Development**: `overwrite` for testing\n",
        "- **Production**: `append` for incremental, `errorifexists` for safety\n",
        "- **Initialization**: `ignore` for first-time setup"
      ],
      "metadata": {
        "id": "2CEl7Tc12Lgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writer -json\n",
        "\n",
        "json_write_path = 'output/json/dataframe_json_overwrite.json'\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('multiLine','true')\\\n",
        "         .mode('overwrite')\\\n",
        "         .json(json_write_path)\n",
        "\n",
        "##--\n",
        "\n",
        "json_write_path = 'output/json/dataframe_json_append.json'\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('multiLine','true')\\\n",
        "         .mode('append')\\\n",
        "         .json(json_write_path)\n",
        "\n",
        "##--\n",
        "\n",
        "json_write_path = 'output/json/dataframe_json_partitioned.json'\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('multiLine','true')\\\n",
        "         .mode('overwrite')\\\n",
        "         .partitionBy('PClass','survived')\\\n",
        "         .json(json_write_path)"
      ],
      "metadata": {
        "id": "epKKQ30x1Ixx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writer - parquet\n",
        "\n",
        "parquet_write_path = 'output/parquet/dataframe_parquet.parquet'\n",
        "\n",
        "dataframe.write\\\n",
        "         .option('mergeSchema','true')\\\n",
        "         .option('compression','gzip')\\\n",
        "         .option('parquet.enable.bloom.filter','true')\\\n",
        "         .partitionBy('PClass','survived')\\\n",
        "         .mode('overwrite')\\\n",
        "         .parquet(parquet_write_path)\n"
      ],
      "metadata": {
        "id": "-W8XEkf24a2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : The Cluster By and Sort By Operation are not available in writer V1, but those are available in writer V2.\n"
      ],
      "metadata": {
        "id": "uhiTxSQmBHlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writer - format\n",
        "\n",
        "format_write_path = 'output/format/dataframe_format_partitioned.csv'\n",
        "\n",
        "dataframe.write\\\n",
        "         .format('csv')\\\n",
        "         .option('header','true')\\\n",
        "         .mode('overwrite')\\\n",
        "         .partitionBy('PClass','survived')\\\n",
        "         .option('compression','gzip')\\\n",
        "         .save(format_write_path)"
      ],
      "metadata": {
        "id": "2RCfMKCt8T2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to test #jdbc\n",
        "\n",
        "# SET THE POSTGRE RDBMS (LOCALLY) TO TEST THE JDBC\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y postgresql postgresql-contrib\n",
        "!service postgresql start\n",
        "!clear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_ewdGHlELVX",
        "outputId": "e6204a92-b3fe-42ea-c22d-fcf4c166e32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common ssl-cert\n",
            "  sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common\n",
            "  postgresql-contrib ssl-cert sysstat\n",
            "0 upgraded, 15 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 42.4 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libjson-xs-perl amd64 4.040-0ubuntu0.22.04.1 [87.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm14 amd64 1:14.0.0-1ubuntu1.1 [24.0 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.19-0ubuntu0.22.04.1 [1,249 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.19-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 42.4 MB in 30s (1,437 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 125586 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../05-libjson-xs-perl_4.040-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libllvm14:amd64.\n",
            "Preparing to unpack .../06-libllvm14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../07-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../08-postgresql-client-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../09-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../10-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../11-postgresql-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../12-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package postgresql-contrib.\n",
            "Preparing to unpack .../13-postgresql-contrib_14+238_all.deb ...\n",
            "Unpacking postgresql-contrib (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../14-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "Setting up libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Setting up postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-common (238) ...\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-contrib (14+238) ...\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "\u001b[H\u001b[2J"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database = 'magic_database'\n",
        "schema = 'magic_schema'\n",
        "\n",
        "user = 'rahul'\n",
        "password = 'rahul_password'\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE USER {user} WITH PASSWORD '{password}';\"\n",
        "!sudo -u postgres psql -c \"CREATE DATABASE {database} OWNER {user};\"\n",
        "!sudo -u postgres psql -d {database} -c \"CREATE SCHEMA {schema} AUTHORIZATION {user};\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQgpnCxsFKZp",
        "outputId": "a138b2b0-cdd4-4e82-ffee-8ae0bdb78200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATE ROLE\n",
            "CREATE DATABASE\n",
            "CREATE SCHEMA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writer - jdbc\n",
        "\n",
        "url =  f\"jdbc:postgresql://localhost:5432/{database}\"\n",
        "table = 'magic_schema.magic_table'\n",
        "\n",
        "properties = {'user':'rahul', 'password':'rahul_password','driver': 'org.postgresql.Driver'}\n",
        "\n",
        "dataframe.repartition(10)\\\n",
        "         .write\\\n",
        "         .mode('overwrite')\\\n",
        "         .option('numPartition',8)\\\n",
        "         .option('partitionColumns','PClass')\\\n",
        "         .jdbc(url = url,\n",
        "               table = table,\n",
        "               properties = properties)\n",
        "\n",
        "#--\n",
        "\n",
        "url =  f\"jdbc:postgresql://localhost:5432/{database}\"\n",
        "\n",
        "table = 'magic_schema.magic_table_apped'\n",
        "\n",
        "properties = {'user':'rahul', 'password':'rahul_password','driver': 'org.postgresql.Driver'}\n",
        "\n",
        "dataframe.repartition(10)\\\n",
        "         .write\\\n",
        "         .mode('append')\\\n",
        "         .option('numPartition',8)\\\n",
        "         .option('partitionColumns','PClass')\\\n",
        "         .jdbc(url = url,\n",
        "               table = table,\n",
        "               properties = properties)"
      ],
      "metadata": {
        "id": "iwwmUOOsFWar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.jdbc(url = url, table = table, properties=properties).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdRmCOCiGfkl",
        "outputId": "2446bf75-5fa9-44bc-c9e3-4990ba7fed66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+----------------------------------------------------+------+----+-----+-----+----------------+--------+-------+--------+\n",
            "|PassengerId|Survived|Pclass|Name                                                |Sex   |Age |SibSp|Parch|Ticket          |Fare    |Cabin  |Embarked|\n",
            "+-----------+--------+------+----------------------------------------------------+------+----+-----+-----+----------------+--------+-------+--------+\n",
            "|119        |0       |1     |Baxter, Mr. Quigg Edmond                            |male  |24.0|0    |1    |PC 17558        |247.5208|B58 B60|C       |\n",
            "|240        |0       |2     |Hunt, Mr. George Henry                              |male  |33.0|0    |0    |SCO/W 1585      |12.275  |NULL   |S       |\n",
            "|138        |0       |1     |Futrelle, Mr. Jacques Heath                         |male  |37.0|1    |0    |113803          |53.1    |C123   |S       |\n",
            "|730        |0       |3     |Ilmakangas, Miss. Pieta Sofia                       |female|25.0|1    |0    |STON/O2. 3101271|7.925   |NULL   |S       |\n",
            "|544        |1       |2     |Beane, Mr. Edward                                   |male  |32.0|1    |0    |2908            |26.0    |NULL   |S       |\n",
            "|861        |0       |3     |Hansen, Mr. Claus Peter                             |male  |41.0|2    |0    |350026          |14.1083 |NULL   |S       |\n",
            "|549        |0       |3     |Goldsmith, Mr. Frank John                           |male  |33.0|1    |1    |363291          |20.525  |NULL   |S       |\n",
            "|480        |1       |3     |Hirvonen, Miss. Hildur E                            |female|2.0 |0    |1    |3101298         |12.2875 |NULL   |S       |\n",
            "|649        |0       |3     |Willey, Mr. Edward                                  |male  |NULL|0    |0    |S.O./P.P. 751   |7.55    |NULL   |S       |\n",
            "|381        |1       |1     |Bidois, Miss. Rosalie                               |female|42.0|0    |0    |PC 17757        |227.525 |NULL   |C       |\n",
            "|493        |0       |1     |Molson, Mr. Harry Markland                          |male  |55.0|0    |0    |113787          |30.5    |C30    |S       |\n",
            "|789        |1       |3     |Dean, Master. Bertram Vere                          |male  |1.0 |1    |2    |C.A. 2315       |20.575  |NULL   |S       |\n",
            "|213        |0       |3     |Perkin, Mr. John Henry                              |male  |22.0|0    |0    |A/5 21174       |7.25    |NULL   |S       |\n",
            "|206        |0       |3     |Strom, Miss. Telma Matilda                          |female|2.0 |0    |1    |347054          |10.4625 |G6     |S       |\n",
            "|96         |0       |3     |Shorney, Mr. Charles Joseph                         |male  |NULL|0    |0    |374910          |8.05    |NULL   |S       |\n",
            "|801        |0       |2     |Ponesell, Mr. Martin                                |male  |34.0|0    |0    |250647          |13.0    |NULL   |S       |\n",
            "|582        |1       |1     |Thayer, Mrs. John Borland (Marian Longstreth Morris)|female|39.0|1    |1    |17421           |110.8833|C68    |C       |\n",
            "|588        |1       |1     |Frolicher-Stehli, Mr. Maxmillian                    |male  |60.0|1    |1    |13567           |79.2    |B41    |C       |\n",
            "|343        |0       |2     |Collander, Mr. Erik Gustaf                          |male  |28.0|0    |0    |248740          |13.0    |NULL   |S       |\n",
            "|788        |0       |3     |Rice, Master. George Hugh                           |male  |8.0 |4    |1    |382652          |29.125  |NULL   |Q       |\n",
            "+-----------+--------+------+----------------------------------------------------+------+----+-----+-----+----------------+--------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!service postgresql stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2LiqvugGx5p",
        "outputId": "a5a88964-cc70-4d7b-f41c-a691a0e53274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Stopping PostgreSQL 14 database server\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saveASTable\n",
        "\n",
        "dataframe.write \\\n",
        "    .mode('overwrite') \\\n",
        "    .option('mergeSchema','true')\\\n",
        "    .partitionBy('PClass') \\\n",
        "    .bucketBy(2, 'survived') \\\n",
        "    .sortBy('PassengerId') \\\n",
        "    .saveAsTable('demo_table')"
      ],
      "metadata": {
        "id": "OHQuZSESKTU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('''select * from demo_table''').printSchema()\n",
        "\n",
        "table_schema =  spark.sql('''select * from demo_table''').schema\n",
        "\n",
        "spark.sql('''select * from demo_table''').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll5SuURQKqCq",
        "outputId": "4b63c1ca-7c3a-4c0f-e805-600b07fb0d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            "\n",
            "+-----------+--------+---------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+------+\n",
            "|PassengerId|Survived|Name                                                     |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|Pclass|\n",
            "+-----------+--------+---------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+------+\n",
            "|1          |0       |Braund, Mr. Owen Harris                                  |male  |22.0|1    |0    |A/5 21171       |7.25   |NULL |S       |3     |\n",
            "|3          |1       |Heikkinen, Miss. Laina                                   |female|26.0|0    |0    |STON/O2. 3101282|7.925  |NULL |S       |3     |\n",
            "|5          |0       |Allen, Mr. William Henry                                 |male  |35.0|0    |0    |373450          |8.05   |NULL |S       |3     |\n",
            "|6          |0       |Moran, Mr. James                                         |male  |NULL|0    |0    |330877          |8.4583 |NULL |Q       |3     |\n",
            "|8          |0       |Palsson, Master. Gosta Leonard                           |male  |2.0 |3    |1    |349909          |21.075 |NULL |S       |3     |\n",
            "|9          |1       |Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)        |female|27.0|0    |2    |347742          |11.1333|NULL |S       |3     |\n",
            "|11         |1       |Sandstrom, Miss. Marguerite Rut                          |female|4.0 |1    |1    |PP 9549         |16.7   |G6   |S       |3     |\n",
            "|13         |0       |Saundercock, Mr. William Henry                           |male  |20.0|0    |0    |A/5. 2151       |8.05   |NULL |S       |3     |\n",
            "|14         |0       |Andersson, Mr. Anders Johan                              |male  |39.0|1    |5    |347082          |31.275 |NULL |S       |3     |\n",
            "|15         |0       |Vestrom, Miss. Hulda Amanda Adolfina                     |female|14.0|0    |0    |350406          |7.8542 |NULL |S       |3     |\n",
            "|17         |0       |Rice, Master. Eugene                                     |male  |2.0 |4    |1    |382652          |29.125 |NULL |Q       |3     |\n",
            "|19         |0       |Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)  |female|31.0|1    |0    |345763          |18.0   |NULL |S       |3     |\n",
            "|20         |1       |Masselmani, Mrs. Fatima                                  |female|NULL|0    |0    |2649            |7.225  |NULL |C       |3     |\n",
            "|23         |1       |\"McGowan, Miss. Anna \"\"Annie\"\"\"                          |female|15.0|0    |0    |330923          |8.0292 |NULL |Q       |3     |\n",
            "|25         |0       |Palsson, Miss. Torborg Danira                            |female|8.0 |3    |1    |349909          |21.075 |NULL |S       |3     |\n",
            "|26         |1       |Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)|female|38.0|1    |5    |347077          |31.3875|NULL |S       |3     |\n",
            "|27         |0       |Emir, Mr. Farred Chehab                                  |male  |NULL|0    |0    |2631            |7.225  |NULL |C       |3     |\n",
            "|29         |1       |\"O'Dwyer, Miss. Ellen \"\"Nellie\"\"\"                        |female|NULL|0    |0    |330959          |7.8792 |NULL |Q       |3     |\n",
            "|30         |0       |Todoroff, Mr. Lalio                                      |male  |NULL|0    |0    |349216          |7.8958 |NULL |S       |3     |\n",
            "|33         |1       |Glynn, Miss. Mary Agatha                                 |female|NULL|0    |0    |335677          |7.75   |NULL |Q       |3     |\n",
            "+-----------+--------+---------------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `partitionBy` vs `bucketBy` vs `sortBy`\n",
        "---\n",
        "##### partitionBy - **Physical Separation**\n",
        "- **What**: Creates **actual folders/directories** on storage\n",
        "- **Visual**: You can SEE the separation in file explorer\n",
        "- **Structure**: `country=US/`, `country=UK/` as separate folders\n",
        "- **Analogy**: Library with separate rooms for each genre\n",
        "---\n",
        "##### bucketBy - **Logical Separation**  \n",
        "- **What**: Splits data into **fixed number of files** using hashing\n",
        "- **Visual**: All files in same folder, separation is INTERNAL\n",
        "- **Structure**: 10 files where user_id hashes determine file location\n",
        "- **Analogy**: Single room with numbered shelves (calculate which shelf)\n",
        "---\n",
        "##### sortBy - **Internal Ordering**\n",
        "- **What**: **Sorts data rows** within each file\n",
        "- **Visual**: No structural change, just internal row order\n",
        "- **Structure**: Records ordered by timestamp within each file\n",
        "- **Analogy**: Books arranged alphabetically on each shelf\n",
        "---\n",
        "##### Availability Matrix\n",
        "\n",
        "| Method | CSV | JSON | Parquet/ORC | JDBC | saveAsTable |\n",
        "|--------|-----|------|-------------|------|-------------|\n",
        "| **partitionBy** | ✅ | ✅ | ✅ | ❌ | ✅ |\n",
        "| **bucketBy** | ❌ | ❌ | ❌ | ❌ | ✅ |\n",
        "| **sortBy** | ❌ | ❌ | ❌ | ❌ | ✅ |\n",
        "---\n",
        "##### Detailed Comparison\n",
        "| Aspect | partitionBy | bucketBy | sortBy |\n",
        "|--------|-------------|----------|---------|\n",
        "| **Separation Level** | Directory | File | Row |\n",
        "| **Visibility** | Visible in file system | Hidden, internal | Hidden, internal |\n",
        "| **Data Access** | Direct folder navigation | Hash calculation | Sequential scanning |\n",
        "| **Optimal Use Case** | Low cardinality (≤1000 values) | High cardinality (millions) | Ordered access patterns |\n",
        "| **Performance Benefit** | Partition pruning | Join optimization | Range query optimization |\n",
        "| **File Impact** | Multiple folders with files | Fixed files in one folder | Same files, sorted internally |\n",
        "---\n",
        "##### When to Use Each Method?\n",
        "---\n",
        "##### ✅ Use partitionBy when:\n",
        "- You have clear categories (country, year, month)\n",
        "- You frequently filter by these categories\n",
        "- You need to manage data lifecycle (drop old partitions)\n",
        "- **Works with**: Files (CSV, JSON, Parquet) + Tables\n",
        "---\n",
        "##### ✅ Use bucketBy when:\n",
        "- You have high-cardinality columns (user_id, product_id)\n",
        "- Tables are frequently joined on these columns\n",
        "- You need even data distribution\n",
        "- **Works with**: saveAsTable ONLY\n",
        "---\n",
        "##### ✅ Use sortBy when:\n",
        "- You perform range queries (BETWEEN, >, <)\n",
        "- Data has natural ordering (timestamps, sequences)\n",
        "- You need better compression\n",
        "- **Works with**: saveAsTable ONLY\n",
        "---\n",
        "## Critical Limitations\n",
        "---\n",
        "##### File Formats (CSV, JSON, Parquet, ORC):\n",
        "- **Only partitionBy** available for physical organization\n",
        "- **No bucketing** - cannot optimize joins at write time\n",
        "- **No sortBy** - must pre-sort DataFrames before writing\n",
        "---\n",
        "##### JDBC Writes:\n",
        "- **No partitioning** - database handles table partitioning\n",
        "- **No bucketing** - database handles indexing\n",
        "- **No sortBy** - database handles query optimization\n",
        "---\n",
        "##### saveAsTable (Hive/Spark Tables):\n",
        "- **Full feature set** available\n",
        "- **Only method** for bucketing and sorting during write\n",
        "- **Requires** metastore integration\n",
        "---\n",
        "##### Best Practices\n",
        "---\n",
        "##### For Maximum Performance (saveAsTable only):\n",
        "Use the **three-layer optimization**:\n",
        "1. **partitionBy** for coarse-grained physical separation\n",
        "2. **bucketBy** for join optimization and even distribution  \n",
        "3. **sortBy** for scan efficiency and compression\n",
        "---\n",
        "##### Cardinality Guidelines:\n",
        "- **partitionBy**: 10-1000 distinct values ideal\n",
        "- **bucketBy**: Millions of distinct values handled well\n",
        "- **sortBy**: No cardinality limits\n",
        "---\n",
        "##### File Management:\n",
        "- **partitionBy**: Risk of too many small files\n",
        "- **bucketBy**: Fixed file count, predictable\n",
        "- **sortBy**: No impact on file count\n",
        "---\n",
        "##### Summary\n",
        "✅ **Supported**:\n",
        "- partitionBy: Universal (files + tables)\n",
        "- bucketBy: saveAsTable exclusive\n",
        "- sortBy: saveAsTable exclusive\n",
        "\n",
        "❌ **Not Supported**:\n",
        "- bucketBy/sortBy with file formats (CSV, JSON, Parquet)\n",
        "- Any organization methods with JDBC writer\n",
        "- partitionBy with JDBC (database handles partitioning)\n",
        "---\n",
        "**Key Insight**: partitionBy organizes your storage, bucketBy organizes your data relationships, sortBy organizes your data access patterns."
      ],
      "metadata": {
        "id": "ThU4D64sOJMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l_gOzbfmg6g",
        "outputId": "3a683df7-5b4c-47ce-e0f3-38175101ef55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('PassengerId', IntegerType(), True), StructField('Survived', IntegerType(), True), StructField('Name', StringType(), True), StructField('Sex', StringType(), True), StructField('Age', DoubleType(), True), StructField('SibSp', IntegerType(), True), StructField('Parch', IntegerType(), True), StructField('Ticket', StringType(), True), StructField('Fare', DoubleType(), True), StructField('Cabin', StringType(), True), StructField('Embarked', StringType(), True), StructField('Pclass', IntegerType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q_SZmDYmjhM",
        "outputId": "74345a2f-0ce0-4d0c-df56-5f6887bdb986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[PassengerId: int, Survived: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string, Pclass: int]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# insertInto\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "compatible_df = dataframe.to(table_schema)\n",
        "\n",
        "compatible_df.write\\\n",
        "             .insertInto('demo_table')"
      ],
      "metadata": {
        "id": "P7It05X2K-F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Spark Warehouse Table vs Spark Temp View - Comparison\n",
        "\n",
        "| Feature | Spark Warehouse Table (Managed) | Spark Temp View |\n",
        "|---------|---------------------------------|-----------------|\n",
        "| **Persistence & Storage** | Yes. Data is physically stored in the Spark SQL warehouse directory (e.g., `spark-warehouse/`). | No. It is purely a logical abstraction or a \"view\" over existing data. It holds no data itself. |\n",
        "| **Lifetime** | Permanent. Persists until explicitly dropped by a `DROP TABLE` command. Survives Spark application restarts. | Session-scoped. Automatically disappears when the SparkSession ends. For GLOBAL TEMP VIEW, it is tied to the Spark application. |\n",
        "| **Metadata Management** | Cataloged. Its schema and location are stored in a metastore (Spark's built-in metastore or Hive). | Not Cataloged. Its definition is only known to the SparkSession that created it. |\n",
        "| **Impact of DROP** | Deletes both the metadata AND the underlying data files. | Only deletes the view definition. The underlying data source is completely unaffected. |\n",
        "| **Underlying Data Source** | The data is the table itself. The table \"owns\" the data. | Can be built on top of an existing table, a file (CSV, Parquet), or the result of a DataFrame transformation. |\n",
        "| **Primary Use Case** | For data that needs to be stored, managed, and shared long-term across multiple jobs and users. The \"single source of truth.\" | For ad-hoc, session-specific data manipulation. Great for breaking down complex queries, providing a friendly name to a complex DataFrame, or during exploratory data analysis. |\n",
        "\n",
        "---\n",
        "\n",
        "##### When to Use Which?\n",
        "\n",
        "### Use a Warehouse Table when...\n",
        "- You need to persist the results of your processing\n",
        "- The data is a shared dimension or fact table for other jobs/users\n",
        "- You are building a curated dataset or a data mart\n",
        "- The data's lifecycle should be managed by Spark\n",
        "\n",
        "##### Use a Temp View when...\n",
        "- You are performing exploratory data analysis in a notebook\n",
        "- You need to simplify a complex SQL query by breaking it into parts\n",
        "- You are working with DataFrames in Python/Scala/Java and want to run SQL on them\n",
        "- The data is temporary and only relevant for the duration of your current session or script\n",
        "\n",
        "---\n",
        "\n",
        "##### Key Analogy\n",
        "\n",
        "**Warehouse Table** = The physical house (permanent structure)\n",
        "\n",
        "**Temp View** = The blueprints or brochure (temporary description)"
      ],
      "metadata": {
        "id": "H1RhleoaoG15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "r2tq9CU-R5gr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}