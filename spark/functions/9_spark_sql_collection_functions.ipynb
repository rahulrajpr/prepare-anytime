{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0eDX/ins66xVJ4JXcVysz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/functions/9_spark_sql_collection_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark Collection Functions**\n",
        "-------------------------------------\n",
        "https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#collection-functions"
      ],
      "metadata": {
        "id": "gGFTNsyfkbzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collections\n",
        "-------------------------------------\n",
        "\n",
        "In **Apache Spark**, **collections** are **complex column types** that allow you to store **multiple values or structured data** inside a **single DataFrame column**.  \n",
        "Collections provide a **flexible way** to manage, query, and process **nested or semi-structured data** efficiently.\n",
        "-------------------------------------"
      ],
      "metadata": {
        "id": "OGeDg5dol6vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-------------------------------------\n",
        "\n",
        "#### Common Collection Types\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "| Collection Type | Description | Example Use Case |\n",
        "|-----------------|------------|-----------------|\n",
        "| **ArrayType**   | Ordered list of elements of the **same type** | Tags for a product, list of skills, numerical arrays |\n",
        "| **MapType**     | **Key-value pairs** where each key maps to a value | User properties, configuration settings, JSON-like data |\n",
        "\n",
        "-------------------------------------\n",
        "-------------------------------------"
      ],
      "metadata": {
        "id": "9sDISpf4mIUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-------------------------------------\n",
        "\n",
        "#### Relationship Between Collections\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "- **Maps** can store **arrays or structs** as values.  \n",
        "  - Example: `map<string, struct<city:string, zip:int>>`\n",
        "- **Arrays** can contain **structs** or **maps** as elements.  \n",
        "  - Example: `array<struct<key:string, value:int>>`\n",
        "\n",
        "These combinations enable Spark to efficiently handle **nested, hierarchical, or semi-structured datasets** like JSON, logs, and complex business data.\n",
        "\n",
        "-------------------------------------\n"
      ],
      "metadata": {
        "id": "3TPBYkXemMpe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjZt9PqukDCl"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('spark-functions').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql = '''\n",
        "create or replace temp view input_view as\n",
        "(\n",
        "select\n",
        "  named_struct('name','rahul','age',28,'profession','dataengineering') as struct1,\n",
        "  sequence(1,15,1) as array1,\n",
        "  array('apple','orange','kiwi') as array2,\n",
        "  map('name','rahul','age',28,'profession','dataengineering') as map1,\n",
        "  map('product','laptop','price',999,'category','electronics','stock',15,'rating','4.5') as map2\n",
        ")\n",
        "'''\n",
        "spark.sql(sql)\n",
        "\n",
        "##----\n",
        "\n",
        "sql = '''select * from input_view'''\n",
        "\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkMwqZ_4nSWU",
        "outputId": "02bc0b44-b443-4016-f4e1-480672f045f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+---------------------------------------------------+---------------------+---------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
            "|struct1                     |array1                                             |array2               |map1                                                     |map2                                                                                  |\n",
            "+----------------------------+---------------------------------------------------+---------------------+---------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
            "|{rahul, 28, dataengineering}|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[apple, orange, kiwi]|{name -> rahul, age -> 28, profession -> dataengineering}|{product -> laptop, price -> 999, category -> electronics, stock -> 15, rating -> 4.5}|\n",
            "+----------------------------+---------------------------------------------------+---------------------+---------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# size\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  size(array1) as arraySize,\n",
        "  size(map1) as mapSize\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfeYw2jEUowb",
        "outputId": "342ff005-ab44-465b-cbff-3984e62c7a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|arraySize|mapSize|\n",
            "+---------+-------+\n",
            "|15       |3      |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cardinality : for array and map\n",
        "\n",
        "sql = '''\n",
        "SELECT\n",
        "  cardinality(array1) as arrayCardinality,\n",
        "  cardinality(map1) as mapCardinality\n",
        "FROM input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cOzmbj8VkoQ",
        "outputId": "110b6c70-b344-4221-c8b4-fedbda662b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------+\n",
            "|arrayCardinality|mapCardinality|\n",
            "+----------------+--------------+\n",
            "|15              |3             |\n",
            "+----------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## size vs cardinality functions\n",
        "\n",
        "✅ **Functionally Identical** - Both return the same results for arrays and maps  \n",
        "✅ **Interchangeable** - Can be used interchangeably without any difference in output  \n",
        "✅ **Personal Preference** - Choose based on readability context:  \n",
        "   - Use `size()` for general programming contexts  \n",
        "   - Use `cardinality()` for SQL-standard compliance"
      ],
      "metadata": {
        "id": "sJOYGAzdVm_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse\n",
        "\n",
        "sql = '''\n",
        "  select array1,\n",
        "  reverse(array1) as reversedArray\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoBGu_rifN6V",
        "outputId": "45e2427e-4dde-4715-8524-200a560a7a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|array1                                             |reversedArray                                      |\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]|\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort_array\n",
        "\n",
        "# - acseding order by default\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  array1,\n",
        "  sort_array(array1) as array_sortOut\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "# - descending order\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  array1,\n",
        "  sort_array(array1,False) as array_sortOut\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ffSjsdbfPTj",
        "outputId": "fdc915a2-b62e-4751-948e-706f229252a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|array1                                             |array_sortOut                                      |\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|array1                                             |array_sortOut                                      |\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]|\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# array_sort : left and right are the function keywords\n",
        "# Note : Array-Sort & Sort-Array are different, comparsion table down below\n",
        "# - acsedingt order by default\n",
        "\n",
        "sql = '''\n",
        "select array1,array_sort(array1) as array_sortOut\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##-- descending order\n",
        "\n",
        "sql = '''\n",
        "select array1,array_sort(array1,(left,right)-> right  - left) as array_sortOut\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##-- custom function 1 -- ordering on ascening on the legth of the element\n",
        "\n",
        "sql = '''\n",
        "select array2,array_sort(array2,(left,right)-> length(left)  - length(right)) as array_sortOut\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##-- custom function 2 -- ordering on descending on the length of the element\n",
        "\n",
        "sql = '''\n",
        "select array2,array_sort(array2,(left,right)-> length(right)  - length(left)) as array_sortOut\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B3z6-G8oItW",
        "outputId": "d9fed841-697c-4f13-fbc4-d0368bf606b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|array1                                             |array_sortOut                                      |\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|array1                                             |array_sortOut                                      |\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]|\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "\n",
            "+---------------------+---------------------+\n",
            "|array2               |array_sortOut        |\n",
            "+---------------------+---------------------+\n",
            "|[apple, orange, kiwi]|[kiwi, apple, orange]|\n",
            "+---------------------+---------------------+\n",
            "\n",
            "+---------------------+---------------------+\n",
            "|array2               |array_sortOut        |\n",
            "+---------------------+---------------------+\n",
            "|[apple, orange, kiwi]|[orange, apple, kiwi]|\n",
            "+---------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------\n",
        "#### `reverse` vs `sort_array` vs `array_sort`\n",
        "-----------------------------------\n",
        "\n",
        "| Aspect | reverse | sort_array | array_sort |\n",
        "|--------|-----------|--------------|--------------|\n",
        "| **Purpose** | Reverses element order | Sorts with explicit order control | Sorts with custom comparator |\n",
        "| **Parameters** | `(array)` | `(array, [asc/desc])` | `(array, [comparator])` |\n",
        "| **Order Control** | None - always reverses | Boolean flag (true=asc, false=desc) | Lambda function for custom logic |\n",
        "| **Custom Sorting** | No | No | Yes |\n",
        "| **Return Type** | Array | Array | Array |\n",
        "| **Default Behavior** | Reverses current order | Ascending sort | Ascending sort |\n",
        "-----------------------------------"
      ],
      "metadata": {
        "id": "8A9E9SSSbs3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## aggregate : aggregate(expr, start, merge, finish)\n",
        "\n",
        "## sum\n",
        "\n",
        "sql = '''\n",
        "select aggregate(array1,0, (res,x) -> res+x) as sumTotal\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMAS2kNIt5w-",
        "outputId": "33ee0cb3-21ad-4876-b9a2-91d9594b71f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|sumTotal|\n",
            "+--------+\n",
            "|120     |\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aggregate : aggregate(expr, start, merge, finish)\n",
        "\n",
        "## max\n",
        "\n",
        "sql = '''\n",
        "select aggregate(array1,0, (res,x) -> case when res < x then x else res end) as maxVal\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##- min\n",
        "\n",
        "sql = '''\n",
        "select aggregate(array1,0, (res,x) -> case when res > x then x else res end) as minVal\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxefTxLPyr9g",
        "outputId": "8eb53402-3081-4080-ba4d-ed684c95f6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|maxVal|\n",
            "+------+\n",
            "|15    |\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|minVal|\n",
            "+------+\n",
            "|0     |\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aggregate : aggregate(expr, start, merge, finish)\n",
        "\n",
        "## average\n",
        "\n",
        "sql = '''\n",
        "SELECT array1,\n",
        "       aggregate(array1,\n",
        "                struct(0 as sm, 0 as cnt),\n",
        "                (res, x) -> struct(res.sm + x as sm, res.cnt + 1 as cnt),\n",
        "                res -> res.sm / NULLIF(res.cnt, 0)\n",
        "                ) as AvCal\n",
        "FROM input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaxHqydWzgtO",
        "outputId": "c214c3b0-68c5-41c8-acdf-a6169ecab291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+-----+\n",
            "|array1                                             |AvCal|\n",
            "+---------------------------------------------------+-----+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|8.0  |\n",
            "+---------------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aggregate : aggregate(expr, start, merge, finish)\n",
        "\n",
        "## find the elements greater than a threshold 10\n",
        "\n",
        "sql = '''\n",
        "SELECT array1,\n",
        "       aggregate(array1,\n",
        "                0,\n",
        "                (res, x) -> res + (case when x > 10 then 1 else 0 end),\n",
        "                res -> res\n",
        "                ) as AvCal\n",
        "FROM input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrbZ4JhD03D-",
        "outputId": "e7369a10-900e-40b5-ea05-5d659cfea1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+-----+\n",
            "|array1                                             |AvCal|\n",
            "+---------------------------------------------------+-----+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|5    |\n",
            "+---------------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aggregate : aggregate(expr, start, merge, finish)\n",
        "\n",
        "## string concatenation\n",
        "\n",
        "sql = '''\n",
        "SELECT array2,\n",
        "       aggregate(array2,\n",
        "                '',\n",
        "                (res, x) -> case when res = '' then x else res ||'-'|| x end,\n",
        "                res -> res\n",
        "                ) as AvCal\n",
        "FROM input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ciXbgz67ri",
        "outputId": "ed3c019e-7d60-42a2-b40e-7e5184d95b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-----------------+\n",
            "|array2               |AvCal            |\n",
            "+---------------------+-----------------+\n",
            "|[apple, orange, kiwi]|apple-orange-kiwi|\n",
            "+---------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce : similar to aggregate function , works with arrays\n",
        "\n",
        "# sum of elements\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  array1,\n",
        "  reduce(array1,\n",
        "         0,\n",
        "         (res,x) -> res + x,\n",
        "         res->res) as sumOfElements\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUub-J5tWjWl",
        "outputId": "c28ef521-2d39-403a-e83e-f26f4fff361e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+-------------+\n",
            "|array1                                             |sumOfElements|\n",
            "+---------------------------------------------------+-------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|120          |\n",
            "+---------------------------------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce : similar to aggregate function , works with arrays\n",
        "\n",
        "# min\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  array1,\n",
        "  reduce(array1,\n",
        "        0,\n",
        "        (res,x) -> case when x < res then x else res end) as minValue\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KGTASEyWjT2",
        "outputId": "4aec9eb4-2249-4b5f-99bc-b7e5b4d675c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+--------+\n",
            "|array1                                             |minValue|\n",
            "+---------------------------------------------------+--------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|0       |\n",
            "+---------------------------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce : similar to aggregate function , works with arrays\n",
        "\n",
        "# average\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  array1,\n",
        "  reduce(array1,\n",
        "         struct(0 as sm, 0 as cnt),\n",
        "         (res,x) -> struct(res.sm + x as sm, res.cnt +1 as cnt),\n",
        "         res -> res.sm/nullif(res.cnt,0)) as ArrayAvg\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_sGUOo9XubM",
        "outputId": "723ff9ff-0787-4d89-f34d-ea487d487681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+--------+\n",
            "|array1                                             |ArrayAvg|\n",
            "+---------------------------------------------------+--------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|8.0     |\n",
            "+---------------------------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce : similar to aggregate function , works with arrays\n",
        "\n",
        "# string conatination\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  array2,\n",
        "  reduce(array2,\n",
        "         '',\n",
        "         (res,x) -> case when res <> '' then res ||'-'|| x else x end,\n",
        "         res -> res) as concatString\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa4x3HE1WjRI",
        "outputId": "e173b115-ed07-485a-ccaa-d8616eebf0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-----------------+\n",
            "|array2               |concatString     |\n",
            "+---------------------+-----------------+\n",
            "|[apple, orange, kiwi]|apple-orange-kiwi|\n",
            "+---------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------\n",
        "#### aggregate vs reduce Comparison\n",
        "---------------------------------------\n",
        "\n",
        "| Aspect | `aggregate` | `reduce` |\n",
        "|--------|-------------|----------|\n",
        "| **Purpose** | Applies binary operator to array with initial state, converts final state using finish function | Applies binary operator to array with initial state, converts final state using finish function |\n",
        "| **Parameters** | `(expr, start, merge, finish)` | `(expr, start, merge, finish)` |\n",
        "| **Functionality** | **Identical** to reduce | **Identical** to aggregate |\n",
        "| **Return Type** | Same as reduce | Same as aggregate |\n",
        "| **Usage** | SQL standard name | More common in functional programming |\n",
        "\n",
        "---------------------------------------\n",
        "##### Conclusion:\n",
        "---------------------------------------\n",
        "✅ **Functionally Identical** - Both perform exactly the same operation  \n",
        "✅ **Interchangeable** - Can be used interchangeably  \n",
        "✅ **Syntax Identical** - Same parameters and behavior\n",
        "\n",
        "---------------------------------------"
      ],
      "metadata": {
        "id": "kTj1wTWOaXPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concat : can be used for string and arrays , NOT for Map and Struct\n",
        "\n",
        "sql = '''select concat('rahul','+','lathika','->','skylr') as concatOut '''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#---\n",
        "\n",
        "sql = '''\n",
        "select concat(array1, array2) as concatOut\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b_PLTi_8dSs",
        "outputId": "326bb8a3-3d80-411c-a0d9-ae8667140e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|concatOut           |\n",
            "+--------------------+\n",
            "|rahul+lathika->skylr|\n",
            "+--------------------+\n",
            "\n",
            "+------------------------------------------------------------------------+\n",
            "|concatOut                                                               |\n",
            "+------------------------------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, apple, orange, kiwi]|\n",
            "+------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# element_at : to access elements, position of map and key for map\n",
        "\n",
        "##- for array\n",
        "\n",
        "sql = '''\n",
        "select array2, element_at(array2,2) as element_atOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##- for map\n",
        "\n",
        "sql = '''\n",
        "select map1, element_at(map1,'name') as element_atOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDbUw_KP-JBt",
        "outputId": "800fa3ed-1fc1-4e51-a29e-56391715ee51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------+\n",
            "|array2               |element_atOut|\n",
            "+---------------------+-------------+\n",
            "|[apple, orange, kiwi]|orange       |\n",
            "+---------------------+-------------+\n",
            "\n",
            "+---------------------------------------------------------+-------------+\n",
            "|map1                                                     |element_atOut|\n",
            "+---------------------------------------------------------+-------------+\n",
            "|{name -> rahul, age -> 28, profession -> dataengineering}|rahul        |\n",
            "+---------------------------------------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try_element_at\n",
        "\n",
        "##- for array\n",
        "\n",
        "sql = '''\n",
        "select array2, try_element_at(array2,2) as element_atOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##- for map\n",
        "\n",
        "sql = '''\n",
        "select map1, try_element_at(map1,'name') as element_atOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Zq84fhluxc",
        "outputId": "ca48371b-adbe-4aeb-9fa2-81808299232d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------+\n",
            "|array2               |element_atOut|\n",
            "+---------------------+-------------+\n",
            "|[apple, orange, kiwi]|orange       |\n",
            "+---------------------+-------------+\n",
            "\n",
            "+---------------------------------------------------------+-------------+\n",
            "|map1                                                     |element_atOut|\n",
            "+---------------------------------------------------------+-------------+\n",
            "|{name -> rahul, age -> 28, profession -> dataengineering}|rahul        |\n",
            "+---------------------------------------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exists :\n",
        "\n",
        "# (only for array) object and test function are required as parameters\n",
        "\n",
        "sql = '''\n",
        "select array1, exists(array1,x->x=15) as existsOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##-- let check the array contains even values\n",
        "\n",
        "sql = '''\n",
        "select array1, exists(array1,x->x%2=0) as existsOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##-- let check the array contains no nulls\n",
        "\n",
        "sql = '''\n",
        "select array1, NOT exists(array1,x->x is null) as existsOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmJtVYjj-frZ",
        "outputId": "3ee62aee-1bb6-40db-f19f-11e24a60b1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+---------+\n",
            "|array1                                             |existsOut|\n",
            "+---------------------------------------------------+---------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|true     |\n",
            "+---------------------------------------------------+---------+\n",
            "\n",
            "+---------------------------------------------------+---------+\n",
            "|array1                                             |existsOut|\n",
            "+---------------------------------------------------+---------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|true     |\n",
            "+---------------------------------------------------+---------+\n",
            "\n",
            "+---------------------------------------------------+---------+\n",
            "|array1                                             |existsOut|\n",
            "+---------------------------------------------------+---------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|true     |\n",
            "+---------------------------------------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forall : checks a predicate is applicable all the elemenets in an array\n",
        "\n",
        "sql = '''\n",
        "SELECT array1,\n",
        "       forall(array1, x -> try_cast(x AS INT) IS NOT NULL) as all_numeric\n",
        "FROM input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4D9KesHPxb6",
        "outputId": "4d50c8a7-ce39-4b58-d52b-9f5cf2f81993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+-----------+\n",
            "|array1                                             |all_numeric|\n",
            "+---------------------------------------------------+-----------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|true       |\n",
            "+---------------------------------------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### exists vs forall\n",
        "---------------------------------\n",
        "\n",
        "| Aspect | `exists` | `forall` |\n",
        "|--------|----------|----------|\n",
        "| **Purpose** | Checks if **AT LEAST ONE** element satisfies condition | Checks if **ALL** elements satisfy condition |\n",
        "| **Return Type** | Boolean | Boolean |\n",
        "| **Returns `true` when** | Any element matches the condition | All elements match the condition |\n",
        "---------------------------------"
      ],
      "metadata": {
        "id": "ksDJdRIRQs_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter\n",
        "\n",
        "# only works with arrays\n",
        "\n",
        "sql = '''\n",
        "select array1, filter(array1,x->x%2=0) as filterOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#---\n",
        "\n",
        "sql = '''\n",
        "select array1, filter(array1,x-> lower(x) like '%a%') as filterOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylc8WTS2ABl2",
        "outputId": "d322d339-7f0b-429b-f80a-b83d39abaf7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+------------------------+\n",
            "|array1                                             |filterOut               |\n",
            "+---------------------------------------------------+------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[2, 4, 6, 8, 10, 12, 14]|\n",
            "+---------------------------------------------------+------------------------+\n",
            "\n",
            "+---------------------------------------------------+---------+\n",
            "|array1                                             |filterOut|\n",
            "+---------------------------------------------------+---------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[]       |\n",
            "+---------------------------------------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map_filter : maps are getting filters through the using a predicate for key or valus\n",
        "\n",
        "# only works for maps\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  map2,\n",
        "  map_filter(map2,(k,v)-> try_cast(v as float) is not null) as resultMap\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "##--\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  map2,\n",
        "  map_filter(map2,(k,v)-> startswith(k,'p')) as resultMap\n",
        "from input_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S11i9a9LO9Yz",
        "outputId": "09ffb14f-b4bd-4869-ad64-93c0282b6270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------+------------------------------------------+\n",
            "|map2                                                                                  |resultMap                                 |\n",
            "+--------------------------------------------------------------------------------------+------------------------------------------+\n",
            "|{product -> laptop, price -> 999, category -> electronics, stock -> 15, rating -> 4.5}|{price -> 999, stock -> 15, rating -> 4.5}|\n",
            "+--------------------------------------------------------------------------------------+------------------------------------------+\n",
            "\n",
            "+--------------------------------------------------------------------------------------+---------------------------------+\n",
            "|map2                                                                                  |resultMap                        |\n",
            "+--------------------------------------------------------------------------------------+---------------------------------+\n",
            "|{product -> laptop, price -> 999, category -> electronics, stock -> 15, rating -> 4.5}|{product -> laptop, price -> 999}|\n",
            "+--------------------------------------------------------------------------------------+---------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------\n",
        "### `filter` vs `map_filter` Comparison\n",
        "-------------------------------------\n",
        "\n",
        "| Aspect | filter | map_filter |\n",
        "|--------|----------|--------------|\n",
        "| **Purpose** | Filters **ARRAY** elements based on condition | Filters **MAP** entries based on condition |\n",
        "| **Input Type** | Array | Map |\n",
        "| **Lambda Parameters** | Single parameter (array element) | Two parameters (key, value) |\n",
        "| **Return Type** | Array (filtered elements) | Map (filtered key-value pairs) |\n",
        "-------------------------------------"
      ],
      "metadata": {
        "id": "0KprqoFlUPvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform\n",
        "\n",
        "## multiply\n",
        "\n",
        "sql = '''\n",
        "select array1,\n",
        "transform(array1,x->x*2) as twoTimesX\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRPgHWz-RRQJ",
        "outputId": "79c7d040-313c-46ff-d8fd-88adb8cd371c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+--------------------------------------------------------+\n",
            "|array1                                             |twoTimesX                                               |\n",
            "+---------------------------------------------------+--------------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]|\n",
            "+---------------------------------------------------+--------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform\n",
        "\n",
        "## cleaning text\n",
        "\n",
        "sql = '''\n",
        "select array2,\n",
        "transform(array2,x->trim(upper(x))) as cleanedText\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kqzzFxPggXA",
        "outputId": "66345a50-7742-4922-b977-f3c49a4a7470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+---------------------+\n",
            "|array2               |cleanedText          |\n",
            "+---------------------+---------------------+\n",
            "|[apple, orange, kiwi]|[APPLE, ORANGE, KIWI]|\n",
            "+---------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform\n",
        "\n",
        "## with a condition\n",
        "\n",
        "sql = '''\n",
        "select array1,\n",
        "transform(array1,x-> case when x > 10 then x*2 else x end) as conditionOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXNskkIch_XQ",
        "outputId": "2cdf6fe9-414f-46f8-ef20-57c8a9568c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|array1                                             |conditionOut                                       |\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 22, 24, 26, 28, 30]|\n",
            "+---------------------------------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform\n",
        "\n",
        "## with the index of an array\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  array1,\n",
        "  transform(array1,(x,i)->i) as indexVal,\n",
        "  transform(array1,(x,i)-> x+i) as IndexedOut\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6d-ASBDix3i",
        "outputId": "e3f43b3f-d6b9-42d1-fde8-da4eba2388d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+--------------------------------------------------+-------------------------------------------------------+\n",
            "|array1                                             |indexVal                                          |IndexedOut                                             |\n",
            "+---------------------------------------------------+--------------------------------------------------+-------------------------------------------------------+\n",
            "|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]|[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]|[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29]|\n",
            "+---------------------------------------------------+--------------------------------------------------+-------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform_keys\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  map1,\n",
        "  transform_keys(map1, (k,v)-> upper(k))\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#--\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  map1,\n",
        "  transform_keys(map1, (k,v)-> 'Key_'||upper(k))\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwMZsSEvi-cg",
        "outputId": "10cbc867-ea1f-443f-8b53-b641f4807b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|map1                                                     |transform_keys(map1, lambdafunction(upper(namedlambdavariable()), namedlambdavariable(), namedlambdavariable()))|\n",
            "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
            "|{name -> rahul, age -> 28, profession -> dataengineering}|{NAME -> rahul, AGE -> 28, PROFESSION -> dataengineering}                                                       |\n",
            "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "+---------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------+\n",
            "|map1                                                     |transform_keys(map1, lambdafunction(concat(Key_, upper(namedlambdavariable())), namedlambdavariable(), namedlambdavariable()))|\n",
            "+---------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{name -> rahul, age -> 28, profession -> dataengineering}|{Key_NAME -> rahul, Key_AGE -> 28, Key_PROFESSION -> dataengineering}                                                         |\n",
            "+---------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tranmsform_values\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  map1,\n",
        "  transform_values(map1, (k,v)-> upper(v))\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#--\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  map1,\n",
        "  transform_values(map1, (k,v)-> 'Value_'||upper(v))\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#--\n",
        "\n",
        "sql = '''\n",
        "select\n",
        "  map1,\n",
        "  transform_values(map1, (k,v)-> k||'-'||v)\n",
        "from input_view'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP9p3ERgkTBa",
        "outputId": "6681bbcd-6530-4572-9414-fe36c3344594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+\n",
            "|map1                                                     |transform_values(map1, lambdafunction(upper(namedlambdavariable()), namedlambdavariable(), namedlambdavariable()))|\n",
            "+---------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+\n",
            "|{name -> rahul, age -> 28, profession -> dataengineering}|{name -> RAHUL, age -> 28, profession -> DATAENGINEERING}                                                         |\n",
            "+---------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "|map1                                                     |transform_values(map1, lambdafunction(concat(Value_, upper(namedlambdavariable())), namedlambdavariable(), namedlambdavariable()))|\n",
            "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{name -> rahul, age -> 28, profession -> dataengineering}|{name -> Value_RAHUL, age -> Value_28, profession -> Value_DATAENGINEERING}                                                       |\n",
            "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|map1                                                     |transform_values(map1, lambdafunction(concat(concat(namedlambdavariable(), -), namedlambdavariable()), namedlambdavariable(), namedlambdavariable()))|\n",
            "+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{name -> rahul, age -> 28, profession -> dataengineering}|{name -> name-rahul, age -> age-28, profession -> profession-dataengineering}                                                                        |\n",
            "+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "#### `transform` vs `transform_keys` vs `transform_values`\n",
        "--------------------------\n",
        "\n",
        "| Aspect | `transform` | `transform_keys` | `transform_values` |\n",
        "|--------|-------------|------------------|-------------------|\n",
        "| **Purpose** | Transforms **ARRAY** elements | Transforms **MAP** keys | Transforms **MAP** values |\n",
        "| **Input Type** | Array | Map | Map |\n",
        "| **Lambda Parameters** | `(element)` or `(element, index)` | `(key, value)` | `(key, value)` |\n",
        "| **What Changes** | Array elements | Map keys | Map values |\n",
        "| **Return Type** | Transformed array | Map with transformed keys | Map with transformed values |\n",
        "| **Original Structure** | Array size unchanged | Map size unchanged | Map size unchanged |\n",
        "--------------------------"
      ],
      "metadata": {
        "id": "xRNf8RMBlXuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# arrays_zip\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select\n",
        "  sequence(1,5,1) as array1,\n",
        "  transform(sequence(1,5,1), x-> x*x) as arrayTrans\n",
        "from input_view\n",
        ")\n",
        "select\n",
        "  array1,\n",
        "  arrayTrans,\n",
        "  arrays_zip(array1,arrayTrans) as zippedArray,\n",
        "  TypeOf(arrays_zip(array1,arrayTrans)) as TypeOut\n",
        "from cte\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Srz7WKmozX",
        "outputId": "c04c34b5-9559-418b-9b80-38b6b89ad2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+------------------------------------------+----------------------------------------+\n",
            "|array1         |arrayTrans       |zippedArray                               |TypeOut                                 |\n",
            "+---------------+-----------------+------------------------------------------+----------------------------------------+\n",
            "|[1, 2, 3, 4, 5]|[1, 4, 9, 16, 25]|[{1, 1}, {2, 4}, {3, 9}, {4, 16}, {5, 25}]|array<struct<array1:int,arrayTrans:int>>|\n",
            "+---------------+-----------------+------------------------------------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zip_with : zipping with a transformation between the element of the array\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select\n",
        "  sequence(1,5,1) as array1,\n",
        "  transform(sequence(1,5,1), x-> x*x) as arrayTrans\n",
        "from input_view\n",
        ")\n",
        "select\n",
        "  array1,\n",
        "  arrayTrans,\n",
        "  zip_with(array1,arrayTrans,(a1,a2)-> a1 = sqrt(a2)) as zippedWithArray\n",
        "from cte\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#--\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select\n",
        "  sequence(1,5,1) as array1,\n",
        "  transform(sequence(1,5,1), x-> x*x) as arrayTrans\n",
        "from input_view\n",
        ")\n",
        "select\n",
        "  array1,\n",
        "  arrayTrans,\n",
        "  zip_with(array1,arrayTrans,(a1,a2)-> case when a1%2 = 0 then a1 else a2 end) as zippedWithArray\n",
        "from cte\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LXn8GwCmvoo",
        "outputId": "8e94ffe1-38b7-4513-a4b7-48aae79f3607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+------------------------------+\n",
            "|array1         |arrayTrans       |zippedWithArray               |\n",
            "+---------------+-----------------+------------------------------+\n",
            "|[1, 2, 3, 4, 5]|[1, 4, 9, 16, 25]|[true, true, true, true, true]|\n",
            "+---------------+-----------------+------------------------------+\n",
            "\n",
            "+---------------+-----------------+----------------+\n",
            "|array1         |arrayTrans       |zippedWithArray |\n",
            "+---------------+-----------------+----------------+\n",
            "|[1, 2, 3, 4, 5]|[1, 4, 9, 16, 25]|[1, 2, 9, 4, 25]|\n",
            "+---------------+-----------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "#### `arrays_zip` vs `zip_with`\n",
        "--------------------------------------\n",
        "\n",
        "| Aspect | `arrays_zip` | `zip_with` |\n",
        "|--------|--------------|------------|\n",
        "| **Purpose** | Zips multiple arrays into array of structs | Zips two arrays with custom transformation |\n",
        "| **Number of Arrays** | Multiple arrays (2+) | Exactly two arrays |\n",
        "| **Return Type** | Array of structs | Array (custom type based on function) |\n",
        "| **Output Structure** | `[{\"0\": val1, \"1\": val2, ...}]` | Custom based on lambda function |\n",
        "| **Lambda Function** | No lambda needed | Requires lambda function |\n",
        "| **Customization** | Fixed struct output | Fully customizable output |\n",
        "| **Null Handling** | Appends nulls to shorter arrays | Appends nulls to shorter arrays |\n",
        "--------------------------------------"
      ],
      "metadata": {
        "id": "FSWJGRQ9pdSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Collection Functions Reference\n",
        "\n",
        "| Function | Description | Parameters | Returns | Related Functions | Collections Supported | Remarks |\n",
        "|----------|-------------|------------|---------|-------------------|---------------------|---------|\n",
        "| **size** | Returns number of elements | `(collection)` | Integer | `cardinality` | Arrays, Maps | More commonly used alias |\n",
        "| **cardinality** | Returns number of elements | `(collection)` | Integer | `size` | Arrays, Maps | SQL standard name |\n",
        "| **reverse** | Reverses element order | `(array)` | Array | `sort_array`, `array_sort` | Arrays | No sorting, just reversal |\n",
        "| **sort_array** | Sorts array with order control | `(array, [asc/desc])` | Array | `array_sort`, `reverse` | Arrays | Simple ascending/descending |\n",
        "| **array_sort** | Sorts with custom comparator | `(array, [comparator])` | Array | `sort_array`, `reverse` | Arrays | Custom sorting logic |\n",
        "| **aggregate** | Reduces array with custom logic | `(array, start, merge, finish)` | Any | `reduce` | Arrays | SQL standard name |\n",
        "| **reduce** | Reduces array with custom logic | `(array, start, merge, finish)` | Any | `aggregate` | Arrays | Functional programming name |\n",
        "| **concat** | Concatenates arrays | `(array1, array2, ...)` | Array | `arrays_zip`, `zip_with` | Arrays | Multiple arrays supported |\n",
        "| **element_at** | Gets element at position | `(collection, index/key)` | Element | `try_element_at` | Arrays, Maps | Throws error if not found |\n",
        "| **try_element_at** | Safe element access | `(collection, index/key)` | Element | `element_at` | Arrays, Maps | Returns null if not found |\n",
        "| **exists** | Checks if any element matches | `(array, condition)` | Boolean | `forall`, `filter` | Arrays | Short-circuits on first match |\n",
        "| **forall** | Checks if all elements match | `(array, condition)` | Boolean | `exists`, `filter` | Arrays | Short-circuits on first failure |\n",
        "| **filter** | Filters array elements | `(array, condition)` | Array | `exists`, `forall` | Arrays | Returns new filtered array |\n",
        "| **map_filter** | Filters map entries | `(map, condition)` | Map | `filter` | Maps | Condition on (key, value) pairs |\n",
        "| **transform** | Transforms array elements | `(array, function)` | Array | `transform_keys`, `transform_values` | Arrays | Element-wise transformation |\n",
        "| **transform_keys** | Transforms map keys | `(map, function)` | Map | `transform`, `transform_values` | Maps | Keys change, values same |\n",
        "| **transform_values** | Transforms map values | `(map, function)` | Map | `transform`, `transform_keys` | Maps | Values change, keys same |\n",
        "| **arrays_zip** | Zips arrays into structs | `(array1, array2, ...)` | Array[Struct] | `zip_with` | Arrays | Multiple arrays, fixed output |\n",
        "| **zip_with** | Zips with transformation | `(array1, array2, function)` | Array | `arrays_zip` | Arrays | Custom output, exactly 2 arrays |\n",
        "\n",
        "## Key Notes:\n",
        "- **Structs are NOT collections** - none of these functions work on structs\n",
        "- **Arrays and Maps** are the only true collections in Spark\n",
        "- **Functionally identical pairs**: `size`/`cardinality`, `aggregate`/`reduce`\n",
        "- **Short-circuiting**: `exists` stops at first match, `forall` stops at first failure"
      ],
      "metadata": {
        "id": "0qdZLzGwtVyV"
      }
    }
  ]
}