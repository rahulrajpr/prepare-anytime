{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyqIKm6QvsIUPNRTZK5+36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/functions/14_spark_sql_window_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark Window Functions**\n",
        "https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#window-functions"
      ],
      "metadata": {
        "id": "ptAYb_3hSs8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXIHqycTSsTi"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('spark-functions').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, desc, rank,col,dense_rank,expr\n",
        "from pyspark.sql.types import StructType,StructField, IntegerType, StringType"
      ],
      "metadata": {
        "id": "DUaPQh3cchI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = [\n",
        "    (\"2023-01-01\", \"Alice\", \"Electronics\", 1000),\n",
        "    (\"2023-01-01\", \"Bob\", \"Electronics\", 1200),\n",
        "    (\"2023-01-01\", \"Charlie\", \"Clothing\", 800),\n",
        "    (\"2023-01-01\", \"Diana\", \"Electronics\", 1500),\n",
        "    (\"2023-01-01\", \"Eve\", \"Clothing\", 600),\n",
        "    (\"2023-02-01\", \"Alice\", \"Electronics\", 1100),\n",
        "    (\"2023-02-01\", \"Bob\", \"Electronics\", 900),\n",
        "    (\"2023-02-01\", \"Charlie\", \"Clothing\", 950),\n",
        "    (\"2023-02-01\", \"Diana\", \"Electronics\", 1300),\n",
        "    (\"2023-02-01\", \"Eve\", \"Clothing\", 700),\n",
        "    (\"2023-02-01\", \"Frank\", \"Electronics\", None),\n",
        "    (\"2023-03-01\", \"Alice\", \"Electronics\", 1400),\n",
        "    (\"2023-03-01\", \"Bob\", \"Electronics\", 1000),\n",
        "    (\"2023-03-01\", \"Charlie\", \"Clothing\", 1200),\n",
        "    (\"2023-03-01\", \"Diana\", \"Electronics\", 1600)\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"mn\", StringType(), True),\n",
        "    StructField(\"salesperson\", StringType(), True),\n",
        "    StructField(\"department\", StringType(), True),\n",
        "    StructField(\"sales\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe = dataframe.withColumn('mn',to_date(col('mn'), 'yyyy-MM-dd'))\n",
        "dataframe.printSchema()\n",
        "dataframe.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESOUZY2Tb451",
        "outputId": "a845c1d2-02b1-4039-fb93-6adb4ce6cec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mn: date (nullable = true)\n",
            " |-- salesperson: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- sales: integer (nullable = true)\n",
            "\n",
            "+----------+-----------+-----------+-----+\n",
            "|mn        |salesperson|department |sales|\n",
            "+----------+-----------+-----------+-----+\n",
            "|2023-01-01|Alice      |Electronics|1000 |\n",
            "|2023-01-01|Bob        |Electronics|1200 |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |\n",
            "|2023-01-01|Diana      |Electronics|1500 |\n",
            "|2023-01-01|Eve        |Clothing   |600  |\n",
            "|2023-02-01|Alice      |Electronics|1100 |\n",
            "|2023-02-01|Bob        |Electronics|900  |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |\n",
            "|2023-02-01|Diana      |Electronics|1300 |\n",
            "|2023-02-01|Eve        |Clothing   |700  |\n",
            "|2023-02-01|Frank      |Electronics|NULL |\n",
            "|2023-03-01|Alice      |Electronics|1400 |\n",
            "|2023-03-01|Bob        |Electronics|1000 |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |\n",
            "|2023-03-01|Diana      |Electronics|1600 |\n",
            "+----------+-----------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.createOrReplaceTempView('dataframe_view')"
      ],
      "metadata": {
        "id": "3LMzRqS1cxU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rank\n",
        "\n",
        "# saprk sqkl\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "  select *, rank() over(partition by department order by sales desc NULLS LAST) as rn\n",
        "  from dataframe_view\n",
        ")\n",
        "select *\n",
        "from cte\n",
        "where rn = 1\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxr5Skvhfhgg",
        "outputId": "839f831e-5435-4dab-ec96-dbd7aa3be9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|mn        |salesperson|department |sales|rn |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1  |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1  |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "ULC1tHyjf7-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rank (psyspark dataframe api)\n",
        "\n",
        "win = Window.partitionBy('department').orderBy(desc('sales'))\n",
        "dataframe.withColumn('rn',rank().over(win)).filter('rn == 1').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYqC36OViQGd",
        "outputId": "c526f4b2-5eed-4d4e-eae9-80b7c8f53566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|        mn|salesperson| department|sales| rn|\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-03-01|    Charlie|   Clothing| 1200|  1|\n",
            "|2023-03-01|      Diana|Electronics| 1600|  1|\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dense_rank()\n",
        "\n",
        "# saprk sql\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "  select *, dense_rank() over(partition by department order by sales desc NULLS LAST) as rn\n",
        "  from dataframe_view\n",
        ")\n",
        "select *\n",
        "from cte\n",
        "where rn = 2\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7w4mrigsLwx",
        "outputId": "15fbe03c-6d6a-4504-8402-f358487b416c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|mn        |salesperson|department |sales|rn |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-02-01|Charlie    |Clothing   |950  |2  |\n",
            "|2023-01-01|Diana      |Electronics|1500 |2  |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dense_rank (psyspark dataframe api)\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import expr, dense_rank, desc, desc_nulls_last\n",
        "\n",
        "win = Window.partitionBy('department').orderBy(desc_nulls_last('sales'))\n",
        "dataframe.withColumn('dnsrk', dense_rank().over(win)).filter('dnsrk == 2').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6lthDmesPUS",
        "outputId": "213e43dd-57d8-4f7b-8e69-0c804a2ff01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-----+\n",
            "|mn        |salesperson|department |sales|dnsrk|\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|2023-02-01|Charlie    |Clothing   |950  |2    |\n",
            "|2023-01-01|Diana      |Electronics|1500 |2    |\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# row_number()\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "  select *, row_number() over(partition by department order by sales desc NULLS LAST) as rn\n",
        "  from dataframe_view\n",
        ")\n",
        "select *\n",
        "from cte\n",
        "where rn = 1\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRIPwj16s6mx",
        "outputId": "550fab42-e0e7-4916-94ae-0367c23cc730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|mn        |salesperson|department |sales|rn |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1  |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1  |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# row_number() - pysprk dataframe api\n",
        "\n",
        "from pyspark.sql.functions import row_number, desc_nulls_last,col, exp\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "win = Window.partitionBy('department').orderBy(desc_nulls_last('sales'))\n",
        "dataframe.withColumn('rwNum',row_number().over(win)).filter(expr('rwNum = 1')).show(truncate = False)\n",
        "\n",
        "dataframe.withColumn('rwNum',expr('row_number() over(partition by department order by sales desc nulls last)')).filter(expr('rwNum = 1')).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TprrAx_QwBq3",
        "outputId": "4e3521b2-1ddc-4c31-f44b-385cc4019083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-----+\n",
            "|mn        |salesperson|department |sales|rwNum|\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1    |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1    |\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|mn        |salesperson|department |sales|rwNum|\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1    |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1    |\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# percent_rank() and cume_dist() :\n",
        "\n",
        "sql = '''\n",
        "select *,\n",
        "    percent_rank() over(partition by department order by sales desc nulls last) as perRank,\n",
        "    cume_dist() over(partition by department order by sales desc nulls last) as CumDist\n",
        "from dataframe_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R4RBYhvKq74",
        "outputId": "5a58df3d-30d4-4adf-d220-a2aacc276fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+------------------+-------+\n",
            "|mn        |salesperson|department |sales|perRank           |CumDist|\n",
            "+----------+-----------+-----------+-----+------------------+-------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |0.0               |0.2    |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |0.25              |0.4    |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |0.5               |0.6    |\n",
            "|2023-02-01|Eve        |Clothing   |700  |0.75              |0.8    |\n",
            "|2023-01-01|Eve        |Clothing   |600  |1.0               |1.0    |\n",
            "|2023-03-01|Diana      |Electronics|1600 |0.0               |0.1    |\n",
            "|2023-01-01|Diana      |Electronics|1500 |0.1111111111111111|0.2    |\n",
            "|2023-03-01|Alice      |Electronics|1400 |0.2222222222222222|0.3    |\n",
            "|2023-02-01|Diana      |Electronics|1300 |0.3333333333333333|0.4    |\n",
            "|2023-01-01|Bob        |Electronics|1200 |0.4444444444444444|0.5    |\n",
            "|2023-02-01|Alice      |Electronics|1100 |0.5555555555555556|0.6    |\n",
            "|2023-01-01|Alice      |Electronics|1000 |0.6666666666666666|0.8    |\n",
            "|2023-03-01|Bob        |Electronics|1000 |0.6666666666666666|0.8    |\n",
            "|2023-02-01|Bob        |Electronics|900  |0.8888888888888888|0.9    |\n",
            "|2023-02-01|Frank      |Electronics|NULL |1.0               |1.0    |\n",
            "+----------+-----------+-----------+-----+------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cume_dist() vs percent_rank()\n",
        "\n",
        "# spark dataframe api (with expr)\n",
        "\n",
        "from pyspark.sql.functions import expr, round\n",
        "\n",
        "dataframe.withColumn('cumDist', round(expr('cume_dist() over(partition by department order by sales desc nulls first)').cast('double'),2))\\\n",
        "         .withColumn('percRank', round(expr('percent_rank() over(partition by department order by sales desc nulls first)').cast('double'),2))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "id": "pOG6qp3hLCAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0370a443-39fb-40da-e0eb-6fdaf8f26805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-------+--------+\n",
            "|mn        |salesperson|department |sales|cumDist|percRank|\n",
            "+----------+-----------+-----------+-----+-------+--------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |0.2    |0.0     |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |0.4    |0.25    |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |0.6    |0.5     |\n",
            "|2023-02-01|Eve        |Clothing   |700  |0.8    |0.75    |\n",
            "|2023-01-01|Eve        |Clothing   |600  |1.0    |1.0     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |0.1    |0.0     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |0.2    |0.11    |\n",
            "|2023-01-01|Diana      |Electronics|1500 |0.3    |0.22    |\n",
            "|2023-03-01|Alice      |Electronics|1400 |0.4    |0.33    |\n",
            "|2023-02-01|Diana      |Electronics|1300 |0.5    |0.44    |\n",
            "|2023-01-01|Bob        |Electronics|1200 |0.6    |0.56    |\n",
            "|2023-02-01|Alice      |Electronics|1100 |0.7    |0.67    |\n",
            "|2023-01-01|Alice      |Electronics|1000 |0.9    |0.78    |\n",
            "|2023-03-01|Bob        |Electronics|1000 |0.9    |0.78    |\n",
            "|2023-02-01|Bob        |Electronics|900  |1.0    |1.0     |\n",
            "+----------+-----------+-----------+-----+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# person_rank() : to access the relative standing of a row\n",
        "\n",
        "# spark dataframe api (with all pyspark)\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import percent_rank, cume_dist,desc_nulls_first, col\n",
        "\n",
        "dataframe.withColumn('cumDist',   round(cume_dist().over(Window.partitionBy(col('department')).orderBy(desc_nulls_first(col('sales')))).cast('double'),2))\\\n",
        "         .withColumn('percRank', round(percent_rank().over(Window.partitionBy(col('department')).orderBy(desc_nulls_first(col('sales')))).cast('double'),2))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrGWs677Xc6Y",
        "outputId": "cab54fc3-bbf6-461d-e026-2d2026fae364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-------+--------+\n",
            "|mn        |salesperson|department |sales|cumDist|percRank|\n",
            "+----------+-----------+-----------+-----+-------+--------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |0.2    |0.0     |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |0.4    |0.25    |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |0.6    |0.5     |\n",
            "|2023-02-01|Eve        |Clothing   |700  |0.8    |0.75    |\n",
            "|2023-01-01|Eve        |Clothing   |600  |1.0    |1.0     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |0.1    |0.0     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |0.2    |0.11    |\n",
            "|2023-01-01|Diana      |Electronics|1500 |0.3    |0.22    |\n",
            "|2023-03-01|Alice      |Electronics|1400 |0.4    |0.33    |\n",
            "|2023-02-01|Diana      |Electronics|1300 |0.5    |0.44    |\n",
            "|2023-01-01|Bob        |Electronics|1200 |0.6    |0.56    |\n",
            "|2023-02-01|Alice      |Electronics|1100 |0.7    |0.67    |\n",
            "|2023-01-01|Alice      |Electronics|1000 |0.9    |0.78    |\n",
            "|2023-03-01|Bob        |Electronics|1000 |0.9    |0.78    |\n",
            "|2023-02-01|Bob        |Electronics|900  |1.0    |1.0     |\n",
            "+----------+-----------+-----------+-----+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **CUME_DIST vs PERCENT_RANK**\n",
        "---\n",
        "##### Key Difference:\n",
        "- **CUME_DIST** = What % of ALL rows have values ≤ **current row's value**\n",
        "- **PERCENT_RANK** = What % of OTHER rows have values < **current row's value**\n",
        "---\n",
        "##### Quick Formulas:\n",
        "- **CUME_DIST** = `(Rows with value ≤ current row) / (Total rows)`\n",
        "- **PERCENT_RANK** = `(Rows with value < current row) / (Total rows - 1)`\n",
        "---\n",
        "##### Example: Test Scores [55, 65, 75, 85, 85, 95]\n",
        "---\n",
        "| Student | Score | CUME_DIST | PERCENT_RANK |\n",
        "|---------|-------|-----------|--------------|\n",
        "| Frank   | 55    | 0.17      | 0.0          |\n",
        "| Eve     | 65    | 0.33      | 0.2          |\n",
        "| Diana   | 75    | 0.50      | 0.4          |\n",
        "| Charlie | 85    | 0.83      | 0.6          |\n",
        "| Bob     | 85    | 0.83      | 0.6          |\n",
        "| Alice   | 95    | 1.00      | 1.0          |\n",
        "\n",
        "---\n",
        "##### When to Use:\n",
        "\n",
        "##### Use CUME_DIST:\n",
        "> *\"What percentile am I in?\"*\n",
        "- Answers: \"Where do I stand in the entire group?\"\n",
        "- Includes yourself in the calculation\n",
        "---\n",
        "##### Use PERCENT_RANK:\n",
        "> *\"What percentage of people did I beat?\"*  \n",
        "- Answers: \"How do I rank against others?\"\n",
        "- Excludes yourself from comparison\n",
        "---\n",
        "##### Bottom Line:\n",
        "> **CUME_DIST** includes you in the count  \n",
        "> **PERCENT_RANK** compares you against others"
      ],
      "metadata": {
        "id": "VgG-5TH-SW6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **`asc_nulls_first` vs `asc_nulls_last`**\n",
        "\n",
        "| Aspect | `asc_nulls_first` | `asc_nulls_last` |\n",
        "|--------|-------------------|------------------|\n",
        "| **NULL Position** | NULLs at start | NULLs at end |\n",
        "| **Sort Order** | NULLs → Ascending values | Ascending values → NULLs |\n",
        "| **Default** | Yes | No |\n",
        "---\n",
        "##### **Example:**\n",
        "- `asc_nulls_first`: `[NULL, NULL, 10, 25, 50]`\n",
        "- `asc_nulls_last`: `[10, 25, 50, NULL, NULL]`\n",
        "---\n",
        "**That's it.** Both sort ascending - only NULL placement differs."
      ],
      "metadata": {
        "id": "ooe_eBWjaeAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# asc_nulls_first : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import asc_nulls_first,col\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.orderBy(col('nums')).show(truncate = False)\n",
        "dataframe.orderBy(asc_nulls_first(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpG0Ab3xYyS9",
        "outputId": "d3740fed-de40-45e2-bfc4-592571bdb993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# asc_nulls_last : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import asc_nulls_last,col\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframeNums = spark.createDataFrame(data, schema)\n",
        "dataframeNums.orderBy(col('nums')).show(truncate = False)\n",
        "dataframeNums.orderBy(asc_nulls_last(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4vejMgTbsNE",
        "outputId": "867a9cbd-94e0-40a3-ac54-59d2f6f64dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desc_nulls_first : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import desc_nulls_first,col, expr, desc\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframeNums = spark.createDataFrame(data, schema)\n",
        "dataframeNums.orderBy(desc(col('nums'))).show(truncate = False)\n",
        "dataframeNums.orderBy(desc_nulls_first(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVgz1C3HdvkS",
        "outputId": "6665dc24-9acb-4d81-e9c3-d9fff2bbe535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desc_nulls_last : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import desc_nulls_last,col, expr, desc\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.orderBy(desc(col('nums'))).show(truncate = False)\n",
        "dataframe.orderBy(desc_nulls_last(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvrlDExOd9hy",
        "outputId": "987ea413-2d89-45ff-d1d0-d81412ea2f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **PySpark NULL Ordering Defaults:**\n",
        "\n",
        "✅ **ASC + NULLs first** = Default behavior  \n",
        "✅ **DESC + NULLs last** = Default behavior  \n",
        "✅ **asc_nulls_first()** and **desc_nulls_last()** make default behavior explicit"
      ],
      "metadata": {
        "id": "P6AjrMH7ec6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ntile : simple bucketing the dataset\n",
        "\n",
        "sql = '''\n",
        "select *, ntile(2) over(partition by department order by sales) as perRank\n",
        "from dataframe_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3upELym2eVd6",
        "outputId": "9188e3f9-6a7a-4de5-8bff-82bf757c7c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-------+\n",
            "|mn        |salesperson|department |sales|perRank|\n",
            "+----------+-----------+-----------+-----+-------+\n",
            "|2023-01-01|Eve        |Clothing   |600  |1      |\n",
            "|2023-02-01|Eve        |Clothing   |700  |1      |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |1      |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |2      |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |2      |\n",
            "|2023-02-01|Frank      |Electronics|NULL |1      |\n",
            "|2023-02-01|Bob        |Electronics|900  |1      |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1      |\n",
            "|2023-03-01|Bob        |Electronics|1000 |1      |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1      |\n",
            "|2023-01-01|Bob        |Electronics|1200 |2      |\n",
            "|2023-02-01|Diana      |Electronics|1300 |2      |\n",
            "|2023-03-01|Alice      |Electronics|1400 |2      |\n",
            "|2023-01-01|Diana      |Electronics|1500 |2      |\n",
            "|2023-03-01|Diana      |Electronics|1600 |2      |\n",
            "+----------+-----------+-----------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ntile : simple bucketing the dataset\n",
        "\n",
        "# pyspark api (with expr)\n",
        "\n",
        "from pyspark.sql.functions import col, asc_nulls_last\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "dataframe.withColumn('nthile_col', expr('''ntile(2) over(partition by department order by sales desc nulls last)'''))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6lcUNjupTo5",
        "outputId": "a22c5490-a57b-4fd3-ad8c-e8abbbcf0885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+----------+\n",
            "|mn        |salesperson|department |sales|nthile_col|\n",
            "+----------+-----------+-----------+-----+----------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1         |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |1         |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |1         |\n",
            "|2023-02-01|Eve        |Clothing   |700  |2         |\n",
            "|2023-01-01|Eve        |Clothing   |600  |2         |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1         |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1         |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1         |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1         |\n",
            "|2023-01-01|Bob        |Electronics|1200 |1         |\n",
            "|2023-02-01|Alice      |Electronics|1100 |2         |\n",
            "|2023-01-01|Alice      |Electronics|1000 |2         |\n",
            "|2023-03-01|Bob        |Electronics|1000 |2         |\n",
            "|2023-02-01|Bob        |Electronics|900  |2         |\n",
            "|2023-02-01|Frank      |Electronics|NULL |2         |\n",
            "+----------+-----------+-----------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ntile : simple bucketing the dataset\n",
        "\n",
        "# pyspark api\n",
        "\n",
        "from pyspark.sql.functions import col, desc_nulls_last,ntile\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "dataframe.withColumn('ntile_col',ntile(2).over(Window.partitionBy('department').orderBy(desc_nulls_last(col('sales')))))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmL0YJZepxXs",
        "outputId": "12901b97-3eab-404e-eb88-5a9e11f6dc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|ntile_col|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1        |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |1        |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |1        |\n",
            "|2023-02-01|Eve        |Clothing   |700  |2        |\n",
            "|2023-01-01|Eve        |Clothing   |600  |2        |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1        |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1        |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1        |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1        |\n",
            "|2023-01-01|Bob        |Electronics|1200 |1        |\n",
            "|2023-02-01|Alice      |Electronics|1100 |2        |\n",
            "|2023-01-01|Alice      |Electronics|1000 |2        |\n",
            "|2023-03-01|Bob        |Electronics|1000 |2        |\n",
            "|2023-02-01|Bob        |Electronics|900  |2        |\n",
            "|2023-02-01|Frank      |Electronics|NULL |2        |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nth_value\n",
        "\n",
        "# full sql value\n",
        "\n",
        "sql = '''\n",
        "select *,\n",
        "nth_value(sales,2) over(partition by department order by sales desc nulls last) as nth_valueValue\n",
        "from dataframe_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiZmj9fEqu6v",
        "outputId": "930386e6-aa25-40e4-a175-b141c7d24292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+--------------+\n",
            "|mn        |salesperson|department |sales|nth_valueValue|\n",
            "+----------+-----------+-----------+-----+--------------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |NULL          |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |950           |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |950           |\n",
            "|2023-02-01|Eve        |Clothing   |700  |950           |\n",
            "|2023-01-01|Eve        |Clothing   |600  |950           |\n",
            "|2023-03-01|Diana      |Electronics|1600 |NULL          |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1500          |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1500          |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1500          |\n",
            "|2023-01-01|Bob        |Electronics|1200 |1500          |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1500          |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1500          |\n",
            "|2023-03-01|Bob        |Electronics|1000 |1500          |\n",
            "|2023-02-01|Bob        |Electronics|900  |1500          |\n",
            "|2023-02-01|Frank      |Electronics|NULL |1500          |\n",
            "+----------+-----------+-----------+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nth_value\n",
        "\n",
        "# spark (with expr)\n",
        "\n",
        "from pyspark.sql.functions import col, nth_value, desc_nulls_last\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "dataframe.withColumn('nthValue', expr('nth_value(sales,2) over(partition by department order by sales desc nulls last)'))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FfbwDPa0PgK",
        "outputId": "3261c619-8aa9-45ae-e02e-d81f78e860fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+--------+\n",
            "|mn        |salesperson|department |sales|nthValue|\n",
            "+----------+-----------+-----------+-----+--------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |NULL    |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |950     |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |950     |\n",
            "|2023-02-01|Eve        |Clothing   |700  |950     |\n",
            "|2023-01-01|Eve        |Clothing   |600  |950     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |NULL    |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1500    |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1500    |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1500    |\n",
            "|2023-01-01|Bob        |Electronics|1200 |1500    |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1500    |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1500    |\n",
            "|2023-03-01|Bob        |Electronics|1000 |1500    |\n",
            "|2023-02-01|Bob        |Electronics|900  |1500    |\n",
            "|2023-02-01|Frank      |Electronics|NULL |1500    |\n",
            "+----------+-----------+-----------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nth_value\n",
        "\n",
        "# spark (with expr)\n",
        "\n",
        "from pyspark.sql.functions import col, nth_value, desc_nulls_last\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "dataframe.withColumn('nthValue',nth_value(col('sales'),2).over(Window.partitionBy('department').orderBy(desc_nulls_last(col('sales')))))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BlyD9bv04cC",
        "outputId": "17edeb68-2177-4299-918a-1b38f3381977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+--------+\n",
            "|mn        |salesperson|department |sales|nthValue|\n",
            "+----------+-----------+-----------+-----+--------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |NULL    |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |950     |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |950     |\n",
            "|2023-02-01|Eve        |Clothing   |700  |950     |\n",
            "|2023-01-01|Eve        |Clothing   |600  |950     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |NULL    |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1500    |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1500    |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1500    |\n",
            "|2023-01-01|Bob        |Electronics|1200 |1500    |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1500    |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1500    |\n",
            "|2023-03-01|Bob        |Electronics|1000 |1500    |\n",
            "|2023-02-01|Bob        |Electronics|900  |1500    |\n",
            "|2023-02-01|Frank      |Electronics|NULL |1500    |\n",
            "+----------+-----------+-----------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lead\n",
        "\n",
        "sql = '''\n",
        "select *,\n",
        "lead(sales,1) over(partition by department,salesperson order by mn asc) as nextSales\n",
        "from dataframe_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rXE7Hut3y2R",
        "outputId": "359fafae-2af1-4a81-ca1b-0b0bda959f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|nextSales|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-01-01|Charlie    |Clothing   |800  |950      |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |1200     |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |NULL     |\n",
            "|2023-01-01|Eve        |Clothing   |600  |700      |\n",
            "|2023-02-01|Eve        |Clothing   |700  |NULL     |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1100     |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1400     |\n",
            "|2023-03-01|Alice      |Electronics|1400 |NULL     |\n",
            "|2023-01-01|Bob        |Electronics|1200 |900      |\n",
            "|2023-02-01|Bob        |Electronics|900  |1000     |\n",
            "|2023-03-01|Bob        |Electronics|1000 |NULL     |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1300     |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1600     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |NULL     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |NULL     |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lead\n",
        "# pyspark (with expr)\n",
        "\n",
        "dataframe.withColumn('nextSales', expr('''lead(sales,1) over(partition by department,salesperson order by mn asc)'''))\\\n",
        "         .orderBy(['department','salesperson'])\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3VxyKWZ4plc",
        "outputId": "2419c945-25f8-49db-c957-70c3ccec2440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|nextSales|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-01-01|Charlie    |Clothing   |800  |950      |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |1200     |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |NULL     |\n",
            "|2023-01-01|Eve        |Clothing   |600  |700      |\n",
            "|2023-02-01|Eve        |Clothing   |700  |NULL     |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1100     |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1400     |\n",
            "|2023-03-01|Alice      |Electronics|1400 |NULL     |\n",
            "|2023-01-01|Bob        |Electronics|1200 |900      |\n",
            "|2023-02-01|Bob        |Electronics|900  |1000     |\n",
            "|2023-03-01|Bob        |Electronics|1000 |NULL     |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1300     |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1600     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |NULL     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |NULL     |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lead\n",
        "# pyspark\n",
        "\n",
        "from pyspark.sql.functions import lead,col, asc_nulls_last\n",
        "\n",
        "dataframe.withColumn('nextSales',lead(col('sales'),1).over(Window.partitionBy('department','salesperson').orderBy(asc_nulls_last(col('mn')))))\\\n",
        "         .orderBy(['department','salesperson','mn'])\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R1POlnY5TQN",
        "outputId": "27e5be9d-dc6c-4c0c-fc78-61ebe8a4daee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|nextSales|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-01-01|Charlie    |Clothing   |800  |950      |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |1200     |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |NULL     |\n",
            "|2023-01-01|Eve        |Clothing   |600  |700      |\n",
            "|2023-02-01|Eve        |Clothing   |700  |NULL     |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1100     |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1400     |\n",
            "|2023-03-01|Alice      |Electronics|1400 |NULL     |\n",
            "|2023-01-01|Bob        |Electronics|1200 |900      |\n",
            "|2023-02-01|Bob        |Electronics|900  |1000     |\n",
            "|2023-03-01|Bob        |Electronics|1000 |NULL     |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1300     |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1600     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |NULL     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |NULL     |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lag\n",
        "\n",
        "sql = '''\n",
        "select *,\n",
        "lag(sales,1) over(partition by department,salesperson order by mn asc) as nextSales\n",
        "from dataframe_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nXGf_4ZIAIE",
        "outputId": "2a37b4bb-861a-4f24-d638-863cb63ad10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|nextSales|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-01-01|Charlie    |Clothing   |800  |NULL     |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |800      |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |950      |\n",
            "|2023-01-01|Eve        |Clothing   |600  |NULL     |\n",
            "|2023-02-01|Eve        |Clothing   |700  |600      |\n",
            "|2023-01-01|Alice      |Electronics|1000 |NULL     |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1000     |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1100     |\n",
            "|2023-01-01|Bob        |Electronics|1200 |NULL     |\n",
            "|2023-02-01|Bob        |Electronics|900  |1200     |\n",
            "|2023-03-01|Bob        |Electronics|1000 |900      |\n",
            "|2023-01-01|Diana      |Electronics|1500 |NULL     |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1500     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1300     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |NULL     |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lag\n",
        "# pyspark (with expr)\n",
        "\n",
        "dataframe.withColumn('nextSales', expr('''lag(sales,1) over(partition by department,salesperson order by mn asc)'''))\\\n",
        "         .orderBy(['department','salesperson'])\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfhou-A6Icrh",
        "outputId": "7f0772de-7ecb-4e28-e2be-3d58240ed807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|nextSales|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-01-01|Charlie    |Clothing   |800  |NULL     |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |800      |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |950      |\n",
            "|2023-01-01|Eve        |Clothing   |600  |NULL     |\n",
            "|2023-02-01|Eve        |Clothing   |700  |600      |\n",
            "|2023-01-01|Alice      |Electronics|1000 |NULL     |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1000     |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1100     |\n",
            "|2023-01-01|Bob        |Electronics|1200 |NULL     |\n",
            "|2023-02-01|Bob        |Electronics|900  |1200     |\n",
            "|2023-03-01|Bob        |Electronics|1000 |900      |\n",
            "|2023-01-01|Diana      |Electronics|1500 |NULL     |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1500     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1300     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |NULL     |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lag\n",
        "# pyspark\n",
        "\n",
        "from pyspark.sql.functions import lag,col, asc_nulls_last\n",
        "\n",
        "dataframe.withColumn('nextSales',lag(col('sales'),1).over(Window.partitionBy('department','salesperson').orderBy(asc_nulls_last(col('mn')))))\\\n",
        "         .orderBy(['department','salesperson','mn'])\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CprBcWxVIhJq",
        "outputId": "8a862f9e-35e0-4a96-cd1a-de0b906bfbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|nextSales|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-01-01|Charlie    |Clothing   |800  |NULL     |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |800      |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |950      |\n",
            "|2023-01-01|Eve        |Clothing   |600  |NULL     |\n",
            "|2023-02-01|Eve        |Clothing   |700  |600      |\n",
            "|2023-01-01|Alice      |Electronics|1000 |NULL     |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1000     |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1100     |\n",
            "|2023-01-01|Bob        |Electronics|1200 |NULL     |\n",
            "|2023-02-01|Bob        |Electronics|900  |1200     |\n",
            "|2023-03-01|Bob        |Electronics|1000 |900      |\n",
            "|2023-01-01|Diana      |Electronics|1500 |NULL     |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1500     |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1300     |\n",
            "|2023-02-01|Frank      |Electronics|NULL |NULL     |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}