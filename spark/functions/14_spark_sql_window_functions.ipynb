{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC+ECrdaE8hRAO2+cez923",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/functions/14_spark_sql_window_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark Window Functions**\n",
        "https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#window-functions"
      ],
      "metadata": {
        "id": "ptAYb_3hSs8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qXIHqycTSsTi"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('spark-functions').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, desc, rank,col,dense_rank,expr\n",
        "from pyspark.sql.types import StructType,StructField, IntegerType, StringType"
      ],
      "metadata": {
        "id": "DUaPQh3cchI9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = [\n",
        "    (\"2023-01-01\", \"Alice\", \"Electronics\", 1000),\n",
        "    (\"2023-01-01\", \"Bob\", \"Electronics\", 1200),\n",
        "    (\"2023-01-01\", \"Charlie\", \"Clothing\", 800),\n",
        "    (\"2023-01-01\", \"Diana\", \"Electronics\", 1500),\n",
        "    (\"2023-01-01\", \"Eve\", \"Clothing\", 600),\n",
        "    (\"2023-02-01\", \"Alice\", \"Electronics\", 1100),\n",
        "    (\"2023-02-01\", \"Bob\", \"Electronics\", 900),\n",
        "    (\"2023-02-01\", \"Charlie\", \"Clothing\", 950),\n",
        "    (\"2023-02-01\", \"Diana\", \"Electronics\", 1300),\n",
        "    (\"2023-02-01\", \"Eve\", \"Clothing\", 700),\n",
        "    (\"2023-02-01\", \"Frank\", \"Electronics\", None),\n",
        "    (\"2023-03-01\", \"Alice\", \"Electronics\", 1400),\n",
        "    (\"2023-03-01\", \"Bob\", \"Electronics\", 1000),\n",
        "    (\"2023-03-01\", \"Charlie\", \"Clothing\", 1200),\n",
        "    (\"2023-03-01\", \"Diana\", \"Electronics\", 1600)\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"mn\", StringType(), True),\n",
        "    StructField(\"salesperson\", StringType(), True),\n",
        "    StructField(\"department\", StringType(), True),\n",
        "    StructField(\"sales\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe = dataframe.withColumn('mn',to_date(col('mn'), 'yyyy-MM-dd'))\n",
        "dataframe.printSchema()\n",
        "dataframe.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESOUZY2Tb451",
        "outputId": "a845c1d2-02b1-4039-fb93-6adb4ce6cec4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mn: date (nullable = true)\n",
            " |-- salesperson: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- sales: integer (nullable = true)\n",
            "\n",
            "+----------+-----------+-----------+-----+\n",
            "|mn        |salesperson|department |sales|\n",
            "+----------+-----------+-----------+-----+\n",
            "|2023-01-01|Alice      |Electronics|1000 |\n",
            "|2023-01-01|Bob        |Electronics|1200 |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |\n",
            "|2023-01-01|Diana      |Electronics|1500 |\n",
            "|2023-01-01|Eve        |Clothing   |600  |\n",
            "|2023-02-01|Alice      |Electronics|1100 |\n",
            "|2023-02-01|Bob        |Electronics|900  |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |\n",
            "|2023-02-01|Diana      |Electronics|1300 |\n",
            "|2023-02-01|Eve        |Clothing   |700  |\n",
            "|2023-02-01|Frank      |Electronics|NULL |\n",
            "|2023-03-01|Alice      |Electronics|1400 |\n",
            "|2023-03-01|Bob        |Electronics|1000 |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |\n",
            "|2023-03-01|Diana      |Electronics|1600 |\n",
            "+----------+-----------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.createOrReplaceTempView('dataframe_view')"
      ],
      "metadata": {
        "id": "3LMzRqS1cxU9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rank\n",
        "\n",
        "# saprk sqkl\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "  select *, rank() over(partition by department order by sales desc NULLS LAST) as rn\n",
        "  from dataframe_view\n",
        ")\n",
        "select *\n",
        "from cte\n",
        "where rn = 1\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxr5Skvhfhgg",
        "outputId": "839f831e-5435-4dab-ec96-dbd7aa3be9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|mn        |salesperson|department |sales|rn |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1  |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1  |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "ULC1tHyjf7-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rank (psyspark dataframe api)\n",
        "\n",
        "win = Window.partitionBy('department').orderBy(desc('sales'))\n",
        "dataframe.withColumn('rn',rank().over(win)).filter('rn == 1').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYqC36OViQGd",
        "outputId": "c526f4b2-5eed-4d4e-eae9-80b7c8f53566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|        mn|salesperson| department|sales| rn|\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-03-01|    Charlie|   Clothing| 1200|  1|\n",
            "|2023-03-01|      Diana|Electronics| 1600|  1|\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dense_rank()\n",
        "\n",
        "# saprk sql\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "  select *, dense_rank() over(partition by department order by sales desc NULLS LAST) as rn\n",
        "  from dataframe_view\n",
        ")\n",
        "select *\n",
        "from cte\n",
        "where rn = 2\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7w4mrigsLwx",
        "outputId": "15fbe03c-6d6a-4504-8402-f358487b416c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|mn        |salesperson|department |sales|rn |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-02-01|Charlie    |Clothing   |950  |2  |\n",
            "|2023-01-01|Diana      |Electronics|1500 |2  |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dense_rank (psyspark dataframe api)\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import expr, dense_rank, desc, desc_nulls_last\n",
        "\n",
        "win = Window.partitionBy('department').orderBy(desc_nulls_last('sales'))\n",
        "dataframe.withColumn('dnsrk', dense_rank().over(win)).filter('dnsrk == 2').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6lthDmesPUS",
        "outputId": "213e43dd-57d8-4f7b-8e69-0c804a2ff01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-----+\n",
            "|mn        |salesperson|department |sales|dnsrk|\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|2023-02-01|Charlie    |Clothing   |950  |2    |\n",
            "|2023-01-01|Diana      |Electronics|1500 |2    |\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# row_number()\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "  select *, row_number() over(partition by department order by sales desc NULLS LAST) as rn\n",
        "  from dataframe_view\n",
        ")\n",
        "select *\n",
        "from cte\n",
        "where rn = 1\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRIPwj16s6mx",
        "outputId": "550fab42-e0e7-4916-94ae-0367c23cc730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---+\n",
            "|mn        |salesperson|department |sales|rn |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1  |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1  |\n",
            "+----------+-----------+-----------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# row_number() - pysprk dataframe api\n",
        "\n",
        "from pyspark.sql.functions import row_number, desc_nulls_last,col, exp\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "win = Window.partitionBy('department').orderBy(desc_nulls_last('sales'))\n",
        "dataframe.withColumn('rwNum',row_number().over(win)).filter(expr('rwNum = 1')).show(truncate = False)\n",
        "\n",
        "dataframe.withColumn('rwNum',expr('row_number() over(partition by department order by sales desc nulls last)')).filter(expr('rwNum = 1')).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TprrAx_QwBq3",
        "outputId": "4e3521b2-1ddc-4c31-f44b-385cc4019083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-----+\n",
            "|mn        |salesperson|department |sales|rwNum|\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1    |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1    |\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|mn        |salesperson|department |sales|rwNum|\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1    |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1    |\n",
            "+----------+-----------+-----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# person_rank() : to access the relative standing of a row\n",
        "\n",
        "sql = '''\n",
        "select *, percent_rank() over(partition by department order by sales desc nulls last) as perRank\n",
        "from dataframe_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R4RBYhvKq74",
        "outputId": "63360945-3040-4206-9463-3acb9f120689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+------------------+\n",
            "|mn        |salesperson|department |sales|perRank           |\n",
            "+----------+-----------+-----------+-----+------------------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |0.0               |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |0.25              |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |0.5               |\n",
            "|2023-02-01|Eve        |Clothing   |700  |0.75              |\n",
            "|2023-01-01|Eve        |Clothing   |600  |1.0               |\n",
            "|2023-03-01|Diana      |Electronics|1600 |0.0               |\n",
            "|2023-01-01|Diana      |Electronics|1500 |0.1111111111111111|\n",
            "|2023-03-01|Alice      |Electronics|1400 |0.2222222222222222|\n",
            "|2023-02-01|Diana      |Electronics|1300 |0.3333333333333333|\n",
            "|2023-01-01|Bob        |Electronics|1200 |0.4444444444444444|\n",
            "|2023-02-01|Alice      |Electronics|1100 |0.5555555555555556|\n",
            "|2023-01-01|Alice      |Electronics|1000 |0.6666666666666666|\n",
            "|2023-03-01|Bob        |Electronics|1000 |0.6666666666666666|\n",
            "|2023-02-01|Bob        |Electronics|900  |0.8888888888888888|\n",
            "|2023-02-01|Frank      |Electronics|NULL |1.0               |\n",
            "+----------+-----------+-----------+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# person_rank() : to access the relative standing of a row\n",
        "\n",
        "# spark dataframe api (with expr)\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "dataframe.withColumn('percRank', expr('percent_rank() over(partition by department order by sales desc nulls first)'))\\\n",
        "         .filter(expr('percRank = 1'))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "id": "pOG6qp3hLCAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d32527c-d36c-4f3d-a951-792671ddfebd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+--------+\n",
            "|mn        |salesperson|department |sales|percRank|\n",
            "+----------+-----------+-----------+-----+--------+\n",
            "|2023-01-01|Eve        |Clothing   |600  |1.0     |\n",
            "|2023-02-01|Bob        |Electronics|900  |1.0     |\n",
            "+----------+-----------+-----------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# person_rank() : to access the relative standing of a row\n",
        "\n",
        "# spark dataframe api (with all pyspark)\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import percent_rank, desc_nulls_first, col\n",
        "\n",
        "dataframe.withColumn('perc_rank', percent_rank().over(Window.partitionBy(col('department')).orderBy(desc_nulls_first(col('sales')))))\\\n",
        "         .filter('perc_rank == 1')\\\n",
        "         .show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrGWs677Xc6Y",
        "outputId": "0bdb033e-dd8f-474c-c6a4-2ebb34820f5a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|perc_rank|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-01-01|Eve        |Clothing   |600  |1.0      |\n",
            "|2023-02-01|Bob        |Electronics|900  |1.0      |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **`asc_nulls_first` vs `asc_nulls_last`**\n",
        "\n",
        "| Aspect | `asc_nulls_first` | `asc_nulls_last` |\n",
        "|--------|-------------------|------------------|\n",
        "| **NULL Position** | NULLs at start | NULLs at end |\n",
        "| **Sort Order** | NULLs → Ascending values | Ascending values → NULLs |\n",
        "| **Default** | Yes | No |\n",
        "---\n",
        "##### **Example:**\n",
        "- `asc_nulls_first`: `[NULL, NULL, 10, 25, 50]`\n",
        "- `asc_nulls_last`: `[10, 25, 50, NULL, NULL]`\n",
        "---\n",
        "**That's it.** Both sort ascending - only NULL placement differs."
      ],
      "metadata": {
        "id": "ooe_eBWjaeAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# asc_nulls_first : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import asc_nulls_first,col\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.orderBy(col('nums')).show(truncate = False)\n",
        "dataframe.orderBy(asc_nulls_first(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpG0Ab3xYyS9",
        "outputId": "d3740fed-de40-45e2-bfc4-592571bdb993"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# asc_nulls_last : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import asc_nulls_last,col\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframeNums = spark.createDataFrame(data, schema)\n",
        "dataframeNums.orderBy(col('nums')).show(truncate = False)\n",
        "dataframeNums.orderBy(asc_nulls_last(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4vejMgTbsNE",
        "outputId": "867a9cbd-94e0-40a3-ac54-59d2f6f64dce"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|-20 |\n",
            "|1   |\n",
            "|19  |\n",
            "|20  |\n",
            "|30  |\n",
            "|40  |\n",
            "|100 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desc_nulls_first : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import desc_nulls_first,col, expr, desc\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframeNums = spark.createDataFrame(data, schema)\n",
        "dataframeNums.orderBy(desc(col('nums'))).show(truncate = False)\n",
        "dataframeNums.orderBy(desc_nulls_first(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVgz1C3HdvkS",
        "outputId": "6665dc24-9acb-4d81-e9c3-d9fff2bbe535"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|NULL|\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desc_nulls_last : this is actually default in the order by clause\n",
        "\n",
        "from pyspark.sql.functions import desc_nulls_last,col, expr, desc\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType\n",
        "\n",
        "data = [[100],[-20],[30],[19],[None],[1],[20],[40]]\n",
        "schema = StructType([StructField('nums',IntegerType())])\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.orderBy(desc(col('nums'))).show(truncate = False)\n",
        "dataframe.orderBy(desc_nulls_last(col('nums'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvrlDExOd9hy",
        "outputId": "987ea413-2d89-45ff-d1d0-d81412ea2f6a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n",
            "+----+\n",
            "|nums|\n",
            "+----+\n",
            "|100 |\n",
            "|40  |\n",
            "|30  |\n",
            "|20  |\n",
            "|19  |\n",
            "|1   |\n",
            "|-20 |\n",
            "|NULL|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **PySpark NULL Ordering Defaults:**\n",
        "\n",
        "✅ **ASC + NULLs first** = Default behavior  \n",
        "✅ **DESC + NULLs last** = Default behavior  \n",
        "✅ **asc_nulls_first()** and **desc_nulls_last()** make default behavior explicit"
      ],
      "metadata": {
        "id": "P6AjrMH7ec6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ntile : simple bucketing the dataset\n",
        "\n",
        "sql = '''\n",
        "select *, ntile(2) over(partition by department order by sales) as perRank\n",
        "from dataframe_view\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3upELym2eVd6",
        "outputId": "9188e3f9-6a7a-4de5-8bff-82bf757c7c8b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+-------+\n",
            "|mn        |salesperson|department |sales|perRank|\n",
            "+----------+-----------+-----------+-----+-------+\n",
            "|2023-01-01|Eve        |Clothing   |600  |1      |\n",
            "|2023-02-01|Eve        |Clothing   |700  |1      |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |1      |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |2      |\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |2      |\n",
            "|2023-02-01|Frank      |Electronics|NULL |1      |\n",
            "|2023-02-01|Bob        |Electronics|900  |1      |\n",
            "|2023-01-01|Alice      |Electronics|1000 |1      |\n",
            "|2023-03-01|Bob        |Electronics|1000 |1      |\n",
            "|2023-02-01|Alice      |Electronics|1100 |1      |\n",
            "|2023-01-01|Bob        |Electronics|1200 |2      |\n",
            "|2023-02-01|Diana      |Electronics|1300 |2      |\n",
            "|2023-03-01|Alice      |Electronics|1400 |2      |\n",
            "|2023-01-01|Diana      |Electronics|1500 |2      |\n",
            "|2023-03-01|Diana      |Electronics|1600 |2      |\n",
            "+----------+-----------+-----------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ntile : simple bucketing the dataset\n",
        "\n",
        "# pyspark api (with expr)\n",
        "\n",
        "from pyspark.sql.functions import col, asc_nulls_last\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "dataframe.withColumn('nthile_col', expr('''ntile(2) over(partition by department order by sales desc nulls last)'''))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6lcUNjupTo5",
        "outputId": "a22c5490-a57b-4fd3-ad8c-e8abbbcf0885"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+----------+\n",
            "|mn        |salesperson|department |sales|nthile_col|\n",
            "+----------+-----------+-----------+-----+----------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1         |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |1         |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |1         |\n",
            "|2023-02-01|Eve        |Clothing   |700  |2         |\n",
            "|2023-01-01|Eve        |Clothing   |600  |2         |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1         |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1         |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1         |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1         |\n",
            "|2023-01-01|Bob        |Electronics|1200 |1         |\n",
            "|2023-02-01|Alice      |Electronics|1100 |2         |\n",
            "|2023-01-01|Alice      |Electronics|1000 |2         |\n",
            "|2023-03-01|Bob        |Electronics|1000 |2         |\n",
            "|2023-02-01|Bob        |Electronics|900  |2         |\n",
            "|2023-02-01|Frank      |Electronics|NULL |2         |\n",
            "+----------+-----------+-----------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ntile : simple bucketing the dataset\n",
        "\n",
        "# pyspark api\n",
        "\n",
        "from pyspark.sql.functions import col, desc_nulls_last,ntile\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "dataframe.withColumn('ntile_col',ntile(2).over(Window.partitionBy('department').orderBy(desc_nulls_last(col('sales')))))\\\n",
        "         .show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmL0YJZepxXs",
        "outputId": "12901b97-3eab-404e-eb88-5a9e11f6dc52"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+-----+---------+\n",
            "|mn        |salesperson|department |sales|ntile_col|\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "|2023-03-01|Charlie    |Clothing   |1200 |1        |\n",
            "|2023-02-01|Charlie    |Clothing   |950  |1        |\n",
            "|2023-01-01|Charlie    |Clothing   |800  |1        |\n",
            "|2023-02-01|Eve        |Clothing   |700  |2        |\n",
            "|2023-01-01|Eve        |Clothing   |600  |2        |\n",
            "|2023-03-01|Diana      |Electronics|1600 |1        |\n",
            "|2023-01-01|Diana      |Electronics|1500 |1        |\n",
            "|2023-03-01|Alice      |Electronics|1400 |1        |\n",
            "|2023-02-01|Diana      |Electronics|1300 |1        |\n",
            "|2023-01-01|Bob        |Electronics|1200 |1        |\n",
            "|2023-02-01|Alice      |Electronics|1100 |2        |\n",
            "|2023-01-01|Alice      |Electronics|1000 |2        |\n",
            "|2023-03-01|Bob        |Electronics|1000 |2        |\n",
            "|2023-02-01|Bob        |Electronics|900  |2        |\n",
            "|2023-02-01|Frank      |Electronics|NULL |2        |\n",
            "+----------+-----------+-----------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "HiZmj9fEqu6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}