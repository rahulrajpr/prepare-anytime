{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN68SB2F+PLfa9D72bpQTMC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/functions/13_spark_sql_aggregate_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark Aggregate Functions**\n",
        "https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions"
      ],
      "metadata": {
        "id": "he4EMqWr-uYj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUE_MzFa-qf6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('spark-functions').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select count(col) as cnt\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dPmC4bM-_cy",
        "outputId": "145af246-b8ee-46ef-fa1b-c3663a51bafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|cnt|\n",
            "+---+\n",
            "|50 |\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count (distinct **)\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select array(1,1,1,1,1,2,2,22,3,3,3,10,100) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select count(distinct col) as cnt\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myD0ftC0AfZY",
        "outputId": "e0e89d5e-7c10-483a-88b1-a0a9a96059a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|cnt|\n",
            "+---+\n",
            "|6  |\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# approx_count_distinct : approximate count distinct using the HyperLooLog++ function\n",
        "\n",
        "data = [['rahul'],['rahul'],['skylr'],['lathika'],['hazel'],['jameela'],['lakshmi'],['lakshmi'],['skylr']]\n",
        "schema = ['names']\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.selectExpr('approx_count_distinct(names) as appxcntDist').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAWVyBpstFyC",
        "outputId": "ccc79293-f216-4c89-a54b-ef21f4ce4b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|appxcntDist|\n",
            "+-----------+\n",
            "|6          |\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### COUNT vs. COUNT(DISTINCT) vs. APPROX_COUNT_DISTINCT\n",
        "\n",
        "| Feature | `COUNT(column)` | `COUNT(DISTINCT column)` | `APPROX_COUNT_DISTINCT(column)` |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Primary Purpose** | Counts the total number of **non-null** values. | Calculates the **exact cardinality** (number of unique, non-null values). | **Estimates the cardinality** (number of unique, non-null values). |\n",
        "| **What it Measures** | **Volume / Quantity** of data. | **Exact Uniqueness** (True Cardinality). | **Approximate Uniqueness** (Estimated Cardinality). |\n",
        "| **Output** | Exact integer. | Exact integer. | Approximate integer (High accuracy, but not exact). |\n",
        "| **Performance / Speed** | ðŸŸ¢ **Very Fast**<br/>A simple scan and increment. | ðŸ”´ **Slow / Expensive**<br/>Requires building a complete in-memory hash set of all unique values. | ðŸŸ¡ **Very Fast**<br/>Uses a fixed-size probabilistic sketch (e.g., HyperLogLog). |\n",
        "| **Accuracy** | 100% Accurate. | 100% Accurate. | **High, but not 100%.**<br/>Typical accuracy is 97-99.9% with a small, predictable error rate. |\n",
        "| **Memory Usage** | Low (a single counter). | High (scales with the number of unique values). | Very Low (constant memory, fixed size of the sketch). |\n",
        "| **Ideal Use Case** | \"How many sales transactions were recorded?\" | \"How many unique customers placed an order?\" (for financial reporting). | \"How many unique visitors are on our site right now?\" (for a real-time dashboard). |"
      ],
      "metadata": {
        "id": "KTEEy-I0wfVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count_min_sketch\n",
        "\n",
        "data = [['rahul'],['rahul'],['skylr'],['lathika'],['hazel'],['jameela'],['lakshmi'],['lakshmi'],['skylr']]\n",
        "schema = ['names']\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.selectExpr('count_min_sketch(names,CAST(0.1 AS DOUBLE),CAST(0.95 AS DOUBLE),42) as count_min_sketchOut').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw0AYzr9EVdp",
        "outputId": "e265318d-1de5-4efd-9718-4930e51b0ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|count_min_sketchOut                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[00 00 00 01 00 00 00 00 00 00 00 09 00 00 00 05 00 00 00 14 00 00 00 00 5D 20 CE 9A 00 00 00 00 06 FF 45 7B 00 00 00 00 57 73 DD F0 00 00 00 00 06 22 E0 14 00 00 00 00 27 84 1E 72 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tabular Comparison: `approx_count_distinct` vs `count_min_sketch`\n",
        "\n",
        "| Feature | `approx_count_distinct` | `count_min_sketch` |\n",
        "|---------|------------------------|-------------------|\n",
        "| **What it measures** | Number of unique values | Frequency of each value |\n",
        "| **Output** | Single number | Probabilistic data structure |\n",
        "| **Answers** | \"How many distinct items?\" | \"How many times per item?\" |\n",
        "| **Use Case** | Unique visitor count | Visit frequency per user |\n",
        "| **Example Result** | `5000` unique users | `user1: 15 visits`, `user2: 8 visits` |\n",
        "| **Complexity** | Simple - ready to use | Complex - needs processing |\n",
        "| **Practical Usage** | Direct results in PySpark | Hard to use in PySpark SQL |\n",
        "| **Serialization** | No serialization needed | Returns serialized binary data |\n",
        "| **Data Access** | Direct value | Requires deserialization to access counts |\n",
        "\n",
        "**One-liner:** `approx_count_distinct` gives direct results, `count_min_sketch` gives serialized binary that's hard to use in PySpark."
      ],
      "metadata": {
        "id": "-8rXpJTJGGPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count_if\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select count_if(col % 3 = 0) as cnt\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xyyfJgn_myr",
        "outputId": "0210a5bc-5115-45f3-8ef8-5c8d037fabef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|cnt|\n",
            "+---+\n",
            "|17 |\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# min\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select min(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isNJZWjFCcx7",
        "outputId": "b80959a9-6e76-418a-8447-42b65c2d73c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|val|\n",
            "+---+\n",
            "|1  |\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# min_by\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select 1 as col1, 2 as col2\n",
        "union all\n",
        "select 2 as col1, 5 as col2\n",
        "union all\n",
        "select 3 as col1, 10 as col2\n",
        "union all\n",
        "select 4 as col1, -3 as col2\n",
        ")\n",
        "select\n",
        "  min_by(col1,col2) as minOfCol1ByCol,\n",
        "  min_by(col2,col1) as minOfCol2ByCol1\n",
        "from cte\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#--\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select 1 as col1, 2 as col2\n",
        "union all\n",
        "select 1 as col1, -3 as col2\n",
        "union all\n",
        "select 3 as col1, 10 as col2\n",
        "union all\n",
        "select 4 as col1, -3 as col2\n",
        ")\n",
        "select\n",
        "  min_by(col1,col2) as minOfCol1ByCol,\n",
        "  min_by(col2,col1) as minOfCol2ByCol1\n",
        "from cte\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfjjb9JWBbZk",
        "outputId": "c302c5fb-4ce6-4d00-fa33-118442bfe413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------+\n",
            "|minOfCol1ByCol|minOfCol2ByCol1|\n",
            "+--------------+---------------+\n",
            "|4             |2              |\n",
            "+--------------+---------------+\n",
            "\n",
            "+--------------+---------------+\n",
            "|minOfCol1ByCol|minOfCol2ByCol1|\n",
            "+--------------+---------------+\n",
            "|4             |-3             |\n",
            "+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Key Point on `min_by`:\n",
        "---\n",
        "When there are duplicate minimum values in the ordering column, `min_by()` uses **first occurrence semantics** rather than returning all possible values or choosing randomly.\n",
        "---"
      ],
      "metadata": {
        "id": "zPzFZFeABfk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select max(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E86USkTuCe-P",
        "outputId": "e3ff6e1e-5110-445b-b266-7e49666eeda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|val|\n",
            "+---+\n",
            "|99 |\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max_by\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select 1 as col1, 2 as col2\n",
        "union all\n",
        "select 2 as col1, 5 as col2\n",
        "union all\n",
        "select 3 as col1, 10 as col2\n",
        "union all\n",
        "select 4 as col1, -3 as col2\n",
        ")\n",
        "select\n",
        "  max_by(col1,col2) as maxOfCol1ByCol,\n",
        "  max_by(col2,col1) as maxOfCol2ByCol1\n",
        "from cte\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#--\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select 1 as col1, 2 as col2\n",
        "union all\n",
        "select 1 as col1, -3 as col2\n",
        "union all\n",
        "select 3 as col1, 10 as col2\n",
        "union all\n",
        "select 4 as col1, -3 as col2\n",
        ")\n",
        "select\n",
        "  max_by(col1,col2) as maxOfCol1ByCol,\n",
        "  max_by(col2,col1) as maxOfCol2ByCol1\n",
        "from cte\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGS2dTj4BrE3",
        "outputId": "e11eb9cb-4576-4549-8fac-12ca4883d470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------+\n",
            "|maxOfCol1ByCol|maxOfCol2ByCol1|\n",
            "+--------------+---------------+\n",
            "|3             |-3             |\n",
            "+--------------+---------------+\n",
            "\n",
            "+--------------+---------------+\n",
            "|maxOfCol1ByCol|maxOfCol2ByCol1|\n",
            "+--------------+---------------+\n",
            "|3             |-3             |\n",
            "+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### Key Point:\n",
        "---\n",
        "When there are duplicate minimum values in the ordering column, `max_by()` uses **first occurrence semantics** rather than returning all possible values or choosing randomly.\n",
        "---"
      ],
      "metadata": {
        "id": "ps3MU1_QB5Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sum\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select sum(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bnIIcpjBDwN",
        "outputId": "b20ad5e4-124a-494a-9550-834af22d6a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|val |\n",
            "+----+\n",
            "|2500|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try_sum\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select try_sum(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dX9QL5UCyyo",
        "outputId": "8d0065c4-503b-4daf-e5e2-e604151148e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|val |\n",
            "+----+\n",
            "|2500|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# avg\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select avg(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOR4wFxLCZx-",
        "outputId": "e6105d4a-35ae-4001-a263-73ade01969ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|val |\n",
            "+----+\n",
            "|50.0|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try_avg\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select try_avg(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTZrTG1pC3Xi",
        "outputId": "927c3908-c92d-4d8a-b052-3c85efa74d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|val |\n",
            "+----+\n",
            "|50.0|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mean\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select mean(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9ul0cnaCkfg",
        "outputId": "8788a146-f455-473f-b57c-5037e25e525e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|val |\n",
            "+----+\n",
            "|50.0|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# median\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select median(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPNedgn2DEZq",
        "outputId": "cac101b6-d3d5-4906-a6a3-d41cd27405d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|val |\n",
            "+----+\n",
            "|50.0|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mode\n",
        "\n",
        "sql = '''\n",
        "with cte as\n",
        "(\n",
        "select sequence(1,100,2) as col\n",
        ")\n",
        ",cte2 as\n",
        "(\n",
        "select\n",
        "   explode(col) as col\n",
        "from cte\n",
        ")\n",
        "select mode(col) as val\n",
        "from cte2\n",
        "'''\n",
        "spark.sql(sql).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4_q2FdqDKu0",
        "outputId": "842c0725-759e-497e-a7ed-b062b7c6502a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|val|\n",
            "+---+\n",
            "|13 |\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### AVG(MEAN) vs MEDIAN vs MODE\n",
        "---\n",
        "##### AVG / MEAN\n",
        "- **Mathematical average**\n",
        "- **Use when**: Data is normally distributed\n",
        "- **Best for**: Symmetrical datasets without extreme values\n",
        "---\n",
        "##### MEDIAN  \n",
        "- **Middle value** in sorted dataset\n",
        "- **Use when**: Data has outliers or is skewed\n",
        "- **Best for**: Income data, housing prices, skewed distributions\n",
        "---\n",
        "##### MODE\n",
        "- **Most frequent value**\n",
        "- **Use when**: Working with categorical data or finding popular items\n",
        "- **Best for**: Survey responses, product preferences, categorical analysis\n",
        "---\n",
        "## Key Takeaway\n",
        "- **Normal data** â†’ Use AVG/MEAN\n",
        "- **Skewed data with outliers** â†’ Use MEDIAN  \n",
        "- **Categorical data** â†’ Use MODE\n",
        "---"
      ],
      "metadata": {
        "id": "iSuXQE1DDsl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# any : return true in case any value is true from the column\n",
        "\n",
        "data = [[True],[True],[False],[False]]\n",
        "schema = ['col1']\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.selectExpr('any(col1) as anyValue').show(truncate = False)"
      ],
      "metadata": {
        "id": "Mhp4z8dT_ATR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce49331e-4b82-4603-b5e9-84aa7a514c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|anyValue|\n",
            "+--------+\n",
            "|true    |\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some : return True in case any value is True in the column\n",
        "\n",
        "data = [[True],[True],[False],[False]]\n",
        "schema = ['col1']\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.selectExpr('some(col1) as someVaue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJrjhq9hoAHu",
        "outputId": "5fb2a0fc-c69d-4db7-e4b9-123195449740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|someVaue|\n",
            "+--------+\n",
            "|true    |\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# every : return True in case all value is True in the column\n",
        "\n",
        "data = [[True],[True],[False],[False]]\n",
        "schema = ['col1']\n",
        "\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.selectExpr('every(col1) as everyValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJAfQhbnCI0h",
        "outputId": "5127c50e-be25-41ca-ae36-ae67ba37370b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|everyValue|\n",
            "+----------+\n",
            "|false     |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bool_and : return true in case all values are true from the column\n",
        "\n",
        "data = [[True],[True],[False],[False]]\n",
        "schema = ['col1']\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.selectExpr('bool_and(col1) as bool_andValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x16DulAY10hP",
        "outputId": "9a16909c-7363-48a0-80bf-a8da07e48ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|bool_andValue|\n",
            "+-------------+\n",
            "|false        |\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bool_or : return true in case any value is true from the column\n",
        "\n",
        "data = [[True],[True],[False],[False]]\n",
        "schema = ['col1']\n",
        "dataframe = spark.createDataFrame(data, schema)\n",
        "dataframe.selectExpr('bool_or(col1) as bool_orValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a52t9Twe2JCW",
        "outputId": "e2f3ed1b-5a71-4259-adc4-9279f759891e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|bool_orValue|\n",
            "+------------+\n",
            "|true        |\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# std : standard deviation\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('std(score) as stdValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLNX5v-NqV1G",
        "outputId": "6f17b55a-c27f-4091-817c-d7a11be5efb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|stdValue        |\n",
            "+----------------+\n",
            "|10.1418510567422|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stddev : standard deviation\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('std(score) as stdValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo_kaiTkrJLR",
        "outputId": "016b600d-2e4f-41de-a2bd-4e799de0550e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|stdValue        |\n",
            "+----------------+\n",
            "|10.1418510567422|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stddev_samp : standard deviation for sample\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('stddev_samp(score) as stddev_sampValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4qC9ylorQ6A",
        "outputId": "07c8ee8c-d3a5-473d-a5bd-a4c647b2f37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|stddev_sampValue|\n",
            "+----------------+\n",
            "|10.1418510567422|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stddev_pop: standard deviation for population\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('stddev_pop(score) as stddev_popValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8k4gluLrZ_u",
        "outputId": "b3893b7c-f266-4a48-91e5-822632116865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|stddev_popValue  |\n",
            "+-----------------+\n",
            "|9.486832980505138|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# variance: variance\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('variance(score) as varianceValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRf43g-7rqym",
        "outputId": "4dad8086-3243-4c70-81b4-d6a24748cd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|varianceValue     |\n",
            "+------------------+\n",
            "|102.85714285714286|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# var_samp : variance for sample data\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('var_samp(score) as var_sampValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yYXpIkYr_M8",
        "outputId": "87498712-d9a5-450c-d85a-8553c28e9476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|var_sampValue     |\n",
            "+------------------+\n",
            "|102.85714285714286|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# var_pop : variance for population data\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('var_pop(score) as var_popValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9R9d3y8sJfL",
        "outputId": "061fad61-33b6-4599-ea52-1b728cbe275b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|var_popValue|\n",
            "+------------+\n",
            "|90.0        |\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corr : correlation between two columns\n",
        "\n",
        "data_scores = [[85,90], [92,10], [78,-30], [65,55], [88,20], [95,12], [72,2], [81,100]]\n",
        "schema = ['col1','col2']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('corr(col1,col2) as corrValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEpsc3rdAdXs",
        "outputId": "47c7069d-bcb6-4bbf-d9f7-06aea8e2e159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|corrValue           |\n",
            "+--------------------+\n",
            "|-0.09010476814935589|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# covar_samp : covariance with sample data\n",
        "\n",
        "data_scores = [[85,90], [92,10], [78,-30], [65,55], [88,20], [95,12], [72,2], [81,100]]\n",
        "schema = ['col1','col2']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('covar_samp(col1,col2) as covar_sampValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDro0aBcA__V",
        "outputId": "da9af2fc-e2a3-4f29-cf4e-e958af2d5f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|covar_sampValue    |\n",
            "+-------------------+\n",
            "|-41.285714285714285|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# covar_pop : covariance with population data\n",
        "\n",
        "data_scores = [[85,90], [92,10], [78,-30], [65,55], [88,20], [95,12], [72,2], [81,100]]\n",
        "schema = ['col1','col2']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('covar_pop(col1,col2) as covar_popValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UlV72hNBNKx",
        "outputId": "e0296135-aed7-4585-df7d-4bf46461956d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|covar_popValue|\n",
            "+--------------+\n",
            "|-36.125       |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kurtosis : # kurtosis : variance for population data\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('kurtosis(score) as kurtosisValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8dgqkPtGuhB",
        "outputId": "86ddc31f-5e8a-4833-be8e-4b08788af01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|kurtosisValue      |\n",
            "+-------------------+\n",
            "|-0.9364814814814815|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# skewness : # kurtosis : variance for population data\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('skewness(score) as skewnessValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkekTAiZHPpA",
        "outputId": "aa84076e-8ca5-4111-8763-b95beb365ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|skewnessValue      |\n",
            "+-------------------+\n",
            "|-0.3715676250697846|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first :\n",
        "# first_value\n",
        "\n",
        "# the second argument stands for isIgnoreNull\n",
        "\n",
        "data_scores = [[85,90], [92,10], [78,-30], [65,55], [88,20], [95,12], [72,2], [81,100]]\n",
        "schema = ['col1','col2']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('first(col1,True) as firstValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlSX3OcMBcCX",
        "outputId": "0061a5eb-3d1a-4f6b-eb12-1d1e19af15fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|firstValue|\n",
            "+----------+\n",
            "|85        |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first_value\n",
        "\n",
        "# the second argument stands for isIgnoreNull\n",
        "\n",
        "data_scores = [[85,90], [92,10], [78,-30], [65,55], [88,20], [95,12], [72,2], [81,100]]\n",
        "schema = ['col1','col2']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('first_value(col1,True) as first_valueValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTcnWWqoCVjy",
        "outputId": "23053dc7-8241-4525-b346-11b6d9316c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|first_valueValue|\n",
            "+----------------+\n",
            "|85              |\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# last :\n",
        "# synonym : last_value\n",
        "\n",
        "# the second argument stands for isIgnoreNull\n",
        "\n",
        "data_scores = [[85,90], [92,10], [78,-30], [65,55], [88,20], [95,12], [72,2], [81,100]]\n",
        "schema = ['col1','col2']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('last(col1,True) as lastValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roSpGkJYB2Cn",
        "outputId": "6a7ec2f5-8782-4d4b-a9db-8a747a979814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|lastValue|\n",
            "+---------+\n",
            "|81       |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# last_value :\n",
        "\n",
        "# the second argument stands for isIgnoreNull\n",
        "\n",
        "data_scores = [[85,90], [92,10], [78,-30], [65,55], [88,20], [95,12], [72,2], [81,100]]\n",
        "schema = ['col1','col2']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema)\n",
        "dataframe.selectExpr('last_value(col1,True) as last_valueValuue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I90zbRyVGadk",
        "outputId": "66bd0b88-5143-4d61-b870-9630f9a876b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|last_valueValuue|\n",
            "+----------------+\n",
            "|81              |\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# any_value : a random value from a column\n",
        "\n",
        "## second argument is for ignoring the NULLS\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import cast,col\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81], [None]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema).withColumn('score', col('score').cast('integer'))\n",
        "dataframe.printSchema()\n",
        "dataframe.selectExpr('any_value(score,True) as any_valueValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwka1ZtrsqcH",
        "outputId": "01012581-d915-4d0f-e551-9aba2e2afe8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- score: integer (nullable = true)\n",
            "\n",
            "+--------------+\n",
            "|any_valueValue|\n",
            "+--------------+\n",
            "|85            |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# percentile : percentile for the given thresholds percentages\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import cast,col\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81], [None]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema).withColumn('score', col('score').cast('integer'))\n",
        "dataframe.printSchema()\n",
        "dataframe.selectExpr('percentile(score, array(0.25,0.5,0.75)) as percentileValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpmGSAm5xhQS",
        "outputId": "c68b7cde-1051-4ecd-c80c-eaa63036870c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- score: integer (nullable = true)\n",
            "\n",
            "+------------------+\n",
            "|percentileValue   |\n",
            "+------------------+\n",
            "|[76.5, 83.0, 89.0]|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# percentile_approx : percentile for the given thresholds percentages (approximate))\n",
        "\n",
        "# synonym : approx_percentile\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import cast,col\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81], [None]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema).withColumn('score', col('score').cast('integer'))\n",
        "dataframe.printSchema()\n",
        "dataframe.selectExpr('percentile_approx(score, array(0.25,0.5,0.75)) as percentile_approxValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ76qXf-3Ty6",
        "outputId": "ada8c42d-8949-4f00-e17f-3889eaaf8f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- score: integer (nullable = true)\n",
            "\n",
            "+----------------------+\n",
            "|percentile_approxValue|\n",
            "+----------------------+\n",
            "|[72, 81, 88]          |\n",
            "+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# approx_percentile : percentile for the given thresholds percentages (approximate)\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import cast,col\n",
        "\n",
        "data_scores = [[85], [92], [78], [65], [88], [95], [72], [81], [None]]\n",
        "schema = ['score']\n",
        "\n",
        "dataframe = spark.createDataFrame(data_scores, schema).withColumn('score', col('score').cast('integer'))\n",
        "dataframe.printSchema()\n",
        "dataframe.selectExpr('approx_percentile(score, array(0.25,0.5,0.75)) as approx_percentileValue').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwXAHAms39vX",
        "outputId": "ecf3f51f-45c5-4a6c-8ff8-a047ea4af537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- score: integer (nullable = true)\n",
            "\n",
            "+----------------------+\n",
            "|approx_percentileValue|\n",
            "+----------------------+\n",
            "|[72, 81, 88]          |\n",
            "+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect_list : creating a list within a group\n",
        "# it maintaines all the elements and with the order\n",
        "\n",
        "from pyspark.sql.functions import collect_list\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", \"Math\", 85),\n",
        "    (\"Alice\", \"Science\", 92),\n",
        "    (\"Alice\", \"English\", 85),  # Duplicate score 85\n",
        "    (\"Bob\", \"Math\", 78),\n",
        "    (\"Bob\", \"Science\", 78),    # Duplicate score 78\n",
        "    (\"Bob\", \"English\", 90)\n",
        "]\n",
        "schema = [\"name\", \"subject\", \"score\"]\n",
        "\n",
        "dataframe = spark.createDataFrame(data,schema)\n",
        "dataframe.groupBy('name').agg(collect_list(col('score'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0rg0XNa4STT",
        "outputId": "1ebb7d91-a08b-4316-de27-6a09b170e998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+\n",
            "|name |collect_list(score)|\n",
            "+-----+-------------------+\n",
            "|Alice|[85, 92, 85]       |\n",
            "|Bob  |[78, 78, 90]       |\n",
            "+-----+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect_set : creating a set within a group\n",
        "# it may not maintain the order, and it will contain only the unique values\"\n",
        "\n",
        "from pyspark.sql.functions import collect_set\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", \"Math\", 85),\n",
        "    (\"Alice\", \"Science\", 92),\n",
        "    (\"Alice\", \"English\", 85),  # Duplicate score 85\n",
        "    (\"Bob\", \"Math\", 78),\n",
        "    (\"Bob\", \"Science\", 78),    # Duplicate score 78\n",
        "    (\"Bob\", \"English\", 90)\n",
        "]\n",
        "schema = [\"name\", \"subject\", \"score\"]\n",
        "\n",
        "dataframe = spark.createDataFrame(data,schema)\n",
        "dataframe.groupBy('name').agg(collect_set(col('score'))).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm0WR6yq9GjW",
        "outputId": "436e8b5c-a21b-4438-8a2e-723ce8e6cf1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "|name |collect_set(score)|\n",
            "+-----+------------------+\n",
            "|Alice|[85, 92]          |\n",
            "|Bob  |[78, 90]          |\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# roll up\n",
        "\n",
        "data = [\n",
        "    (\"North\", \"Apple\", 100),\n",
        "    (\"North\", \"Banana\", 150),\n",
        "    (\"North\", \"Apple\", 50),  # Another sale for North/Apple\n",
        "    (\"South\", \"Apple\", 200),\n",
        "    (\"South\", \"Banana\", 250),\n",
        "    (\"South\", \"Orange\", 75),\n",
        "    (\"East\", \"Apple\", 120),\n",
        "    (\"East\", \"Banana\", 80)\n",
        "]\n",
        "\n",
        "columns = [\"Region\", \"Product\", \"Sales\"]\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "#- group by without roll up\n",
        "\n",
        "dataframe.createOrReplaceTempView('dataframe_view')\n",
        "sql = '''\n",
        "select region,product, sum(sales) as salesTotal\n",
        "from dataframe_view\n",
        "group by region,product\n",
        "order by region,product\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n",
        "#- group by WITH roll up\n",
        "\n",
        "dataframe.createOrReplaceTempView('dataframe_view')\n",
        "sql = '''\n",
        "SELECT region, product, SUM(sales) as salesTotal\n",
        "FROM dataframe_view\n",
        "GROUP BY ROLLUP(region, product)\n",
        "ORDER BY region NULLS LAST, product NULLS LAST\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QExBqeVbM447",
        "outputId": "edeb1d1c-65fe-4fd7-e0b5-a263ca269a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+----------+\n",
            "|region|product|salesTotal|\n",
            "+------+-------+----------+\n",
            "|East  |Apple  |120       |\n",
            "|East  |Banana |80        |\n",
            "|North |Apple  |150       |\n",
            "|North |Banana |150       |\n",
            "|South |Apple  |200       |\n",
            "|South |Banana |250       |\n",
            "|South |Orange |75        |\n",
            "+------+-------+----------+\n",
            "\n",
            "+------+-------+----------+\n",
            "|region|product|salesTotal|\n",
            "+------+-------+----------+\n",
            "|East  |Apple  |120       |\n",
            "|East  |Banana |80        |\n",
            "|East  |NULL   |200       |\n",
            "|North |Apple  |150       |\n",
            "|North |Banana |150       |\n",
            "|North |NULL   |300       |\n",
            "|South |Apple  |200       |\n",
            "|South |Banana |250       |\n",
            "|South |Orange |75        |\n",
            "|South |NULL   |525       |\n",
            "|NULL  |NULL   |1025      |\n",
            "+------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##### GROUP BY vs GROUP BY WITH ROLLUP\n",
        "---\n",
        "| Aspect | GROUP BY (Without ROLLUP) | GROUP BY WITH ROLLUP |\n",
        "|--------|---------------------------|----------------------|\n",
        "| **Purpose** | Aggregates data at specified grouping levels only | Creates subtotals and grand totals across hierarchy |\n",
        "| **Output Rows** | One row per unique combination of grouping columns | Multiple rows: details + subtotals + grand total |\n",
        "| **Hierarchy** | Flat structure - single level | Multi-level hierarchical structure |\n",
        "| **NULL Handling** | NULL represents actual NULL values from data | NULL represents summary rows (all groups) |\n",
        "| **Result Types** | Only detailed aggregated rows | Detailed rows + subtotal rows + grand total row |\n",
        "| **Data Completeness** | Partial view - only the specified groupings | Complete view with hierarchical summaries |"
      ],
      "metadata": {
        "id": "n5q8H6SUP4cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping : to indicate the the subtotal row of the grouping heirarchy\n",
        "\n",
        "# this is only valid when you use the rollups\n",
        "# if rollups is there, then there is no meaning of grouping\n",
        "\n",
        "data = [\n",
        "    (\"North\", \"Apple\", 100),\n",
        "    (\"North\", \"Banana\", 150),\n",
        "    (\"North\", \"Apple\", 50),  # Another sale for North/Apple\n",
        "    (\"South\", \"Apple\", 200),\n",
        "    (\"South\", \"Banana\", 250),\n",
        "    (\"South\", \"Orange\", 75),\n",
        "    (\"East\", \"Apple\", 120),\n",
        "    (\"East\", \"Banana\", 80)\n",
        "]\n",
        "\n",
        "columns = [\"Region\", \"Product\", \"Sales\"]\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "dataframe.createOrReplaceTempView('dataframe_view')\n",
        "sql = '''\n",
        "SELECT\n",
        "    region,\n",
        "    product,\n",
        "    SUM(sales) as salesTotal,\n",
        "    GROUPING(region) as regionGrouping,\n",
        "    GROUPING(product) as productGrouping\n",
        "FROM dataframe_view\n",
        "GROUP BY ROLLUP(region, product)\n",
        "ORDER BY region NULLS LAST, product NULLS LAST\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0So1hFdNMbj",
        "outputId": "cd72d942-a96b-4739-c47b-69410268f3d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+----------+--------------+---------------+\n",
            "|region|product|salesTotal|regionGrouping|productGrouping|\n",
            "+------+-------+----------+--------------+---------------+\n",
            "|East  |Apple  |120       |0             |0              |\n",
            "|East  |Banana |80        |0             |0              |\n",
            "|East  |NULL   |200       |0             |1              |\n",
            "|North |Apple  |150       |0             |0              |\n",
            "|North |Banana |150       |0             |0              |\n",
            "|North |NULL   |300       |0             |1              |\n",
            "|South |Apple  |200       |0             |0              |\n",
            "|South |Banana |250       |0             |0              |\n",
            "|South |Orange |75        |0             |0              |\n",
            "|South |NULL   |525       |0             |1              |\n",
            "|NULL  |NULL   |1025      |1             |1              |\n",
            "+------+-------+----------+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping_id : to indicate the the subtotal row of the grouping heirarchy\n",
        "\n",
        "# this is only valid when you use the rollups\n",
        "# if rollups is there, then there is no meaning of grouping_id\n",
        "# it represents the binary of the group id\n",
        "\n",
        "data = [\n",
        "    (\"North\", \"Apple\", 100),\n",
        "    (\"North\", \"Banana\", 150),\n",
        "    (\"North\", \"Apple\", 50),  # Another sale for North/Apple\n",
        "    (\"South\", \"Apple\", 200),\n",
        "    (\"South\", \"Banana\", 250),\n",
        "    (\"South\", \"Orange\", 75),\n",
        "    (\"East\", \"Apple\", 120),\n",
        "    (\"East\", \"Banana\", 80)\n",
        "]\n",
        "\n",
        "columns = [\"Region\", \"Product\", \"Sales\"]\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "dataframe.createOrReplaceTempView('dataframe_view')\n",
        "sql = '''\n",
        "SELECT\n",
        "    region,\n",
        "    product,\n",
        "    SUM(sales) as salesTotal,\n",
        "    GROUPING(region) as regionGrouping,\n",
        "    GROUPING(product) as productGrouping,\n",
        "    GROUPING_ID() AS groupingId\n",
        "FROM dataframe_view\n",
        "GROUP BY ROLLUP(region, product)\n",
        "ORDER BY region NULLS LAST, product NULLS LAST\n",
        "'''\n",
        "spark.sql(sql).show(truncate = False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfImVNYJQw-e",
        "outputId": "bc6c51a7-8d8d-4df3-8ba6-948d379a93b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+----------+--------------+---------------+----------+\n",
            "|region|product|salesTotal|regionGrouping|productGrouping|groupingId|\n",
            "+------+-------+----------+--------------+---------------+----------+\n",
            "|East  |Apple  |120       |0             |0              |0         |\n",
            "|East  |Banana |80        |0             |0              |0         |\n",
            "|East  |NULL   |200       |0             |1              |1         |\n",
            "|North |Apple  |150       |0             |0              |0         |\n",
            "|North |Banana |150       |0             |0              |0         |\n",
            "|North |NULL   |300       |0             |1              |1         |\n",
            "|South |Apple  |200       |0             |0              |0         |\n",
            "|South |Banana |250       |0             |0              |0         |\n",
            "|South |Orange |75        |0             |0              |0         |\n",
            "|South |NULL   |525       |0             |1              |1         |\n",
            "|NULL  |NULL   |1025      |1             |1              |3         |\n",
            "+------+-------+----------+--------------+---------------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}