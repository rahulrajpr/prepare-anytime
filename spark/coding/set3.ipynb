{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/coding/set3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview-section"
      },
      "source": [
        "# PySpark Interview Preparation - Set 3 (Medium)\n",
        "\n",
        "## Overview & Instructions\n",
        "\n",
        "### How to run this notebook in Google Colab:\n",
        "1. Upload this .ipynb file to Google Colab\n",
        "2. Run the installation cells below\n",
        "3. Execute each problem cell sequentially\n",
        "\n",
        "### Installation Commands:\n",
        "The following cell installs Java and PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "installation-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b1b83d-6614-4697-d335-95d115a750bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 1.8.0_462\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ],
      "source": [
        "# Install Java and PySpark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!pyspark --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sparksession-section"
      },
      "source": [
        "### SparkSession Initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sparksession-cell"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "    .appName(\"PySparkInterviewSet3\")\\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assert-function-section"
      },
      "source": [
        "### DataFrame Assertion Function:\n",
        "\n",
        "This function compares DataFrames ignoring order and with floating-point tolerance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "assert-function-cell"
      },
      "outputs": [],
      "source": [
        "def assert_dataframe_equal(df_actual, df_expected, epsilon=1e-6, check_schema_strict=False):\n",
        "    \"\"\"Compare two DataFrames using PySpark operations\"\"\"\n",
        "\n",
        "    if check_schema_strict:\n",
        "        # Check schema exactly\n",
        "        if df_actual.schema != df_expected.schema:\n",
        "            print(\"Schema mismatch!\")\n",
        "            print(\"Actual schema:\", df_actual.schema)\n",
        "            print(\"Expected schema:\", df_expected.schema)\n",
        "            raise AssertionError(\"Schema mismatch\")\n",
        "    else:\n",
        "        # Check column names and basic types\n",
        "        actual_fields = df_actual.schema\n",
        "        expected_fields = df_expected.schema\n",
        "\n",
        "        if len(actual_fields) != len(expected_fields):\n",
        "            print(\"Column count mismatch!\")\n",
        "            raise AssertionError(\"Column count mismatch\")\n",
        "\n",
        "        for i, (actual_field, expected_field) in enumerate(zip(actual_fields, expected_fields)):\n",
        "            if actual_field.name != expected_field.name:\n",
        "                print(f\"Column name mismatch at position {i}: {actual_field.name} vs {expected_field.name}\")\n",
        "                raise AssertionError(\"Column name mismatch\")\n",
        "\n",
        "    # Rest of your comparison logic remains the same\n",
        "    if df_actual.count() != df_expected.count():\n",
        "        print(f\"Row count mismatch! Actual: {df_actual.count()}, Expected: {df_expected.count()}\")\n",
        "        raise AssertionError(\"Row count mismatch\")\n",
        "\n",
        "    diff_actual = df_actual.exceptAll(df_expected)\n",
        "    diff_expected = df_expected.exceptAll(df_actual)\n",
        "\n",
        "    if diff_actual.count() > 0 or diff_expected.count() > 0:\n",
        "        print(\"Data mismatch!\")\n",
        "        print(\"Rows in actual but not in expected:\")\n",
        "        diff_actual.show()\n",
        "        print(\"Rows in expected but not in actual:\")\n",
        "        diff_expected.show()\n",
        "        raise AssertionError(\"Data content mismatch\")\n",
        "\n",
        "    print(\"‚úì DataFrames are equal!\\n\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toc-section"
      },
      "source": [
        "## Table of Contents - Set 3 (Medium)\n",
        "\n",
        "**Difficulty Distribution:** 30 Medium Problems\n",
        "\n",
        "**Topics Covered:**\n",
        "- Complex Joins & Relationship Analysis (7 problems)\n",
        "- Advanced Window Functions & Analytics (7 problems)\n",
        "- Multi-level Aggregations & Rollups (6 problems)\n",
        "- Complex UDFs & Data Transformations (5 problems)\n",
        "- Performance Optimization & Partitioning (5 problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-1"
      },
      "source": [
        "## Problem 1: Customer Churn Prediction Features\n",
        "\n",
        "**Requirement:** Analytics team needs features for customer churn prediction model.\n",
        "\n",
        "**Scenario:** Calculate customer engagement metrics: purchase frequency, recency, and monetary value.\n",
        "\n",
        "* Frequency = total number of purchases\n",
        "\n",
        "* Recency = days between customer's last purchase and the dataset's most recent order date.\n",
        "\n",
        "* Monetary = total amount spent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871a1234-9200-4cac-cbdf-f11af01f5763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------+\n",
            "|customer_id|order_date|amount|\n",
            "+-----------+----------+------+\n",
            "|       C001|2023-01-15| 100.0|\n",
            "|       C001|2023-02-10| 150.0|\n",
            "|       C001|2023-03-05| 200.0|\n",
            "|       C002|2023-01-20| 300.0|\n",
            "|       C002|2023-03-15| 250.0|\n",
            "|       C003|2023-02-01| 500.0|\n",
            "|       C004|2023-01-05| 150.0|\n",
            "|       C004|2023-01-25| 175.0|\n",
            "|       C004|2023-02-20| 200.0|\n",
            "|       C004|2023-03-10| 225.0|\n",
            "+-----------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_engagement_data = [\n",
        "    (\"C001\", \"2023-01-15\", 100.0),\n",
        "    (\"C001\", \"2023-02-10\", 150.0),\n",
        "    (\"C001\", \"2023-03-05\", 200.0),\n",
        "    (\"C002\", \"2023-01-20\", 300.0),\n",
        "    (\"C002\", \"2023-03-15\", 250.0),\n",
        "    (\"C003\", \"2023-02-01\", 500.0),\n",
        "    (\"C004\", \"2023-01-05\", 150.0),\n",
        "    (\"C004\", \"2023-01-25\", 175.0),\n",
        "    (\"C004\", \"2023-02-20\", 200.0),\n",
        "    (\"C004\", \"2023-03-10\", 225.0)\n",
        "]\n",
        "\n",
        "customer_engagement_df = spark.createDataFrame(customer_engagement_data, [\"customer_id\", \"order_date\", \"amount\"])\n",
        "customer_engagement_df = customer_engagement_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
        "customer_engagement_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab391c2-bc24-4b57-a400-276e26ff6b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+--------+---------------+------------+\n",
            "|customer_id|frequency|monetary|avg_order_value|recency_days|\n",
            "+-----------+---------+--------+---------------+------------+\n",
            "|       C004|        4|   750.0|          187.5|           5|\n",
            "|       C001|        3|   450.0|          150.0|          10|\n",
            "|       C002|        2|   550.0|          275.0|           0|\n",
            "|       C003|        1|   500.0|          500.0|          42|\n",
            "+-----------+---------+--------+---------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Corrected Expected Output based on your logic\n",
        "# Max date in dataset: 2023-03-15\n",
        "# Recency = days between max date and customer's last purchase\n",
        "\n",
        "expected_data = [\n",
        "    (\"C004\", 4, 750.0, 187.5, 5),   # last purchase: 2023-03-10 ‚Üí 5 days\n",
        "    (\"C001\", 3, 450.0, 150.0, 10),  # last purchase: 2023-03-05 ‚Üí 10 days\n",
        "    (\"C002\", 2, 550.0, 275.0, 0),   # last purchase: 2023-03-15 ‚Üí 0 days\n",
        "    (\"C003\", 1, 500.0, 500.0, 42)   # last purchase: 2023-02-01 ‚Üí 42 days\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"frequency\", \"monetary\", \"avg_order_value\", \"recency_days\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd91f4a-0971-459b-a777-2ebc965bfb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+--------+---------------+------------+\n",
            "|customer_id|frequency|monetary|avg_order_value|recency_days|\n",
            "+-----------+---------+--------+---------------+------------+\n",
            "|       C001|        3|   450.0|          150.0|          10|\n",
            "|       C002|        2|   550.0|          275.0|           0|\n",
            "|       C003|        1|   500.0|          500.0|          42|\n",
            "|       C004|        4|   750.0|          187.5|           5|\n",
            "+-----------+---------+--------+---------------+------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql import types as tp\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "win = Window.partitionBy()\n",
        "\n",
        "result_df = \\\n",
        "      customer_engagement_df\\\n",
        "        .withColumn('maxdate', fn.max('order_date').over(win))\\\n",
        "        .groupBy('customer_id')\\\n",
        "        .agg(fn.expr(''' count(customer_id) as frequency '''),\n",
        "            fn.expr(''' sum(amount) as monetary '''),\n",
        "            fn.expr(''' avg(amount) as avg_order_value '''),\n",
        "            fn.expr(''' date_diff(max(maxdate),max(order_date)) as recency_days '''),)\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-1"
      },
      "source": [
        "**Instructor Notes:** RFM analysis implementation. Tests date calculations and multi-metric aggregation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-2"
      },
      "source": [
        "## Problem 2: Inventory Stock Analysis\n",
        "\n",
        "**Requirement:** Supply chain needs current stock levels with lead time calculations.\n",
        "\n",
        "**Scenario:** Calculate current inventory levels considering incoming and outgoing shipments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3a0736-433f-41db-9621-a4265201b142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inventory:\n",
            "+----------+------------+-------------+\n",
            "|product_id|product_name|current_stock|\n",
            "+----------+------------+-------------+\n",
            "|      P001|      Laptop|           50|\n",
            "|      P002|       Mouse|          100|\n",
            "|      P003|    Keyboard|           75|\n",
            "+----------+------------+-------------+\n",
            "\n",
            "Incoming Shipments:\n",
            "+-----------+----------+------------+--------+\n",
            "|shipment_id|product_id|arrival_date|quantity|\n",
            "+-----------+----------+------------+--------+\n",
            "|       S001|      P001|  2023-03-01|      20|\n",
            "|       S002|      P002|  2023-03-02|      50|\n",
            "|       S003|      P001|  2023-03-03|      10|\n",
            "+-----------+----------+------------+--------+\n",
            "\n",
            "Outgoing Orders:\n",
            "+--------+----------+----------+--------+\n",
            "|order_id|product_id|order_date|quantity|\n",
            "+--------+----------+----------+--------+\n",
            "|    O001|      P001|2023-03-01|      15|\n",
            "|    O002|      P002|2023-03-02|      30|\n",
            "|    O003|      P001|2023-03-03|      25|\n",
            "|    O004|      P003|2023-03-03|      20|\n",
            "+--------+----------+----------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "inventory_data = [\n",
        "    (\"P001\", \"Laptop\", 50),\n",
        "    (\"P002\", \"Mouse\", 100),\n",
        "    (\"P003\", \"Keyboard\", 75)\n",
        "]\n",
        "\n",
        "incoming_shipments_data = [\n",
        "    (\"S001\", \"P001\", \"2023-03-01\", 20),\n",
        "    (\"S002\", \"P002\", \"2023-03-02\", 50),\n",
        "    (\"S003\", \"P001\", \"2023-03-03\", 10)\n",
        "]\n",
        "\n",
        "outgoing_orders_data = [\n",
        "    (\"O001\", \"P001\", \"2023-03-01\", 15),\n",
        "    (\"O002\", \"P002\", \"2023-03-02\", 30),\n",
        "    (\"O003\", \"P001\", \"2023-03-03\", 25),\n",
        "    (\"O004\", \"P003\", \"2023-03-03\", 20)\n",
        "]\n",
        "\n",
        "inventory_df = spark.createDataFrame(inventory_data, [\"product_id\", \"product_name\", \"current_stock\"])\n",
        "incoming_df = spark.createDataFrame(incoming_shipments_data, [\"shipment_id\", \"product_id\", \"arrival_date\", \"quantity\"])\n",
        "outgoing_df = spark.createDataFrame(outgoing_orders_data, [\"order_id\", \"product_id\", \"order_date\", \"quantity\"])\n",
        "\n",
        "print(\"Inventory:\")\n",
        "inventory_df.show()\n",
        "print(\"Incoming Shipments:\")\n",
        "incoming_df.show()\n",
        "print(\"Outgoing Orders:\")\n",
        "outgoing_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5f22b3-12a1-42eb-f37b-ab79d9c79d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+------------+------------+---------------+\n",
            "|product_id|product_name|current_stock|incoming_qty|outgoing_qty|projected_stock|\n",
            "+----------+------------+-------------+------------+------------+---------------+\n",
            "|      P001|      Laptop|           50|          30|          40|             40|\n",
            "|      P002|       Mouse|          100|          50|          30|            120|\n",
            "|      P003|    Keyboard|           75|           0|          20|             55|\n",
            "+----------+------------+-------------+------------+------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"P001\", \"Laptop\", 50, 30, 40, 40),\n",
        "    (\"P002\", \"Mouse\", 100, 50, 30, 120),\n",
        "    (\"P003\", \"Keyboard\", 75, 0, 20, 55)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"current_stock\", \"incoming_qty\", \"outgoing_qty\", \"projected_stock\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5363557b-00f7-4e33-9b34-f3558310b829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+\n",
            "|product_id|quantity|\n",
            "+----------+--------+\n",
            "|      P001|      30|\n",
            "|      P002|      50|\n",
            "+----------+--------+\n",
            "\n",
            "+----------+--------+\n",
            "|product_id|quantity|\n",
            "+----------+--------+\n",
            "|      P002|      30|\n",
            "|      P001|      40|\n",
            "|      P003|      20|\n",
            "+----------+--------+\n",
            "\n",
            "+----------+------------+-------------+------------+------------+---------------+\n",
            "|product_id|product_name|current_stock|incoming_qty|outgoing_qty|projected_stock|\n",
            "+----------+------------+-------------+------------+------------+---------------+\n",
            "|      P001|      Laptop|           50|          30|          40|             40|\n",
            "|      P003|    Keyboard|           75|           0|          20|             55|\n",
            "|      P002|       Mouse|          100|          50|          30|            120|\n",
            "+----------+------------+-------------+------------+------------+---------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "\n",
        "incom = \\\n",
        "    incoming_df\\\n",
        "        .groupBy('product_id')\\\n",
        "        .agg(fn.expr('sum(quantity) as quantity'))\n",
        "\n",
        "incom.show()\n",
        "\n",
        "outgo = \\\n",
        "    outgoing_df\\\n",
        "        .groupBy('product_id')\\\n",
        "        .agg(fn.expr('sum(quantity) as quantity'))\n",
        "\n",
        "outgo.show()\n",
        "\n",
        "join_on1 = fn.expr(''' inv.product_id = incom.product_id''')\n",
        "join_on2 = fn.expr(''' inv.product_id = outgo.product_id''')\n",
        "\n",
        "result_df = \\\n",
        "    inventory_df.alias('inv')\\\n",
        "      .join(incom.alias('incom'),join_on1,'left')\\\n",
        "      .join(outgo.alias('outgo'),join_on2,'left')\\\n",
        "      .select(fn.col('inv.product_id'),\n",
        "              fn.col('inv.product_name'),\n",
        "              fn.col('inv.current_stock'),\n",
        "              fn.expr('nvl(incom.quantity,0)').alias('incoming_qty'),\n",
        "              fn.expr('nvl(outgo.quantity,0)').alias('outgoing_qty'))\\\n",
        "      .withColumn('projected_stock', fn.expr('current_stock + incoming_qty - outgoing_qty'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-2"
      },
      "source": [
        "**Instructor Notes:** Multi-table aggregation with conditional sums. Tests complex join scenarios with multiple data sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-3"
      },
      "source": [
        "## Problem 3: Employee Attendance Pattern Analysis\n",
        "\n",
        "**Requirement:** HR needs to analyze employee attendance patterns for workforce planning.\n",
        "\n",
        "**Scenario:** Calculate consecutive work days and identify attendance patterns using window functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62799748-53b3-436a-a1df-95f483f52263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+\n",
            "|employee_id|      date| status|\n",
            "+-----------+----------+-------+\n",
            "|       E001|2023-03-01|Present|\n",
            "|       E001|2023-03-02|Present|\n",
            "|       E001|2023-03-03| Absent|\n",
            "|       E001|2023-03-04|Present|\n",
            "|       E001|2023-03-05|Present|\n",
            "|       E001|2023-03-06|Present|\n",
            "|       E002|2023-03-01|Present|\n",
            "|       E002|2023-03-02|Present|\n",
            "|       E002|2023-03-03|Present|\n",
            "|       E002|2023-03-04| Absent|\n",
            "|       E002|2023-03-05|Present|\n",
            "+-----------+----------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "attendance_data = [\n",
        "    (\"E001\", \"2023-03-01\", \"Present\"),\n",
        "    (\"E001\", \"2023-03-02\", \"Present\"),\n",
        "    (\"E001\", \"2023-03-03\", \"Absent\"),\n",
        "    (\"E001\", \"2023-03-04\", \"Present\"),\n",
        "    (\"E001\", \"2023-03-05\", \"Present\"),\n",
        "    (\"E001\", \"2023-03-06\", \"Present\"),\n",
        "    (\"E002\", \"2023-03-01\", \"Present\"),\n",
        "    (\"E002\", \"2023-03-02\", \"Present\"),\n",
        "    (\"E002\", \"2023-03-03\", \"Present\"),\n",
        "    (\"E002\", \"2023-03-04\", \"Absent\"),\n",
        "    (\"E002\", \"2023-03-05\", \"Present\")\n",
        "]\n",
        "\n",
        "attendance_df = spark.createDataFrame(attendance_data, [\"employee_id\", \"date\", \"status\"])\n",
        "attendance_df = attendance_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "attendance_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f792257-ca54-4fea-fd78-d19d82a1de9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+----------------+\n",
            "|employee_id|      date| status|consecutive_days|\n",
            "+-----------+----------+-------+----------------+\n",
            "|       E001|2023-03-01|Present|               1|\n",
            "|       E001|2023-03-02|Present|               2|\n",
            "|       E001|2023-03-03| Absent|               0|\n",
            "|       E001|2023-03-04|Present|               1|\n",
            "|       E001|2023-03-05|Present|               2|\n",
            "|       E001|2023-03-06|Present|               3|\n",
            "|       E002|2023-03-01|Present|               1|\n",
            "|       E002|2023-03-02|Present|               2|\n",
            "|       E002|2023-03-03|Present|               3|\n",
            "|       E002|2023-03-04| Absent|               0|\n",
            "|       E002|2023-03-05|Present|               1|\n",
            "+-----------+----------+-------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"E001\", \"2023-03-01\", \"Present\", 1),\n",
        "    (\"E001\", \"2023-03-02\", \"Present\", 2),\n",
        "    (\"E001\", \"2023-03-03\", \"Absent\", 0),\n",
        "    (\"E001\", \"2023-03-04\", \"Present\", 1),\n",
        "    (\"E001\", \"2023-03-05\", \"Present\", 2),\n",
        "    (\"E001\", \"2023-03-06\", \"Present\", 3),\n",
        "    (\"E002\", \"2023-03-01\", \"Present\", 1),\n",
        "    (\"E002\", \"2023-03-02\", \"Present\", 2),\n",
        "    (\"E002\", \"2023-03-03\", \"Present\", 3),\n",
        "    (\"E002\", \"2023-03-04\", \"Absent\", 0),\n",
        "    (\"E002\", \"2023-03-05\", \"Present\", 1)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"employee_id\", \"date\", \"status\", \"consecutive_days\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3561d8ab-cd16-4fba-954d-37a3cd499d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------+---------+\n",
            "|employee_id|      date| status|dateOrder|\n",
            "+-----------+----------+-------+---------+\n",
            "|       E001|2023-03-01|Present|        1|\n",
            "|       E001|2023-03-02|Present|        2|\n",
            "|       E001|2023-03-03| Absent|        3|\n",
            "|       E001|2023-03-04|Present|        4|\n",
            "|       E001|2023-03-05|Present|        5|\n",
            "|       E001|2023-03-06|Present|        6|\n",
            "|       E002|2023-03-01|Present|        1|\n",
            "|       E002|2023-03-02|Present|        2|\n",
            "|       E002|2023-03-03|Present|        3|\n",
            "|       E002|2023-03-04| Absent|        4|\n",
            "|       E002|2023-03-05|Present|        5|\n",
            "+-----------+----------+-------+---------+\n",
            "\n",
            "+-----------+----------+-------+---------+\n",
            "|employee_id|      date| status|dateOrder|\n",
            "+-----------+----------+-------+---------+\n",
            "|       E001|2023-03-01|Present|        1|\n",
            "|       E001|2023-03-02|Present|        2|\n",
            "|       E001|2023-03-04|Present|        3|\n",
            "|       E001|2023-03-05|Present|        4|\n",
            "|       E001|2023-03-06|Present|        5|\n",
            "|       E002|2023-03-01|Present|        1|\n",
            "|       E002|2023-03-02|Present|        2|\n",
            "|       E002|2023-03-03|Present|        3|\n",
            "|       E002|2023-03-05|Present|        4|\n",
            "+-----------+----------+-------+---------+\n",
            "\n",
            "+-----------+----------+-------+----------------+\n",
            "|employee_id|      date| status|consecutive_days|\n",
            "+-----------+----------+-------+----------------+\n",
            "|       E001|2023-03-01|Present|               1|\n",
            "|       E001|2023-03-02|Present|               2|\n",
            "|       E001|2023-03-03| Absent|               0|\n",
            "|       E001|2023-03-04|Present|               1|\n",
            "|       E001|2023-03-05|Present|               2|\n",
            "|       E001|2023-03-06|Present|               3|\n",
            "|       E002|2023-03-01|Present|               1|\n",
            "|       E002|2023-03-02|Present|               2|\n",
            "|       E002|2023-03-03|Present|               3|\n",
            "|       E002|2023-03-04| Absent|               0|\n",
            "|       E002|2023-03-05|Present|               1|\n",
            "+-----------+----------+-------+----------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "winDate = Window.partitionBy('employee_id').orderBy(fn.col('date').asc_nulls_last())\n",
        "\n",
        "normal_order = attendance_df\\\n",
        "                  .withColumn('dateOrder', fn.row_number().over(winDate))\n",
        "\n",
        "normal_order.show()\n",
        "\n",
        "present_order = attendance_df\\\n",
        "                  .filter(''' status = 'Present' ''')\\\n",
        "                  .withColumn('dateOrder', fn.row_number().over(winDate))\n",
        "\n",
        "present_order.show()\n",
        "\n",
        "result_df = normal_order.alias('normal')\\\n",
        "                .join(present_order.alias('present'),\n",
        "                      fn.expr(''' normal.employee_id = present.employee_id\n",
        "                                  and normal.date = present.date  '''),\n",
        "                      'left')\\\n",
        "                .withColumn('gap',fn.expr(''' normal.dateOrder - present.dateOrder '''))\\\n",
        "                .withColumn('gapOrder', fn.expr(' row_number() over(partition by normal.employee_id, gap order by normal.date asc )'))\\\n",
        "                .withColumn('consecutive_days', fn.expr(''' case when normal.status = 'Absent'  then 0 else gapOrder end '''))\\\n",
        "                .select('normal.employee_id', 'normal.date', 'normal.status','consecutive_days')\\\n",
        "                .orderBy('normal.employee_id', 'normal.date')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-3"
      },
      "source": [
        "**Instructor Notes:** Complex window functions with conditional reset. Tests pattern detection and state management in window operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-4"
      },
      "source": [
        "## Problem 4: Financial Portfolio Analysis\n",
        "\n",
        "**Requirement:** Investment team needs portfolio performance analysis with risk metrics.\n",
        "\n",
        "**Scenario:** Calculate portfolio weights, returns, and risk metrics across different assets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198a8d1f-1262-4c5e-c4da-cb3c594e39d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------------+-------------+\n",
            "|symbol|investment|purchase_price|current_price|\n",
            "+------+----------+--------------+-------------+\n",
            "|  AAPL|   10000.0|         150.0|        155.0|\n",
            "| GOOGL|   15000.0|        2800.0|       2850.0|\n",
            "|  MSFT|    8000.0|         300.0|        295.0|\n",
            "|  TSLA|   12000.0|         200.0|        210.0|\n",
            "+------+----------+--------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "portfolio_data = [\n",
        "    (\"AAPL\", 10000.0, 150.0, 155.0),\n",
        "    (\"GOOGL\", 15000.0, 2800.0, 2850.0),\n",
        "    (\"MSFT\", 8000.0, 300.0, 295.0),\n",
        "    (\"TSLA\", 12000.0, 200.0, 210.0)\n",
        "]\n",
        "\n",
        "portfolio_df = spark.createDataFrame(portfolio_data, [\"symbol\", \"investment\", \"purchase_price\", \"current_price\"])\n",
        "portfolio_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d57fc8-9a0b-49d7-b209-c7b0d4a55d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------------+-------------+------+-------------+----------+----------+\n",
            "|symbol|investment|purchase_price|current_price|shares|current_value|return_pct|return_amt|\n",
            "+------+----------+--------------+-------------+------+-------------+----------+----------+\n",
            "|  AAPL|   10000.0|         150.0|        155.0| 66.67|     10333.85|      3.34|    333.85|\n",
            "| GOOGL|   15000.0|        2800.0|       2850.0|  5.36|      15276.0|      1.84|     276.0|\n",
            "|  MSFT|    8000.0|         300.0|        295.0| 26.67|      7867.65|     -1.65|   -132.35|\n",
            "|  TSLA|   12000.0|         200.0|        210.0|  60.0|      12600.0|       5.0|     600.0|\n",
            "+------+----------+--------------+-------------+------+-------------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "\n",
        "expected_data = [\n",
        "    (\"AAPL\", 10000.0, 150.0, 155.0, 66.67, 10333.85, 3.34, 333.85),\n",
        "    (\"GOOGL\", 15000.0, 2800.0, 2850.0, 5.36, 15276.0, 1.84, 276.0),\n",
        "    (\"MSFT\", 8000.0, 300.0, 295.0, 26.67, 7867.65, -1.65, -132.35),\n",
        "    (\"TSLA\", 12000.0, 200.0, 210.0, 60.0, 12600.0, 5.0, 600.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"symbol\", \"investment\", \"purchase_price\", \"current_price\", \"shares\", \"current_value\", \"return_pct\", \"return_amt\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131adc37-9ff4-4530-e835-7c9facbb29b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------------+-------------+------+-------------+----------+----------+\n",
            "|symbol|investment|purchase_price|current_price|shares|current_value|return_pct|return_amt|\n",
            "+------+----------+--------------+-------------+------+-------------+----------+----------+\n",
            "|  AAPL|   10000.0|         150.0|        155.0| 66.67|     10333.85|      3.34|    333.85|\n",
            "| GOOGL|   15000.0|        2800.0|       2850.0|  5.36|      15276.0|      1.84|     276.0|\n",
            "|  MSFT|    8000.0|         300.0|        295.0| 26.67|      7867.65|     -1.65|   -132.35|\n",
            "|  TSLA|   12000.0|         200.0|        210.0|  60.0|      12600.0|       5.0|     600.0|\n",
            "+------+----------+--------------+-------------+------+-------------+----------+----------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    portfolio_df\\\n",
        "      .withColumn('shares', fn.expr('round(investment / purchase_price,2)'))\\\n",
        "      .withColumn('current_value', fn.expr('round(current_price * shares,2)'))\\\n",
        "      .withColumn('return_pct', fn.expr('round(100 * (current_value - investment) / investment, 2)'))\\\n",
        "      .withColumn('return_amt', fn.expr('round(current_value - investment,2)'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-4"
      },
      "source": [
        "**Instructor Notes:** Financial calculations with multiple derived metrics. Tests mathematical operations and percentage calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-5"
      },
      "source": [
        "## Problem 5: Healthcare Patient Journey Analysis\n",
        "\n",
        "**Requirement:** Medical analytics needs patient treatment pathway analysis.\n",
        "\n",
        "**Scenario:** Analyze patient journeys through different medical departments and treatments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a25216-7382-409e-9a65-ffa063e2cd7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-------------------+\n",
            "|patient_id|department|          timestamp|\n",
            "+----------+----------+-------------------+\n",
            "|      P001| Emergency|2023-01-15 10:00:00|\n",
            "|      P001| Radiology|2023-01-15 11:30:00|\n",
            "|      P001|   Surgery|2023-01-15 14:00:00|\n",
            "|      P001|       ICU|2023-01-15 18:00:00|\n",
            "|      P002|       OPD|2023-01-16 09:00:00|\n",
            "|      P002|       Lab|2023-01-16 10:00:00|\n",
            "|      P002|  Pharmacy|2023-01-16 11:00:00|\n",
            "|      P003| Emergency|2023-01-17 15:00:00|\n",
            "|      P003| Radiology|2023-01-17 16:00:00|\n",
            "+----------+----------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "patient_journey_data = [\n",
        "    (\"P001\", \"Emergency\", \"2023-01-15 10:00:00\"),\n",
        "    (\"P001\", \"Radiology\", \"2023-01-15 11:30:00\"),\n",
        "    (\"P001\", \"Surgery\", \"2023-01-15 14:00:00\"),\n",
        "    (\"P001\", \"ICU\", \"2023-01-15 18:00:00\"),\n",
        "    (\"P002\", \"OPD\", \"2023-01-16 09:00:00\"),\n",
        "    (\"P002\", \"Lab\", \"2023-01-16 10:00:00\"),\n",
        "    (\"P002\", \"Pharmacy\", \"2023-01-16 11:00:00\"),\n",
        "    (\"P003\", \"Emergency\", \"2023-01-17 15:00:00\"),\n",
        "    (\"P003\", \"Radiology\", \"2023-01-17 16:00:00\")\n",
        "]\n",
        "\n",
        "patient_journey_df = spark.createDataFrame(patient_journey_data, [\"patient_id\", \"department\", \"timestamp\"])\n",
        "patient_journey_df = patient_journey_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "patient_journey_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33031f25-bc3c-430d-9189-81528bbc1930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+------------+\n",
            "|patient_id|from_dept|  to_dept|time_minutes|\n",
            "+----------+---------+---------+------------+\n",
            "|      P001|Emergency|Radiology|          90|\n",
            "|      P001|Radiology|  Surgery|         150|\n",
            "|      P001|  Surgery|      ICU|         240|\n",
            "|      P002|      OPD|      Lab|          60|\n",
            "|      P002|      Lab| Pharmacy|          60|\n",
            "|      P003|Emergency|Radiology|          60|\n",
            "+----------+---------+---------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"P001\", \"Emergency\", \"Radiology\", 90),\n",
        "    (\"P001\", \"Radiology\", \"Surgery\", 150),\n",
        "    (\"P001\", \"Surgery\", \"ICU\", 240),\n",
        "    (\"P002\", \"OPD\", \"Lab\", 60),\n",
        "    (\"P002\", \"Lab\", \"Pharmacy\", 60),\n",
        "    (\"P003\", \"Emergency\", \"Radiology\", 60)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"patient_id\", \"from_dept\", \"to_dept\", \"time_minutes\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c12718a-e2fb-4ab1-9a7d-9e0957fbaf56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+------------+\n",
            "|patient_id|from_dept|  to_dept|time_minutes|\n",
            "+----------+---------+---------+------------+\n",
            "|      P001|Emergency|Radiology|          90|\n",
            "|      P001|Radiology|  Surgery|         150|\n",
            "|      P001|  Surgery|      ICU|         240|\n",
            "|      P002|      OPD|      Lab|          60|\n",
            "|      P002|      Lab| Pharmacy|          60|\n",
            "|      P003|Emergency|Radiology|          60|\n",
            "+----------+---------+---------+------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      patient_journey_df\\\n",
        "        .withColumn('next_dept', fn.expr(''' lead(department,1) over(partition by patient_id order by timestamp) '''))\\\n",
        "        .withColumn('next_dept_timestamp', fn.expr(''' lead(timestamp,1) over(partition by patient_id order by timestamp) '''))\\\n",
        "        .filter('next_dept IS NOT NULL')\\\n",
        "        .withColumn('time_minutes', fn.expr(''' cast((unix_timestamp(next_dept_timestamp) - unix_timestamp(timestamp))/60 as int) '''))\\\n",
        "        .withColumnsRenamed( {'department' : 'from_dept',\n",
        "                              'next_dept':'to_dept'})\\\n",
        "        .select('patient_id','from_dept','to_dept','time_minutes')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-5"
      },
      "source": [
        "**Instructor Notes:** Time-based analysis with lead/lag operations. Tests patient journey analysis and time interval calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-6"
      },
      "source": [
        "## Problem 6: E-commerce Customer Segmentation\n",
        "\n",
        "**Requirement:** Marketing needs advanced customer segmentation for targeted campaigns.\n",
        "\n",
        "**Scenario:** Segment customers based on RFM (Recency, Frequency, Monetary) scores and clustering logic.\n",
        "\n",
        "# üéØ RFM Segmentation Rules\n",
        "\n",
        "## üèÜ Platinum Segment\n",
        "- **Last purchased within 30 days** AND\n",
        "- **Made 20 or more purchases** AND  \n",
        "- **Spent $4,000 or more**\n",
        "\n",
        "## ü•á Gold Segment\n",
        "- **Last purchased within 60 days** AND\n",
        "- **Made 10 or more purchases** AND\n",
        "- **Spent $2,000 or more**\n",
        "\n",
        "## ü•à Silver Segment  \n",
        "- **Last purchased within 90 days** AND\n",
        "- **Made 5 or more purchases** AND\n",
        "- **Spent $1,000 or more**\n",
        "\n",
        "## ü•â Bronze Segment\n",
        "- **All customers who don't meet the above criteria**\n",
        "\n",
        "---\n",
        "\n",
        "**Priority Order:** Platinum ‚Üí Gold ‚Üí Silver ‚Üí Bronze  \n",
        "*Note: Customers are assigned to the highest segment they qualify for*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df1e430-5afc-4131-ce3a-2076570a6f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+---------+--------+\n",
            "|customer_id|recency_days|frequency|monetary|\n",
            "+-----------+------------+---------+--------+\n",
            "|       C001|          45|       15|  2500.0|\n",
            "|       C002|         120|        3|   800.0|\n",
            "|       C003|          10|       25|  5000.0|\n",
            "|       C004|          80|        8|  1500.0|\n",
            "|       C005|         200|        2|   400.0|\n",
            "|       C006|           5|       30|  7500.0|\n",
            "|       C007|          60|       12|  3000.0|\n",
            "+-----------+------------+---------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_segmentation_data = [\n",
        "    (\"C001\", 45, 15, 2500.0),\n",
        "    (\"C002\", 120, 3, 800.0),\n",
        "    (\"C003\", 10, 25, 5000.0),\n",
        "    (\"C004\", 80, 8, 1500.0),\n",
        "    (\"C005\", 200, 2, 400.0),\n",
        "    (\"C006\", 5, 30, 7500.0),\n",
        "    (\"C007\", 60, 12, 3000.0)\n",
        "]\n",
        "\n",
        "customer_segmentation_df = spark.createDataFrame(customer_segmentation_data, [\"customer_id\", \"recency_days\", \"frequency\", \"monetary\"])\n",
        "customer_segmentation_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521fc49d-1476-4254-d2b5-3796ee48eec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+---------+--------+--------+\n",
            "|customer_id|recency_days|frequency|monetary| segment|\n",
            "+-----------+------------+---------+--------+--------+\n",
            "|       C001|          45|       15|  2500.0|    Gold|\n",
            "|       C002|         120|        3|   800.0|  Bronze|\n",
            "|       C003|          10|       25|  5000.0|Platinum|\n",
            "|       C004|          80|        8|  1500.0|  Silver|\n",
            "|       C005|         200|        2|   400.0|  Bronze|\n",
            "|       C006|           5|       30|  7500.0|Platinum|\n",
            "|       C007|          60|       12|  3000.0|    Gold|\n",
            "+-----------+------------+---------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", 45, 15, 2500.0, \"Gold\"),\n",
        "    (\"C002\", 120, 3, 800.0, \"Bronze\"),\n",
        "    (\"C003\", 10, 25, 5000.0, \"Platinum\"),\n",
        "    (\"C004\", 80, 8, 1500.0, \"Silver\"),\n",
        "    (\"C005\", 200, 2, 400.0, \"Bronze\"),\n",
        "    (\"C006\", 5, 30, 7500.0, \"Platinum\"),\n",
        "    (\"C007\", 60, 12, 3000.0, \"Gold\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"recency_days\", \"frequency\", \"monetary\", \"segment\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0259944c-7036-4d6a-db41-8d501788cecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+---------+--------+--------+\n",
            "|customer_id|recency_days|frequency|monetary| segment|\n",
            "+-----------+------------+---------+--------+--------+\n",
            "|       C001|          45|       15|  2500.0|    Gold|\n",
            "|       C002|         120|        3|   800.0|  Bronze|\n",
            "|       C003|          10|       25|  5000.0|Platinum|\n",
            "|       C004|          80|        8|  1500.0|  Silver|\n",
            "|       C005|         200|        2|   400.0|  Bronze|\n",
            "|       C006|           5|       30|  7500.0|Platinum|\n",
            "|       C007|          60|       12|  3000.0|    Gold|\n",
            "+-----------+------------+---------+--------+--------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "case_statement = fn.expr('''\n",
        "      case when recency_days <= 30 and frequency >= 20 and monetary >= 4000 then 'Platinum'\n",
        "          when recency_days <= 60 and frequency >= 10 and monetary >= 2000 then 'Gold'\n",
        "          when recency_days <= 90 and frequency >= 5 and monetary >= 1000 then 'Silver'\n",
        "          else 'Bronze' end\n",
        "          ''')\n",
        "\n",
        "result_df = \\\n",
        "        customer_segmentation_df\\\n",
        "          .withColumn('segment', case_statement)\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-6"
      },
      "source": [
        "**Instructor Notes:** Customer segmentation with business rules. Tests conditional logic and multi-criteria classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-7"
      },
      "source": [
        "## Problem 7: Supply Chain Route Optimization\n",
        "\n",
        "**Requirement:** Logistics needs optimal delivery route analysis with cost calculations.\n",
        "\n",
        "**Scenario:** Calculate delivery routes, distances, and costs considering multiple stops and constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1dd0779-4c3d-4c90-c048-4e2079ccc69f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+-----------+-----------+-----+\n",
            "|route_id|from_location|to_location|distance_km| cost|\n",
            "+--------+-------------+-----------+-----------+-----+\n",
            "|    R001|    Warehouse|    Store_A|       50.0|100.0|\n",
            "|    R001|      Store_A|    Store_B|       30.0| 60.0|\n",
            "|    R001|      Store_B|  Warehouse|       40.0| 80.0|\n",
            "|    R002|    Warehouse|    Store_C|       70.0|140.0|\n",
            "|    R002|      Store_C|    Store_D|       25.0| 50.0|\n",
            "|    R002|      Store_D|  Warehouse|       60.0|120.0|\n",
            "|    R003|    Warehouse|    Store_E|       90.0|180.0|\n",
            "+--------+-------------+-----------+-----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "delivery_routes_data = [\n",
        "    (\"R001\", \"Warehouse\", \"Store_A\", 50.0, 100.0),\n",
        "    (\"R001\", \"Store_A\", \"Store_B\", 30.0, 60.0),\n",
        "    (\"R001\", \"Store_B\", \"Warehouse\", 40.0, 80.0),\n",
        "    (\"R002\", \"Warehouse\", \"Store_C\", 70.0, 140.0),\n",
        "    (\"R002\", \"Store_C\", \"Store_D\", 25.0, 50.0),\n",
        "    (\"R002\", \"Store_D\", \"Warehouse\", 60.0, 120.0),\n",
        "    (\"R003\", \"Warehouse\", \"Store_E\", 90.0, 180.0)\n",
        "]\n",
        "\n",
        "delivery_routes_df = spark.createDataFrame(delivery_routes_data, [\"route_id\", \"from_location\", \"to_location\", \"distance_km\", \"cost\"])\n",
        "delivery_routes_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d1ffa6-e68f-4e39-d8b2-0261610dd2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------+----------+-----+\n",
            "|route_id|total_distance|total_cost|stops|\n",
            "+--------+--------------+----------+-----+\n",
            "|    R001|         120.0|     240.0|    3|\n",
            "|    R002|         155.0|     310.0|    3|\n",
            "|    R003|          90.0|     180.0|    1|\n",
            "+--------+--------------+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"R001\", 120.0, 240.0, 3),\n",
        "    (\"R002\", 155.0, 310.0, 3),\n",
        "    (\"R003\", 90.0, 180.0, 1)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"route_id\", \"total_distance\", \"total_cost\", \"stops\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2304c7-beca-444a-bcac-5f1ec28be0b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------+----------+-----+\n",
            "|route_id|total_distance|total_cost|stops|\n",
            "+--------+--------------+----------+-----+\n",
            "|    R001|         120.0|     240.0|    3|\n",
            "|    R003|          90.0|     180.0|    1|\n",
            "|    R002|         155.0|     310.0|    3|\n",
            "+--------+--------------+----------+-----+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      delivery_routes_df\\\n",
        "          .groupBy('route_id')\\\n",
        "          .agg(fn.sum(fn.col('distance_km')).alias('total_distance'),\n",
        "              fn.sum(fn.col('cost')).alias('total_cost'),\n",
        "              fn.countDistinct(fn.col('to_location')).alias('stops'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-7"
      },
      "source": [
        "**Instructor Notes:** Route optimization with aggregation. Tests group-based calculations and multi-leg journey analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-8"
      },
      "source": [
        "## Problem 8: Media Content Performance Analysis\n",
        "\n",
        "**Requirement:** Media analytics needs content engagement metrics and performance trends.\n",
        "\n",
        "**Scenario:** Calculate content engagement rates, completion rates, and audience retention metrics.\n",
        "\n",
        "* View Rate = (Views √∑ Impressions) √ó 100\n",
        "* Engagement Rate = (Engagements √∑ Impressions) √ó 100\n",
        "* Completion Rate = (Completions √∑ Impressions) √ó 100\n",
        "* Retention Rate = (Completions √∑ Views) √ó 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3acff37d-a2c4-4db9-cc18-450d11e3b680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-----------+-----+-----------+-----------+\n",
            "|content_id|     category|impressions|views|engagements|completions|\n",
            "+----------+-------------+-----------+-----+-----------+-----------+\n",
            "|      V001|     Tutorial|      10000| 8500|       7500|       6000|\n",
            "|      V002|Entertainment|      15000|12000|      11000|       9000|\n",
            "|      V003|         News|       8000| 6000|       5000|       3500|\n",
            "|      V004|  Documentary|       5000| 4500|       4200|       3800|\n",
            "|      V005|       Sports|      20000|18000|      16000|      14000|\n",
            "+----------+-------------+-----------+-----+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "content_performance_data = [\n",
        "    (\"V001\", \"Tutorial\", 10000, 8500, 7500, 6000),\n",
        "    (\"V002\", \"Entertainment\", 15000, 12000, 11000, 9000),\n",
        "    (\"V003\", \"News\", 8000, 6000, 5000, 3500),\n",
        "    (\"V004\", \"Documentary\", 5000, 4500, 4200, 3800),\n",
        "    (\"V005\", \"Sports\", 20000, 18000, 16000, 14000)\n",
        "]\n",
        "\n",
        "content_performance_df = spark.createDataFrame(content_performance_data, [\"content_id\", \"category\", \"impressions\", \"views\", \"engagements\", \"completions\"])\n",
        "content_performance_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5e7fc2-8659-4e45-cd42-e56140ac94af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------+---------------+---------------+--------------+\n",
            "|content_id|     category|view_rate|engagement_rate|completion_rate|retention_rate|\n",
            "+----------+-------------+---------+---------------+---------------+--------------+\n",
            "|      V001|     Tutorial|     85.0|           75.0|           60.0|          70.6|\n",
            "|      V002|Entertainment|     80.0|           73.3|           60.0|          75.0|\n",
            "|      V003|         News|     75.0|           62.5|           43.8|          58.3|\n",
            "|      V004|  Documentary|     90.0|           84.0|           76.0|          84.4|\n",
            "|      V005|       Sports|     90.0|           80.0|           70.0|          77.8|\n",
            "+----------+-------------+---------+---------------+---------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"V001\", \"Tutorial\", 85.0, 75.0, 60.0, 70.6),\n",
        "    (\"V002\", \"Entertainment\", 80.0, 73.3, 60.0, 75.0),\n",
        "    (\"V003\", \"News\", 75.0, 62.5, 43.8, 58.3),\n",
        "    (\"V004\", \"Documentary\", 90.0, 84.0, 76.0, 84.4),\n",
        "    (\"V005\", \"Sports\", 90.0, 80.0, 70.0, 77.8)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"content_id\", \"category\", \"view_rate\", \"engagement_rate\", \"completion_rate\", \"retention_rate\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64aebab9-f35e-4d3d-fdb0-a63f0ed48da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------+---------------+---------------+--------------+\n",
            "|content_id|     category|view_rate|engagement_rate|completion_rate|retention_rate|\n",
            "+----------+-------------+---------+---------------+---------------+--------------+\n",
            "|      V001|     Tutorial|     85.0|           75.0|           60.0|          70.6|\n",
            "|      V002|Entertainment|     80.0|           73.3|           60.0|          75.0|\n",
            "|      V003|         News|     75.0|           62.5|           43.8|          58.3|\n",
            "|      V004|  Documentary|     90.0|           84.0|           76.0|          84.4|\n",
            "|      V005|       Sports|     90.0|           80.0|           70.0|          77.8|\n",
            "+----------+-------------+---------+---------------+---------------+--------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    content_performance_df\\\n",
        "        .withColumn('view_rate', fn.expr(''' round(views*100/nullif(impressions,0),1) '''))\\\n",
        "        .withColumn('engagement_rate', fn.expr(''' round(engagements*100/nullif(impressions,0),1) '''))\\\n",
        "        .withColumn('completion_rate', fn.expr(''' round(completions*100/nullif(impressions,0),1) '''))\\\n",
        "        .withColumn('retention_rate', fn.expr(''' round(completions*100/nullif(views,0),1) '''))\\\n",
        "        .select('content_id','category','view_rate','engagement_rate','completion_rate','retention_rate')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-8"
      },
      "source": [
        "**Instructor Notes:** Media analytics with percentage calculations. Tests ratio computations and performance metric derivations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-9"
      },
      "source": [
        "## Problem 9: Educational Course Progress Tracking\n",
        "\n",
        "**Requirement:** Education platform needs student progress analytics and course completion tracking.\n",
        "\n",
        "**Scenario:** Calculate student progress, completion rates, and identify at-risk students.\n",
        "\n",
        "* Excellent = Completion Rate ‚â• 80% AND Average Score ‚â• 85\n",
        "* At Risk = Completion Rate ‚â• 50% AND Average Score < 85\n",
        "* Critical = Completion Rate < 50% OR Average Score < 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06f75df-caa7-4a5b-80b6-be115476256e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+-------------+-----------------+---------+\n",
            "|student_id|course_id|total_modules|completed_modules|avg_score|\n",
            "+----------+---------+-------------+-----------------+---------+\n",
            "|      S001|     C001|           10|                8|     85.0|\n",
            "|      S001|     C002|           15|                5|     65.0|\n",
            "|      S002|     C001|           10|               10|     95.0|\n",
            "|      S002|     C003|           20|               15|     88.0|\n",
            "|      S003|     C001|           10|                3|     55.0|\n",
            "|      S003|     C002|           15|                2|     45.0|\n",
            "|      S004|     C003|           20|               18|     92.0|\n",
            "+----------+---------+-------------+-----------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "student_progress_data = [\n",
        "    (\"S001\", \"C001\", 10, 8, 85.0),\n",
        "    (\"S001\", \"C002\", 15, 5, 65.0),\n",
        "    (\"S002\", \"C001\", 10, 10, 95.0),\n",
        "    (\"S002\", \"C003\", 20, 15, 88.0),\n",
        "    (\"S003\", \"C001\", 10, 3, 55.0),\n",
        "    (\"S003\", \"C002\", 15, 2, 45.0),\n",
        "    (\"S004\", \"C003\", 20, 18, 92.0)\n",
        "]\n",
        "\n",
        "student_progress_df = spark.createDataFrame(student_progress_data, [\"student_id\", \"course_id\", \"total_modules\", \"completed_modules\", \"avg_score\"])\n",
        "student_progress_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57de6b33-31fb-4c48-bebb-e5777834c8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-----------------+---------------+---------+---------+\n",
            "|student_id|total_modules|completed_modules|completion_rate|avg_score|   status|\n",
            "+----------+-------------+-----------------+---------------+---------+---------+\n",
            "|      S001|           25|               13|           52.0|     75.0|  At Risk|\n",
            "|      S002|           30|               25|           83.3|     91.5|Excellent|\n",
            "|      S003|           25|                5|           20.0|     50.0| Critical|\n",
            "|      S004|           20|               18|           90.0|     92.0|Excellent|\n",
            "+----------+-------------+-----------------+---------------+---------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"S001\", 25, 13, 52.0, 75.0, \"At Risk\"),\n",
        "    (\"S002\", 30, 25, 83.3, 91.5, \"Excellent\"),\n",
        "    (\"S003\", 25, 5, 20.0, 50.0, \"Critical\"),\n",
        "    (\"S004\", 20, 18, 90.0, 92.0, \"Excellent\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"student_id\", \"total_modules\", \"completed_modules\", \"completion_rate\", \"avg_score\", \"status\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb916ca-143e-4447-980f-cb3c2b6f4f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-----------------+---------------+---------+---------+\n",
            "|student_id|total_modules|completed_modules|completion_rate|avg_score|   status|\n",
            "+----------+-------------+-----------------+---------------+---------+---------+\n",
            "|      S001|           25|               13|           52.0|     75.0|  At Risk|\n",
            "|      S002|           30|               25|           83.3|     91.5|Excellent|\n",
            "|      S004|           20|               18|           90.0|     92.0|Excellent|\n",
            "|      S003|           25|                5|           20.0|     50.0| Critical|\n",
            "+----------+-------------+-----------------+---------------+---------+---------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      student_progress_df\\\n",
        "          .groupBy('student_id')\\\n",
        "          .agg(fn.expr(''' sum(total_modules) as total_modules '''),\n",
        "              fn.expr(''' sum(completed_modules) as completed_modules '''),\n",
        "              fn.expr(''' round(sum(completed_modules)*100/cast(nullif(sum(total_modules),0) as float),1) as completion_rate '''),\n",
        "              fn.expr(''' avg(avg_score) as avg_score '''))\\\n",
        "          .withColumn('status', fn.when((fn.col('completion_rate') >= 80) & (fn.col('avg_score')>= 85),'Excellent')\\\n",
        "                                  .when((fn.col('completion_rate') >= 50) & (fn.col('avg_score')< 85),'At Risk')\\\n",
        "                                  .when((fn.col('completion_rate') < 50) | (fn.col('avg_score')< 60),'Critical')\\\n",
        "                                  .otherwise(None)\n",
        "\n",
        "                  )\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-9"
      },
      "source": [
        "**Instructor Notes:** Student analytics with multi-criteria status classification. Tests aggregation and conditional business logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-10"
      },
      "source": [
        "## Problem 10: IoT Sensor Data Anomaly Detection\n",
        "\n",
        "**Requirement:** IoT monitoring needs real-time anomaly detection in sensor data streams.\n",
        "\n",
        "**Scenario:** Identify sensor readings that deviate significantly from historical patterns using statistical methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d584ed4f-299a-4929-98ef-ad014a35e22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+-----+\n",
            "|sensor_id|          timestamp|value|\n",
            "+---------+-------------------+-----+\n",
            "| Sensor_A|2023-03-01 10:00:00| 26.5|\n",
            "| Sensor_A|2023-03-01 11:00:00| 26.1|\n",
            "| Sensor_A|2023-03-01 12:00:00| 25.8|\n",
            "| Sensor_A|2023-03-01 13:00:00| 45.2|\n",
            "| Sensor_A|2023-03-01 14:00:00| 27.9|\n",
            "| Sensor_B|2023-03-01 10:00:00| 31.2|\n",
            "| Sensor_B|2023-03-01 11:00:00| 31.0|\n",
            "| Sensor_B|2023-03-01 12:00:00| 12.1|\n",
            "| Sensor_B|2023-03-01 13:00:00| 30.5|\n",
            "| Sensor_B|2023-03-01 14:00:00| 30.8|\n",
            "+---------+-------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "sensor_data = [\n",
        "    (\"Sensor_A\", \"2023-03-01 10:00:00\", 26.5),\n",
        "    (\"Sensor_A\", \"2023-03-01 11:00:00\", 26.1),\n",
        "    (\"Sensor_A\", \"2023-03-01 12:00:00\", 25.8),\n",
        "    (\"Sensor_A\", \"2023-03-01 13:00:00\", 45.2),  # Anomaly\n",
        "    (\"Sensor_A\", \"2023-03-01 14:00:00\", 27.9),\n",
        "    (\"Sensor_B\", \"2023-03-01 10:00:00\", 31.2),\n",
        "    (\"Sensor_B\", \"2023-03-01 11:00:00\", 31.0),\n",
        "    (\"Sensor_B\", \"2023-03-01 12:00:00\", 12.1),  # Anomaly\n",
        "    (\"Sensor_B\", \"2023-03-01 13:00:00\", 30.5),\n",
        "    (\"Sensor_B\", \"2023-03-01 14:00:00\", 30.8)\n",
        "]\n",
        "\n",
        "sensor_df = spark.createDataFrame(sensor_data, [\"sensor_id\", \"timestamp\", \"value\"])\n",
        "sensor_df = sensor_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "sensor_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b091730d-c1a4-4e93-a341-a678c4f785e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+-----+-------+\n",
            "|sensor_id|          timestamp|value| status|\n",
            "+---------+-------------------+-----+-------+\n",
            "| Sensor_A|2023-03-01 13:00:00| 45.2|Anomaly|\n",
            "| Sensor_B|2023-03-01 12:00:00| 12.1|Anomaly|\n",
            "+---------+-------------------+-----+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Sensor_A\", \"2023-03-01 13:00:00\", 45.2, \"Anomaly\"),\n",
        "    (\"Sensor_B\", \"2023-03-01 12:00:00\", 12.1, \"Anomaly\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"sensor_id\", \"timestamp\", \"value\", \"status\"])\n",
        "expected_df = expected_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328f5fa1-34fd-4bf5-81de-e79572b028d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.549018181640903 44.8709818183591\n",
            "+---------+-------------------+-----+-------+\n",
            "|sensor_id|          timestamp|value| status|\n",
            "+---------+-------------------+-----+-------+\n",
            "| Sensor_A|2023-03-01 13:00:00| 45.2|Anomaly|\n",
            "| Sensor_B|2023-03-01 12:00:00| 12.1|Anomaly|\n",
            "+---------+-------------------+-----+-------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "thresholds = sensor_df.\\\n",
        "        selectExpr('mean(value) as meanvalue', 'std(value) as stdvalue')\\\n",
        "         .withColumn('minThreshold',fn.expr(''' meanvalue - 2* stdvalue'''))\\\n",
        "         .withColumn('maxThreshold',fn.expr(''' meanvalue + 2* stdvalue'''))\n",
        "\n",
        "minThreshold = thresholds.collect()[0]['minThreshold']\n",
        "maxThreshold = thresholds.collect()[0]['maxThreshold']\n",
        "print(minThreshold,maxThreshold)\n",
        "\n",
        "result_df = \\\n",
        "    sensor_df\\\n",
        "        .withColumn('status', fn.expr(f''' case when value > {maxThreshold} or value < {minThreshold} then 'Anomaly' end'''))\\\n",
        "        .filter('''status = 'Anomaly' ''')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-10"
      },
      "source": [
        "**Instructor Notes:** Statistical anomaly detection with window functions. Tests standard deviation calculations and outlier identification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-11"
      },
      "source": [
        "## Problem 11: Financial Transaction Pattern Analysis\n",
        "\n",
        "**Requirement:** Fraud detection needs transaction pattern analysis for suspicious activity identification.\n",
        "\n",
        "**Scenario:** Analyze transaction patterns to identify unusual spending behaviors and potential fraud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d255a49-a1c6-41b8-9948-2fd2c8200a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+-------------------+------+-------------+\n",
            "|transaction_id|customer_id|          timestamp|amount|     category|\n",
            "+--------------+-----------+-------------------+------+-------------+\n",
            "|          T001|       C001|2023-03-01 09:00:00| 100.0|       Retail|\n",
            "|          T002|       C001|2023-03-01 10:30:00|  50.0|       Dining|\n",
            "|          T003|       C001|2023-03-01 15:00:00| 200.0|  Electronics|\n",
            "|          T004|       C001|2023-03-02 08:00:00|5000.0|      Jewelry|\n",
            "|          T005|       C002|2023-03-01 11:00:00|  75.0|    Groceries|\n",
            "|          T006|       C002|2023-03-01 14:00:00| 120.0|Entertainment|\n",
            "|          T007|       C002|2023-03-02 10:00:00|  80.0|       Dining|\n",
            "+--------------+-----------+-------------------+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "transaction_patterns_data = [\n",
        "    (\"T001\", \"C001\", \"2023-03-01 09:00:00\", 100.0, \"Retail\"),\n",
        "    (\"T002\", \"C001\", \"2023-03-01 10:30:00\", 50.0, \"Dining\"),\n",
        "    (\"T003\", \"C001\", \"2023-03-01 15:00:00\", 200.0, \"Electronics\"),\n",
        "    (\"T004\", \"C001\", \"2023-03-02 08:00:00\", 5000.0, \"Jewelry\"),  # Suspicious\n",
        "    (\"T005\", \"C002\", \"2023-03-01 11:00:00\", 75.0, \"Groceries\"),\n",
        "    (\"T006\", \"C002\", \"2023-03-01 14:00:00\", 120.0, \"Entertainment\"),\n",
        "    (\"T007\", \"C002\", \"2023-03-02 10:00:00\", 80.0, \"Dining\")\n",
        "]\n",
        "\n",
        "transaction_patterns_df = spark.createDataFrame(transaction_patterns_data, [\"transaction_id\", \"customer_id\", \"timestamp\", \"amount\", \"category\"])\n",
        "transaction_patterns_df = transaction_patterns_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "transaction_patterns_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a84e80-0ba6-4435-f940-cfe7dddf4451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+-------------------+------+--------+----------+\n",
            "|transaction_id|customer_id|          timestamp|amount|category|risk_level|\n",
            "+--------------+-----------+-------------------+------+--------+----------+\n",
            "|          T004|       C001|2023-03-02 08:00:00|5000.0| Jewelry|High Value|\n",
            "+--------------+-----------+-------------------+------+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"T004\", \"C001\", \"2023-03-02 08:00:00\", 5000.0, \"Jewelry\", \"High Value\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"transaction_id\", \"customer_id\", \"timestamp\", \"amount\", \"category\", \"risk_level\"])\n",
        "expected_df = expected_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1a2572-acf7-4280-f4c4-4b7be4069790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+-------------------+------+--------+----------+\n",
            "|transaction_id|customer_id|          timestamp|amount|category|risk_level|\n",
            "+--------------+-----------+-------------------+------+--------+----------+\n",
            "|          T004|       C001|2023-03-02 08:00:00|5000.0| Jewelry|High Value|\n",
            "+--------------+-----------+-------------------+------+--------+----------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      transaction_patterns_df\\\n",
        "        .withColumn('risk_level',fn.expr(''' case when amount > 1000 then 'High Value' else 'Normal' end '''))\\\n",
        "        .filter(''' risk_level = 'High Value' ''')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-11"
      },
      "source": [
        "**Instructor Notes:** Fraud detection with pattern analysis. Tests statistical comparisons and anomaly flagging based on historical patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-12"
      },
      "source": [
        "## Problem 12: Multi-Dimensional Sales Analysis\n",
        "\n",
        "**Requirement:** Business intelligence needs sales analysis across multiple dimensions.\n",
        "\n",
        "**Scenario:** Analyze sales performance across time, geography, and product categories with rollup aggregations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0b6e7f-eefd-469b-86f3-9f12862f46f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+-------+-----+\n",
            "|quarter|region|   category|product|sales|\n",
            "+-------+------+-----------+-------+-----+\n",
            "|2023-Q1| North|Electronics| Laptop|50000|\n",
            "|2023-Q1| North|Electronics| Tablet|30000|\n",
            "|2023-Q1| South|Electronics| Laptop|45000|\n",
            "|2023-Q1| South|Electronics| Tablet|25000|\n",
            "|2023-Q1| North|   Clothing|  Shirt|20000|\n",
            "|2023-Q1| South|   Clothing|  Shirt|22000|\n",
            "|2023-Q2| North|Electronics| Laptop|55000|\n",
            "|2023-Q2| North|Electronics| Tablet|32000|\n",
            "+-------+------+-----------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "multi_dim_sales_data = [\n",
        "    (\"2023-Q1\", \"North\", \"Electronics\", \"Laptop\", 50000),\n",
        "    (\"2023-Q1\", \"North\", \"Electronics\", \"Tablet\", 30000),\n",
        "    (\"2023-Q1\", \"South\", \"Electronics\", \"Laptop\", 45000),\n",
        "    (\"2023-Q1\", \"South\", \"Electronics\", \"Tablet\", 25000),\n",
        "    (\"2023-Q1\", \"North\", \"Clothing\", \"Shirt\", 20000),\n",
        "    (\"2023-Q1\", \"South\", \"Clothing\", \"Shirt\", 22000),\n",
        "    (\"2023-Q2\", \"North\", \"Electronics\", \"Laptop\", 55000),\n",
        "    (\"2023-Q2\", \"North\", \"Electronics\", \"Tablet\", 32000)\n",
        "]\n",
        "\n",
        "multi_dim_sales_df = spark.createDataFrame(multi_dim_sales_data, [\"quarter\", \"region\", \"category\", \"product\", \"sales\"])\n",
        "multi_dim_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a8995f-d3ae-413a-eab8-bbfcc950deda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+-----------+\n",
            "|quarter|region|   category|total_sales|\n",
            "+-------+------+-----------+-----------+\n",
            "|2023-Q1|   All|        All|     192000|\n",
            "|2023-Q1| North|        All|     100000|\n",
            "|2023-Q1| North|   Clothing|      20000|\n",
            "|2023-Q1| North|Electronics|      80000|\n",
            "|2023-Q1| South|        All|      92000|\n",
            "|2023-Q1| South|   Clothing|      22000|\n",
            "|2023-Q1| South|Electronics|      70000|\n",
            "|2023-Q2|   All|        All|      87000|\n",
            "|2023-Q2| North|        All|      87000|\n",
            "|2023-Q2| North|Electronics|      87000|\n",
            "+-------+------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  Expected Output matching your actual result\n",
        "\n",
        "expected_data = [\n",
        "    (\"2023-Q1\", \"All\", \"All\", 192000),\n",
        "    (\"2023-Q1\", \"North\", \"All\", 100000),\n",
        "    (\"2023-Q1\", \"North\", \"Clothing\", 20000),\n",
        "    (\"2023-Q1\", \"North\", \"Electronics\", 80000),\n",
        "    (\"2023-Q1\", \"South\", \"All\", 92000),\n",
        "    (\"2023-Q1\", \"South\", \"Clothing\", 22000),\n",
        "    (\"2023-Q1\", \"South\", \"Electronics\", 70000),\n",
        "    (\"2023-Q2\", \"All\", \"All\", 87000),\n",
        "    (\"2023-Q2\", \"North\", \"All\", 87000),\n",
        "    (\"2023-Q2\", \"North\", \"Electronics\", 87000)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"quarter\", \"region\", \"category\", \"total_sales\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37705da-8508-4047-d4de-8e001f6657a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+-----------+\n",
            "|quarter|region|   category|total_sales|\n",
            "+-------+------+-----------+-----------+\n",
            "|2023-Q1|   All|        All|     192000|\n",
            "|2023-Q1| North|        All|     100000|\n",
            "|2023-Q1| North|   Clothing|      20000|\n",
            "|2023-Q1| North|Electronics|      80000|\n",
            "|2023-Q1| South|        All|      92000|\n",
            "|2023-Q1| South|   Clothing|      22000|\n",
            "|2023-Q1| South|Electronics|      70000|\n",
            "|2023-Q2|   All|        All|      87000|\n",
            "|2023-Q2| North|        All|      87000|\n",
            "|2023-Q2| North|Electronics|      87000|\n",
            "+-------+------+-----------+-----------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "\n",
        "result_df =\\\n",
        "    multi_dim_sales_df\\\n",
        "      .rollup('quarter','region','category')\\\n",
        "      .agg(fn.sum(fn.col('sales')).alias('total_sales'))\\\n",
        "      .filter('quarter IS NOT NULL')\\\n",
        "      .withColumn('region', fn.nvl(fn.col('region'),fn.lit('All')))\\\n",
        "      .withColumn('category', fn.nvl(fn.col('category'),fn.lit('All')))\\\n",
        "      .orderBy('quarter','region','category')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-12"
      },
      "source": [
        "**Instructor Notes:** Multi-dimensional analysis with rollup aggregations. Tests cube/rollup operations for hierarchical reporting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-13"
      },
      "source": [
        "## Problem 13: Complex UDF for Natural Language Processing\n",
        "\n",
        "**Requirement:** Customer feedback analysis needs text processing for sentiment and topic extraction.\n",
        "\n",
        "**Scenario:** Create advanced UDFs to process customer feedback text for sentiment analysis and key topic identification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660ee201-b92c-4834-f47d-ee936f43d101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------------------------------------------------------------------------+\n",
            "|feedback_id|feedback_text                                                                    |\n",
            "+-----------+---------------------------------------------------------------------------------+\n",
            "|1          |The product is amazing! Great quality and fast delivery.                         |\n",
            "|2          |Terrible experience. The item arrived damaged and customer service was unhelpful.|\n",
            "|3          |Average product, nothing special but gets the job done.                          |\n",
            "|4          |Excellent service! Will definitely buy again. Highly recommended.                |\n",
            "|5          |Poor quality product. Broke after first use. Very disappointed.                  |\n",
            "+-----------+---------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_feedback_data = [\n",
        "    (1, \"The product is amazing! Great quality and fast delivery.\"),\n",
        "    (2, \"Terrible experience. The item arrived damaged and customer service was unhelpful.\"),\n",
        "    (3, \"Average product, nothing special but gets the job done.\"),\n",
        "    (4, \"Excellent service! Will definitely buy again. Highly recommended.\"),\n",
        "    (5, \"Poor quality product. Broke after first use. Very disappointed.\")\n",
        "]\n",
        "\n",
        "customer_feedback_df = spark.createDataFrame(customer_feedback_data, [\"feedback_id\", \"feedback_text\"])\n",
        "customer_feedback_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa628bc5-abe8-4495-eb79-027332e2271b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------------------------------------------------------------------------+---------+----------------+\n",
            "|feedback_id|feedback_text                                                                    |sentiment|main_topic      |\n",
            "+-----------+---------------------------------------------------------------------------------+---------+----------------+\n",
            "|1          |The product is amazing! Great quality and fast delivery.                         |Positive |product quality |\n",
            "|2          |Terrible experience. The item arrived damaged and customer service was unhelpful.|Negative |customer service|\n",
            "|3          |Average product, nothing special but gets the job done.                          |Neutral  |product quality |\n",
            "|4          |Excellent service! Will definitely buy again. Highly recommended.                |Positive |customer service|\n",
            "|5          |Poor quality product. Broke after first use. Very disappointed.                  |Negative |product quality |\n",
            "+-----------+---------------------------------------------------------------------------------+---------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"The product is amazing! Great quality and fast delivery.\", \"Positive\", \"product quality\"),\n",
        "    (2, \"Terrible experience. The item arrived damaged and customer service was unhelpful.\", \"Negative\", \"customer service\"),\n",
        "    (3, \"Average product, nothing special but gets the job done.\", \"Neutral\", \"product quality\"),\n",
        "    (4, \"Excellent service! Will definitely buy again. Highly recommended.\", \"Positive\", \"customer service\"),\n",
        "    (5, \"Poor quality product. Broke after first use. Very disappointed.\", \"Negative\", \"product quality\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"feedback_id\", \"feedback_text\", \"sentiment\", \"main_topic\"])\n",
        "expected_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-13"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "# I will skip this question, idont want solve a natural langugae processing questions\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-13"
      },
      "source": [
        "**Instructor Notes:** Advanced UDFs for text processing. Tests string analysis, keyword matching, and sentiment classification logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-14"
      },
      "source": [
        "## Problem 14: Time-Series Forecasting Features\n",
        "\n",
        "**Requirement:** Forecasting team needs feature engineering for time-series prediction models.\n",
        "\n",
        "**Scenario:** Create lag features, moving averages, and trend indicators for sales forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eccad99d-cb70-4b6b-a149-8857d292b483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|      date| sales|\n",
            "+----------+------+\n",
            "|2023-01-01|1000.0|\n",
            "|2023-01-02|1200.0|\n",
            "|2023-01-03|1100.0|\n",
            "|2023-01-04|1300.0|\n",
            "|2023-01-05|1400.0|\n",
            "|2023-01-06|1250.0|\n",
            "|2023-01-07|1500.0|\n",
            "|2023-01-08|1600.0|\n",
            "+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "sales_forecasting_data = [\n",
        "    (\"2023-01-01\", 1000.0),\n",
        "    (\"2023-01-02\", 1200.0),\n",
        "    (\"2023-01-03\", 1100.0),\n",
        "    (\"2023-01-04\", 1300.0),\n",
        "    (\"2023-01-05\", 1400.0),\n",
        "    (\"2023-01-06\", 1250.0),\n",
        "    (\"2023-01-07\", 1500.0),\n",
        "    (\"2023-01-08\", 1600.0)\n",
        "]\n",
        "\n",
        "sales_forecasting_df = spark.createDataFrame(sales_forecasting_data, [\"date\", \"sales\"])\n",
        "sales_forecasting_df = sales_forecasting_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "sales_forecasting_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e429da2a-c2a0-4f21-c10e-d8e8e2dd1c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+------+------+------------+------------+\n",
            "|      date| sales| lag_1| lag_2|moving_avg_3|daily_change|\n",
            "+----------+------+------+------+------------+------------+\n",
            "|2023-01-01|1000.0|  NULL|  NULL|        NULL|        NULL|\n",
            "|2023-01-02|1200.0|1000.0|  NULL|        NULL|       200.0|\n",
            "|2023-01-03|1100.0|1200.0|1000.0|      1100.0|      -100.0|\n",
            "|2023-01-04|1300.0|1100.0|1200.0|      1200.0|       200.0|\n",
            "|2023-01-05|1400.0|1300.0|1100.0|     1266.67|       100.0|\n",
            "|2023-01-06|1250.0|1400.0|1300.0|     1316.67|      -150.0|\n",
            "|2023-01-07|1500.0|1250.0|1400.0|     1383.33|       250.0|\n",
            "|2023-01-08|1600.0|1500.0|1250.0|      1450.0|       100.0|\n",
            "+----------+------+------+------+------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", 1000.0, None, None, None, None),\n",
        "    (\"2023-01-02\", 1200.0, 1000.0, None, None, 200.0),\n",
        "    (\"2023-01-03\", 1100.0, 1200.0, 1000.0, 1100.0, -100.0),\n",
        "    (\"2023-01-04\", 1300.0, 1100.0, 1200.0, 1200.0, 200.0),\n",
        "    (\"2023-01-05\", 1400.0, 1300.0, 1100.0, 1266.67, 100.0),\n",
        "    (\"2023-01-06\", 1250.0, 1400.0, 1300.0, 1316.67, -150.0),\n",
        "    (\"2023-01-07\", 1500.0, 1250.0, 1400.0, 1383.33, 250.0),\n",
        "    (\"2023-01-08\", 1600.0, 1500.0, 1250.0, 1450.0, 100.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"sales\", \"lag_1\", \"lag_2\", \"moving_avg_3\", \"daily_change\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6da96e-2f4b-48cb-9b2a-36e0b8d15896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+------+------+------------+------------+\n",
            "|      date| sales| lag_1| lag_2|moving_avg_3|daily_change|\n",
            "+----------+------+------+------+------------+------------+\n",
            "|2023-01-01|1000.0|  NULL|  NULL|        NULL|        NULL|\n",
            "|2023-01-02|1200.0|1000.0|  NULL|        NULL|       200.0|\n",
            "|2023-01-03|1100.0|1200.0|1000.0|      1100.0|      -100.0|\n",
            "|2023-01-04|1300.0|1100.0|1200.0|      1200.0|       200.0|\n",
            "|2023-01-05|1400.0|1300.0|1100.0|     1266.67|       100.0|\n",
            "|2023-01-06|1250.0|1400.0|1300.0|     1316.67|      -150.0|\n",
            "|2023-01-07|1500.0|1250.0|1400.0|     1383.33|       250.0|\n",
            "|2023-01-08|1600.0|1500.0|1250.0|      1450.0|       100.0|\n",
            "+----------+------+------+------+------------+------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      sales_forecasting_df\\\n",
        "          .withColumn('lag_1', fn.expr(''' lag(sales,1) over(order by date asc) '''))\\\n",
        "          .withColumn('lag_2', fn.expr(''' lag(sales,2) over(order by date asc) '''))\\\n",
        "          .withColumn('moving_avg_row_num', fn.expr(''' row_number() over(order by date asc ) '''))\\\n",
        "          .withColumn('moving_avg_3', fn.expr(''' avg(sales) over(order by date asc rows between 2 preceding and current row) '''))\\\n",
        "          .withColumn('moving_avg_3', fn.expr(''' case when moving_avg_row_num <= 2 then NULL else round(moving_avg_3,2) end'''))\\\n",
        "          .withColumn('daily_change', fn.expr(''' sales - lag_1 '''))\\\n",
        "          .drop('moving_avg_row_num')\\\n",
        "          .orderBy('date')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-14"
      },
      "source": [
        "**Instructor Notes:** Time-series feature engineering. Tests lag features, moving averages, and trend calculations for forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-15"
      },
      "source": [
        "## Problem 15: Complex Data Validation Framework\n",
        "\n",
        "**Requirement:** Data governance needs comprehensive data quality validation framework.\n",
        "\n",
        "**Scenario:** Implement multi-level data validation checks with custom business rules and cross-field validation.\n",
        "\n",
        "* Email Format Check - Email must contain \"@\" symbol\n",
        "* Balance Check - Balance must be non-negative (‚â• 0)\n",
        "* Age Check - Customer must be at least 18 years old at the time of signup\n",
        "* Signup Date Check - Signup date must be on or after 2023-01-01\n",
        "* Name Presence Check - Name must not be empty or blank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e538f9-bdd6-4dd9-a962-d4f0524543a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------------+----------+-----------+-------+\n",
            "|customer_id|       name|            email|birth_date|signup_date|balance|\n",
            "+-----------+-----------+-----------------+----------+-----------+-------+\n",
            "|          1|   John Doe|   john@email.com|1990-01-15| 2023-01-01| 5000.0|\n",
            "|          2| Jane Smith|    invalid-email|1985-12-20| 2023-01-15| -100.0|\n",
            "|          3|Bob Johnson|  bob@company.com|2005-06-10| 2023-02-01| 3000.0|\n",
            "|          4|Alice Brown| alice@domain.com|1975-03-25| 2022-12-01| 7500.0|\n",
            "|          5|           |charlie@email.com|1988-07-30| 2023-01-10| 4000.0|\n",
            "+-----------+-----------+-----------------+----------+-----------+-------+\n",
            "\n",
            "root\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- birth_date: string (nullable = true)\n",
            " |-- signup_date: string (nullable = true)\n",
            " |-- balance: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "data_validation_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"1990-01-15\", \"2023-01-01\", 5000.0),\n",
        "    (2, \"Jane Smith\", \"invalid-email\", \"1985-12-20\", \"2023-01-15\", -100.0),  # Invalid\n",
        "    (3, \"Bob Johnson\", \"bob@company.com\", \"2005-06-10\", \"2023-02-01\", 3000.0),  # Underage\n",
        "    (4, \"Alice Brown\", \"alice@domain.com\", \"1975-03-25\", \"2022-12-01\", 7500.0),  # Future date\n",
        "    (5, \"\", \"charlie@email.com\", \"1988-07-30\", \"2023-01-10\", 4000.0)  # Empty name\n",
        "]\n",
        "\n",
        "data_validation_df = spark.createDataFrame(data_validation_data, [\"customer_id\", \"name\", \"email\", \"birth_date\", \"signup_date\", \"balance\"])\n",
        "data_validation_df.show()\n",
        "data_validation_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7412ce8f-0d84-40a7-cc9e-643c2e743b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------------+----------+-----------+-------+------------------------------+\n",
            "|customer_id|name       |email            |birth_date|signup_date|balance|validation_errors             |\n",
            "+-----------+-----------+-----------------+----------+-----------+-------+------------------------------+\n",
            "|1          |John Doe   |john@email.com   |1990-01-15|2023-01-01 |5000.0 |Valid                         |\n",
            "|2          |Jane Smith |invalid-email    |1985-12-20|2023-01-15 |-100.0 |Invalid Email,Negative Balance|\n",
            "|3          |Bob Johnson|bob@company.com  |2005-06-10|2023-02-01 |3000.0 |Underage                      |\n",
            "|4          |Alice Brown|alice@domain.com |1975-03-25|2022-12-01 |7500.0 |Future Signup Date            |\n",
            "|5          |           |charlie@email.com|1988-07-30|2023-01-10 |4000.0 |Empty Name                    |\n",
            "+-----------+-----------+-----------------+----------+-----------+-------+------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"1990-01-15\", \"2023-01-01\", 5000.0, \"Valid\"),\n",
        "    (2, \"Jane Smith\", \"invalid-email\", \"1985-12-20\", \"2023-01-15\", -100.0, \"Invalid Email,Negative Balance\"),\n",
        "    (3, \"Bob Johnson\", \"bob@company.com\", \"2005-06-10\", \"2023-02-01\", 3000.0, \"Underage\"),\n",
        "    (4, \"Alice Brown\", \"alice@domain.com\", \"1975-03-25\", \"2022-12-01\", 7500.0, \"Future Signup Date\"),\n",
        "    (5, \"\", \"charlie@email.com\", \"1988-07-30\", \"2023-01-10\", 4000.0, \"Empty Name\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"email\", \"birth_date\", \"signup_date\", \"balance\", \"validation_errors\"])\n",
        "expected_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82336846-4e1d-4936-ab3b-e2b2c2e76afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------------+----------+-----------+-------+------------------------------+\n",
            "|customer_id|name       |email            |birth_date|signup_date|balance|validation_errors             |\n",
            "+-----------+-----------+-----------------+----------+-----------+-------+------------------------------+\n",
            "|1          |John Doe   |john@email.com   |1990-01-15|2023-01-01 |5000.0 |Valid                         |\n",
            "|2          |Jane Smith |invalid-email    |1985-12-20|2023-01-15 |-100.0 |Invalid Email,Negative Balance|\n",
            "|3          |Bob Johnson|bob@company.com  |2005-06-10|2023-02-01 |3000.0 |Underage                      |\n",
            "|4          |Alice Brown|alice@domain.com |1975-03-25|2022-12-01 |7500.0 |Future Signup Date            |\n",
            "|5          |           |charlie@email.com|1988-07-30|2023-01-10 |4000.0 |Empty Name                    |\n",
            "+-----------+-----------+-----------------+----------+-----------+-------+------------------------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "email_reg = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "\n",
        "result_df = \\\n",
        "      data_validation_df\\\n",
        "        .withColumn('email_val', fn.regexp_like(fn.col('email'),fn.lit(email_reg)))\\\n",
        "        .withColumn('email_val', fn.when(fn.col('email_val') == False,'Invalid Email'))\\\n",
        "        .withColumn('balance_val', fn.expr(''' case when balance < 0 then 'Negative Balance' else NULL end '''))\\\n",
        "        .withColumn('age_val', fn.expr(''' (date_diff(to_date(signup_date),to_date(birth_date))) / float(365.25) '''))\\\n",
        "        .withColumn('age_val', fn.expr(''' case when age_val < 18 then 'Underage' ELSE NULL END  '''))\\\n",
        "        .withColumn('signup_val',fn.expr(''' case when signup_date < to_date('2023-01-01') then 'Future Signup Date' else null end '''))\\\n",
        "        .withColumn('name_val', fn.expr(''' case when nullif(trim(name),'') is null then 'Empty Name' ELSE NULL END '''))\\\n",
        "        .withColumn('validation_errors',fn.expr(''' concat_ws(',',email_val,balance_val,age_val,signup_val,name_val) '''))\\\n",
        "        .withColumn('validation_errors',fn.expr(''' case when validation_errors = '' then 'Valid' else validation_errors end '''))\\\n",
        "        .drop('email_val','balance_val','age_val','signup_val','name_val')\n",
        "\n",
        "result_df.show(truncate = False)\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-15"
      },
      "source": [
        "**Instructor Notes:** Comprehensive data validation framework. Tests multiple validation rules and error message aggregation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-16"
      },
      "source": [
        "## Problem 16: Advanced Window Functions for Gap Analysis\n",
        "\n",
        "**Requirement:** Business operations needs gap analysis in service delivery timelines.\n",
        "\n",
        "**Scenario:** Identify service gaps and calculate downtime between consecutive service events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6698f8-a291-442c-995c-ae529c176eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-------------------+\n",
            "|service_id|         start_time|           end_time|\n",
            "+----------+-------------------+-------------------+\n",
            "|      S001|2023-03-01 09:00:00|2023-03-01 10:00:00|\n",
            "|      S001|2023-03-01 11:30:00|2023-03-01 12:30:00|\n",
            "|      S001|2023-03-01 14:00:00|2023-03-01 15:00:00|\n",
            "|      S002|2023-03-01 08:00:00|2023-03-01 09:00:00|\n",
            "|      S002|2023-03-01 10:00:00|2023-03-01 11:00:00|\n",
            "|      S002|2023-03-01 13:00:00|2023-03-01 14:00:00|\n",
            "+----------+-------------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- service_id: string (nullable = true)\n",
            " |-- start_time: timestamp (nullable = true)\n",
            " |-- end_time: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "service_events_data = [\n",
        "    (\"S001\", \"2023-03-01 09:00:00\", \"2023-03-01 10:00:00\"),\n",
        "    (\"S001\", \"2023-03-01 11:30:00\", \"2023-03-01 12:30:00\"),\n",
        "    (\"S001\", \"2023-03-01 14:00:00\", \"2023-03-01 15:00:00\"),\n",
        "    (\"S002\", \"2023-03-01 08:00:00\", \"2023-03-01 09:00:00\"),\n",
        "    (\"S002\", \"2023-03-01 10:00:00\", \"2023-03-01 11:00:00\"),\n",
        "    (\"S002\", \"2023-03-01 13:00:00\", \"2023-03-01 14:00:00\")\n",
        "]\n",
        "\n",
        "service_events_df = spark.createDataFrame(service_events_data, [\"service_id\", \"start_time\", \"end_time\"])\n",
        "service_events_df = service_events_df.withColumn(\"start_time\", col(\"start_time\").cast(\"timestamp\"))\\\n",
        "                                   .withColumn(\"end_time\", col(\"end_time\").cast(\"timestamp\"))\n",
        "service_events_df.show()\n",
        "service_events_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8d667a-bee7-44cf-b3f6-7571a61afda5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-------------------+-------------------+-----------+\n",
            "|service_id|         start_time|           end_time|      prev_end_time|gap_minutes|\n",
            "+----------+-------------------+-------------------+-------------------+-----------+\n",
            "|      S001|2023-03-01 09:00:00|2023-03-01 10:00:00|               NULL|       NULL|\n",
            "|      S001|2023-03-01 11:30:00|2023-03-01 12:30:00|2023-03-01 10:00:00|         90|\n",
            "|      S001|2023-03-01 14:00:00|2023-03-01 15:00:00|2023-03-01 12:30:00|         90|\n",
            "|      S002|2023-03-01 08:00:00|2023-03-01 09:00:00|               NULL|       NULL|\n",
            "|      S002|2023-03-01 10:00:00|2023-03-01 11:00:00|2023-03-01 09:00:00|         60|\n",
            "|      S002|2023-03-01 13:00:00|2023-03-01 14:00:00|2023-03-01 11:00:00|        120|\n",
            "+----------+-------------------+-------------------+-------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"S001\", \"2023-03-01 09:00:00\", \"2023-03-01 10:00:00\", None, None),\n",
        "    (\"S001\", \"2023-03-01 11:30:00\", \"2023-03-01 12:30:00\", \"2023-03-01 10:00:00\", 90),\n",
        "    (\"S001\", \"2023-03-01 14:00:00\", \"2023-03-01 15:00:00\", \"2023-03-01 12:30:00\", 90),\n",
        "    (\"S002\", \"2023-03-01 08:00:00\", \"2023-03-01 09:00:00\", None, None),\n",
        "    (\"S002\", \"2023-03-01 10:00:00\", \"2023-03-01 11:00:00\", \"2023-03-01 09:00:00\", 60),\n",
        "    (\"S002\", \"2023-03-01 13:00:00\", \"2023-03-01 14:00:00\", \"2023-03-01 11:00:00\", 120)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"service_id\", \"start_time\", \"end_time\", \"prev_end_time\", \"gap_minutes\"])\n",
        "expected_df = expected_df.withColumn(\"start_time\", col(\"start_time\").cast(\"timestamp\"))\\\n",
        "                       .withColumn(\"end_time\", col(\"end_time\").cast(\"timestamp\"))\\\n",
        "                       .withColumn(\"prev_end_time\", col(\"prev_end_time\").cast(\"timestamp\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3cd0c7-1076-4f4c-9a0b-8dce011e17fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-------------------+-------------------+-----------+\n",
            "|service_id|         start_time|           end_time|      prev_end_time|gap_minutes|\n",
            "+----------+-------------------+-------------------+-------------------+-----------+\n",
            "|      S001|2023-03-01 09:00:00|2023-03-01 10:00:00|               NULL|       NULL|\n",
            "|      S001|2023-03-01 11:30:00|2023-03-01 12:30:00|2023-03-01 10:00:00|       90.0|\n",
            "|      S001|2023-03-01 14:00:00|2023-03-01 15:00:00|2023-03-01 12:30:00|       90.0|\n",
            "|      S002|2023-03-01 08:00:00|2023-03-01 09:00:00|               NULL|       NULL|\n",
            "|      S002|2023-03-01 10:00:00|2023-03-01 11:00:00|2023-03-01 09:00:00|       60.0|\n",
            "|      S002|2023-03-01 13:00:00|2023-03-01 14:00:00|2023-03-01 11:00:00|      120.0|\n",
            "+----------+-------------------+-------------------+-------------------+-----------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    service_events_df\\\n",
        "      .withColumn('prev_end_time', fn.expr(' lag(end_time,1) over(partition by service_id order by end_time asc )'))\\\n",
        "      .withColumn('gap_minutes', fn.expr(''' (unix_timestamp(start_time) - unix_timestamp(prev_end_time))/float(60) '''))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-16"
      },
      "source": [
        "**Instructor Notes:** Gap analysis with window functions. Tests time interval calculations and service continuity analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-17"
      },
      "source": [
        "## Problem 17: Complex Business Rule Engine\n",
        "\n",
        "**Requirement:** Insurance claims processing needs automated rule-based decision engine.\n",
        "\n",
        "**Scenario:** Implement complex business rules for insurance claim approval with multiple conditions and scoring.\n",
        "\n",
        "\n",
        "- Auto Approved - Claim amount ‚â§ $10,000\n",
        "\n",
        "- Manual Review Required - Claim amount between $10,001 - $20,000  \n",
        "- High Risk - Approved - Claim amount between $20,001 - $30,000\n",
        "- Exceeds Limit - Claim amount > $30,000\n",
        "- Frequent Claimant - Review** - Previous claims ‚â• 5 (regardless of amount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4158db22-cf1c-4830-d90c-ef0035c72593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+---------------+----------+--------------+\n",
            "|claim_id|claim_amount|previous_claims|claim_date|current_status|\n",
            "+--------+------------+---------------+----------+--------------+\n",
            "|   CL001|      5000.0|              2|2023-01-15|      Approved|\n",
            "|   CL002|     15000.0|              1|2023-02-20|       Pending|\n",
            "|   CL003|     25000.0|              3|2023-03-05|      Approved|\n",
            "|   CL004|     50000.0|              0|2023-03-10|      Rejected|\n",
            "|   CL005|      8000.0|              5|2023-03-15|      Approved|\n",
            "+--------+------------+---------------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "insurance_claims_data = [\n",
        "    (\"CL001\", 5000.0, 2, \"2023-01-15\", \"Approved\"),\n",
        "    (\"CL002\", 15000.0, 1, \"2023-02-20\", \"Pending\"),\n",
        "    (\"CL003\", 25000.0, 3, \"2023-03-05\", \"Approved\"),\n",
        "    (\"CL004\", 50000.0, 0, \"2023-03-10\", \"Rejected\"),\n",
        "    (\"CL005\", 8000.0, 5, \"2023-03-15\", \"Approved\")\n",
        "]\n",
        "\n",
        "insurance_claims_df = spark.createDataFrame(insurance_claims_data, [\"claim_id\", \"claim_amount\", \"previous_claims\", \"claim_date\", \"current_status\"])\n",
        "insurance_claims_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2e2f87-eca3-4ff0-edf0-2dfa20fd5df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+---------------+----------+--------------+--------------------------+\n",
            "|claim_id|claim_amount|previous_claims|claim_date|current_status|decision_reason           |\n",
            "+--------+------------+---------------+----------+--------------+--------------------------+\n",
            "|CL001   |5000.0      |2              |2023-01-15|Approved      |Auto Approved             |\n",
            "|CL002   |15000.0     |1              |2023-02-20|Pending       |Manual Review Required    |\n",
            "|CL003   |25000.0     |3              |2023-03-05|Approved      |High Risk - Approved      |\n",
            "|CL004   |50000.0     |0              |2023-03-10|Rejected      |Exceeds Limit             |\n",
            "|CL005   |8000.0      |5              |2023-03-15|Approved      |Frequent Claimant - Review|\n",
            "+--------+------------+---------------+----------+--------------+--------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"CL001\", 5000.0, 2, \"2023-01-15\", \"Approved\", \"Auto Approved\"),\n",
        "    (\"CL002\", 15000.0, 1, \"2023-02-20\", \"Pending\", \"Manual Review Required\"),\n",
        "    (\"CL003\", 25000.0, 3, \"2023-03-05\", \"Approved\", \"High Risk - Approved\"),\n",
        "    (\"CL004\", 50000.0, 0, \"2023-03-10\", \"Rejected\", \"Exceeds Limit\"),\n",
        "    (\"CL005\", 8000.0, 5, \"2023-03-15\", \"Approved\", \"Frequent Claimant - Review\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"claim_id\", \"claim_amount\", \"previous_claims\", \"claim_date\", \"current_status\", \"decision_reason\"])\n",
        "expected_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675f7d22-2e3e-426f-90d2-7d5058da4010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+---------------+----------+--------------+--------------------------+\n",
            "|claim_id|claim_amount|previous_claims|claim_date|current_status|decision_reason           |\n",
            "+--------+------------+---------------+----------+--------------+--------------------------+\n",
            "|CL001   |5000.0      |2              |2023-01-15|Approved      |Auto Approved             |\n",
            "|CL002   |15000.0     |1              |2023-02-20|Pending       |Manual Review Required    |\n",
            "|CL003   |25000.0     |3              |2023-03-05|Approved      |High Risk - Approved      |\n",
            "|CL004   |50000.0     |0              |2023-03-10|Rejected      |Exceeds Limit             |\n",
            "|CL005   |8000.0      |5              |2023-03-15|Approved      |Frequent Claimant - Review|\n",
            "+--------+------------+---------------+----------+--------------+--------------------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      insurance_claims_df\\\n",
        "        .withColumn('decision_reason', fn.when(fn.col('previous_claims') >= 5, 'Frequent Claimant - Review')\\\n",
        "                                        .when(fn.col('claim_amount') <= 10000, 'Auto Approved')\\\n",
        "                                        .when(fn.col('claim_amount').between(10001, 20000), 'Manual Review Required')\\\n",
        "                                        .when(fn.col('claim_amount').between(20001,30000), 'High Risk - Approved')\\\n",
        "                                        .when(fn.col('claim_amount') > 30000, 'Exceeds Limit')\\\n",
        "                                        .otherwise(None))\n",
        "result_df.show(truncate = False)\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-17"
      },
      "source": [
        "**Instructor Notes:** Complex business rule engine implementation. Tests multi-condition decision logic and business rule application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-18"
      },
      "source": [
        "## Problem 18: Advanced Data Partitioning Strategy\n",
        "\n",
        "**Requirement:** Big data processing needs optimized partitioning for performance.\n",
        "\n",
        "**Scenario:** Implement custom partitioning strategy for large-scale customer transaction data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67c27cb-dca2-4286-ee65-c93f198cc722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------+-----------+------+\n",
            "|transaction_id|customer_id|      date|   category|amount|\n",
            "+--------------+-----------+----------+-----------+------+\n",
            "|          T001|       C001|2023-03-01|Electronics|1000.0|\n",
            "|          T002|       C002|2023-03-01|   Clothing| 500.0|\n",
            "|          T003|       C001|2023-03-02|Electronics|1500.0|\n",
            "|          T004|       C003|2023-03-02|       Home|2000.0|\n",
            "|          T005|       C002|2023-03-03|Electronics| 800.0|\n",
            "|          T006|       C004|2023-03-03|   Clothing| 300.0|\n",
            "|          T007|       C001|2023-03-04|       Home|1200.0|\n",
            "|          T008|       C003|2023-03-04|Electronics|2500.0|\n",
            "+--------------+-----------+----------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "large_transactions_data = [\n",
        "    (\"T001\", \"C001\", \"2023-03-01\", \"Electronics\", 1000.0),\n",
        "    (\"T002\", \"C002\", \"2023-03-01\", \"Clothing\", 500.0),\n",
        "    (\"T003\", \"C001\", \"2023-03-02\", \"Electronics\", 1500.0),\n",
        "    (\"T004\", \"C003\", \"2023-03-02\", \"Home\", 2000.0),\n",
        "    (\"T005\", \"C002\", \"2023-03-03\", \"Electronics\", 800.0),\n",
        "    (\"T006\", \"C004\", \"2023-03-03\", \"Clothing\", 300.0),\n",
        "    (\"T007\", \"C001\", \"2023-03-04\", \"Home\", 1200.0),\n",
        "    (\"T008\", \"C003\", \"2023-03-04\", \"Electronics\", 2500.0)\n",
        "]\n",
        "\n",
        "large_transactions_df = spark.createDataFrame(large_transactions_data, [\"transaction_id\", \"customer_id\", \"date\", \"category\", \"amount\"])\n",
        "large_transactions_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409f9d87-12c9-4f4d-b859-f1285ff5931c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------------+\n",
            "|      date|   category|total_amount|\n",
            "+----------+-----------+------------+\n",
            "|2023-03-01|Electronics|      1000.0|\n",
            "|2023-03-01|   Clothing|       500.0|\n",
            "|2023-03-02|Electronics|      1500.0|\n",
            "|2023-03-02|       Home|      2000.0|\n",
            "|2023-03-03|Electronics|       800.0|\n",
            "|2023-03-03|   Clothing|       300.0|\n",
            "|2023-03-04|       Home|      1200.0|\n",
            "|2023-03-04|Electronics|      2500.0|\n",
            "+----------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-03-01\", \"Electronics\", 1000.0),\n",
        "    (\"2023-03-01\", \"Clothing\", 500.0),\n",
        "    (\"2023-03-02\", \"Electronics\", 1500.0),\n",
        "    (\"2023-03-02\", \"Home\", 2000.0),\n",
        "    (\"2023-03-03\", \"Electronics\", 800.0),\n",
        "    (\"2023-03-03\", \"Clothing\", 300.0),\n",
        "    (\"2023-03-04\", \"Home\", 1200.0),\n",
        "    (\"2023-03-04\", \"Electronics\", 2500.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"category\", \"total_amount\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc2537f-cab3-404f-f170-999a91b752ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------+-----------+\n",
            "|namespace|        tableName|isTemporary|\n",
            "+---------+-----------------+-----------+\n",
            "|    rahul|transaction_table|      false|\n",
            "+---------+-----------------+-----------+\n",
            "\n",
            "+--------------+---------+-------+\n",
            "|      col_name|data_type|comment|\n",
            "+--------------+---------+-------+\n",
            "|transaction_id|   string|   NULL|\n",
            "|   customer_id|   string|   NULL|\n",
            "|          date|   string|   NULL|\n",
            "|      category|   string|   NULL|\n",
            "|        amount|   double|   NULL|\n",
            "+--------------+---------+-------+\n",
            "\n",
            "+----------------------------+--------------------------------------------------------+-------+\n",
            "|col_name                    |data_type                                               |comment|\n",
            "+----------------------------+--------------------------------------------------------+-------+\n",
            "|transaction_id              |string                                                  |NULL   |\n",
            "|customer_id                 |string                                                  |NULL   |\n",
            "|date                        |string                                                  |NULL   |\n",
            "|category                    |string                                                  |NULL   |\n",
            "|amount                      |double                                                  |NULL   |\n",
            "|                            |                                                        |       |\n",
            "|# Detailed Table Information|                                                        |       |\n",
            "|Catalog                     |spark_catalog                                           |       |\n",
            "|Database                    |rahul                                                   |       |\n",
            "|Table                       |transaction_table                                       |       |\n",
            "|Created Time                |Thu Nov 20 17:29:15 UTC 2025                            |       |\n",
            "|Last Access                 |UNKNOWN                                                 |       |\n",
            "|Created By                  |Spark 3.5.1                                             |       |\n",
            "|Type                        |MANAGED                                                 |       |\n",
            "|Provider                    |parquet                                                 |       |\n",
            "|Num Buckets                 |10                                                      |       |\n",
            "|Bucket Columns              |[`date`]                                                |       |\n",
            "|Sort Columns                |[`category`]                                            |       |\n",
            "|Location                    |file:/content/spark-warehouse/rahul.db/transaction_table|       |\n",
            "+----------------------------+--------------------------------------------------------+-------+\n",
            "\n",
            "+----------+-----------+------------+\n",
            "|      date|   category|total_amount|\n",
            "+----------+-----------+------------+\n",
            "|2023-03-03|   Clothing|       300.0|\n",
            "|2023-03-03|Electronics|       800.0|\n",
            "|2023-03-02|Electronics|      1500.0|\n",
            "|2023-03-02|       Home|      2000.0|\n",
            "|2023-03-01|   Clothing|       500.0|\n",
            "|2023-03-01|Electronics|      1000.0|\n",
            "|2023-03-04|Electronics|      2500.0|\n",
            "|2023-03-04|       Home|      1200.0|\n",
            "+----------+-----------+------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "\n",
        "#approach 1\n",
        "\n",
        "spark.sql('CREATE DATABASE IF NOT EXISTS rahul')\n",
        "spark.sql('USE rahul')\n",
        "\n",
        "large_transactions_df\\\n",
        "      .write\\\n",
        "      .bucketBy(10,'date')\\\n",
        "      .sortBy('category')\\\n",
        "      .format('parquet')\\\n",
        "      .mode('overwrite')\\\n",
        "      .saveAsTable('rahul.transaction_table')\n",
        "\n",
        "spark.sql('SHOW TABLES').show()\n",
        "spark.sql('DESCRIBE rahul.transaction_table').show()\n",
        "spark.sql('DESCRIBE FORMATTED rahul.transaction_table').show(truncate = False)\n",
        "\n",
        "result_df = spark.sql(''' select date, category, sum(amount) as total_amount\n",
        "                from rahul.transaction_table\n",
        "                group by date, category ''')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-18"
      },
      "source": [
        "**Instructor Notes:** Advanced partitioning and aggregation strategy. Tests efficient data organization for large-scale processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-19"
      },
      "source": [
        "## Problem 19: Multi-Source Data Integration\n",
        "\n",
        "**Requirement:** Data warehouse needs integration of multiple source systems with conflict resolution.\n",
        "\n",
        "**Scenario:** Merge customer data from different source systems with priority-based conflict resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55984e1b-1c8d-441c-d53e-4d734a03943e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRM Customers:\n",
            "+-----------+-----------+------------------+------------+\n",
            "|customer_id|       name|             email|       phone|\n",
            "+-----------+-----------+------------------+------------+\n",
            "|       C001|   John Doe|john@old-email.com|123-456-7890|\n",
            "|       C002| Jane Smith|    jane@email.com|987-654-3210|\n",
            "|       C003|Bob Johnson|   bob@company.com|555-123-4567|\n",
            "+-----------+-----------+------------------+------------+\n",
            "\n",
            "ERP Customers:\n",
            "+-----------+-----------+------------------+------------+\n",
            "|customer_id|       name|             email|       phone|\n",
            "+-----------+-----------+------------------+------------+\n",
            "|       C001|   John Doe|john@new-email.com|123-456-7890|\n",
            "|       C002| Jane Smith|    jane@email.com|987-654-0000|\n",
            "|       C004|Alice Brown|  alice@domain.com|111-222-3333|\n",
            "+-----------+-----------+------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "\n",
        "crm_customers_data = [\n",
        "    (\"C001\", \"John Doe\", \"john@old-email.com\", \"123-456-7890\"),\n",
        "    (\"C002\", \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"bob@company.com\", \"555-123-4567\")\n",
        "]\n",
        "\n",
        "erp_customers_data = [\n",
        "    (\"C001\", \"John Doe\", \"john@new-email.com\", \"123-456-7890\"),\n",
        "    (\"C002\", \"Jane Smith\", \"jane@email.com\", \"987-654-0000\"),\n",
        "    (\"C004\", \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\")\n",
        "]\n",
        "\n",
        "crm_customers_df = spark.createDataFrame(crm_customers_data, [\"customer_id\", \"name\", \"email\", \"phone\"])\n",
        "erp_customers_df = spark.createDataFrame(erp_customers_data, [\"customer_id\", \"name\", \"email\", \"phone\"])\n",
        "\n",
        "print(\"CRM Customers:\")\n",
        "crm_customers_df.show()\n",
        "print(\"ERP Customers:\")\n",
        "erp_customers_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a2a136-1cf5-4243-d8ed-a2c596aa081b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+------------------+------------+\n",
            "|customer_id|       name|             email|       phone|\n",
            "+-----------+-----------+------------------+------------+\n",
            "|       C001|   John Doe|john@new-email.com|123-456-7890|\n",
            "|       C002| Jane Smith|    jane@email.com|987-654-0000|\n",
            "|       C003|Bob Johnson|   bob@company.com|555-123-4567|\n",
            "|       C004|Alice Brown|  alice@domain.com|111-222-3333|\n",
            "+-----------+-----------+------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John Doe\", \"john@new-email.com\", \"123-456-7890\"),\n",
        "    (\"C002\", \"Jane Smith\", \"jane@email.com\", \"987-654-0000\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"bob@company.com\", \"555-123-4567\"),\n",
        "    (\"C004\", \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"email\", \"phone\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883c98e3-5c2e-4d74-ca87-43eef767f7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+------------------+------------+\n",
            "|customer_id|       name|             email|       phone|\n",
            "+-----------+-----------+------------------+------------+\n",
            "|       C001|   John Doe|john@new-email.com|123-456-7890|\n",
            "|       C002| Jane Smith|    jane@email.com|987-654-0000|\n",
            "|       C003|Bob Johnson|   bob@company.com|555-123-4567|\n",
            "|       C004|Alice Brown|  alice@domain.com|111-222-3333|\n",
            "+-----------+-----------+------------------+------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "        erp_customers_df\\\n",
        "          .unionAll(crm_customers_df)\\\n",
        "          .dropDuplicates(['customer_id','name'])\\\n",
        "          .orderBy('customer_id')\\\n",
        "          .select(fn.col('customer_id').alias('customer_id'),\n",
        "                  fn.col('name').alias('name'),\n",
        "                  fn.col('email').alias('email'),\n",
        "                  fn.col('phone').alias('phone'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-19"
      },
      "source": [
        "**Instructor Notes:** Multi-source data integration with conflict resolution. Tests complex join logic and priority-based merging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-20"
      },
      "source": [
        "## Problem 20: Complex Hierarchical Calculations\n",
        "\n",
        "**Requirement:** Financial reporting needs hierarchical profit center calculations.\n",
        "\n",
        "**Scenario:** Calculate rolling up financial metrics across organizational hierarchy with weighted allocations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca284f36-d441-4b4f-be51-59c9a7f92106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+--------+---------+---------+\n",
            "|center_id| center_name|   level|parent_id|  revenue|\n",
            "+---------+------------+--------+---------+---------+\n",
            "|    PC001|North Region|  Region|     NULL|1000000.0|\n",
            "|    PC002| NY Division|Division|    PC001| 400000.0|\n",
            "|    PC003| NJ Division|Division|    PC001| 350000.0|\n",
            "|    PC004| CT Division|Division|    PC001| 250000.0|\n",
            "|    PC005|  NY Store A|   Store|    PC002| 150000.0|\n",
            "|    PC006|  NY Store B|   Store|    PC002| 120000.0|\n",
            "|    PC007|  NY Store C|   Store|    PC002| 130000.0|\n",
            "+---------+------------+--------+---------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "profit_centers_data = [\n",
        "    (\"PC001\", \"North Region\", \"Region\", None, 1000000.0),\n",
        "    (\"PC002\", \"NY Division\", \"Division\", \"PC001\", 400000.0),\n",
        "    (\"PC003\", \"NJ Division\", \"Division\", \"PC001\", 350000.0),\n",
        "    (\"PC004\", \"CT Division\", \"Division\", \"PC001\", 250000.0),\n",
        "    (\"PC005\", \"NY Store A\", \"Store\", \"PC002\", 150000.0),\n",
        "    (\"PC006\", \"NY Store B\", \"Store\", \"PC002\", 120000.0),\n",
        "    (\"PC007\", \"NY Store C\", \"Store\", \"PC002\", 130000.0)\n",
        "]\n",
        "\n",
        "profit_centers_df = spark.createDataFrame(profit_centers_data, [\"center_id\", \"center_name\", \"level\", \"parent_id\", \"revenue\"])\n",
        "profit_centers_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-20"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Corrected Expected Output\n",
        "expected_data = [\n",
        "    (\"PC001\", \"North Region\", \"Region\", None, 1000000.0, 1000000.0),  # Should be sum of PC002+PC003+PC004 = 400k+350k+250k = 1000k ‚úì\n",
        "    (\"PC002\", \"NY Division\", \"Division\", \"PC001\", 400000.0, 400000.0),  # Should be sum of PC005+PC006+PC007 = 150k+120k+130k = 400k ‚úì\n",
        "    (\"PC003\", \"NJ Division\", \"Division\", \"PC001\", 350000.0, 350000.0),  # No children, so same as revenue ‚úì\n",
        "    (\"PC004\", \"CT Division\", \"Division\", \"PC001\", 250000.0, 250000.0),  # No children, so same as revenue ‚úì\n",
        "    (\"PC005\", \"NY Store A\", \"Store\", \"PC002\", 150000.0, 150000.0),  # Leaf node ‚úì\n",
        "    (\"PC006\", \"NY Store B\", \"Store\", \"PC002\", 120000.0, 120000.0),  # Leaf node ‚úì\n",
        "    (\"PC007\", \"NY Store C\", \"Store\", \"PC002\", 130000.0, 130000.0)   # Leaf node ‚úì\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"center_id\", \"center_name\", \"level\", \"parent_id\", \"revenue\", \"rolled_up_revenue\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4d2df3-4fd0-4c6d-ded0-e93bd57bb17d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+--------+---------+---------+-----------------+\n",
            "|center_id| center_name|   level|parent_id|  revenue|rolled_up_revenue|\n",
            "+---------+------------+--------+---------+---------+-----------------+\n",
            "|    PC001|North Region|  Region|     NULL|1000000.0|        1000000.0|\n",
            "|    PC003| NJ Division|Division|    PC001| 350000.0|         350000.0|\n",
            "|    PC002| NY Division|Division|    PC001| 400000.0|         400000.0|\n",
            "|    PC006|  NY Store B|   Store|    PC002| 120000.0|         120000.0|\n",
            "|    PC005|  NY Store A|   Store|    PC002| 150000.0|         150000.0|\n",
            "|    PC007|  NY Store C|   Store|    PC002| 130000.0|         130000.0|\n",
            "|    PC004| CT Division|Division|    PC001| 250000.0|         250000.0|\n",
            "+---------+------------+--------+---------+---------+-----------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      profit_centers_df\\\n",
        "        .groupBy('center_id','center_name','level','parent_id')\\\n",
        "        .agg(fn.sum(fn.col('revenue')).alias('revenue'),\n",
        "            fn.sum(fn.col('revenue')).alias('rolled_up_revenue'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-20"
      },
      "source": [
        "**Instructor Notes:** Hierarchical calculations with recursive relationships. Tests complex organizational structure processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-21"
      },
      "source": [
        "## Problem 21: Advanced Time-Series Correlation\n",
        "\n",
        "**Requirement:** Financial analytics needs correlation analysis between different time-series.\n",
        "\n",
        "**Scenario:** Calculate rolling correlations between stock prices and market indicators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46edf306-eab6-49d8-827f-6decd116e6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+------------+\n",
            "|      date|symbol|price|market_index|\n",
            "+----------+------+-----+------------+\n",
            "|2023-01-01|  AAPL|150.0|      4500.0|\n",
            "|2023-01-02|  AAPL|152.0|      4520.0|\n",
            "|2023-01-03|  AAPL|151.5|      4480.0|\n",
            "|2023-01-04|  AAPL|153.0|      4550.0|\n",
            "|2023-01-05|  AAPL|154.5|      4600.0|\n",
            "|2023-01-06|  AAPL|153.5|      4580.0|\n",
            "|2023-01-07|  AAPL|155.0|      4620.0|\n",
            "|2023-01-08|  AAPL|156.0|      4650.0|\n",
            "+----------+------+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "stock_correlation_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0, 4500.0),\n",
        "    (\"2023-01-02\", \"AAPL\", 152.0, 4520.0),\n",
        "    (\"2023-01-03\", \"AAPL\", 151.5, 4480.0),\n",
        "    (\"2023-01-04\", \"AAPL\", 153.0, 4550.0),\n",
        "    (\"2023-01-05\", \"AAPL\", 154.5, 4600.0),\n",
        "    (\"2023-01-06\", \"AAPL\", 153.5, 4580.0),\n",
        "    (\"2023-01-07\", \"AAPL\", 155.0, 4620.0),\n",
        "    (\"2023-01-08\", \"AAPL\", 156.0, 4650.0)\n",
        "]\n",
        "\n",
        "stock_correlation_df = spark.createDataFrame(stock_correlation_data, [\"date\", \"symbol\", \"price\", \"market_index\"])\n",
        "stock_correlation_df = stock_correlation_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "stock_correlation_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b46f83-c6b6-49f9-e8bf-b22550531fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+------------+--------------+\n",
            "|      date|symbol|price|market_index|correlation_5d|\n",
            "+----------+------+-----+------------+--------------+\n",
            "|2023-01-01|  AAPL|150.0|      4500.0|          NULL|\n",
            "|2023-01-02|  AAPL|152.0|      4520.0|          NULL|\n",
            "|2023-01-03|  AAPL|151.5|      4480.0|          NULL|\n",
            "|2023-01-04|  AAPL|153.0|      4550.0|          0.87|\n",
            "|2023-01-05|  AAPL|154.5|      4600.0|          0.92|\n",
            "|2023-01-06|  AAPL|153.5|      4580.0|          0.89|\n",
            "|2023-01-07|  AAPL|155.0|      4620.0|          0.91|\n",
            "|2023-01-08|  AAPL|156.0|      4650.0|          0.93|\n",
            "+----------+------+-----+------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0, 4500.0, None),\n",
        "    (\"2023-01-02\", \"AAPL\", 152.0, 4520.0, None),\n",
        "    (\"2023-01-03\", \"AAPL\", 151.5, 4480.0, None),\n",
        "    (\"2023-01-04\", \"AAPL\", 153.0, 4550.0, 0.87),\n",
        "    (\"2023-01-05\", \"AAPL\", 154.5, 4600.0, 0.92),\n",
        "    (\"2023-01-06\", \"AAPL\", 153.5, 4580.0, 0.89),\n",
        "    (\"2023-01-07\", \"AAPL\", 155.0, 4620.0, 0.91),\n",
        "    (\"2023-01-08\", \"AAPL\", 156.0, 4650.0, 0.93)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\", \"market_index\", \"correlation_5d\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-21"
      },
      "outputs": [],
      "source": [
        "# # YOUR SOLUTION HERE\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-21"
      },
      "source": [
        "**Instructor Notes:** Advanced time-series correlation analysis. Tests statistical calculations and rolling window correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-22"
      },
      "source": [
        "## Problem 22: Complex Data Enrichment Pipeline\n",
        "\n",
        "**Requirement:** Customer analytics needs comprehensive data enrichment from multiple sources.\n",
        "\n",
        "**Scenario:** Enrich customer data with demographic, geographic, and behavioral attributes from external sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f99a85-771b-4487-d1d3-13f1ffd52b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Customers:\n",
            "+-----------+-----------+-----------+\n",
            "|customer_id|       name|postal_code|\n",
            "+-----------+-----------+-----------+\n",
            "|       C001|   John Doe|      10001|\n",
            "|       C002| Jane Smith|      90001|\n",
            "|       C003|Bob Johnson|      60601|\n",
            "+-----------+-----------+-----------+\n",
            "\n",
            "Demographic Data:\n",
            "+-----------+-------+--------------+---------+\n",
            "|postal_code|avg_age|marital_status|education|\n",
            "+-----------+-------+--------------+---------+\n",
            "|      10001|     35|       Married| Bachelor|\n",
            "|      90001|     28|        Single|   Master|\n",
            "|      60601|     42|       Married|      PhD|\n",
            "+-----------+-------+--------------+---------+\n",
            "\n",
            "Behavioral Data:\n",
            "+-----------+--------------+------------------+-------------+\n",
            "|customer_id|spending_level|purchase_frequency|customer_tier|\n",
            "+-----------+--------------+------------------+-------------+\n",
            "|       C001|          High|          Frequent|      Premium|\n",
            "|       C002|        Medium|        Occasional|     Standard|\n",
            "|       C003|           Low|              Rare|        Basic|\n",
            "+-----------+--------------+------------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "customers_base_data = [\n",
        "    (\"C001\", \"John Doe\", \"10001\"),\n",
        "    (\"C002\", \"Jane Smith\", \"90001\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"60601\")\n",
        "]\n",
        "\n",
        "demographic_data = [\n",
        "    (\"10001\", 35, \"Married\", \"Bachelor\"),\n",
        "    (\"90001\", 28, \"Single\", \"Master\"),\n",
        "    (\"60601\", 42, \"Married\", \"PhD\")\n",
        "]\n",
        "\n",
        "behavioral_data = [\n",
        "    (\"C001\", \"High\", \"Frequent\", \"Premium\"),\n",
        "    (\"C002\", \"Medium\", \"Occasional\", \"Standard\"),\n",
        "    (\"C003\", \"Low\", \"Rare\", \"Basic\")\n",
        "]\n",
        "\n",
        "customers_base_df = spark.createDataFrame(customers_base_data, [\"customer_id\", \"name\", \"postal_code\"])\n",
        "demographic_df = spark.createDataFrame(demographic_data, [\"postal_code\", \"avg_age\", \"marital_status\", \"education\"])\n",
        "behavioral_df = spark.createDataFrame(behavioral_data, [\"customer_id\", \"spending_level\", \"purchase_frequency\", \"customer_tier\"])\n",
        "\n",
        "print(\"Base Customers:\")\n",
        "customers_base_df.show()\n",
        "print(\"Demographic Data:\")\n",
        "demographic_df.show()\n",
        "print(\"Behavioral Data:\")\n",
        "behavioral_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4de729-d750-4e34-dc8a-cf8b80a2fd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-------+--------------+---------+--------------+------------------+-------------+\n",
            "|customer_id|       name|postal_code|avg_age|marital_status|education|spending_level|purchase_frequency|customer_tier|\n",
            "+-----------+-----------+-----------+-------+--------------+---------+--------------+------------------+-------------+\n",
            "|       C001|   John Doe|      10001|     35|       Married| Bachelor|          High|          Frequent|      Premium|\n",
            "|       C002| Jane Smith|      90001|     28|        Single|   Master|        Medium|        Occasional|     Standard|\n",
            "|       C003|Bob Johnson|      60601|     42|       Married|      PhD|           Low|              Rare|        Basic|\n",
            "+-----------+-----------+-----------+-------+--------------+---------+--------------+------------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John Doe\", \"10001\", 35, \"Married\", \"Bachelor\", \"High\", \"Frequent\", \"Premium\"),\n",
        "    (\"C002\", \"Jane Smith\", \"90001\", 28, \"Single\", \"Master\", \"Medium\", \"Occasional\", \"Standard\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"60601\", 42, \"Married\", \"PhD\", \"Low\", \"Rare\", \"Basic\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"postal_code\", \"avg_age\", \"marital_status\", \"education\", \"spending_level\", \"purchase_frequency\", \"customer_tier\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f92d281-b54f-4152-acc2-756ac1b1a77e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-------+--------------+---------+--------------+------------------+-------------+\n",
            "|customer_id|       name|postal_code|avg_age|marital_status|education|spending_level|purchase_frequency|customer_tier|\n",
            "+-----------+-----------+-----------+-------+--------------+---------+--------------+------------------+-------------+\n",
            "|       C001|   John Doe|      10001|     35|       Married| Bachelor|          High|          Frequent|      Premium|\n",
            "|       C003|Bob Johnson|      60601|     42|       Married|      PhD|           Low|              Rare|        Basic|\n",
            "|       C002| Jane Smith|      90001|     28|        Single|   Master|        Medium|        Occasional|     Standard|\n",
            "+-----------+-----------+-----------+-------+--------------+---------+--------------+------------------+-------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    customers_base_df.alias('customers')\\\n",
        "      .join(demographic_df.alias('demographics'),fn.expr(''' customers.postal_code = demographics.postal_code ''') ,'inner')\\\n",
        "      .join(behavioral_df.alias('behavioral'),fn.expr(''' behavioral.customer_id = customers.customer_id '''),'inner')\\\n",
        "      .drop(fn.col('demographics.postal_code'), fn.col('behavioral.customer_id'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-22"
      },
      "source": [
        "**Instructor Notes:** Complex data enrichment pipeline. Tests multi-source joins and comprehensive data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-23"
      },
      "source": [
        "## Problem 23: Advanced Statistical Analysis\n",
        "\n",
        "**Requirement:** Data science needs advanced statistical metrics for model feature engineering.\n",
        "\n",
        "**Scenario:** Calculate z-scores, percentiles, and other statistical measures for data normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebbbcbe-dc0a-4306-bffc-28763d2e0147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|product_id|price|\n",
            "+----------+-----+\n",
            "|      P001|150.0|\n",
            "|      P002|175.0|\n",
            "|      P003|200.0|\n",
            "|      P004|125.0|\n",
            "|      P005|225.0|\n",
            "|      P006|180.0|\n",
            "|      P007|160.0|\n",
            "|      P008|190.0|\n",
            "|      P009|210.0|\n",
            "|      P010|140.0|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "statistical_data = [\n",
        "    (\"P001\", 150.0),\n",
        "    (\"P002\", 175.0),\n",
        "    (\"P003\", 200.0),\n",
        "    (\"P004\", 125.0),\n",
        "    (\"P005\", 225.0),\n",
        "    (\"P006\", 180.0),\n",
        "    (\"P007\", 160.0),\n",
        "    (\"P008\", 190.0),\n",
        "    (\"P009\", 210.0),\n",
        "    (\"P010\", 140.0)\n",
        "]\n",
        "\n",
        "statistical_df = spark.createDataFrame(statistical_data, [\"product_id\", \"price\"])\n",
        "statistical_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f698d588-c074-46d2-95b3-f9783769b078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-------+----------+\n",
            "|product_id|price|z_score|percentile|\n",
            "+----------+-----+-------+----------+\n",
            "|      P001|150.0|  -0.82|       0.2|\n",
            "|      P002|175.0|  -0.16|       0.4|\n",
            "|      P003|200.0|   0.49|       0.6|\n",
            "|      P004|125.0|  -1.48|       0.1|\n",
            "|      P005|225.0|   1.15|       0.9|\n",
            "|      P006|180.0|    0.0|       0.5|\n",
            "|      P007|160.0|  -0.65|       0.3|\n",
            "|      P008|190.0|   0.33|       0.7|\n",
            "|      P009|210.0|   0.82|       0.8|\n",
            "|      P010|140.0|  -1.15|       0.0|\n",
            "+----------+-----+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"P001\", 150.0, -0.82, 0.2),\n",
        "    (\"P002\", 175.0, -0.16, 0.4),\n",
        "    (\"P003\", 200.0, 0.49, 0.6),\n",
        "    (\"P004\", 125.0, -1.48, 0.1),\n",
        "    (\"P005\", 225.0, 1.15, 0.9),\n",
        "    (\"P006\", 180.0, 0.0, 0.5),\n",
        "    (\"P007\", 160.0, -0.65, 0.3),\n",
        "    (\"P008\", 190.0, 0.33, 0.7),\n",
        "    (\"P009\", 210.0, 0.82, 0.8),\n",
        "    (\"P010\", 140.0, -1.15, 0.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"price\", \"z_score\", \"percentile\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ada0f8-b6a1-435a-98b8-645451284769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-------+----------+\n",
            "|product_id|price|z_score|percentile|\n",
            "+----------+-----+-------+----------+\n",
            "|      P001|150.0|   -0.8|       0.2|\n",
            "|      P002|175.0|  -0.02|       0.4|\n",
            "|      P003|200.0|   0.77|       0.8|\n",
            "|      P004|125.0|  -1.58|       0.0|\n",
            "|      P005|225.0|   1.55|       1.0|\n",
            "|      P006|180.0|   0.14|       0.6|\n",
            "|      P007|160.0|  -0.49|       0.3|\n",
            "|      P008|190.0|   0.45|       0.7|\n",
            "|      P009|210.0|   1.08|       0.9|\n",
            "|      P010|140.0|  -1.11|       0.1|\n",
            "+----------+-----+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    statistical_df\\\n",
        "      .withColumn('meanValue', fn.expr(''' avg(price) over()'''))\\\n",
        "      .withColumn('stdValue', fn.expr(''' std(price) over()'''))\\\n",
        "      .withColumn('z_score', fn.expr(''' round((price-meanValue)/cast(nullif(stdValue,0) as float),2) '''))\\\n",
        "      .withColumn('percentile', fn.expr(''' round(percent_rank() over(order by price asc),1) '''))\\\n",
        "      .drop('meanValue','stdValue')\\\n",
        "      .orderBy('product_id')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-23"
      },
      "source": [
        "**Instructor Notes:** Advanced statistical analysis with window functions. Tests z-score calculations and percentile rankings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-24"
      },
      "source": [
        "## Problem 24: Complex Data Quality Monitoring\n",
        "\n",
        "**Requirement:** Data governance needs automated data quality monitoring with trend analysis.\n",
        "\n",
        "**Scenario:** Implement data quality metrics tracking with trend analysis and alerting capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b987bc-53c1-4ca7-934c-ff238da4f553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----+\n",
            "|      date|      metric|score|\n",
            "+----------+------------+-----+\n",
            "|2023-01-01|Completeness| 95.5|\n",
            "|2023-01-02|Completeness| 96.2|\n",
            "|2023-01-03|Completeness| 94.8|\n",
            "|2023-01-04|Completeness| 97.1|\n",
            "|2023-01-05|Completeness| 93.5|\n",
            "|2023-01-01|    Accuracy| 98.0|\n",
            "|2023-01-02|    Accuracy| 97.5|\n",
            "|2023-01-03|    Accuracy| 96.8|\n",
            "|2023-01-04|    Accuracy| 98.2|\n",
            "|2023-01-05|    Accuracy| 95.9|\n",
            "+----------+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "data_quality_metrics_data = [\n",
        "    (\"2023-01-01\", \"Completeness\", 95.5),\n",
        "    (\"2023-01-02\", \"Completeness\", 96.2),\n",
        "    (\"2023-01-03\", \"Completeness\", 94.8),\n",
        "    (\"2023-01-04\", \"Completeness\", 97.1),\n",
        "    (\"2023-01-05\", \"Completeness\", 93.5),\n",
        "    (\"2023-01-01\", \"Accuracy\", 98.0),\n",
        "    (\"2023-01-02\", \"Accuracy\", 97.5),\n",
        "    (\"2023-01-03\", \"Accuracy\", 96.8),\n",
        "    (\"2023-01-04\", \"Accuracy\", 98.2),\n",
        "    (\"2023-01-05\", \"Accuracy\", 95.9)\n",
        "]\n",
        "\n",
        "data_quality_metrics_df = spark.createDataFrame(data_quality_metrics_data, [\"date\", \"metric\", \"score\"])\n",
        "data_quality_metrics_df = data_quality_metrics_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "data_quality_metrics_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a05afb8-8f82-4fa3-afc6-341b848248b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----+------------+\n",
            "|      date|      metric|score|daily_change|\n",
            "+----------+------------+-----+------------+\n",
            "|2023-01-01|Completeness| 95.5|        NULL|\n",
            "|2023-01-02|Completeness| 96.2|         0.7|\n",
            "|2023-01-03|Completeness| 94.8|        -1.4|\n",
            "|2023-01-04|Completeness| 97.1|         2.3|\n",
            "|2023-01-05|Completeness| 93.5|        -3.6|\n",
            "|2023-01-01|    Accuracy| 98.0|        NULL|\n",
            "|2023-01-02|    Accuracy| 97.5|        -0.5|\n",
            "|2023-01-03|    Accuracy| 96.8|        -0.7|\n",
            "|2023-01-04|    Accuracy| 98.2|         1.4|\n",
            "|2023-01-05|    Accuracy| 95.9|        -2.3|\n",
            "+----------+------------+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", \"Completeness\", 95.5, None),\n",
        "    (\"2023-01-02\", \"Completeness\", 96.2, 0.7),\n",
        "    (\"2023-01-03\", \"Completeness\", 94.8, -1.4),\n",
        "    (\"2023-01-04\", \"Completeness\", 97.1, 2.3),\n",
        "    (\"2023-01-05\", \"Completeness\", 93.5, -3.6),\n",
        "    (\"2023-01-01\", \"Accuracy\", 98.0, None),\n",
        "    (\"2023-01-02\", \"Accuracy\", 97.5, -0.5),\n",
        "    (\"2023-01-03\", \"Accuracy\", 96.8, -0.7),\n",
        "    (\"2023-01-04\", \"Accuracy\", 98.2, 1.4),\n",
        "    (\"2023-01-05\", \"Accuracy\", 95.9, -2.3)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"metric\", \"score\", \"daily_change\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8026e3-5602-4767-87c2-a3ebdaff40c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----+------------+\n",
            "|      date|      metric|score|daily_change|\n",
            "+----------+------------+-----+------------+\n",
            "|2023-01-01|    Accuracy| 98.0|        NULL|\n",
            "|2023-01-02|    Accuracy| 97.5|        -0.5|\n",
            "|2023-01-03|    Accuracy| 96.8|        -0.7|\n",
            "|2023-01-04|    Accuracy| 98.2|         1.4|\n",
            "|2023-01-05|    Accuracy| 95.9|        -2.3|\n",
            "|2023-01-01|Completeness| 95.5|        NULL|\n",
            "|2023-01-02|Completeness| 96.2|         0.7|\n",
            "|2023-01-03|Completeness| 94.8|        -1.4|\n",
            "|2023-01-04|Completeness| 97.1|         2.3|\n",
            "|2023-01-05|Completeness| 93.5|        -3.6|\n",
            "+----------+------------+-----+------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    data_quality_metrics_df\\\n",
        "      .withColumn('prev_day_stat', fn.expr(''' lag(score,1) over(partition by metric order by date asc) '''))\\\n",
        "      .withColumn('daily_change' , fn.expr(''' round(score - prev_day_stat,1) '''))\\\n",
        "      .drop('prev_day_stat')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-24"
      },
      "source": [
        "**Instructor Notes:** Data quality monitoring with trend analysis. Tests time-series analysis for quality metric tracking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-25"
      },
      "source": [
        "## Problem 25: Complex Business Metric Calculation\n",
        "\n",
        "**Requirement:** Executive dashboard needs complex business KPIs with multiple calculation steps.\n",
        "\n",
        "**Scenario:** Calculate customer acquisition cost, lifetime value, and return on investment metrics.\n",
        "\n",
        "* CAC (Customer Acquisition Cost) : `cac = acquisition_cost / new_customers`\n",
        "\n",
        "* CLV (Customer Lifetime Value) : `clv = customer_revenue / new_customers`\n",
        "* ROI (Return on Investment) : `roi = customer_revenue / acquisition_cost`\n",
        "* Net Profit net_profit = `customer_revenue - operating_cost`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8cefb6-de67-4b65-ca83-4e7b4a6b5100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+----------------+----------------+--------------+\n",
            "|quarter|new_customers|acquisition_cost|customer_revenue|operating_cost|\n",
            "+-------+-------------+----------------+----------------+--------------+\n",
            "|2023-Q1|         1000|         50000.0|        250000.0|        5000.0|\n",
            "|2023-Q2|         1200|         60000.0|        300000.0|        5500.0|\n",
            "|2023-Q3|         1500|         75000.0|        400000.0|        6000.0|\n",
            "|2023-Q4|         1800|         90000.0|        500000.0|        6500.0|\n",
            "+-------+-------------+----------------+----------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "business_metrics_data = [\n",
        "    (\"2023-Q1\", 1000, 50000.0, 250000.0, 5000.0),\n",
        "    (\"2023-Q2\", 1200, 60000.0, 300000.0, 5500.0),\n",
        "    (\"2023-Q3\", 1500, 75000.0, 400000.0, 6000.0),\n",
        "    (\"2023-Q4\", 1800, 90000.0, 500000.0, 6500.0)\n",
        "]\n",
        "\n",
        "business_metrics_df = spark.createDataFrame(business_metrics_data, [\"quarter\", \"new_customers\", \"acquisition_cost\", \"customer_revenue\", \"operating_cost\"])\n",
        "business_metrics_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7adea81-1a5d-435f-97b7-ecee2906d359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+----------------+----------------+--------------+----+------+----+----------+\n",
            "|quarter|new_customers|acquisition_cost|customer_revenue|operating_cost| cac|   clv| roi|net_profit|\n",
            "+-------+-------------+----------------+----------------+--------------+----+------+----+----------+\n",
            "|2023-Q1|         1000|         50000.0|        250000.0|        5000.0|50.0| 250.0| 5.0|  245000.0|\n",
            "|2023-Q2|         1200|         60000.0|        300000.0|        5500.0|50.0| 250.0| 5.0|  294500.0|\n",
            "|2023-Q3|         1500|         75000.0|        400000.0|        6000.0|50.0|266.67|5.33|  394000.0|\n",
            "|2023-Q4|         1800|         90000.0|        500000.0|        6500.0|50.0|277.78|5.56|  493500.0|\n",
            "+-------+-------------+----------------+----------------+--------------+----+------+----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-Q1\", 1000, 50000.0, 250000.0, 5000.0, 50.0, 250.0, 5.0, 245000.0),\n",
        "    (\"2023-Q2\", 1200, 60000.0, 300000.0, 5500.0, 50.0, 250.0, 5.0, 294500.0),\n",
        "    (\"2023-Q3\", 1500, 75000.0, 400000.0, 6000.0, 50.0, 266.67, 5.33, 394000.0),\n",
        "    (\"2023-Q4\", 1800, 90000.0, 500000.0, 6500.0, 50.0, 277.78, 5.56, 493500.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"quarter\", \"new_customers\", \"acquisition_cost\", \"customer_revenue\", \"operating_cost\", \"cac\", \"clv\", \"roi\", \"net_profit\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6208ca0-fce6-4c3c-e4f5-54403282467f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+----------------+----------------+--------------+----+------+----+----------+\n",
            "|quarter|new_customers|acquisition_cost|customer_revenue|operating_cost| cac|   clv| roi|net_profit|\n",
            "+-------+-------------+----------------+----------------+--------------+----+------+----+----------+\n",
            "|2023-Q1|         1000|         50000.0|        250000.0|        5000.0|50.0| 250.0| 5.0|  245000.0|\n",
            "|2023-Q2|         1200|         60000.0|        300000.0|        5500.0|50.0| 250.0| 5.0|  294500.0|\n",
            "|2023-Q3|         1500|         75000.0|        400000.0|        6000.0|50.0|266.67|5.33|  394000.0|\n",
            "|2023-Q4|         1800|         90000.0|        500000.0|        6500.0|50.0|277.78|5.56|  493500.0|\n",
            "+-------+-------------+----------------+----------------+--------------+----+------+----+----------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "  business_metrics_df \\\n",
        "    .withColumn('cac', fn.col('acquisition_cost') / fn.col('new_customers')) \\\n",
        "    .withColumn('clv', fn.round(fn.col('customer_revenue') / fn.col('new_customers'), 2)) \\\n",
        "    .withColumn('roi', fn.round(fn.col('customer_revenue') / fn.expr('nullif(acquisition_cost,0)'), 2)) \\\n",
        "    .withColumn('net_profit', fn.col('customer_revenue') - fn.col('operating_cost'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-25"
      },
      "source": [
        "**Instructor Notes:** Complex business metric calculations. Tests multi-step financial calculations and KPI derivations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-26"
      },
      "source": [
        "## Problem 26: Advanced Data Transformation Pipeline\n",
        "\n",
        "**Requirement:** ETL pipeline needs complex multi-stage data transformation with error handling.\n",
        "\n",
        "**Scenario:** Implement a robust ETL pipeline with data validation, transformation, and error logging.\n",
        "\n",
        "### Data Transformation Rules\n",
        "\n",
        "## 1. Customer Name Formatting\n",
        "- Convert to proper case (capitalize first letter of each word)\n",
        "- Remove any leading/trailing whitespace\n",
        "\n",
        "## 2. Amount Validation and Conversion\n",
        "- Convert string amount to numeric/decimal type\n",
        "- If amount contains non-numeric values (\"invalid\"), set to NULL\n",
        "- Handle decimal values properly\n",
        "\n",
        "## 3. Status Assignment\n",
        "- Assign \"Success\" status for records with valid numeric amounts\n",
        "- Assign \"Invalid Amount\" status for records with non-numeric amount values\n",
        "\n",
        "## 4. Data Preservation\n",
        "- Keep original customer_id unchanged\n",
        "- Keep original date field unchanged\n",
        "- Preserve all records (no filtering out invalid records)\n",
        "\n",
        "**Note**: The transformation maintains all source records while flagging data quality issues and standardizing formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b20de69-cd18-4d50-b3d4-0990e42884c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------+----------+\n",
            "|customer_id|customer_name| amount|      date|\n",
            "+-----------+-------------+-------+----------+\n",
            "|       C001|   john doe  |1000.50|2023-01-15|\n",
            "|       C002|   Jane Smith|invalid|2023-01-16|\n",
            "|       C003|  bob johnson|1200.25|2023-01-17|\n",
            "|       C004|  Alice Brown| 950.00|2023-01-18|\n",
            "+-----------+-------------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "etl_source_data = [\n",
        "    (\"C001\", \"john doe  \", \"1000.50\", \"2023-01-15\"),\n",
        "    (\"C002\", \"Jane Smith\", \"invalid\", \"2023-01-16\"),\n",
        "    (\"C003\", \"bob johnson\", \"1200.25\", \"2023-01-17\"),\n",
        "    (\"C004\", \"Alice Brown\", \"950.00\", \"2023-01-18\")\n",
        "]\n",
        "\n",
        "etl_source_df = spark.createDataFrame(etl_source_data, [\"customer_id\", \"customer_name\", \"amount\", \"date\"])\n",
        "etl_source_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e94b76c-c4b4-4234-d6d4-b7e0574cfe80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------+----------+--------------+\n",
            "|customer_id|customer_name| amount|      date|        status|\n",
            "+-----------+-------------+-------+----------+--------------+\n",
            "|       C001|     John Doe| 1000.5|2023-01-15|       Success|\n",
            "|       C002|   Jane Smith|   NULL|2023-01-16|Invalid Amount|\n",
            "|       C003|  Bob Johnson|1200.25|2023-01-17|       Success|\n",
            "|       C004|  Alice Brown|  950.0|2023-01-18|       Success|\n",
            "+-----------+-------------+-------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John Doe\", 1000.5, \"2023-01-15\", \"Success\"),\n",
        "    (\"C002\", \"Jane Smith\", None, \"2023-01-16\", \"Invalid Amount\"),\n",
        "    (\"C003\", \"Bob Johnson\", 1200.25, \"2023-01-17\", \"Success\"),\n",
        "    (\"C004\", \"Alice Brown\", 950.0, \"2023-01-18\", \"Success\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"amount\", \"date\", \"status\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c9407b-5342-4707-b52e-9a2efd3ffbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------+----------+--------------+\n",
            "|customer_id|customer_name| amount|      date|        status|\n",
            "+-----------+-------------+-------+----------+--------------+\n",
            "|       C001|     John Doe| 1000.5|2023-01-15|       Success|\n",
            "|       C002|   Jane Smith|   NULL|2023-01-16|Invalid Amount|\n",
            "|       C003|  Bob Johnson|1200.25|2023-01-17|       Success|\n",
            "|       C004|  Alice Brown|  950.0|2023-01-18|       Success|\n",
            "+-----------+-------------+-------+----------+--------------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "        etl_source_df\\\n",
        "          .withColumn('customer_name', fn.expr(''' initcap(trim(customer_name)) '''))\\\n",
        "          .withColumn('amount', fn.expr(''' try_cast(amount as float) '''))\\\n",
        "          .withColumn('status', fn.expr(''' nvl2(amount, 'Success','Invalid Amount')'''))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-26"
      },
      "source": [
        "**Instructor Notes:** Robust ETL pipeline with error handling. Tests data validation, transformation, and error management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-27"
      },
      "source": [
        "## Problem 27: Complex Join Optimization\n",
        "\n",
        "**Requirement:** Performance tuning needs optimized join strategies for large datasets.\n",
        "\n",
        "**Scenario:** Implement efficient join strategies for large customer and transaction datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d597a73-9276-4778-e500-cd1851a9d0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers:\n",
            "+-----------+-------------+\n",
            "|customer_id|customer_name|\n",
            "+-----------+-------------+\n",
            "|       C001|     John Doe|\n",
            "|       C002|   Jane Smith|\n",
            "|       C003|  Bob Johnson|\n",
            "|       C004|  Alice Brown|\n",
            "+-----------+-------------+\n",
            "\n",
            "Transactions:\n",
            "+--------------+-----------+------+\n",
            "|transaction_id|customer_id|amount|\n",
            "+--------------+-----------+------+\n",
            "|          T001|       C001| 100.0|\n",
            "|          T002|       C001| 150.0|\n",
            "|          T003|       C002| 200.0|\n",
            "|          T004|       C003|  75.0|\n",
            "|          T005|       C004| 300.0|\n",
            "|          T006|       C001| 125.0|\n",
            "+--------------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "customers_large_data = [\n",
        "    (\"C001\", \"John Doe\"),\n",
        "    (\"C002\", \"Jane Smith\"),\n",
        "    (\"C003\", \"Bob Johnson\"),\n",
        "    (\"C004\", \"Alice Brown\")\n",
        "]\n",
        "\n",
        "transactions_large_data = [\n",
        "    (\"T001\", \"C001\", 100.0),\n",
        "    (\"T002\", \"C001\", 150.0),\n",
        "    (\"T003\", \"C002\", 200.0),\n",
        "    (\"T004\", \"C003\", 75.0),\n",
        "    (\"T005\", \"C004\", 300.0),\n",
        "    (\"T006\", \"C001\", 125.0)\n",
        "]\n",
        "\n",
        "customers_large_df = spark.createDataFrame(customers_large_data, [\"customer_id\", \"customer_name\"])\n",
        "transactions_large_df = spark.createDataFrame(transactions_large_data, [\"transaction_id\", \"customer_id\", \"amount\"])\n",
        "\n",
        "print(\"Customers:\")\n",
        "customers_large_df.show()\n",
        "print(\"Transactions:\")\n",
        "transactions_large_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b668b302-cb52-48f6-fbc7-d0b03f1155b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------------+------------+\n",
            "|customer_id|customer_name|transaction_count|total_amount|\n",
            "+-----------+-------------+-----------------+------------+\n",
            "|       C001|     John Doe|                3|       375.0|\n",
            "|       C002|   Jane Smith|                1|       200.0|\n",
            "|       C003|  Bob Johnson|                1|        75.0|\n",
            "|       C004|  Alice Brown|                1|       300.0|\n",
            "+-----------+-------------+-----------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John Doe\", 3, 375.0),\n",
        "    (\"C002\", \"Jane Smith\", 1, 200.0),\n",
        "    (\"C003\", \"Bob Johnson\", 1, 75.0),\n",
        "    (\"C004\", \"Alice Brown\", 1, 300.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"transaction_count\", \"total_amount\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0c71f5-7ede-4ac9-f32b-ee74628a63e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------------+------------+\n",
            "|customer_id|customer_name|transaction_count|total_amount|\n",
            "+-----------+-------------+-----------------+------------+\n",
            "|       C001|     John Doe|                3|       375.0|\n",
            "|       C002|   Jane Smith|                1|       200.0|\n",
            "|       C003|  Bob Johnson|                1|        75.0|\n",
            "|       C004|  Alice Brown|                1|       300.0|\n",
            "+-----------+-------------+-----------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "spark.conf.set('spark.sql.adaptive.enabled','true')\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "\n",
        "cust_partitioned = customers_large_df.repartition('customer_id').alias('cust')\n",
        "trans_partitioned = transactions_large_df.repartition('customer_id').alias('trans')\n",
        "\n",
        "join_on = fn.expr('''cust.customer_id = trans.customer_id ''')\n",
        "\n",
        "merge_partitioned = cust_partitioned\\\n",
        "                          .join(trans_partitioned,join_on,'inner')\\\n",
        "                          .drop(fn.col('trans.customer_id'))\n",
        "result_df = \\\n",
        "      merge_partitioned\\\n",
        "      .groupBy('cust.customer_id','cust.customer_name')\\\n",
        "      .agg(fn.expr(''' count(trans.transaction_id) as transaction_count '''),\n",
        "           fn.expr(''' sum(trans.amount) as total_amount '''))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-27"
      },
      "source": [
        "**Instructor Notes:** Join optimization strategies. Tests efficient aggregation and join patterns for large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-28"
      },
      "source": [
        "## Problem 28: Complex Window Function Patterns\n",
        "\n",
        "**Requirement:** Advanced analytics needs complex window function patterns for time-series analysis.\n",
        "\n",
        "**Scenario:** Implement advanced window function patterns for financial time-series analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448955ba-d5fb-4b6d-9d95-4f4557d01ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+\n",
            "|      date|symbol|price|\n",
            "+----------+------+-----+\n",
            "|2023-01-01|  AAPL|150.0|\n",
            "|2023-01-02|  AAPL|152.0|\n",
            "|2023-01-03|  AAPL|151.5|\n",
            "|2023-01-04|  AAPL|153.0|\n",
            "|2023-01-05|  AAPL|154.5|\n",
            "|2023-01-06|  AAPL|153.5|\n",
            "|2023-01-07|  AAPL|155.0|\n",
            "|2023-01-08|  AAPL|156.0|\n",
            "+----------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "financial_series_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0),\n",
        "    (\"2023-01-02\", \"AAPL\", 152.0),\n",
        "    (\"2023-01-03\", \"AAPL\", 151.5),\n",
        "    (\"2023-01-04\", \"AAPL\", 153.0),\n",
        "    (\"2023-01-05\", \"AAPL\", 154.5),\n",
        "    (\"2023-01-06\", \"AAPL\", 153.5),\n",
        "    (\"2023-01-07\", \"AAPL\", 155.0),\n",
        "    (\"2023-01-08\", \"AAPL\", 156.0)\n",
        "]\n",
        "\n",
        "financial_series_df = spark.createDataFrame(financial_series_data, [\"date\", \"symbol\", \"price\"])\n",
        "financial_series_df = financial_series_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "financial_series_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e741ae9b-cb53-49ea-9cd0-1d5f9ee6eb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----------+------------+----------+\n",
            "|      date|symbol|price|prev_price|price_change|pct_change|\n",
            "+----------+------+-----+----------+------------+----------+\n",
            "|2023-01-01|  AAPL|150.0|      NULL|        NULL|      NULL|\n",
            "|2023-01-02|  AAPL|152.0|     150.0|         2.0|      1.32|\n",
            "|2023-01-03|  AAPL|151.5|     152.0|        -0.5|     -0.33|\n",
            "|2023-01-04|  AAPL|153.0|     151.5|         1.5|      0.98|\n",
            "|2023-01-05|  AAPL|154.5|     153.0|         1.5|      0.97|\n",
            "|2023-01-06|  AAPL|153.5|     154.5|        -1.0|     -0.65|\n",
            "|2023-01-07|  AAPL|155.0|     153.5|         1.5|      0.97|\n",
            "|2023-01-08|  AAPL|156.0|     155.0|         1.0|      0.64|\n",
            "+----------+------+-----+----------+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output - Modified to match your actual calculation (truncation to 2 decimal places)\n",
        "\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0, None, None, None),\n",
        "    (\"2023-01-02\", \"AAPL\", 152.0, 150.0, 2.0, 1.32),    # (2.0/150.0)*100 = 1.333... ‚Üí truncated to 1.32\n",
        "    (\"2023-01-03\", \"AAPL\", 151.5, 152.0, -0.5, -0.33),  # (-0.5/152.0)*100 = -0.3289... ‚Üí truncated to -0.33\n",
        "    (\"2023-01-04\", \"AAPL\", 153.0, 151.5, 1.5, 0.98),    # (1.5/151.5)*100 = 0.9900... ‚Üí truncated to 0.98\n",
        "    (\"2023-01-05\", \"AAPL\", 154.5, 153.0, 1.5, 0.97),    # (1.5/153.0)*100 = 0.9803... ‚Üí truncated to 0.97\n",
        "    (\"2023-01-06\", \"AAPL\", 153.5, 154.5, -1.0, -0.65),  # (-1.0/154.5)*100 = -0.6472... ‚Üí truncated to -0.65\n",
        "    (\"2023-01-07\", \"AAPL\", 155.0, 153.5, 1.5, 0.97),    # (1.5/153.5)*100 = 0.9771... ‚Üí truncated to 0.97\n",
        "    (\"2023-01-08\", \"AAPL\", 156.0, 155.0, 1.0, 0.64)     # (1.0/155.0)*100 = 0.6451... ‚Üí truncated to 0.64\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\", \"prev_price\", \"price_change\", \"pct_change\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a53eab-657c-4829-9d08-94bf6d79c66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----------+------------+----------+\n",
            "|      date|symbol|price|prev_price|price_change|pct_change|\n",
            "+----------+------+-----+----------+------------+----------+\n",
            "|2023-01-01|  AAPL|150.0|      NULL|        NULL|      NULL|\n",
            "|2023-01-02|  AAPL|152.0|     150.0|         2.0|      1.32|\n",
            "|2023-01-03|  AAPL|151.5|     152.0|        -0.5|     -0.33|\n",
            "|2023-01-04|  AAPL|153.0|     151.5|         1.5|      0.98|\n",
            "|2023-01-05|  AAPL|154.5|     153.0|         1.5|      0.97|\n",
            "|2023-01-06|  AAPL|153.5|     154.5|        -1.0|     -0.65|\n",
            "|2023-01-07|  AAPL|155.0|     153.5|         1.5|      0.97|\n",
            "|2023-01-08|  AAPL|156.0|     155.0|         1.0|      0.64|\n",
            "+----------+------+-----+----------+------------+----------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    financial_series_df\\\n",
        "      .withColumn('prev_price',fn.expr(''' lag(price,1) over(partition by symbol order by date asc) '''))\\\n",
        "      .withColumn('price_change', fn.expr(''' round(price - prev_price,1)  '''))\\\n",
        "      .withColumn('pct_change', fn.expr(''' round(price_change*100/price,2) '''))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-28"
      },
      "source": [
        "**Instructor Notes:** Advanced window function patterns. Tests financial calculations and time-series analysis techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-29"
      },
      "source": [
        "## Problem 29: Complex Data Aggregation Strategy\n",
        "\n",
        "**Requirement:** Business reporting needs complex multi-level aggregation with custom logic.\n",
        "\n",
        "**Scenario:** Implement custom aggregation logic for hierarchical business reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267e3397-43f2-45c8-fbce-9f9407468cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------+--------+\n",
            "|  region|  division|  department| revenue|\n",
            "+--------+----------+------------+--------+\n",
            "|Region_A|Division_1|Department_X|100000.0|\n",
            "|Region_A|Division_1|Department_Y|150000.0|\n",
            "|Region_A|Division_2|Department_Z|200000.0|\n",
            "|Region_B|Division_3|Department_W|120000.0|\n",
            "|Region_B|Division_3|Department_V|180000.0|\n",
            "|Region_B|Division_4|Department_U|220000.0|\n",
            "+--------+----------+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "business_aggregation_data = [\n",
        "    (\"Region_A\", \"Division_1\", \"Department_X\", 100000.0),\n",
        "    (\"Region_A\", \"Division_1\", \"Department_Y\", 150000.0),\n",
        "    (\"Region_A\", \"Division_2\", \"Department_Z\", 200000.0),\n",
        "    (\"Region_B\", \"Division_3\", \"Department_W\", 120000.0),\n",
        "    (\"Region_B\", \"Division_3\", \"Department_V\", 180000.0),\n",
        "    (\"Region_B\", \"Division_4\", \"Department_U\", 220000.0)\n",
        "]\n",
        "\n",
        "business_aggregation_df = spark.createDataFrame(business_aggregation_data, [\"region\", \"division\", \"department\", \"revenue\"])\n",
        "business_aggregation_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8d813d-7c51-46d7-9f0c-40ea48794087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------+--------+\n",
            "|  region|  division|  department| revenue|\n",
            "+--------+----------+------------+--------+\n",
            "|Region_A|Division_1|Department_X|100000.0|\n",
            "|Region_A|Division_1|Department_Y|150000.0|\n",
            "|Region_A|Division_1|         All|250000.0|\n",
            "|Region_A|Division_2|Department_Z|200000.0|\n",
            "|Region_A|Division_2|         All|200000.0|\n",
            "|Region_A|       All|         All|450000.0|\n",
            "|Region_B|Division_3|Department_W|120000.0|\n",
            "|Region_B|Division_3|Department_V|180000.0|\n",
            "|Region_B|Division_3|         All|300000.0|\n",
            "|Region_B|Division_4|Department_U|220000.0|\n",
            "|Region_B|Division_4|         All|220000.0|\n",
            "|Region_B|       All|         All|520000.0|\n",
            "|     All|       All|         All|970000.0|\n",
            "+--------+----------+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Region_A\", \"Division_1\", \"Department_X\", 100000.0),\n",
        "    (\"Region_A\", \"Division_1\", \"Department_Y\", 150000.0),\n",
        "    (\"Region_A\", \"Division_1\", \"All\", 250000.0),\n",
        "    (\"Region_A\", \"Division_2\", \"Department_Z\", 200000.0),\n",
        "    (\"Region_A\", \"Division_2\", \"All\", 200000.0),\n",
        "    (\"Region_A\", \"All\", \"All\", 450000.0),\n",
        "    (\"Region_B\", \"Division_3\", \"Department_W\", 120000.0),\n",
        "    (\"Region_B\", \"Division_3\", \"Department_V\", 180000.0),\n",
        "    (\"Region_B\", \"Division_3\", \"All\", 300000.0),\n",
        "    (\"Region_B\", \"Division_4\", \"Department_U\", 220000.0),\n",
        "    (\"Region_B\", \"Division_4\", \"All\", 220000.0),\n",
        "    (\"Region_B\", \"All\", \"All\", 520000.0),\n",
        "    (\"All\", \"All\", \"All\", 970000.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"region\", \"division\", \"department\", \"revenue\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998cb56c-5cf7-4730-d8a1-cc1416fade35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------+--------+\n",
            "|  region|  division|  department| revenue|\n",
            "+--------+----------+------------+--------+\n",
            "|     All|       All|         All|970000.0|\n",
            "|Region_A|Division_2|         All|200000.0|\n",
            "|Region_A|Division_1|         All|250000.0|\n",
            "|Region_A|Division_1|Department_X|100000.0|\n",
            "|Region_A|Division_2|Department_Z|200000.0|\n",
            "|Region_A|       All|         All|450000.0|\n",
            "|Region_A|Division_1|Department_Y|150000.0|\n",
            "|Region_B|Division_4|Department_U|220000.0|\n",
            "|Region_B|       All|         All|520000.0|\n",
            "|Region_B|Division_3|         All|300000.0|\n",
            "|Region_B|Division_3|Department_W|120000.0|\n",
            "|Region_B|Division_4|         All|220000.0|\n",
            "|Region_B|Division_3|Department_V|180000.0|\n",
            "+--------+----------+------------+--------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    business_aggregation_df\\\n",
        "      .rollup('region','division','department')\\\n",
        "      .agg(fn.expr('sum(revenue) as revenue'))\\\n",
        "      .fillna('All', subset = ['region','division','department'])\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-29"
      },
      "source": [
        "**Instructor Notes:** Complex multi-level aggregation. Tests hierarchical rollup operations and custom aggregation logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-30"
      },
      "source": [
        "## Problem 30: Advanced Performance Optimization\n",
        "\n",
        "**Requirement:** Large-scale data processing needs advanced performance optimization techniques.\n",
        "\n",
        "**Scenario:** Implement performance optimization strategies for complex data processing pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de4c888-e73c-4c8f-e677-842d97821bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------+------+\n",
            "|product_id|   category|region| sales|\n",
            "+----------+-----------+------+------+\n",
            "|      P001|Electronics| North|1000.0|\n",
            "|      P001|Electronics| South|1500.0|\n",
            "|      P002|   Clothing| North| 800.0|\n",
            "|      P002|   Clothing| South|1200.0|\n",
            "|      P003|       Home| North|2000.0|\n",
            "|      P003|       Home| South|1800.0|\n",
            "|      P004|Electronics| North| 900.0|\n",
            "|      P004|Electronics| South|1100.0|\n",
            "+----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "performance_data = [\n",
        "    (\"P001\", \"Electronics\", \"North\", 1000.0),\n",
        "    (\"P001\", \"Electronics\", \"South\", 1500.0),\n",
        "    (\"P002\", \"Clothing\", \"North\", 800.0),\n",
        "    (\"P002\", \"Clothing\", \"South\", 1200.0),\n",
        "    (\"P003\", \"Home\", \"North\", 2000.0),\n",
        "    (\"P003\", \"Home\", \"South\", 1800.0),\n",
        "    (\"P004\", \"Electronics\", \"North\", 900.0),\n",
        "    (\"P004\", \"Electronics\", \"South\", 1100.0)\n",
        "]\n",
        "\n",
        "performance_df = spark.createDataFrame(performance_data, [\"product_id\", \"category\", \"region\", \"sales\"])\n",
        "performance_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06419da-2b09-49b0-f7ff-d1813eda0c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+\n",
            "|   category|total_sales|north_sales|south_sales|\n",
            "+-----------+-----------+-----------+-----------+\n",
            "|Electronics|     4500.0|     1900.0|     2600.0|\n",
            "|   Clothing|     2000.0|      800.0|     1200.0|\n",
            "|       Home|     3800.0|     2000.0|     1800.0|\n",
            "+-----------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Electronics\", 4500.0, 1900.0, 2600.0),\n",
        "    (\"Clothing\", 2000.0, 800.0, 1200.0),\n",
        "    (\"Home\", 3800.0, 2000.0, 1800.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"category\", \"total_sales\", \"north_sales\", \"south_sales\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623310a1-47e5-45c8-ec86-0841b73a9e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+\n",
            "|   category|total_sales|north_sales|south_sales|\n",
            "+-----------+-----------+-----------+-----------+\n",
            "|       Home|     3800.0|     2000.0|     1800.0|\n",
            "|   Clothing|     2000.0|      800.0|     1200.0|\n",
            "|Electronics|     4500.0|     1900.0|     2600.0|\n",
            "+-----------+-----------+-----------+-----------+\n",
            "\n",
            "‚úì DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "spark.conf.set('spark.sql.adaptive.enabled', 'true')\n",
        "spark.conf.set('spark.sql.adaptive.coalescePartitions.enabled', 'true')\n",
        "spark.conf.set('spark.sql.adaptive.skewJoin.enabled', 'true')\n",
        "spark.conf.set('spark.sql.adaptive.coalescePartitions.initialPartitionNum', '1000')\n",
        "spark.conf.set('spark.sql.adaptive.advisoryPartitionSizeInBytes', '134217728')  # 128MB\n",
        "spark.conf.set('spark.sql.adaptive.skewedPartitionFactor', '5')\n",
        "spark.conf.set('spark.sql.adaptive.skewedPartitionMaxSplits', '5')\n",
        "\n",
        "spark.conf.set('spark.sql.adaptive.localShuffleReader.enabled', 'true')\n",
        "spark.conf.set('spark.sql.shuffle.partitions', '200')\n",
        "\n",
        "result_df = (\n",
        "    performance_df\n",
        "    .withColumn('pivot_col', fn.concat(fn.lower('region'), fn.lit('_sales')))\n",
        "    .groupBy('category')\n",
        "    .pivot('pivot_col')\n",
        "    .agg(fn.sum('sales').alias('sales'))\n",
        "    .withColumn('total_sales', fn.col('north_sales') + fn.col('south_sales'))\n",
        "    .select('category', 'total_sales', 'north_sales', 'south_sales')\n",
        ")\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-30"
      },
      "source": [
        "**Instructor Notes:** Advanced performance optimization. Tests efficient aggregation patterns and data processing strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "completion-note"
      },
      "source": [
        "# Set 3 Complete!\n",
        "\n",
        "You've completed all 30 Medium problems in Set 3. These problems cover:\n",
        "- Complex joins and relationship analysis\n",
        "- Advanced window functions and analytics\n",
        "- Multi-level aggregations and rollups\n",
        "- Complex UDFs and data transformations\n",
        "- Performance optimization and partitioning\n",
        "- Statistical analysis and business metrics\n",
        "- Data quality monitoring and validation\n",
        "\n",
        "Ready for Set 4 with Medium/Hard difficulty problems?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}