{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview-section"
   },
   "source": [
    "# PySpark Interview Preparation - Set 5 (Hard)\n\n## Overview & Instructions\n\n### How to run this notebook in Google Colab:\n1. Upload this .ipynb file to Google Colab\n2. Run the installation cells below\n3. Execute each problem cell sequentially\n\n### Installation Commands:\nThe following cell installs Java and PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "installation-cell"
   },
   "outputs": [],
   "source": [
    "# Install Java and PySpark\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n!pip install -q pyspark\n\nimport os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sparksession-section"
   },
   "source": [
    "### SparkSession Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sparksession-cell"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\n\nspark = SparkSession.builder\\\n    .appName(\"PySparkInterviewSet5\")\\\n    .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n    .getOrCreate()\n\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assert-function-section"
   },
   "source": [
    "### DataFrame Assertion Function:\n\nThis function compares DataFrames ignoring order and with floating-point tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "assert-function-cell"
   },
   "outputs": [],
   "source": [
    "def assert_dataframe_equal(df_actual, df_expected, epsilon=1e-6):\n    \"\"\"Compare two DataFrames ignoring order and with floating-point tolerance\"\"\"\n    \n    # Check schema first\n    if df_actual.schema != df_expected.schema:\n        print(\"Schema mismatch!\")\n        print(\"Actual schema:\", df_actual.schema)\n        print(\"Expected schema:\", df_expected.schema)\n        raise AssertionError(\"Schema mismatch\")\n    \n    # Collect data\n    actual_data = df_actual.collect()\n    expected_data = df_expected.collect()\n    \n    if len(actual_data) != len(expected_data):\n        print(f\"Row count mismatch! Actual: {len(actual_data)}, Expected: {len(expected_data)}\")\n        raise AssertionError(\"Row count mismatch\")\n    \n    # Convert to sets of tuples for order-insensitive comparison\n    def row_to_comparable(row):\n        values = []\n        for field in row:\n            if isinstance(field, float):\n                # Handle floating point comparison\n                values.append(round(field / epsilon) * epsilon)\n            elif isinstance(field, list):\n                # Handle arrays\n                values.append(tuple(sorted(field)) if field else tuple())\n            elif isinstance(field, dict):\n                # Handle structs\n                values.append(tuple(sorted(field.items())))\n            else:\n                values.append(field)\n        return tuple(values)\n    \n    actual_set = set(row_to_comparable(row) for row in actual_data)\n    expected_set = set(row_to_comparable(row) for row in expected_data)\n    \n    if actual_set != expected_set:\n        print(\"Data mismatch!\")\n        print(\"Actual data:\", actual_set)\n        print(\"Expected data:\", expected_set)\n        raise AssertionError(\"Data content mismatch\")\n    \n    print(\"\u2713 DataFrames are equal!\")\n    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toc-section"
   },
   "source": [
    "## Table of Contents - Set 5 (Hard)\n\n**Difficulty Distribution:** 30 Hard Problems\n\n**Topics Covered:**\n- Advanced Joins & Complex Deduplication (9 problems)\n- Sophisticated Window Functions (4 problems)\n- Multi-level Aggregations & OLAP (3 problems)\n- Advanced Pandas UDFs & Performance (3 problems)\n- Production File Format Handling (8 problems)\n- Complex Nested Data Structures (7 problems)\n- Performance & Optimization (2 problems)\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-1"
   },
   "source": [
    "## Problem 1: Customer Churn Prediction Features\n\n**Requirement:** Analytics team needs features for customer churn prediction model.\n\n**Scenario:** Calculate customer engagement metrics: purchase frequency, recency, and monetary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-1"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ncustomer_engagement_data = [\n    (\"C001\", \"2023-01-15\", 100.0),\n    (\"C001\", \"2023-02-10\", 150.0),\n    (\"C001\", \"2023-03-05\", 200.0),\n    (\"C002\", \"2023-01-20\", 300.0),\n    (\"C002\", \"2023-03-15\", 250.0),\n    (\"C003\", \"2023-02-01\", 500.0),\n    (\"C004\", \"2023-01-05\", 150.0),\n    (\"C004\", \"2023-01-25\", 175.0),\n    (\"C004\", \"2023-02-20\", 200.0),\n    (\"C004\", \"2023-03-10\", 225.0)\n]\n\ncustomer_engagement_df = spark.createDataFrame(customer_engagement_data, [\"customer_id\", \"order_date\", \"amount\"])\ncustomer_engagement_df = customer_engagement_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\ncustomer_engagement_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-1"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"C004\", 4, 750.0, 187.5, 64),\n    (\"C001\", 3, 450.0, 150.0, 54),\n    (\"C002\", 2, 550.0, 275.0, 54),\n    (\"C003\", 1, 500.0, 500.0, 37)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"frequency\", \"monetary\", \"avg_order_value\", \"recency_days\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-1"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-1"
   },
   "source": [
    "**Instructor Notes:** RFM analysis implementation. Tests date calculations and multi-metric aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-2"
   },
   "source": [
    "## Problem 2: Inventory Stock Analysis\n\n**Requirement:** Supply chain needs current stock levels with lead time calculations.\n\n**Scenario:** Calculate current inventory levels considering incoming and outgoing shipments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-2"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\ninventory_data = [\n    (\"P001\", \"Laptop\", 50),\n    (\"P002\", \"Mouse\", 100),\n    (\"P003\", \"Keyboard\", 75)\n]\n\nincoming_shipments_data = [\n    (\"S001\", \"P001\", \"2023-03-01\", 20),\n    (\"S002\", \"P002\", \"2023-03-02\", 50),\n    (\"S003\", \"P001\", \"2023-03-03\", 10)\n]\n\noutgoing_orders_data = [\n    (\"O001\", \"P001\", \"2023-03-01\", 15),\n    (\"O002\", \"P002\", \"2023-03-02\", 30),\n    (\"O003\", \"P001\", \"2023-03-03\", 25),\n    (\"O004\", \"P003\", \"2023-03-03\", 20)\n]\n\ninventory_df = spark.createDataFrame(inventory_data, [\"product_id\", \"product_name\", \"current_stock\"])\nincoming_df = spark.createDataFrame(incoming_shipments_data, [\"shipment_id\", \"product_id\", \"arrival_date\", \"quantity\"])\noutgoing_df = spark.createDataFrame(outgoing_orders_data, [\"order_id\", \"product_id\", \"order_date\", \"quantity\"])\n\nprint(\"Inventory:\")\ninventory_df.show()\nprint(\"Incoming Shipments:\")\nincoming_df.show()\nprint(\"Outgoing Orders:\")\noutgoing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-2"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"P001\", \"Laptop\", 50, 30, 40, 40),\n    (\"P002\", \"Mouse\", 100, 50, 30, 120),\n    (\"P003\", \"Keyboard\", 75, 0, 20, 55)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"current_stock\", \"incoming_qty\", \"outgoing_qty\", \"projected_stock\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-2"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-2"
   },
   "source": [
    "**Instructor Notes:** Multi-table aggregation with conditional sums. Tests complex join scenarios with multiple data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-3"
   },
   "source": [
    "## Problem 3: Employee Attendance Pattern Analysis\n\n**Requirement:** HR needs to analyze employee attendance patterns for workforce planning.\n\n**Scenario:** Calculate consecutive work days and identify attendance patterns using window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-3"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nattendance_data = [\n    (\"E001\", \"2023-03-01\", \"Present\"),\n    (\"E001\", \"2023-03-02\", \"Present\"),\n    (\"E001\", \"2023-03-03\", \"Absent\"),\n    (\"E001\", \"2023-03-04\", \"Present\"),\n    (\"E001\", \"2023-03-05\", \"Present\"),\n    (\"E001\", \"2023-03-06\", \"Present\"),\n    (\"E002\", \"2023-03-01\", \"Present\"),\n    (\"E002\", \"2023-03-02\", \"Present\"),\n    (\"E002\", \"2023-03-03\", \"Present\"),\n    (\"E002\", \"2023-03-04\", \"Absent\"),\n    (\"E002\", \"2023-03-05\", \"Present\")\n]\n\nattendance_df = spark.createDataFrame(attendance_data, [\"employee_id\", \"date\", \"status\"])\nattendance_df = attendance_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nattendance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-3"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"E001\", \"2023-03-01\", \"Present\", 1),\n    (\"E001\", \"2023-03-02\", \"Present\", 2),\n    (\"E001\", \"2023-03-03\", \"Absent\", 0),\n    (\"E001\", \"2023-03-04\", \"Present\", 1),\n    (\"E001\", \"2023-03-05\", \"Present\", 2),\n    (\"E001\", \"2023-03-06\", \"Present\", 3),\n    (\"E002\", \"2023-03-01\", \"Present\", 1),\n    (\"E002\", \"2023-03-02\", \"Present\", 2),\n    (\"E002\", \"2023-03-03\", \"Present\", 3),\n    (\"E002\", \"2023-03-04\", \"Absent\", 0),\n    (\"E002\", \"2023-03-05\", \"Present\", 1)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"employee_id\", \"date\", \"status\", \"consecutive_days\"])\nexpected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-3"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-3"
   },
   "source": [
    "**Instructor Notes:** Complex window functions with conditional reset. Tests pattern detection and state management in window operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-4"
   },
   "source": [
    "## Problem 4: Financial Portfolio Analysis\n\n**Requirement:** Investment team needs portfolio performance analysis with risk metrics.\n\n**Scenario:** Calculate portfolio weights, returns, and risk metrics across different assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-4"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nportfolio_data = [\n    (\"AAPL\", 10000.0, 150.0, 155.0),\n    (\"GOOGL\", 15000.0, 2800.0, 2850.0),\n    (\"MSFT\", 8000.0, 300.0, 295.0),\n    (\"TSLA\", 12000.0, 200.0, 210.0)\n]\n\nportfolio_df = spark.createDataFrame(portfolio_data, [\"symbol\", \"investment\", \"purchase_price\", \"current_price\"])\nportfolio_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-4"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"AAPL\", 10000.0, 150.0, 155.0, 6666.67, 10333.33, 3.33, 22.22),\n    (\"GOOGL\", 15000.0, 2800.0, 2850.0, 5357.14, 15267.86, 1.79, 267.86),\n    (\"MSFT\", 8000.0, 300.0, 295.0, 2666.67, 7866.67, -1.67, -133.33),\n    (\"TSLA\", 12000.0, 200.0, 210.0, 6000.0, 12600.0, 5.0, 600.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"symbol\", \"investment\", \"purchase_price\", \"current_price\", \"shares\", \"current_value\", \"return_pct\", \"return_amt\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-4"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-4"
   },
   "source": [
    "**Instructor Notes:** Financial calculations with multiple derived metrics. Tests mathematical operations and percentage calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-5"
   },
   "source": [
    "## Problem 5: Healthcare Patient Journey Analysis\n\n**Requirement:** Medical analytics needs patient treatment pathway analysis.\n\n**Scenario:** Analyze patient journeys through different medical departments and treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-5"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\npatient_journey_data = [\n    (\"P001\", \"Emergency\", \"2023-01-15 10:00:00\"),\n    (\"P001\", \"Radiology\", \"2023-01-15 11:30:00\"),\n    (\"P001\", \"Surgery\", \"2023-01-15 14:00:00\"),\n    (\"P001\", \"ICU\", \"2023-01-15 18:00:00\"),\n    (\"P002\", \"OPD\", \"2023-01-16 09:00:00\"),\n    (\"P002\", \"Lab\", \"2023-01-16 10:00:00\"),\n    (\"P002\", \"Pharmacy\", \"2023-01-16 11:00:00\"),\n    (\"P003\", \"Emergency\", \"2023-01-17 15:00:00\"),\n    (\"P003\", \"Radiology\", \"2023-01-17 16:00:00\")\n]\n\npatient_journey_df = spark.createDataFrame(patient_journey_data, [\"patient_id\", \"department\", \"timestamp\"])\npatient_journey_df = patient_journey_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\npatient_journey_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-5"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"P001\", \"Emergency\", \"Radiology\", 90),\n    (\"P001\", \"Radiology\", \"Surgery\", 150),\n    (\"P001\", \"Surgery\", \"ICU\", 240),\n    (\"P002\", \"OPD\", \"Lab\", 60),\n    (\"P002\", \"Lab\", \"Pharmacy\", 60),\n    (\"P003\", \"Emergency\", \"Radiology\", 60)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"patient_id\", \"from_dept\", \"to_dept\", \"time_minutes\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-5"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-5"
   },
   "source": [
    "**Instructor Notes:** Time-based analysis with lead/lag operations. Tests patient journey analysis and time interval calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-6"
   },
   "source": [
    "## Problem 6: E-commerce Customer Segmentation\n\n**Requirement:** Marketing needs advanced customer segmentation for targeted campaigns.\n\n**Scenario:** Segment customers based on RFM (Recency, Frequency, Monetary) scores and clustering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-6"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ncustomer_segmentation_data = [\n    (\"C001\", 45, 15, 2500.0),\n    (\"C002\", 120, 3, 800.0),\n    (\"C003\", 10, 25, 5000.0),\n    (\"C004\", 80, 8, 1500.0),\n    (\"C005\", 200, 2, 400.0),\n    (\"C006\", 5, 30, 7500.0),\n    (\"C007\", 60, 12, 3000.0)\n]\n\ncustomer_segmentation_df = spark.createDataFrame(customer_segmentation_data, [\"customer_id\", \"recency_days\", \"frequency\", \"monetary\"])\ncustomer_segmentation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-6"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"C001\", 45, 15, 2500.0, \"Gold\"),\n    (\"C002\", 120, 3, 800.0, \"Bronze\"),\n    (\"C003\", 10, 25, 5000.0, \"Platinum\"),\n    (\"C004\", 80, 8, 1500.0, \"Silver\"),\n    (\"C005\", 200, 2, 400.0, \"Bronze\"),\n    (\"C006\", 5, 30, 7500.0, \"Platinum\"),\n    (\"C007\", 60, 12, 3000.0, \"Gold\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"recency_days\", \"frequency\", \"monetary\", \"segment\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-6"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-6"
   },
   "source": [
    "**Instructor Notes:** Customer segmentation with business rules. Tests conditional logic and multi-criteria classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-7"
   },
   "source": [
    "## Problem 7: Supply Chain Route Optimization\n\n**Requirement:** Logistics needs optimal delivery route analysis with cost calculations.\n\n**Scenario:** Calculate delivery routes, distances, and costs considering multiple stops and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-7"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ndelivery_routes_data = [\n    (\"R001\", \"Warehouse\", \"Store_A\", 50.0, 100.0),\n    (\"R001\", \"Store_A\", \"Store_B\", 30.0, 60.0),\n    (\"R001\", \"Store_B\", \"Warehouse\", 40.0, 80.0),\n    (\"R002\", \"Warehouse\", \"Store_C\", 70.0, 140.0),\n    (\"R002\", \"Store_C\", \"Store_D\", 25.0, 50.0),\n    (\"R002\", \"Store_D\", \"Warehouse\", 60.0, 120.0),\n    (\"R003\", \"Warehouse\", \"Store_E\", 90.0, 180.0)\n]\n\ndelivery_routes_df = spark.createDataFrame(delivery_routes_data, [\"route_id\", \"from_location\", \"to_location\", \"distance_km\", \"cost\"])\ndelivery_routes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-7"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"R001\", 120.0, 240.0, 3),\n    (\"R002\", 155.0, 310.0, 3),\n    (\"R003\", 90.0, 180.0, 1)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"route_id\", \"total_distance\", \"total_cost\", \"stops\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-7"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-7"
   },
   "source": [
    "**Instructor Notes:** Route optimization with aggregation. Tests group-based calculations and multi-leg journey analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-8"
   },
   "source": [
    "## Problem 8: Media Content Performance Analysis\n\n**Requirement:** Media analytics needs content engagement metrics and performance trends.\n\n**Scenario:** Calculate content engagement rates, completion rates, and audience retention metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-8"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ncontent_performance_data = [\n    (\"V001\", \"Tutorial\", 10000, 8500, 7500, 6000),\n    (\"V002\", \"Entertainment\", 15000, 12000, 11000, 9000),\n    (\"V003\", \"News\", 8000, 6000, 5000, 3500),\n    (\"V004\", \"Documentary\", 5000, 4500, 4200, 3800),\n    (\"V005\", \"Sports\", 20000, 18000, 16000, 14000)\n]\n\ncontent_performance_df = spark.createDataFrame(content_performance_data, [\"content_id\", \"category\", \"impressions\", \"views\", \"engagements\", \"completions\"])\ncontent_performance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-8"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"V001\", \"Tutorial\", 85.0, 75.0, 60.0, 70.6),\n    (\"V002\", \"Entertainment\", 80.0, 73.3, 60.0, 75.0),\n    (\"V003\", \"News\", 75.0, 62.5, 43.8, 58.3),\n    (\"V004\", \"Documentary\", 90.0, 84.0, 76.0, 84.4),\n    (\"V005\", \"Sports\", 90.0, 80.0, 70.0, 77.8)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"content_id\", \"category\", \"view_rate\", \"engagement_rate\", \"completion_rate\", \"retention_rate\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-8"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-8"
   },
   "source": [
    "**Instructor Notes:** Media analytics with percentage calculations. Tests ratio computations and performance metric derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-9"
   },
   "source": [
    "## Problem 9: Educational Course Progress Tracking\n\n**Requirement:** Education platform needs student progress analytics and course completion tracking.\n\n**Scenario:** Calculate student progress, completion rates, and identify at-risk students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-9"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nstudent_progress_data = [\n    (\"S001\", \"C001\", 10, 8, 85.0),\n    (\"S001\", \"C002\", 15, 5, 65.0),\n    (\"S002\", \"C001\", 10, 10, 95.0),\n    (\"S002\", \"C003\", 20, 15, 88.0),\n    (\"S003\", \"C001\", 10, 3, 55.0),\n    (\"S003\", \"C002\", 15, 2, 45.0),\n    (\"S004\", \"C003\", 20, 18, 92.0)\n]\n\nstudent_progress_df = spark.createDataFrame(student_progress_data, [\"student_id\", \"course_id\", \"total_modules\", \"completed_modules\", \"avg_score\"])\nstudent_progress_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-9"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"S001\", 25, 13, 52.0, 75.0, \"At Risk\"),\n    (\"S002\", 30, 25, 83.3, 91.5, \"Excellent\"),\n    (\"S003\", 25, 5, 20.0, 50.0, \"Critical\"),\n    (\"S004\", 20, 18, 90.0, 92.0, \"Excellent\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"student_id\", \"total_modules\", \"completed_modules\", \"completion_rate\", \"avg_score\", \"status\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-9"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-9"
   },
   "source": [
    "**Instructor Notes:** Student analytics with multi-criteria status classification. Tests aggregation and conditional business logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-10"
   },
   "source": [
    "## Problem 10: IoT Sensor Data Anomaly Detection\n\n**Requirement:** IoT monitoring needs real-time anomaly detection in sensor data streams.\n\n**Scenario:** Identify sensor readings that deviate significantly from historical patterns using statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-10"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nsensor_data = [\n    (\"Sensor_A\", \"2023-03-01 10:00:00\", 25.5),\n    (\"Sensor_A\", \"2023-03-01 11:00:00\", 26.1),\n    (\"Sensor_A\", \"2023-03-01 12:00:00\", 25.8),\n    (\"Sensor_A\", \"2023-03-01 13:00:00\", 45.2),  # Anomaly\n    (\"Sensor_A\", \"2023-03-01 14:00:00\", 25.9),\n    (\"Sensor_B\", \"2023-03-01 10:00:00\", 30.2),\n    (\"Sensor_B\", \"2023-03-01 11:00:00\", 31.0),\n    (\"Sensor_B\", \"2023-03-01 12:00:00\", 15.8),  # Anomaly\n    (\"Sensor_B\", \"2023-03-01 13:00:00\", 30.5),\n    (\"Sensor_B\", \"2023-03-01 14:00:00\", 30.8)\n]\n\nsensor_df = spark.createDataFrame(sensor_data, [\"sensor_id\", \"timestamp\", \"value\"])\nsensor_df = sensor_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\nsensor_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-10"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"Sensor_A\", \"2023-03-01 13:00:00\", 45.2, \"Anomaly\"),\n    (\"Sensor_B\", \"2023-03-01 12:00:00\", 15.8, \"Anomaly\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"sensor_id\", \"timestamp\", \"value\", \"status\"])\nexpected_df = expected_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-10"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-10"
   },
   "source": [
    "**Instructor Notes:** Statistical anomaly detection with window functions. Tests standard deviation calculations and outlier identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-11"
   },
   "source": [
    "## Problem 11: Financial Transaction Pattern Analysis\n\n**Requirement:** Fraud detection needs transaction pattern analysis for suspicious activity identification.\n\n**Scenario:** Analyze transaction patterns to identify unusual spending behaviors and potential fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-11"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ntransaction_patterns_data = [\n    (\"T001\", \"C001\", \"2023-03-01 09:00:00\", 100.0, \"Retail\"),\n    (\"T002\", \"C001\", \"2023-03-01 10:30:00\", 50.0, \"Dining\"),\n    (\"T003\", \"C001\", \"2023-03-01 15:00:00\", 200.0, \"Electronics\"),\n    (\"T004\", \"C001\", \"2023-03-02 08:00:00\", 5000.0, \"Jewelry\"),  # Suspicious\n    (\"T005\", \"C002\", \"2023-03-01 11:00:00\", 75.0, \"Groceries\"),\n    (\"T006\", \"C002\", \"2023-03-01 14:00:00\", 120.0, \"Entertainment\"),\n    (\"T007\", \"C002\", \"2023-03-02 10:00:00\", 80.0, \"Dining\")\n]\n\ntransaction_patterns_df = spark.createDataFrame(transaction_patterns_data, [\"transaction_id\", \"customer_id\", \"timestamp\", \"amount\", \"category\"])\ntransaction_patterns_df = transaction_patterns_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\ntransaction_patterns_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-11"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"T004\", \"C001\", \"2023-03-02 08:00:00\", 5000.0, \"Jewelry\", \"High Value\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"transaction_id\", \"customer_id\", \"timestamp\", \"amount\", \"category\", \"risk_level\"])\nexpected_df = expected_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-11"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-11"
   },
   "source": [
    "**Instructor Notes:** Fraud detection with pattern analysis. Tests statistical comparisons and anomaly flagging based on historical patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-12"
   },
   "source": [
    "## Problem 12: Multi-Dimensional Sales Analysis\n\n**Requirement:** Business intelligence needs sales analysis across multiple dimensions.\n\n**Scenario:** Analyze sales performance across time, geography, and product categories with rollup aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-12"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nmulti_dim_sales_data = [\n    (\"2023-Q1\", \"North\", \"Electronics\", \"Laptop\", 50000),\n    (\"2023-Q1\", \"North\", \"Electronics\", \"Tablet\", 30000),\n    (\"2023-Q1\", \"South\", \"Electronics\", \"Laptop\", 45000),\n    (\"2023-Q1\", \"South\", \"Electronics\", \"Tablet\", 25000),\n    (\"2023-Q1\", \"North\", \"Clothing\", \"Shirt\", 20000),\n    (\"2023-Q1\", \"South\", \"Clothing\", \"Shirt\", 22000),\n    (\"2023-Q2\", \"North\", \"Electronics\", \"Laptop\", 55000),\n    (\"2023-Q2\", \"North\", \"Electronics\", \"Tablet\", 32000)\n]\n\nmulti_dim_sales_df = spark.createDataFrame(multi_dim_sales_data, [\"quarter\", \"region\", \"category\", \"product\", \"sales\"])\nmulti_dim_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-12"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"2023-Q1\", \"North\", \"Electronics\", 80000),\n    (\"2023-Q1\", \"North\", \"Clothing\", 20000),\n    (\"2023-Q1\", \"South\", \"Electronics\", 70000),\n    (\"2023-Q1\", \"South\", \"Clothing\", 22000),\n    (\"2023-Q2\", \"North\", \"Electronics\", 87000),\n    (\"2023-Q1\", \"North\", \"All\", 100000),\n    (\"2023-Q1\", \"South\", \"All\", 92000),\n    (\"2023-Q1\", \"All\", \"Electronics\", 150000),\n    (\"2023-Q1\", \"All\", \"Clothing\", 42000)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"quarter\", \"region\", \"category\", \"total_sales\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-12"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-12"
   },
   "source": [
    "**Instructor Notes:** Multi-dimensional analysis with rollup aggregations. Tests cube/rollup operations for hierarchical reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-13"
   },
   "source": [
    "## Problem 13: Complex UDF for Natural Language Processing\n\n**Requirement:** Customer feedback analysis needs text processing for sentiment and topic extraction.\n\n**Scenario:** Create advanced UDFs to process customer feedback text for sentiment analysis and key topic identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-13"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ncustomer_feedback_data = [\n    (1, \"The product is amazing! Great quality and fast delivery.\"),\n    (2, \"Terrible experience. The item arrived damaged and customer service was unhelpful.\"),\n    (3, \"Average product, nothing special but gets the job done.\"),\n    (4, \"Excellent service! Will definitely buy again. Highly recommended.\"),\n    (5, \"Poor quality product. Broke after first use. Very disappointed.\")\n]\n\ncustomer_feedback_df = spark.createDataFrame(customer_feedback_data, [\"feedback_id\", \"feedback_text\"])\ncustomer_feedback_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-13"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (1, \"The product is amazing! Great quality and fast delivery.\", \"Positive\", \"product quality\"),\n    (2, \"Terrible experience. The item arrived damaged and customer service was unhelpful.\", \"Negative\", \"customer service\"),\n    (3, \"Average product, nothing special but gets the job done.\", \"Neutral\", \"product quality\"),\n    (4, \"Excellent service! Will definitely buy again. Highly recommended.\", \"Positive\", \"customer service\"),\n    (5, \"Poor quality product. Broke after first use. Very disappointed.\", \"Negative\", \"product quality\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"feedback_id\", \"feedback_text\", \"sentiment\", \"main_topic\"])\nexpected_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-13"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-13"
   },
   "source": [
    "**Instructor Notes:** Advanced UDFs for text processing. Tests string analysis, keyword matching, and sentiment classification logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-14"
   },
   "source": [
    "## Problem 14: Time-Series Forecasting Features\n\n**Requirement:** Forecasting team needs feature engineering for time-series prediction models.\n\n**Scenario:** Create lag features, moving averages, and trend indicators for sales forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-14"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nsales_forecasting_data = [\n    (\"2023-01-01\", 1000.0),\n    (\"2023-01-02\", 1200.0),\n    (\"2023-01-03\", 1100.0),\n    (\"2023-01-04\", 1300.0),\n    (\"2023-01-05\", 1400.0),\n    (\"2023-01-06\", 1250.0),\n    (\"2023-01-07\", 1500.0),\n    (\"2023-01-08\", 1600.0)\n]\n\nsales_forecasting_df = spark.createDataFrame(sales_forecasting_data, [\"date\", \"sales\"])\nsales_forecasting_df = sales_forecasting_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nsales_forecasting_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-14"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"2023-01-01\", 1000.0, None, None, None, None),\n    (\"2023-01-02\", 1200.0, 1000.0, None, None, 200.0),\n    (\"2023-01-03\", 1100.0, 1200.0, 1000.0, 1100.0, -100.0),\n    (\"2023-01-04\", 1300.0, 1100.0, 1200.0, 1200.0, 200.0),\n    (\"2023-01-05\", 1400.0, 1300.0, 1100.0, 1266.67, 100.0),\n    (\"2023-01-06\", 1250.0, 1400.0, 1300.0, 1316.67, -150.0),\n    (\"2023-01-07\", 1500.0, 1250.0, 1400.0, 1383.33, 250.0),\n    (\"2023-01-08\", 1600.0, 1500.0, 1250.0, 1450.0, 100.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"date\", \"sales\", \"lag_1\", \"lag_2\", \"moving_avg_3\", \"daily_change\"])\nexpected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-14"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-14"
   },
   "source": [
    "**Instructor Notes:** Time-series feature engineering. Tests lag features, moving averages, and trend calculations for forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-15"
   },
   "source": [
    "## Problem 15: Complex Data Validation Framework\n\n**Requirement:** Data governance needs comprehensive data quality validation framework.\n\n**Scenario:** Implement multi-level data validation checks with custom business rules and cross-field validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-15"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ndata_validation_data = [\n    (1, \"John Doe\", \"john@email.com\", \"1990-01-15\", \"2023-01-01\", 5000.0),\n    (2, \"Jane Smith\", \"invalid-email\", \"1985-12-20\", \"2023-01-15\", -100.0),  # Invalid\n    (3, \"Bob Johnson\", \"bob@company.com\", \"2005-06-10\", \"2023-02-01\", 3000.0),  # Underage\n    (4, \"Alice Brown\", \"alice@domain.com\", \"1975-03-25\", \"2022-12-01\", 7500.0),  # Future date\n    (5, \"\", \"charlie@email.com\", \"1988-07-30\", \"2023-01-10\", 4000.0)  # Empty name\n]\n\ndata_validation_df = spark.createDataFrame(data_validation_data, [\"customer_id\", \"name\", \"email\", \"birth_date\", \"signup_date\", \"balance\"])\ndata_validation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-15"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (1, \"John Doe\", \"john@email.com\", \"1990-01-15\", \"2023-01-01\", 5000.0, \"Valid\"),\n    (2, \"Jane Smith\", \"invalid-email\", \"1985-12-20\", \"2023-01-15\", -100.0, \"Invalid Email, Negative Balance\"),\n    (3, \"Bob Johnson\", \"bob@company.com\", \"2005-06-10\", \"2023-02-01\", 3000.0, \"Underage\"),\n    (4, \"Alice Brown\", \"alice@domain.com\", \"1975-03-25\", \"2022-12-01\", 7500.0, \"Future Signup Date\"),\n    (5, \"\", \"charlie@email.com\", \"1988-07-30\", \"2023-01-10\", 4000.0, \"Empty Name\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"email\", \"birth_date\", \"signup_date\", \"balance\", \"validation_errors\"])\nexpected_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-15"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-15"
   },
   "source": [
    "**Instructor Notes:** Comprehensive data validation framework. Tests multiple validation rules and error message aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-16"
   },
   "source": [
    "## Problem 16: Advanced Window Functions for Gap Analysis\n\n**Requirement:** Business operations needs gap analysis in service delivery timelines.\n\n**Scenario:** Identify service gaps and calculate downtime between consecutive service events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-16"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nservice_events_data = [\n    (\"S001\", \"2023-03-01 09:00:00\", \"2023-03-01 10:00:00\"),\n    (\"S001\", \"2023-03-01 11:30:00\", \"2023-03-01 12:30:00\"),\n    (\"S001\", \"2023-03-01 14:00:00\", \"2023-03-01 15:00:00\"),\n    (\"S002\", \"2023-03-01 08:00:00\", \"2023-03-01 09:00:00\"),\n    (\"S002\", \"2023-03-01 10:00:00\", \"2023-03-01 11:00:00\"),\n    (\"S002\", \"2023-03-01 13:00:00\", \"2023-03-01 14:00:00\")\n]\n\nservice_events_df = spark.createDataFrame(service_events_data, [\"service_id\", \"start_time\", \"end_time\"])\nservice_events_df = service_events_df.withColumn(\"start_time\", col(\"start_time\").cast(\"timestamp\"))\\\n                                   .withColumn(\"end_time\", col(\"end_time\").cast(\"timestamp\"))\nservice_events_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-16"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"S001\", \"2023-03-01 09:00:00\", \"2023-03-01 10:00:00\", None, None),\n    (\"S001\", \"2023-03-01 11:30:00\", \"2023-03-01 12:30:00\", \"2023-03-01 10:00:00\", 90),\n    (\"S001\", \"2023-03-01 14:00:00\", \"2023-03-01 15:00:00\", \"2023-03-01 12:30:00\", 90),\n    (\"S002\", \"2023-03-01 08:00:00\", \"2023-03-01 09:00:00\", None, None),\n    (\"S002\", \"2023-03-01 10:00:00\", \"2023-03-01 11:00:00\", \"2023-03-01 09:00:00\", 60),\n    (\"S002\", \"2023-03-01 13:00:00\", \"2023-03-01 14:00:00\", \"2023-03-01 11:00:00\", 120)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"service_id\", \"start_time\", \"end_time\", \"prev_end_time\", \"gap_minutes\"])\nexpected_df = expected_df.withColumn(\"start_time\", col(\"start_time\").cast(\"timestamp\"))\\\n                       .withColumn(\"end_time\", col(\"end_time\").cast(\"timestamp\"))\\\n                       .withColumn(\"prev_end_time\", col(\"prev_end_time\").cast(\"timestamp\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-16"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-16"
   },
   "source": [
    "**Instructor Notes:** Gap analysis with window functions. Tests time interval calculations and service continuity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-17"
   },
   "source": [
    "## Problem 17: Complex Business Rule Engine\n\n**Requirement:** Insurance claims processing needs automated rule-based decision engine.\n\n**Scenario:** Implement complex business rules for insurance claim approval with multiple conditions and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-17"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ninsurance_claims_data = [\n    (\"CL001\", 5000.0, 2, \"2023-01-15\", \"Approved\"),\n    (\"CL002\", 15000.0, 1, \"2023-02-20\", \"Pending\"),\n    (\"CL003\", 25000.0, 3, \"2023-03-05\", \"Approved\"),\n    (\"CL004\", 50000.0, 0, \"2023-03-10\", \"Rejected\"),\n    (\"CL005\", 8000.0, 5, \"2023-03-15\", \"Approved\")\n]\n\ninsurance_claims_df = spark.createDataFrame(insurance_claims_data, [\"claim_id\", \"claim_amount\", \"previous_claims\", \"claim_date\", \"current_status\"])\ninsurance_claims_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-17"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"CL001\", 5000.0, 2, \"2023-01-15\", \"Approved\", \"Auto Approved\"),\n    (\"CL002\", 15000.0, 1, \"2023-02-20\", \"Pending\", \"Manual Review Required\"),\n    (\"CL003\", 25000.0, 3, \"2023-03-05\", \"Approved\", \"High Risk - Approved\"),\n    (\"CL004\", 50000.0, 0, \"2023-03-10\", \"Rejected\", \"Exceeds Limit\"),\n    (\"CL005\", 8000.0, 5, \"2023-03-15\", \"Approved\", \"Frequent Claimant - Review\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"claim_id\", \"claim_amount\", \"previous_claims\", \"claim_date\", \"current_status\", \"decision_reason\"])\nexpected_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-17"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-17"
   },
   "source": [
    "**Instructor Notes:** Complex business rule engine implementation. Tests multi-condition decision logic and business rule application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-18"
   },
   "source": [
    "## Problem 18: Advanced Data Partitioning Strategy\n\n**Requirement:** Big data processing needs optimized partitioning for performance.\n\n**Scenario:** Implement custom partitioning strategy for large-scale customer transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-18"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nlarge_transactions_data = [\n    (\"T001\", \"C001\", \"2023-03-01\", \"Electronics\", 1000.0),\n    (\"T002\", \"C002\", \"2023-03-01\", \"Clothing\", 500.0),\n    (\"T003\", \"C001\", \"2023-03-02\", \"Electronics\", 1500.0),\n    (\"T004\", \"C003\", \"2023-03-02\", \"Home\", 2000.0),\n    (\"T005\", \"C002\", \"2023-03-03\", \"Electronics\", 800.0),\n    (\"T006\", \"C004\", \"2023-03-03\", \"Clothing\", 300.0),\n    (\"T007\", \"C001\", \"2023-03-04\", \"Home\", 1200.0),\n    (\"T008\", \"C003\", \"2023-03-04\", \"Electronics\", 2500.0)\n]\n\nlarge_transactions_df = spark.createDataFrame(large_transactions_data, [\"transaction_id\", \"customer_id\", \"date\", \"category\", \"amount\"])\nlarge_transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-18"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"2023-03-01\", \"Electronics\", 1000.0),\n    (\"2023-03-01\", \"Clothing\", 500.0),\n    (\"2023-03-02\", \"Electronics\", 1500.0),\n    (\"2023-03-02\", \"Home\", 2000.0),\n    (\"2023-03-03\", \"Electronics\", 800.0),\n    (\"2023-03-03\", \"Clothing\", 300.0),\n    (\"2023-03-04\", \"Home\", 1200.0),\n    (\"2023-03-04\", \"Electronics\", 2500.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"date\", \"category\", \"total_amount\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-18"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-18"
   },
   "source": [
    "**Instructor Notes:** Advanced partitioning and aggregation strategy. Tests efficient data organization for large-scale processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-19"
   },
   "source": [
    "## Problem 19: Multi-Source Data Integration\n\n**Requirement:** Data warehouse needs integration of multiple source systems with conflict resolution.\n\n**Scenario:** Merge customer data from different source systems with priority-based conflict resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-19"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\ncrm_customers_data = [\n    (\"C001\", \"John Doe\", \"john@old-email.com\", \"123-456-7890\"),\n    (\"C002\", \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n    (\"C003\", \"Bob Johnson\", \"bob@company.com\", \"555-123-4567\")\n]\n\nerp_customers_data = [\n    (\"C001\", \"John Doe\", \"john@new-email.com\", \"123-456-7890\"),\n    (\"C002\", \"Jane Smith\", \"jane@email.com\", \"987-654-0000\"),\n    (\"C004\", \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\")\n]\n\ncrm_customers_df = spark.createDataFrame(crm_customers_data, [\"customer_id\", \"name\", \"email\", \"phone\"])\nerp_customers_df = spark.createDataFrame(erp_customers_data, [\"customer_id\", \"name\", \"email\", \"phone\"])\n\nprint(\"CRM Customers:\")\ncrm_customers_df.show()\nprint(\"ERP Customers:\")\nerp_customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-19"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"C001\", \"John Doe\", \"john@new-email.com\", \"123-456-7890\"),\n    (\"C002\", \"Jane Smith\", \"jane@email.com\", \"987-654-0000\"),\n    (\"C003\", \"Bob Johnson\", \"bob@company.com\", \"555-123-4567\"),\n    (\"C004\", \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"email\", \"phone\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-19"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-19"
   },
   "source": [
    "**Instructor Notes:** Multi-source data integration with conflict resolution. Tests complex join logic and priority-based merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-20"
   },
   "source": [
    "## Problem 20: Complex Hierarchical Calculations\n\n**Requirement:** Financial reporting needs hierarchical profit center calculations.\n\n**Scenario:** Calculate rolling up financial metrics across organizational hierarchy with weighted allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-20"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nprofit_centers_data = [\n    (\"PC001\", \"North Region\", \"Region\", None, 1000000.0),\n    (\"PC002\", \"NY Division\", \"Division\", \"PC001\", 400000.0),\n    (\"PC003\", \"NJ Division\", \"Division\", \"PC001\", 350000.0),\n    (\"PC004\", \"CT Division\", \"Division\", \"PC001\", 250000.0),\n    (\"PC005\", \"NY Store A\", \"Store\", \"PC002\", 150000.0),\n    (\"PC006\", \"NY Store B\", \"Store\", \"PC002\", 120000.0),\n    (\"PC007\", \"NY Store C\", \"Store\", \"PC002\", 130000.0)\n]\n\nprofit_centers_df = spark.createDataFrame(profit_centers_data, [\"center_id\", \"center_name\", \"level\", \"parent_id\", \"revenue\"])\nprofit_centers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-20"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"PC001\", \"North Region\", \"Region\", None, 1000000.0, 1000000.0),\n    (\"PC002\", \"NY Division\", \"Division\", \"PC001\", 400000.0, 400000.0),\n    (\"PC003\", \"NJ Division\", \"Division\", \"PC001\", 350000.0, 350000.0),\n    (\"PC004\", \"CT Division\", \"Division\", \"PC001\", 250000.0, 250000.0),\n    (\"PC005\", \"NY Store A\", \"Store\", \"PC002\", 150000.0, 150000.0),\n    (\"PC006\", \"NY Store B\", \"Store\", \"PC002\", 120000.0, 120000.0),\n    (\"PC007\", \"NY Store C\", \"Store\", \"PC002\", 130000.0, 130000.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"center_id\", \"center_name\", \"level\", \"parent_id\", \"revenue\", \"rolled_up_revenue\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-20"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-20"
   },
   "source": [
    "**Instructor Notes:** Hierarchical calculations with recursive relationships. Tests complex organizational structure processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-21"
   },
   "source": [
    "## Problem 21: Advanced Time-Series Correlation\n\n**Requirement:** Financial analytics needs correlation analysis between different time-series.\n\n**Scenario:** Calculate rolling correlations between stock prices and market indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-21"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nstock_correlation_data = [\n    (\"2023-01-01\", \"AAPL\", 150.0, 4500.0),\n    (\"2023-01-02\", \"AAPL\", 152.0, 4520.0),\n    (\"2023-01-03\", \"AAPL\", 151.5, 4480.0),\n    (\"2023-01-04\", \"AAPL\", 153.0, 4550.0),\n    (\"2023-01-05\", \"AAPL\", 154.5, 4600.0),\n    (\"2023-01-06\", \"AAPL\", 153.5, 4580.0),\n    (\"2023-01-07\", \"AAPL\", 155.0, 4620.0),\n    (\"2023-01-08\", \"AAPL\", 156.0, 4650.0)\n]\n\nstock_correlation_df = spark.createDataFrame(stock_correlation_data, [\"date\", \"symbol\", \"price\", \"market_index\"])\nstock_correlation_df = stock_correlation_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nstock_correlation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-21"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"2023-01-01\", \"AAPL\", 150.0, 4500.0, None),\n    (\"2023-01-02\", \"AAPL\", 152.0, 4520.0, None),\n    (\"2023-01-03\", \"AAPL\", 151.5, 4480.0, None),\n    (\"2023-01-04\", \"AAPL\", 153.0, 4550.0, 0.87),\n    (\"2023-01-05\", \"AAPL\", 154.5, 4600.0, 0.92),\n    (\"2023-01-06\", \"AAPL\", 153.5, 4580.0, 0.89),\n    (\"2023-01-07\", \"AAPL\", 155.0, 4620.0, 0.91),\n    (\"2023-01-08\", \"AAPL\", 156.0, 4650.0, 0.93)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\", \"market_index\", \"correlation_5d\"])\nexpected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-21"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-21"
   },
   "source": [
    "**Instructor Notes:** Advanced time-series correlation analysis. Tests statistical calculations and rolling window correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-22"
   },
   "source": [
    "## Problem 22: Complex Data Enrichment Pipeline\n\n**Requirement:** Customer analytics needs comprehensive data enrichment from multiple sources.\n\n**Scenario:** Enrich customer data with demographic, geographic, and behavioral attributes from external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-22"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\ncustomers_base_data = [\n    (\"C001\", \"John Doe\", \"10001\"),\n    (\"C002\", \"Jane Smith\", \"90001\"),\n    (\"C003\", \"Bob Johnson\", \"60601\")\n]\n\ndemographic_data = [\n    (\"10001\", 35, \"Married\", \"Bachelor\"),\n    (\"90001\", 28, \"Single\", \"Master\"),\n    (\"60601\", 42, \"Married\", \"PhD\")\n]\n\nbehavioral_data = [\n    (\"C001\", \"High\", \"Frequent\", \"Premium\"),\n    (\"C002\", \"Medium\", \"Occasional\", \"Standard\"),\n    (\"C003\", \"Low\", \"Rare\", \"Basic\")\n]\n\ncustomers_base_df = spark.createDataFrame(customers_base_data, [\"customer_id\", \"name\", \"postal_code\"])\ndemographic_df = spark.createDataFrame(demographic_data, [\"postal_code\", \"avg_age\", \"marital_status\", \"education\"])\nbehavioral_df = spark.createDataFrame(behavioral_data, [\"customer_id\", \"spending_level\", \"purchase_frequency\", \"customer_tier\"])\n\nprint(\"Base Customers:\")\ncustomers_base_df.show()\nprint(\"Demographic Data:\")\ndemographic_df.show()\nprint(\"Behavioral Data:\")\nbehavioral_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-22"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"C001\", \"John Doe\", \"10001\", 35, \"Married\", \"Bachelor\", \"High\", \"Frequent\", \"Premium\"),\n    (\"C002\", \"Jane Smith\", \"90001\", 28, \"Single\", \"Master\", \"Medium\", \"Occasional\", \"Standard\"),\n    (\"C003\", \"Bob Johnson\", \"60601\", 42, \"Married\", \"PhD\", \"Low\", \"Rare\", \"Basic\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"postal_code\", \"avg_age\", \"marital_status\", \"education\", \"spending_level\", \"purchase_frequency\", \"customer_tier\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-22"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-22"
   },
   "source": [
    "**Instructor Notes:** Complex data enrichment pipeline. Tests multi-source joins and comprehensive data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-23"
   },
   "source": [
    "## Problem 23: Advanced Statistical Analysis\n\n**Requirement:** Data science needs advanced statistical metrics for model feature engineering.\n\n**Scenario:** Calculate z-scores, percentiles, and other statistical measures for data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-23"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nstatistical_data = [\n    (\"P001\", 150.0),\n    (\"P002\", 175.0),\n    (\"P003\", 200.0),\n    (\"P004\", 125.0),\n    (\"P005\", 225.0),\n    (\"P006\", 180.0),\n    (\"P007\", 160.0),\n    (\"P008\", 190.0),\n    (\"P009\", 210.0),\n    (\"P010\", 140.0)\n]\n\nstatistical_df = spark.createDataFrame(statistical_data, [\"product_id\", \"price\"])\nstatistical_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-23"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"P001\", 150.0, -0.82, 0.2),\n    (\"P002\", 175.0, -0.16, 0.4),\n    (\"P003\", 200.0, 0.49, 0.6),\n    (\"P004\", 125.0, -1.48, 0.1),\n    (\"P005\", 225.0, 1.15, 0.9),\n    (\"P006\", 180.0, 0.0, 0.5),\n    (\"P007\", 160.0, -0.65, 0.3),\n    (\"P008\", 190.0, 0.33, 0.7),\n    (\"P009\", 210.0, 0.82, 0.8),\n    (\"P010\", 140.0, -1.15, 0.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"product_id\", \"price\", \"z_score\", \"percentile\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-23"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-23"
   },
   "source": [
    "**Instructor Notes:** Advanced statistical analysis with window functions. Tests z-score calculations and percentile rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-24"
   },
   "source": [
    "## Problem 24: Complex Data Quality Monitoring\n\n**Requirement:** Data governance needs automated data quality monitoring with trend analysis.\n\n**Scenario:** Implement data quality metrics tracking with trend analysis and alerting capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-24"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\ndata_quality_metrics_data = [\n    (\"2023-01-01\", \"Completeness\", 95.5),\n    (\"2023-01-02\", \"Completeness\", 96.2),\n    (\"2023-01-03\", \"Completeness\", 94.8),\n    (\"2023-01-04\", \"Completeness\", 97.1),\n    (\"2023-01-05\", \"Completeness\", 93.5),\n    (\"2023-01-01\", \"Accuracy\", 98.0),\n    (\"2023-01-02\", \"Accuracy\", 97.5),\n    (\"2023-01-03\", \"Accuracy\", 96.8),\n    (\"2023-01-04\", \"Accuracy\", 98.2),\n    (\"2023-01-05\", \"Accuracy\", 95.9)\n]\n\ndata_quality_metrics_df = spark.createDataFrame(data_quality_metrics_data, [\"date\", \"metric\", \"score\"])\ndata_quality_metrics_df = data_quality_metrics_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\ndata_quality_metrics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-24"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"2023-01-01\", \"Completeness\", 95.5, None),\n    (\"2023-01-02\", \"Completeness\", 96.2, 0.7),\n    (\"2023-01-03\", \"Completeness\", 94.8, -1.4),\n    (\"2023-01-04\", \"Completeness\", 97.1, 2.3),\n    (\"2023-01-05\", \"Completeness\", 93.5, -3.6),\n    (\"2023-01-01\", \"Accuracy\", 98.0, None),\n    (\"2023-01-02\", \"Accuracy\", 97.5, -0.5),\n    (\"2023-01-03\", \"Accuracy\", 96.8, -0.7),\n    (\"2023-01-04\", \"Accuracy\", 98.2, 1.4),\n    (\"2023-01-05\", \"Accuracy\", 95.9, -2.3)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"date\", \"metric\", \"score\", \"daily_change\"])\nexpected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-24"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-24"
   },
   "source": [
    "**Instructor Notes:** Data quality monitoring with trend analysis. Tests time-series analysis for quality metric tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-25"
   },
   "source": [
    "## Problem 25: Complex Business Metric Calculation\n\n**Requirement:** Executive dashboard needs complex business KPIs with multiple calculation steps.\n\n**Scenario:** Calculate customer acquisition cost, lifetime value, and return on investment metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-25"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nbusiness_metrics_data = [\n    (\"2023-Q1\", 1000, 50000.0, 250000.0, 5000.0),\n    (\"2023-Q2\", 1200, 60000.0, 300000.0, 5500.0),\n    (\"2023-Q3\", 1500, 75000.0, 400000.0, 6000.0),\n    (\"2023-Q4\", 1800, 90000.0, 500000.0, 6500.0)\n]\n\nbusiness_metrics_df = spark.createDataFrame(business_metrics_data, [\"quarter\", \"new_customers\", \"acquisition_cost\", \"customer_revenue\", \"operating_cost\"])\nbusiness_metrics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-25"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"2023-Q1\", 1000, 50000.0, 250000.0, 5000.0, 50.0, 250.0, 5.0, 245000.0),\n    (\"2023-Q2\", 1200, 60000.0, 300000.0, 5500.0, 50.0, 250.0, 5.0, 294500.0),\n    (\"2023-Q3\", 1500, 75000.0, 400000.0, 6000.0, 50.0, 266.67, 5.33, 394000.0),\n    (\"2023-Q4\", 1800, 90000.0, 500000.0, 6500.0, 50.0, 277.78, 5.56, 493500.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"quarter\", \"new_customers\", \"acquisition_cost\", \"customer_revenue\", \"operating_cost\", \"cac\", \"clv\", \"roi\", \"net_profit\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-25"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-25"
   },
   "source": [
    "**Instructor Notes:** Complex business metric calculations. Tests multi-step financial calculations and KPI derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-26"
   },
   "source": [
    "## Problem 26: Advanced Data Transformation Pipeline\n\n**Requirement:** ETL pipeline needs complex multi-stage data transformation with error handling.\n\n**Scenario:** Implement a robust ETL pipeline with data validation, transformation, and error logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-26"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\netl_source_data = [\n    (\"C001\", \"john doe  \", \"1000.50\", \"2023-01-15\"),\n    (\"C002\", \"Jane Smith\", \"invalid\", \"2023-01-16\"),\n    (\"C003\", \"bob johnson\", \"1200.25\", \"2023-01-17\"),\n    (\"C004\", \"Alice Brown\", \"950.00\", \"2023-01-18\")\n]\n\netl_source_df = spark.createDataFrame(etl_source_data, [\"customer_id\", \"customer_name\", \"amount\", \"date\"])\netl_source_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-26"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"C001\", \"John Doe\", 1000.5, \"2023-01-15\", \"Success\"),\n    (\"C002\", \"Jane Smith\", None, \"2023-01-16\", \"Invalid Amount\"),\n    (\"C003\", \"Bob Johnson\", 1200.25, \"2023-01-17\", \"Success\"),\n    (\"C004\", \"Alice Brown\", 950.0, \"2023-01-18\", \"Success\")\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"amount\", \"date\", \"status\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-26"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-26"
   },
   "source": [
    "**Instructor Notes:** Robust ETL pipeline with error handling. Tests data validation, transformation, and error management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-27"
   },
   "source": [
    "## Problem 27: Complex Join Optimization\n\n**Requirement:** Performance tuning needs optimized join strategies for large datasets.\n\n**Scenario:** Implement efficient join strategies for large customer and transaction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-27"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\ncustomers_large_data = [\n    (\"C001\", \"John Doe\"),\n    (\"C002\", \"Jane Smith\"),\n    (\"C003\", \"Bob Johnson\"),\n    (\"C004\", \"Alice Brown\")\n]\n\ntransactions_large_data = [\n    (\"T001\", \"C001\", 100.0),\n    (\"T002\", \"C001\", 150.0),\n    (\"T003\", \"C002\", 200.0),\n    (\"T004\", \"C003\", 75.0),\n    (\"T005\", \"C004\", 300.0),\n    (\"T006\", \"C001\", 125.0)\n]\n\ncustomers_large_df = spark.createDataFrame(customers_large_data, [\"customer_id\", \"customer_name\"])\ntransactions_large_df = spark.createDataFrame(transactions_large_data, [\"transaction_id\", \"customer_id\", \"amount\"])\n\nprint(\"Customers:\")\ncustomers_large_df.show()\nprint(\"Transactions:\")\ntransactions_large_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-27"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"C001\", \"John Doe\", 3, 375.0),\n    (\"C002\", \"Jane Smith\", 1, 200.0),\n    (\"C003\", \"Bob Johnson\", 1, 75.0),\n    (\"C004\", \"Alice Brown\", 1, 300.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"transaction_count\", \"total_amount\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-27"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-27"
   },
   "source": [
    "**Instructor Notes:** Join optimization strategies. Tests efficient aggregation and join patterns for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-28"
   },
   "source": [
    "## Problem 28: Complex Window Function Patterns\n\n**Requirement:** Advanced analytics needs complex window function patterns for time-series analysis.\n\n**Scenario:** Implement advanced window function patterns for financial time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-28"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nfinancial_series_data = [\n    (\"2023-01-01\", \"AAPL\", 150.0),\n    (\"2023-01-02\", \"AAPL\", 152.0),\n    (\"2023-01-03\", \"AAPL\", 151.5),\n    (\"2023-01-04\", \"AAPL\", 153.0),\n    (\"2023-01-05\", \"AAPL\", 154.5),\n    (\"2023-01-06\", \"AAPL\", 153.5),\n    (\"2023-01-07\", \"AAPL\", 155.0),\n    (\"2023-01-08\", \"AAPL\", 156.0)\n]\n\nfinancial_series_df = spark.createDataFrame(financial_series_data, [\"date\", \"symbol\", \"price\"])\nfinancial_series_df = financial_series_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nfinancial_series_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-28"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"2023-01-01\", \"AAPL\", 150.0, None, None, None),\n    (\"2023-01-02\", \"AAPL\", 152.0, 150.0, 2.0, 1.33),\n    (\"2023-01-03\", \"AAPL\", 151.5, 152.0, -0.5, -0.33),\n    (\"2023-01-04\", \"AAPL\", 153.0, 151.5, 1.5, 0.99),\n    (\"2023-01-05\", \"AAPL\", 154.5, 153.0, 1.5, 0.98),\n    (\"2023-01-06\", \"AAPL\", 153.5, 154.5, -1.0, -0.65),\n    (\"2023-01-07\", \"AAPL\", 155.0, 153.5, 1.5, 0.98),\n    (\"2023-01-08\", \"AAPL\", 156.0, 155.0, 1.0, 0.65)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\", \"prev_price\", \"price_change\", \"pct_change\"])\nexpected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-28"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-28"
   },
   "source": [
    "**Instructor Notes:** Advanced window function patterns. Tests financial calculations and time-series analysis techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-29"
   },
   "source": [
    "## Problem 29: Complex Data Aggregation Strategy\n\n**Requirement:** Business reporting needs complex multi-level aggregation with custom logic.\n\n**Scenario:** Implement custom aggregation logic for hierarchical business reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-29"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nbusiness_aggregation_data = [\n    (\"Region_A\", \"Division_1\", \"Department_X\", 100000.0),\n    (\"Region_A\", \"Division_1\", \"Department_Y\", 150000.0),\n    (\"Region_A\", \"Division_2\", \"Department_Z\", 200000.0),\n    (\"Region_B\", \"Division_3\", \"Department_W\", 120000.0),\n    (\"Region_B\", \"Division_3\", \"Department_V\", 180000.0),\n    (\"Region_B\", \"Division_4\", \"Department_U\", 220000.0)\n]\n\nbusiness_aggregation_df = spark.createDataFrame(business_aggregation_data, [\"region\", \"division\", \"department\", \"revenue\"])\nbusiness_aggregation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-29"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"Region_A\", \"Division_1\", \"Department_X\", 100000.0),\n    (\"Region_A\", \"Division_1\", \"Department_Y\", 150000.0),\n    (\"Region_A\", \"Division_1\", \"All\", 250000.0),\n    (\"Region_A\", \"Division_2\", \"Department_Z\", 200000.0),\n    (\"Region_A\", \"Division_2\", \"All\", 200000.0),\n    (\"Region_A\", \"All\", \"All\", 450000.0),\n    (\"Region_B\", \"Division_3\", \"Department_W\", 120000.0),\n    (\"Region_B\", \"Division_3\", \"Department_V\", 180000.0),\n    (\"Region_B\", \"Division_3\", \"All\", 300000.0),\n    (\"Region_B\", \"Division_4\", \"Department_U\", 220000.0),\n    (\"Region_B\", \"Division_4\", \"All\", 220000.0),\n    (\"Region_B\", \"All\", \"All\", 520000.0),\n    (\"All\", \"All\", \"All\", 970000.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"region\", \"division\", \"department\", \"revenue\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-29"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-29"
   },
   "source": [
    "**Instructor Notes:** Complex multi-level aggregation. Tests hierarchical rollup operations and custom aggregation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-30"
   },
   "source": [
    "## Problem 30: Advanced Performance Optimization\n\n**Requirement:** Large-scale data processing needs advanced performance optimization techniques.\n\n**Scenario:** Implement performance optimization strategies for complex data processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-30"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\nperformance_data = [\n    (\"P001\", \"Electronics\", \"North\", 1000.0),\n    (\"P001\", \"Electronics\", \"South\", 1500.0),\n    (\"P002\", \"Clothing\", \"North\", 800.0),\n    (\"P002\", \"Clothing\", \"South\", 1200.0),\n    (\"P003\", \"Home\", \"North\", 2000.0),\n    (\"P003\", \"Home\", \"South\", 1800.0),\n    (\"P004\", \"Electronics\", \"North\", 900.0),\n    (\"P004\", \"Electronics\", \"South\", 1100.0)\n]\n\nperformance_df = spark.createDataFrame(performance_data, [\"product_id\", \"category\", \"region\", \"sales\"])\nperformance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-30"
   },
   "outputs": [],
   "source": [
    "# Expected Output\nexpected_data = [\n    (\"Electronics\", 4500.0, 2000.0, 2500.0),\n    (\"Clothing\", 2000.0, 800.0, 1200.0),\n    (\"Home\", 3800.0, 2000.0, 1800.0)\n]\n\nexpected_df = spark.createDataFrame(expected_data, [\"category\", \"total_sales\", \"north_sales\", \"south_sales\"])\nexpected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-30"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n\n# Test your solution\nassert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-30"
   },
   "source": [
    "**Instructor Notes:** Advanced performance optimization. Tests efficient aggregation patterns and data processing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "completion-note"
   },
   "source": [
    "# Set 5 Complete!\n\nYou've completed all 30 Hard problems in Set 3. These problems cover:\n- Complex joins and relationship analysis\n- Advanced window functions and analytics\n- Multi-level aggregations and rollups\n- Complex UDFs and data transformations\n- Performance optimization and partitioning\n- Statistical analysis and business metrics\n- Data quality monitoring and validation\n\nCongratulations! You have completed all 150 PySpark interview problems across 5 difficulty levels! with Medium/Hard difficulty problems?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}