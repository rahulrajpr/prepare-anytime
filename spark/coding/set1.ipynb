{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/coding/set1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview-section"
      },
      "source": [
        "# PySpark Interview Preparation - Set 1 (Easy)\n",
        "\n",
        "## Overview & Instructions\n",
        "\n",
        "### How to run this notebook in Google Colab:\n",
        "1. Upload this .ipynb file to Google Colab\n",
        "2. Run the installation cells below\n",
        "3. Execute each problem cell sequentially\n",
        "\n",
        "### Installation Commands:\n",
        "The following cell installs Java and PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "installation-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eee3af8-3a61-4b40-9f79-f1254762f424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 1.8.0_462\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ],
      "source": [
        "# Install Java and PySpark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!pyspark --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sparksession-section"
      },
      "source": [
        "### SparkSession Initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sparksession-cell"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "    .appName(\"PySparkInterviewSet1\")\\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assert-function-section"
      },
      "source": [
        "### DataFrame Assertion Function:\n",
        "\n",
        "This function compares DataFrames ignoring order and with floating-point tolerance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "assert-function-cell"
      },
      "outputs": [],
      "source": [
        "def assert_dataframe_equal(df_actual, df_expected, epsilon=1e-6, check_schema_strict=False):\n",
        "    \"\"\"Compare two DataFrames using PySpark operations\"\"\"\n",
        "\n",
        "    if check_schema_strict:\n",
        "        # Check schema exactly\n",
        "        if df_actual.schema != df_expected.schema:\n",
        "            print(\"Schema mismatch!\")\n",
        "            print(\"Actual schema:\", df_actual.schema)\n",
        "            print(\"Expected schema:\", df_expected.schema)\n",
        "            raise AssertionError(\"Schema mismatch\")\n",
        "    else:\n",
        "        # Check column names and basic types\n",
        "        actual_fields = df_actual.schema\n",
        "        expected_fields = df_expected.schema\n",
        "\n",
        "        if len(actual_fields) != len(expected_fields):\n",
        "            print(\"Column count mismatch!\")\n",
        "            raise AssertionError(\"Column count mismatch\")\n",
        "\n",
        "        for i, (actual_field, expected_field) in enumerate(zip(actual_fields, expected_fields)):\n",
        "            if actual_field.name != expected_field.name:\n",
        "                print(f\"Column name mismatch at position {i}: {actual_field.name} vs {expected_field.name}\")\n",
        "                raise AssertionError(\"Column name mismatch\")\n",
        "\n",
        "    # Rest of your comparison logic remains the same\n",
        "    if df_actual.count() != df_expected.count():\n",
        "        print(f\"Row count mismatch! Actual: {df_actual.count()}, Expected: {df_expected.count()}\")\n",
        "        raise AssertionError(\"Row count mismatch\")\n",
        "\n",
        "    diff_actual = df_actual.exceptAll(df_expected)\n",
        "    diff_expected = df_expected.exceptAll(df_actual)\n",
        "\n",
        "    if diff_actual.count() > 0 or diff_expected.count() > 0:\n",
        "        print(\"Data mismatch!\")\n",
        "        print(\"Rows in actual but not in expected:\")\n",
        "        diff_actual.show()\n",
        "        print(\"Rows in expected but not in actual:\")\n",
        "        diff_expected.show()\n",
        "        raise AssertionError(\"Data content mismatch\")\n",
        "\n",
        "    print(\"✓ DataFrames are equal!\\n\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toc-section"
      },
      "source": [
        "## Table of Contents - Set 1 (Easy)\n",
        "\n",
        "**Difficulty Distribution:** 30 Easy Problems\n",
        "\n",
        "**Topics Covered:**\n",
        "- Basic Filtering & Selection (6 problems)\n",
        "- Simple Aggregations (6 problems)\n",
        "- Basic Joins (6 problems)\n",
        "- Window Functions (4 problems)\n",
        "- String & Date Operations (4 problems)\n",
        "- UDFs (4 problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-1"
      },
      "source": [
        "## Problem 1: Active Customer Filter\n",
        "\n",
        "**Requirement:** The marketing team needs a list of all active customers for a promotional campaign.\n",
        "\n",
        "**Scenario:** Filter customers where status is 'Active' from the customer database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244fa5b9-0a98-44ab-c084-4174baf0934f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+--------+----------+\n",
            "|customer_id| customer_name|  status| join_date|\n",
            "+-----------+--------------+--------+----------+\n",
            "|          1|      John Doe|  Active|2023-01-15|\n",
            "|          2|    Jane Smith|Inactive|2023-02-20|\n",
            "|          3|   Bob Johnson|  Active|2023-03-10|\n",
            "|          4|   Alice Brown|  Active|2023-01-05|\n",
            "|          5|Charlie Wilson|Inactive|2023-04-01|\n",
            "+-----------+--------------+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_data = [\n",
        "    (1, \"John Doe\", \"Active\", \"2023-01-15\"),\n",
        "    (2, \"Jane Smith\", \"Inactive\", \"2023-02-20\"),\n",
        "    (3, \"Bob Johnson\", \"Active\", \"2023-03-10\"),\n",
        "    (4, \"Alice Brown\", \"Active\", \"2023-01-05\"),\n",
        "    (5, \"Charlie Wilson\", \"Inactive\", \"2023-04-01\")\n",
        "]\n",
        "\n",
        "customer_df = spark.createDataFrame(customer_data, [\"customer_id\", \"customer_name\", \"status\", \"join_date\"])\n",
        "customer_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43e72f0-88d5-46d5-d626-e1754fdfa030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+------+----------+\n",
            "|customer_id|customer_name|status| join_date|\n",
            "+-----------+-------------+------+----------+\n",
            "|          1|     John Doe|Active|2023-01-15|\n",
            "|          3|  Bob Johnson|Active|2023-03-10|\n",
            "|          4|  Alice Brown|Active|2023-01-05|\n",
            "+-----------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John Doe\", \"Active\", \"2023-01-15\"),\n",
        "    (3, \"Bob Johnson\", \"Active\", \"2023-03-10\"),\n",
        "    (4, \"Alice Brown\", \"Active\", \"2023-01-05\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"status\", \"join_date\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc2e4a2-ee45-4d45-c0d2-e94b99344cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+------+----------+\n",
            "|customer_id|customer_name|status| join_date|\n",
            "+-----------+-------------+------+----------+\n",
            "|          1|     John Doe|Active|2023-01-15|\n",
            "|          3|  Bob Johnson|Active|2023-03-10|\n",
            "|          4|  Alice Brown|Active|2023-01-05|\n",
            "+-----------+-------------+------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "result_df = customer_df.filter(\"status = 'Active'\")\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-1"
      },
      "source": [
        "**Instructor Notes:** Basic filtering operation. Tests understanding of filter() or where() methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-2"
      },
      "source": [
        "## Problem 2: Total Sales by Product\n",
        "\n",
        "**Requirement:** Sales department wants total revenue by product for quarterly reporting.\n",
        "\n",
        "**Scenario:** Calculate sum of sales amount grouped by product_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f572d63-206f-42f4-d57b-a6b9a6c93467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+------+\n",
            "|product_id|product_name|amount|\n",
            "+----------+------------+------+\n",
            "|         1|           A| 100.0|\n",
            "|         1|           A| 150.0|\n",
            "|         2|           B| 200.0|\n",
            "|         1|           A|  75.0|\n",
            "|         3|           C| 300.0|\n",
            "|         2|           B| 250.0|\n",
            "|         3|           C| 100.0|\n",
            "+----------+------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "sales_data = [\n",
        "    (1, \"A\", 100.0),\n",
        "    (1, \"A\", 150.0),\n",
        "    (2, \"B\", 200.0),\n",
        "    (1, \"A\", 75.0),\n",
        "    (3, \"C\", 300.0),\n",
        "    (2, \"B\", 250.0),\n",
        "    (3, \"C\", 100.0)\n",
        "]\n",
        "\n",
        "sales_df = spark.createDataFrame(sales_data, [\"product_id\", \"product_name\", \"amount\"])\n",
        "sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9e4aea-915b-4390-c6da-f297af9fa372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+\n",
            "|product_id|product_name|total_sales|\n",
            "+----------+------------+-----------+\n",
            "|         3|           C|      400.0|\n",
            "|         1|           A|      325.0|\n",
            "|         2|           B|      450.0|\n",
            "+----------+------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (3, \"C\", 400.0),\n",
        "    (1, \"A\", 325.0),\n",
        "    (2, \"B\", 450.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"total_sales\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8325bc7-66cb-4447-fbbc-7a4b31765719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+\n",
            "|product_id|product_name|total_sales|\n",
            "+----------+------------+-----------+\n",
            "|         2|           B|      450.0|\n",
            "|         1|           A|      325.0|\n",
            "|         3|           C|      400.0|\n",
            "+----------+------------+-----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "result_df = sales_df.groupBy('product_id','product_name')\\\n",
        "                    .agg(fn.expr('sum(amount) as total_sales'))\\\n",
        "                    .select('product_id','product_name','total_sales')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-2"
      },
      "source": [
        "**Instructor Notes:** Basic groupBy and aggregation. Tests sum() function and alias usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-3"
      },
      "source": [
        "## Problem 3: Employee Department Join\n",
        "\n",
        "**Requirement:** HR needs a combined view of employees with their department names.\n",
        "\n",
        "**Scenario:** Join employees DataFrame with departments DataFrame on department_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c03b77-be56-4477-c298-f4f31d19e11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees:\n",
            "+------+--------+-------+\n",
            "|emp_id|emp_name|dept_id|\n",
            "+------+--------+-------+\n",
            "|     1|    John|    101|\n",
            "|     2|    Jane|    102|\n",
            "|     3|     Bob|    101|\n",
            "|     4|   Alice|    103|\n",
            "|     5| Charlie|    102|\n",
            "+------+--------+-------+\n",
            "\n",
            "Departments:\n",
            "+-------+-----------+\n",
            "|dept_id|  dept_name|\n",
            "+-------+-----------+\n",
            "|    101|Engineering|\n",
            "|    102|  Marketing|\n",
            "|    103|      Sales|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "employees_data = [\n",
        "    (1, \"John\", 101),\n",
        "    (2, \"Jane\", 102),\n",
        "    (3, \"Bob\", 101),\n",
        "    (4, \"Alice\", 103),\n",
        "    (5, \"Charlie\", 102)\n",
        "]\n",
        "\n",
        "departments_data = [\n",
        "    (101, \"Engineering\"),\n",
        "    (102, \"Marketing\"),\n",
        "    (103, \"Sales\")\n",
        "]\n",
        "\n",
        "employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"emp_name\", \"dept_id\"])\n",
        "departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "print(\"Employees:\")\n",
        "employees_df.show()\n",
        "print(\"Departments:\")\n",
        "departments_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c72ef96-4e13-4d49-8b92-60798400f38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-------+-----------+\n",
            "|emp_id|emp_name|dept_id|  dept_name|\n",
            "+------+--------+-------+-----------+\n",
            "|     1|    John|    101|Engineering|\n",
            "|     2|    Jane|    102|  Marketing|\n",
            "|     3|     Bob|    101|Engineering|\n",
            "|     4|   Alice|    103|      Sales|\n",
            "|     5| Charlie|    102|  Marketing|\n",
            "+------+--------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John\", 101, \"Engineering\"),\n",
        "    (2, \"Jane\", 102, \"Marketing\"),\n",
        "    (3, \"Bob\", 101, \"Engineering\"),\n",
        "    (4, \"Alice\", 103, \"Sales\"),\n",
        "    (5, \"Charlie\", 102, \"Marketing\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"dept_id\", \"dept_name\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bf1dc5-f526-4ef9-f3e5-f2f605cf1570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-------+-----------+\n",
            "|emp_id|emp_name|dept_id|  dept_name|\n",
            "+------+--------+-------+-----------+\n",
            "|     1|    John|    101|Engineering|\n",
            "|     2|    Jane|    102|  Marketing|\n",
            "|     3|     Bob|    101|Engineering|\n",
            "|     4|   Alice|    103|      Sales|\n",
            "|     5| Charlie|    102|  Marketing|\n",
            "+------+--------+-------+-----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "join_expr = employees_df.dept_id == departments_df.dept_id\n",
        "\n",
        "result_df = employees_df.join(departments_df, join_expr,'inner')\\\n",
        "                                .drop(departments_df.dept_id)\\\n",
        "                                .select('emp_id','emp_name','dept_id','dept_name')\\\n",
        "                                .orderBy('emp_id')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-3"
      },
      "source": [
        "**Instructor Notes:** Basic inner join operation. Tests join syntax and column handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-4"
      },
      "source": [
        "## Problem 4: Top N Products by Sales\n",
        "\n",
        "**Requirement:** Business stakeholders want to identify top 3 best-selling products.\n",
        "\n",
        "**Scenario:** Use window functions to rank products by total sales and select top 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82a4290-9734-491d-a154-3669dfe93c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+\n",
            "|product_id|product_name|total_sales|\n",
            "+----------+------------+-----------+\n",
            "|      P001|      Laptop|    50000.0|\n",
            "|      P002|       Mouse|    15000.0|\n",
            "|      P003|    Keyboard|    25000.0|\n",
            "|      P004|     Monitor|    45000.0|\n",
            "|      P005|  Headphones|    18000.0|\n",
            "|      P006|      Tablet|    35000.0|\n",
            "+----------+------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "product_sales_data = [\n",
        "    (\"P001\", \"Laptop\", 50000.0),\n",
        "    (\"P002\", \"Mouse\", 15000.0),\n",
        "    (\"P003\", \"Keyboard\", 25000.0),\n",
        "    (\"P004\", \"Monitor\", 45000.0),\n",
        "    (\"P005\", \"Headphones\", 18000.0),\n",
        "    (\"P006\", \"Tablet\", 35000.0)\n",
        "]\n",
        "\n",
        "product_sales_df = spark.createDataFrame(product_sales_data, [\"product_id\", \"product_name\", \"total_sales\"])\n",
        "product_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c3f9cb-a4be-465e-c964-b2adde94ecd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+----+\n",
            "|product_id|product_name|total_sales|rank|\n",
            "+----------+------------+-----------+----+\n",
            "|      P001|      Laptop|    50000.0|   1|\n",
            "|      P004|     Monitor|    45000.0|   2|\n",
            "|      P006|      Tablet|    35000.0|   3|\n",
            "+----------+------------+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"P001\", \"Laptop\", 50000.0, 1),\n",
        "    (\"P004\", \"Monitor\", 45000.0, 2),\n",
        "    (\"P006\", \"Tablet\", 35000.0, 3)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"total_sales\", \"rank\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507a0594-70a6-48de-f9df-53307a44cac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+----+\n",
            "|product_id|product_name|total_sales|rank|\n",
            "+----------+------------+-----------+----+\n",
            "|      P001|      Laptop|    50000.0|   1|\n",
            "|      P004|     Monitor|    45000.0|   2|\n",
            "|      P006|      Tablet|    35000.0|   3|\n",
            "+----------+------------+-----------+----+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "win = Window.orderBy(fn.col('total_sales').desc_nulls_last(), fn.col('product_id').asc_nulls_last())\n",
        "\n",
        "result_df = product_sales_df\\\n",
        "          .withColumn('rank',fn.row_number().over(win))\\\n",
        "          .filter('rank <= 3')\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-4"
      },
      "source": [
        "**Instructor Notes:** Basic window function with ranking. Tests Window specification and rank() function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-5"
      },
      "source": [
        "## Problem 5: Customer Email Domain Extraction\n",
        "\n",
        "**Requirement:** Marketing team wants to analyze customer distribution by email domain.\n",
        "\n",
        "**Scenario:** Extract domain from email addresses and count customers by domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca00e673-9b33-4302-8874-bcadefd38e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------------+\n",
            "|customer_id|            email|\n",
            "+-----------+-----------------+\n",
            "|          1|   john@gmail.com|\n",
            "|          2|   jane@yahoo.com|\n",
            "|          3|    bob@gmail.com|\n",
            "|          4|alice@company.com|\n",
            "|          5|charlie@gmail.com|\n",
            "|          6|  diana@yahoo.com|\n",
            "+-----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customers_data = [\n",
        "    (1, \"john@gmail.com\"),\n",
        "    (2, \"jane@yahoo.com\"),\n",
        "    (3, \"bob@gmail.com\"),\n",
        "    (4, \"alice@company.com\"),\n",
        "    (5, \"charlie@gmail.com\"),\n",
        "    (6, \"diana@yahoo.com\")\n",
        "]\n",
        "\n",
        "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"email\"])\n",
        "customers_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb4daed-67be-44dc-c28b-a50afaf2382f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|     domain|customer_count|\n",
            "+-----------+--------------+\n",
            "|  gmail.com|             3|\n",
            "|  yahoo.com|             2|\n",
            "|company.com|             1|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"gmail.com\", 3),\n",
        "    (\"yahoo.com\", 2),\n",
        "    (\"company.com\", 1)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"domain\", \"customer_count\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f929b8-e199-4e24-905c-751c6348e745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|     domain|customer_count|\n",
            "+-----------+--------------+\n",
            "|  gmail.com|             3|\n",
            "|  yahoo.com|             2|\n",
            "|company.com|             1|\n",
            "+-----------+--------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "result_df = customers_df\\\n",
        "                .withColumn('domain', fn.split(fn.col('email'),'@').getItem(1))\\\n",
        "                .groupby('domain')\\\n",
        "                .agg(fn.expr('count(customer_id) as customer_count'))\\\n",
        "                .orderBy(fn.col('customer_count').desc_nulls_last())\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-5"
      },
      "source": [
        "**Instructor Notes:** String manipulation with split function. Tests string operations and array element access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-6"
      },
      "source": [
        "## Problem 6: Age Category UDF\n",
        "\n",
        "**Requirement:** Analytics team needs to categorize customers by age groups for segmentation.\n",
        "\n",
        "**Scenario:** Create a UDF to categorize ages and apply it to customer data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959a5046-0b18-41db-899d-cc715dffd09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+---+\n",
            "|customer_id|   name|age|\n",
            "+-----------+-------+---+\n",
            "|          1|   John| 25|\n",
            "|          2|   Jane| 35|\n",
            "|          3|    Bob| 17|\n",
            "|          4|  Alice| 45|\n",
            "|          5|Charlie| 60|\n",
            "|          6|  Diana| 15|\n",
            "+-----------+-------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_ages_data = [\n",
        "    (1, \"John\", 25),\n",
        "    (2, \"Jane\", 35),\n",
        "    (3, \"Bob\", 17),\n",
        "    (4, \"Alice\", 45),\n",
        "    (5, \"Charlie\", 60),\n",
        "    (6, \"Diana\", 15)\n",
        "]\n",
        "\n",
        "customer_ages_df = spark.createDataFrame(customer_ages_data, [\"customer_id\", \"name\", \"age\"])\n",
        "customer_ages_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3373dc45-bc54-4320-b18c-efe3a32ecb94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+---+------------+\n",
            "|customer_id|   name|age|age_category|\n",
            "+-----------+-------+---+------------+\n",
            "|          1|   John| 25|       Adult|\n",
            "|          2|   Jane| 35|       Adult|\n",
            "|          3|    Bob| 17|        Teen|\n",
            "|          4|  Alice| 45|       Adult|\n",
            "|          5|Charlie| 60|      Senior|\n",
            "|          6|  Diana| 15|        Teen|\n",
            "+-----------+-------+---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John\", 25, \"Adult\"),\n",
        "    (2, \"Jane\", 35, \"Adult\"),\n",
        "    (3, \"Bob\", 17, \"Teen\"),\n",
        "    (4, \"Alice\", 45, \"Adult\"),\n",
        "    (5, \"Charlie\", 60, \"Senior\"),\n",
        "    (6, \"Diana\", 15, \"Teen\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"age\", \"age_category\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590a44a8-a7a8-4fc4-a554-86cf142f80dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+---+------------+\n",
            "|customer_id|   name|age|age_category|\n",
            "+-----------+-------+---+------------+\n",
            "|          1|   John| 25|       Adult|\n",
            "|          2|   Jane| 35|       Adult|\n",
            "|          3|    Bob| 17|        Teen|\n",
            "|          4|  Alice| 45|       Adult|\n",
            "|          5|Charlie| 60|      Senior|\n",
            "|          6|  Diana| 15|        Teen|\n",
            "+-----------+-------+---+------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql import types as tp\n",
        "\n",
        "def categorise_age(value):\n",
        "  if value < 18:\n",
        "    return 'Teen'\n",
        "  elif value < 60:\n",
        "    return 'Adult'\n",
        "  else:\n",
        "    return 'Senior'\n",
        "\n",
        "categorise_age_udf = fn.udf(categorise_age, tp.StringType())\n",
        "\n",
        "result_df = customer_ages_df\\\n",
        "        .withColumn('age_category',categorise_age_udf(fn.col('age')))\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-6"
      },
      "source": [
        "**Instructor Notes:** Basic UDF creation and application. Tests UDF registration and usage with column operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-7"
      },
      "source": [
        "## Problem 7: Monthly Sales Growth\n",
        "\n",
        "**Requirement:** Finance team needs month-over-month sales growth percentage.\n",
        "\n",
        "**Scenario:** Calculate percentage growth compared to previous month using window functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2907c638-f456-482d-ba1e-e9e7ca7a0246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|  month|   sales|\n",
            "+-------+--------+\n",
            "|2023-01|100000.0|\n",
            "|2023-02|120000.0|\n",
            "|2023-03|110000.0|\n",
            "|2023-04|130000.0|\n",
            "|2023-05|150000.0|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "monthly_sales_data = [\n",
        "    (\"2023-01\", 100000.0),\n",
        "    (\"2023-02\", 120000.0),\n",
        "    (\"2023-03\", 110000.0),\n",
        "    (\"2023-04\", 130000.0),\n",
        "    (\"2023-05\", 150000.0)\n",
        "]\n",
        "\n",
        "monthly_sales_df = spark.createDataFrame(monthly_sales_data, [\"month\", \"sales\"])\n",
        "monthly_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b7f518-dcd5-49fe-f2e8-fff269a5afa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+----------+\n",
            "|  month|   sales|growth_pct|\n",
            "+-------+--------+----------+\n",
            "|2023-01|100000.0|      NULL|\n",
            "|2023-02|120000.0|      20.0|\n",
            "|2023-03|110000.0|     -8.33|\n",
            "|2023-04|130000.0|     18.18|\n",
            "|2023-05|150000.0|     15.38|\n",
            "+-------+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01\", 100000.0, None),\n",
        "    (\"2023-02\", 120000.0, 20.0),\n",
        "    (\"2023-03\", 110000.0, -8.33),\n",
        "    (\"2023-04\", 130000.0, 18.18),\n",
        "    (\"2023-05\", 150000.0, 15.38)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"month\", \"sales\", \"growth_pct\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d11ef2b-223e-4753-8012-aee386e50d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+----------+\n",
            "|  month|   sales|growth_pct|\n",
            "+-------+--------+----------+\n",
            "|2023-01|100000.0|      NULL|\n",
            "|2023-02|120000.0|      20.0|\n",
            "|2023-03|110000.0|     -8.33|\n",
            "|2023-04|130000.0|     18.18|\n",
            "|2023-05|150000.0|     15.38|\n",
            "+-------+--------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql import types as tp\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "win = Window.orderBy(fn.col('month').asc_nulls_last())\n",
        "\n",
        "result_df = monthly_sales_df\\\n",
        "              .withColumn('lastMonthSales',fn.lag(fn.col('sales')).over(win))\\\n",
        "              .withColumn('growth_pct', fn.expr('round((sales-lastMonthSales)*100/lastMonthSales,2)'))\\\n",
        "              .orderBy(fn.col('month').asc_nulls_last())\\\n",
        "              .drop('lastMonthSales')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-7"
      },
      "source": [
        "**Instructor Notes:** Window function with lag for time-series analysis. Tests lag() and percentage calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-8"
      },
      "source": [
        "## Problem 8: Duplicate Order Detection\n",
        "\n",
        "**Requirement:** Operations team needs to identify duplicate orders for fraud detection.\n",
        "\n",
        "**Scenario:** Find orders with same customer_id, product_id, and order_date within 1 hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64527885-9a40-4b1d-8f23-02b99c18a9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+-------------------+----------+------+\n",
            "|order_id|customer_id|         order_time|product_id|amount|\n",
            "+--------+-----------+-------------------+----------+------+\n",
            "|       1|        101|2023-01-01 10:00:00|         2| 100.0|\n",
            "|       2|        101|2023-01-01 10:30:00|         1|  50.0|\n",
            "|       3|        102|2023-01-01 11:00:00|         1|  75.0|\n",
            "|       4|        101|2023-01-01 10:45:00|         1|  50.0|\n",
            "|       5|        103|2023-01-01 12:00:00|         3| 200.0|\n",
            "|       6|        102|2023-01-01 13:00:00|         2| 150.0|\n",
            "+--------+-----------+-------------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "orders_data = [\n",
        "    (1, 101, \"2023-01-01 10:00:00\", 2, 100.0),\n",
        "    (2, 101, \"2023-01-01 10:30:00\", 1, 50.0),\n",
        "    (3, 102, \"2023-01-01 11:00:00\", 1, 75.0),\n",
        "    (4, 101, \"2023-01-01 10:45:00\", 1, 50.0),  # Duplicate\n",
        "    (5, 103, \"2023-01-01 12:00:00\", 3, 200.0),\n",
        "    (6, 102, \"2023-01-01 13:00:00\", 2, 150.0)\n",
        "]\n",
        "\n",
        "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"order_time\", \"product_id\", \"amount\"])\n",
        "orders_df = orders_df.withColumn(\"order_time\", col(\"order_time\").cast(\"timestamp\"))\n",
        "orders_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52552b6-b939-4707-ed45-d4e64703e49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+-------------------+----------+------+\n",
            "|order_id|customer_id|         order_time|product_id|amount|\n",
            "+--------+-----------+-------------------+----------+------+\n",
            "|       2|        101|2023-01-01 10:30:00|         1|  50.0|\n",
            "|       4|        101|2023-01-01 10:45:00|         1|  50.0|\n",
            "+--------+-----------+-------------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (2, 101, \"2023-01-01 10:30:00\", 1, 50.0),\n",
        "    (4, 101, \"2023-01-01 10:45:00\", 1, 50.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"customer_id\", \"order_time\", \"product_id\", \"amount\"])\n",
        "expected_df = expected_df.withColumn(\"order_time\", col(\"order_time\").cast(\"timestamp\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8915ec1-2e36-4243-8e60-c1ceac11ad92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+-------------------+----------+------+\n",
            "|order_id|customer_id|         order_time|product_id|amount|\n",
            "+--------+-----------+-------------------+----------+------+\n",
            "|       2|        101|2023-01-01 10:30:00|         1|  50.0|\n",
            "|       4|        101|2023-01-01 10:45:00|         1|  50.0|\n",
            "+--------+-----------+-------------------+----------+------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "win  = Window.partitionBy('customer_id','product_id').orderBy('order_time')\n",
        "\n",
        "result_df = orders_df\\\n",
        "              .withColumn('lastTime', fn.lag('order_time',1).over(win))\\\n",
        "              .withColumn('nextTime', fn.lead('order_time',1).over(win))\\\n",
        "              .withColumn('timeIntervalLastTime', fn.expr('order_time - lastTime'))\\\n",
        "              .withColumn('timeIntervalNextTime', fn.expr('nextTime - order_time'))\\\n",
        "              .filter('''(timeIntervalLastTime < INTERVAL 1 HOUR) OR\n",
        "                        (timeIntervalNextTime < INTERVAL 1 HOUR) ''')\\\n",
        "              .drop('lastTime','nextTime','timeIntervalLastTime','timeIntervalNextTime')\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-8"
      },
      "source": [
        "**Instructor Notes:** Window functions with time-based duplicate detection. Tests timestamp operations and conditional filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-9"
      },
      "source": [
        "## Problem 9: Product Price Range Categorization\n",
        "\n",
        "**Requirement:** Pricing team wants to categorize products into price ranges for analysis.\n",
        "\n",
        "**Scenario:** Use CASE WHEN statements to categorize products by price ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ee8acd-8161-4463-f08a-6090947f7f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+------+\n",
            "|product_id|product_name| price|\n",
            "+----------+------------+------+\n",
            "|      P001|      Laptop|999.99|\n",
            "|      P002|       Mouse|  25.5|\n",
            "|      P003|    Keyboard|  75.0|\n",
            "|      P004|     Monitor|299.99|\n",
            "|      P005|  Headphones| 150.0|\n",
            "|      P006|      Tablet| 450.0|\n",
            "+----------+------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "products_data = [\n",
        "    (\"P001\", \"Laptop\", 999.99),\n",
        "    (\"P002\", \"Mouse\", 25.50),\n",
        "    (\"P003\", \"Keyboard\", 75.00),\n",
        "    (\"P004\", \"Monitor\", 299.99),\n",
        "    (\"P005\", \"Headphones\", 150.00),\n",
        "    (\"P006\", \"Tablet\", 450.00)\n",
        "]\n",
        "\n",
        "products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"price\"])\n",
        "products_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247fe31d-54e2-4d62-82da-235e9b91b81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+------+--------------+\n",
            "|product_id|product_name| price|price_category|\n",
            "+----------+------------+------+--------------+\n",
            "|      P001|      Laptop|999.99|       Premium|\n",
            "|      P002|       Mouse|  25.5|        Budget|\n",
            "|      P003|    Keyboard|  75.0|      Standard|\n",
            "|      P004|     Monitor|299.99|      Standard|\n",
            "|      P005|  Headphones| 150.0|      Standard|\n",
            "|      P006|      Tablet| 450.0|       Premium|\n",
            "+----------+------------+------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"P001\", \"Laptop\", 999.99, \"Premium\"),\n",
        "    (\"P002\", \"Mouse\", 25.50, \"Budget\"),\n",
        "    (\"P003\", \"Keyboard\", 75.00, \"Standard\"),\n",
        "    (\"P004\", \"Monitor\", 299.99, \"Standard\"),\n",
        "    (\"P005\", \"Headphones\", 150.00, \"Standard\"),\n",
        "    (\"P006\", \"Tablet\", 450.00, \"Premium\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"price\", \"price_category\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49951887-7a5b-4059-ac29-1080351e1f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+------+--------------+\n",
            "|product_id|product_name| price|price_category|\n",
            "+----------+------------+------+--------------+\n",
            "|      P001|      Laptop|999.99|       Premium|\n",
            "|      P002|       Mouse|  25.5|        Budget|\n",
            "|      P003|    Keyboard|  75.0|        Budget|\n",
            "|      P004|     Monitor|299.99|      Standard|\n",
            "|      P005|  Headphones| 150.0|      Standard|\n",
            "|      P006|      Tablet| 450.0|       Premium|\n",
            "+----------+------------+------+--------------+\n",
            "\n",
            "+----------+------------+------+--------------+\n",
            "|product_id|product_name| price|price_category|\n",
            "+----------+------------+------+--------------+\n",
            "|      P001|      Laptop|999.99|       Premium|\n",
            "|      P002|       Mouse|  25.5|        Budget|\n",
            "|      P003|    Keyboard|  75.0|      Standard|\n",
            "|      P004|     Monitor|299.99|      Standard|\n",
            "|      P005|  Headphones| 150.0|      Standard|\n",
            "|      P006|      Tablet| 450.0|       Premium|\n",
            "+----------+------------+------+--------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "products_df\\\n",
        "        .withColumn('price_category',\n",
        "          fn.when(fn.expr('price <= 75'),'Budget')\\\n",
        "          .when(fn.expr('price <= 300'),'Standard')\\\n",
        "          .otherwise('Premium'))\\\n",
        "      .show()\n",
        "\n",
        "\n",
        "result_df = products_df\\\n",
        "              .withColumn('price_category',\n",
        "                fn.expr('''CASE\n",
        "                  WHEN price <= 70 THEN 'Budget'\n",
        "                  WHEN price <= 300 THEN 'Standard'\n",
        "                  ELSE 'Premium' END '''))\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-9"
      },
      "source": [
        "**Instructor Notes:** Conditional logic with CASE WHEN. Tests when().otherwise() pattern for categorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-10"
      },
      "source": [
        "## Problem 10: Customer Order Summary\n",
        "\n",
        "**Requirement:** Sales team wants a summary of each customer's order history.\n",
        "\n",
        "**Scenario:** For each customer, calculate total orders, total amount, and average order value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba17900-3c4b-45ae-d402-bf0f2f7b1d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+\n",
            "|order_id|customer_id|amount|\n",
            "+--------+-----------+------+\n",
            "|       1|        101| 100.0|\n",
            "|       2|        101| 150.0|\n",
            "|       3|        102| 200.0|\n",
            "|       4|        101|  75.0|\n",
            "|       5|        103| 300.0|\n",
            "|       6|        102| 250.0|\n",
            "|       7|        103| 100.0|\n",
            "|       8|        104| 500.0|\n",
            "+--------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_orders_data = [\n",
        "    (1, 101, 100.0),\n",
        "    (2, 101, 150.0),\n",
        "    (3, 102, 200.0),\n",
        "    (4, 101, 75.0),\n",
        "    (5, 103, 300.0),\n",
        "    (6, 102, 250.0),\n",
        "    (7, 103, 100.0),\n",
        "    (8, 104, 500.0)\n",
        "]\n",
        "\n",
        "customer_orders_df = spark.createDataFrame(customer_orders_data, [\"order_id\", \"customer_id\", \"amount\"])\n",
        "customer_orders_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339344e3-4323-4f22-a363-7753a5d65092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+------------+---------------+\n",
            "|customer_id|total_orders|total_amount|avg_order_value|\n",
            "+-----------+------------+------------+---------------+\n",
            "|        104|           1|       500.0|          500.0|\n",
            "|        103|           2|       400.0|          200.0|\n",
            "|        101|           3|       325.0|         108.33|\n",
            "|        102|           2|       450.0|          225.0|\n",
            "+-----------+------------+------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (104, 1, 500.0, 500.0),\n",
        "    (103, 2, 400.0, 200.0),\n",
        "    (101, 3, 325.0, 108.33),\n",
        "    (102, 2, 450.0, 225.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"total_orders\", \"total_amount\", \"avg_order_value\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5e9173-aca0-4bdf-dc5f-151564fe76d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+------------+---------------+\n",
            "|customer_id|total_orders|total_amount|avg_order_value|\n",
            "+-----------+------------+------------+---------------+\n",
            "|        101|           3|       325.0|         108.33|\n",
            "|        102|           2|       450.0|          225.0|\n",
            "|        103|           2|       400.0|          200.0|\n",
            "|        104|           1|       500.0|          500.0|\n",
            "+-----------+------------+------------+---------------+\n",
            "\n",
            "+-----------+------------+------------+---------------+\n",
            "|customer_id|total_orders|total_amount|avg_order_value|\n",
            "+-----------+------------+------------+---------------+\n",
            "|        101|           3|       325.0|         108.33|\n",
            "|        102|           2|       450.0|          225.0|\n",
            "|        103|           2|       400.0|          200.0|\n",
            "|        104|           1|       500.0|          500.0|\n",
            "+-----------+------------+------------+---------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "customer_orders_df\\\n",
        "              .groupBy('customer_id')\\\n",
        "              .agg(\n",
        "                  fn.count(col('order_id')).alias('total_orders'),\n",
        "                  fn.sum(col('amount')).alias('total_amount'),\n",
        "                  fn.round(fn.avg(col('amount')),2).alias('avg_order_value')\n",
        "                  )\\\n",
        "              .show()\n",
        "\n",
        "result_df = customer_orders_df\\\n",
        "              .groupBy('customer_id')\\\n",
        "              .agg(\n",
        "                  fn.expr('count(order_id) as total_orders'),\n",
        "                  fn.expr('sum(amount) as total_amount'),\n",
        "                  fn.expr('round(avg(amount),2) as avg_order_value')\n",
        "                  )\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-10"
      },
      "source": [
        "**Instructor Notes:** Multiple aggregations in single groupBy. Tests count, sum, avg functions together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-11"
      },
      "source": [
        "## Problem 11: Employee Salary Percentile\n",
        "\n",
        "**Requirement:** HR analytics needs to calculate salary percentiles by department.\n",
        "\n",
        "**Scenario:** Use window functions to calculate percentile rank of salaries within each department."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7ba862-a8da-40dd-84b9-20a9edcc6265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+------+\n",
            "|emp_id|emp_name| department|salary|\n",
            "+------+--------+-----------+------+\n",
            "|     1|    John|Engineering| 80000|\n",
            "|     2|    Jane|Engineering| 95000|\n",
            "|     3|     Bob|Engineering| 70000|\n",
            "|     4|   Alice|  Marketing| 60000|\n",
            "|     5| Charlie|  Marketing| 75000|\n",
            "|     6|   Diana|      Sales| 65000|\n",
            "|     7|     Eve|      Sales| 85000|\n",
            "+------+--------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "employees_salary_data = [\n",
        "    (1, \"John\", \"Engineering\", 80000),\n",
        "    (2, \"Jane\", \"Engineering\", 95000),\n",
        "    (3, \"Bob\", \"Engineering\", 70000),\n",
        "    (4, \"Alice\", \"Marketing\", 60000),\n",
        "    (5, \"Charlie\", \"Marketing\", 75000),\n",
        "    (6, \"Diana\", \"Sales\", 65000),\n",
        "    (7, \"Eve\", \"Sales\", 85000)\n",
        "]\n",
        "\n",
        "employees_salary_df = spark.createDataFrame(employees_salary_data, [\"emp_id\", \"emp_name\", \"department\", \"salary\"])\n",
        "employees_salary_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e77d4bc-4c65-4958-978b-be275879d44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+------+----------+\n",
            "|emp_id|emp_name| department|salary|percentile|\n",
            "+------+--------+-----------+------+----------+\n",
            "|     1|    John|Engineering| 80000|       0.5|\n",
            "|     2|    Jane|Engineering| 95000|       1.0|\n",
            "|     3|     Bob|Engineering| 70000|       0.0|\n",
            "|     4|   Alice|  Marketing| 60000|       0.0|\n",
            "|     5| Charlie|  Marketing| 75000|       1.0|\n",
            "|     6|   Diana|      Sales| 65000|       0.0|\n",
            "|     7|     Eve|      Sales| 85000|       1.0|\n",
            "+------+--------+-----------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John\", \"Engineering\", 80000, 0.5),\n",
        "    (2, \"Jane\", \"Engineering\", 95000, 1.0),\n",
        "    (3, \"Bob\", \"Engineering\", 70000, 0.0),\n",
        "    (4, \"Alice\", \"Marketing\", 60000, 0.0),\n",
        "    (5, \"Charlie\", \"Marketing\", 75000, 1.0),\n",
        "    (6, \"Diana\", \"Sales\", 65000, 0.0),\n",
        "    (7, \"Eve\", \"Sales\", 85000, 1.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"department\", \"salary\", \"percentile\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e087116d-eae4-42e8-f30e-b6b2006bc688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+------+----------+\n",
            "|emp_id|emp_name| department|salary|percentile|\n",
            "+------+--------+-----------+------+----------+\n",
            "|     3|     Bob|Engineering| 70000|       0.0|\n",
            "|     1|    John|Engineering| 80000|       0.5|\n",
            "|     2|    Jane|Engineering| 95000|       1.0|\n",
            "|     4|   Alice|  Marketing| 60000|       0.0|\n",
            "|     5| Charlie|  Marketing| 75000|       1.0|\n",
            "|     6|   Diana|      Sales| 65000|       0.0|\n",
            "|     7|     Eve|      Sales| 85000|       1.0|\n",
            "+------+--------+-----------+------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "win = Window.partitionBy('department').orderBy(fn.col('salary').asc_nulls_last())\n",
        "\n",
        "result_df = employees_salary_df\\\n",
        "          .withColumn('percentile',fn.round(fn.percent_rank().over(win),1))\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-11"
      },
      "source": [
        "**Instructor Notes:** Window function with percent_rank for percentile calculations. Tests partitioning and ranking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-12"
      },
      "source": [
        "## Problem 12: Product Sales Pivot\n",
        "\n",
        "**Requirement:** Business intelligence needs monthly sales data in pivot table format.\n",
        "\n",
        "**Scenario:** Pivot sales data to show product sales by month as columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c0a5ea-6960-4785-a058-b458463d41b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----+\n",
            "|product_id|  month|sales|\n",
            "+----------+-------+-----+\n",
            "|      P001|2023-01| 1000|\n",
            "|      P001|2023-02| 1200|\n",
            "|      P001|2023-03| 1100|\n",
            "|      P002|2023-01|  500|\n",
            "|      P002|2023-02|  600|\n",
            "|      P002|2023-03|  550|\n",
            "|      P003|2023-01|  800|\n",
            "|      P003|2023-02|  900|\n",
            "|      P003|2023-03|  850|\n",
            "+----------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "product_monthly_sales_data = [\n",
        "    (\"P001\", \"2023-01\", 1000),\n",
        "    (\"P001\", \"2023-02\", 1200),\n",
        "    (\"P001\", \"2023-03\", 1100),\n",
        "    (\"P002\", \"2023-01\", 500),\n",
        "    (\"P002\", \"2023-02\", 600),\n",
        "    (\"P002\", \"2023-03\", 550),\n",
        "    (\"P003\", \"2023-01\", 800),\n",
        "    (\"P003\", \"2023-02\", 900),\n",
        "    (\"P003\", \"2023-03\", 850)\n",
        "]\n",
        "\n",
        "product_monthly_sales_df = spark.createDataFrame(product_monthly_sales_data, [\"product_id\", \"month\", \"sales\"])\n",
        "product_monthly_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c7ff2c-e81b-47ca-b54f-512ab72f11c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+\n",
            "|product_id|2023-01|2023-02|2023-03|\n",
            "+----------+-------+-------+-------+\n",
            "|      P001|   1000|   1200|   1100|\n",
            "|      P002|    500|    600|    550|\n",
            "|      P003|    800|    900|    850|\n",
            "+----------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"P001\", 1000, 1200, 1100),\n",
        "    (\"P002\", 500, 600, 550),\n",
        "    (\"P003\", 800, 900, 850)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"2023-01\", \"2023-02\", \"2023-03\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95602440-42cd-46fb-8f12-745f57f586eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+\n",
            "|product_id|2023-01|2023-02|2023-03|\n",
            "+----------+-------+-------+-------+\n",
            "|      P003|    800|    900|    850|\n",
            "|      P002|    500|    600|    550|\n",
            "|      P001|   1000|   1200|   1100|\n",
            "+----------+-------+-------+-------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "result_df = \\\n",
        "       product_monthly_sales_df\\\n",
        "          .groupBy('product_id')\\\n",
        "          .pivot('month')\\\n",
        "          .agg(fn.expr('cast(avg(sales) as int)'))\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-12"
      },
      "source": [
        "**Instructor Notes:** Pivot table operation. Tests pivot() method with aggregation for data reshaping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-13"
      },
      "source": [
        "## Problem 13: Customer Purchase Intervals\n",
        "\n",
        "**Requirement:** Marketing team wants to analyze time between customer purchases for retention.\n",
        "\n",
        "**Scenario:** Calculate days between consecutive purchases for each customer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2440c2f5-534c-4b63-c108-000db5201b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+\n",
            "|order_id|customer_id|order_date|\n",
            "+--------+-----------+----------+\n",
            "|       1|        101|2023-01-01|\n",
            "|       2|        101|2023-01-05|\n",
            "|       3|        101|2023-01-12|\n",
            "|       4|        102|2023-01-02|\n",
            "|       5|        102|2023-01-15|\n",
            "|       6|        103|2023-01-03|\n",
            "+--------+-----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_purchases_data = [\n",
        "    (1, 101, \"2023-01-01\"),\n",
        "    (2, 101, \"2023-01-05\"),\n",
        "    (3, 101, \"2023-01-12\"),\n",
        "    (4, 102, \"2023-01-02\"),\n",
        "    (5, 102, \"2023-01-15\"),\n",
        "    (6, 103, \"2023-01-03\")\n",
        "]\n",
        "\n",
        "customer_purchases_df = spark.createDataFrame(customer_purchases_data, [\"order_id\", \"customer_id\", \"order_date\"])\n",
        "customer_purchases_df = customer_purchases_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
        "customer_purchases_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2675553-a4c6-4bec-dd82-8e0a5d1b922c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+------------------------+\n",
            "|order_id|customer_id|order_date|days_since_last_purchase|\n",
            "+--------+-----------+----------+------------------------+\n",
            "|       2|        101|2023-01-05|                       4|\n",
            "|       3|        101|2023-01-12|                       7|\n",
            "|       5|        102|2023-01-15|                      13|\n",
            "+--------+-----------+----------+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (2, 101, \"2023-01-05\", 4),\n",
        "    (3, 101, \"2023-01-12\", 7),\n",
        "    (5, 102, \"2023-01-15\", 13)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"customer_id\", \"order_date\", \"days_since_last_purchase\"])\n",
        "expected_df = expected_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5328730-16a1-4950-f12f-9e22f0467081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+------------------------+\n",
            "|order_id|customer_id|order_date|days_since_last_purchase|\n",
            "+--------+-----------+----------+------------------------+\n",
            "|       2|        101|2023-01-05|                       4|\n",
            "|       3|        101|2023-01-12|                       7|\n",
            "|       5|        102|2023-01-15|                      13|\n",
            "+--------+-----------+----------+------------------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "win = Window.partitionBy('customer_id').orderBy(fn.col('order_date').asc_nulls_last())\n",
        "\n",
        "result_df = \\\n",
        "    customer_purchases_df\\\n",
        "        .withColumn('lastPurchasedOn', fn.lag(fn.col('order_date')).over(win))\\\n",
        "        .withColumn('days_since_last_purchase', fn.col('order_date') - fn.col('lastPurchasedOn'))\\\n",
        "        .withColumn('days_since_last_purchase', fn.expr('extract (days from days_since_last_purchase)'))\\\n",
        "        .filter('days_since_last_purchase is not null')\\\n",
        "        .drop('lastPurchasedOn')\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-13"
      },
      "source": [
        "**Instructor Notes:** Date operations with window functions. Tests datediff and lag for time interval calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-14"
      },
      "source": [
        "## Problem 14: String Pattern Matching\n",
        "\n",
        "**Requirement:** Support team needs to find customers with specific email patterns for outreach.\n",
        "\n",
        "**Scenario:** Filter customers whose email contains 'support' or 'help' in any case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f14d41-1ca8-41d5-8a34-5e82c657f2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+\n",
            "|customer_id|               email|\n",
            "+-----------+--------------------+\n",
            "|          1|      john@gmail.com|\n",
            "|          2| support@company.com|\n",
            "|          3|      jane@yahoo.com|\n",
            "|          4|HELPdesk@business...|\n",
            "|          5|       bob@gmail.com|\n",
            "|          6|    info@company.com|\n",
            "+-----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_emails_data = [\n",
        "    (1, \"john@gmail.com\"),\n",
        "    (2, \"support@company.com\"),\n",
        "    (3, \"jane@yahoo.com\"),\n",
        "    (4, \"HELPdesk@business.com\"),\n",
        "    (5, \"bob@gmail.com\"),\n",
        "    (6, \"info@company.com\")\n",
        "]\n",
        "\n",
        "customer_emails_df = spark.createDataFrame(customer_emails_data, [\"customer_id\", \"email\"])\n",
        "customer_emails_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a9dfa2-f038-470e-d2d5-5183a80c3e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+\n",
            "|customer_id|               email|\n",
            "+-----------+--------------------+\n",
            "|          2| support@company.com|\n",
            "|          4|HELPdesk@business...|\n",
            "+-----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (2, \"support@company.com\"),\n",
        "    (4, \"HELPdesk@business.com\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"email\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab7ac13-f72f-460c-92b7-103d652e197e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+\n",
            "|customer_id|               email|\n",
            "+-----------+--------------------+\n",
            "|          2| support@company.com|\n",
            "|          4|HELPdesk@business...|\n",
            "+-----------+--------------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "result_df = \\\n",
        "    customer_emails_df\\\n",
        "        .filter(fn.expr(''' contains(lower(email),'support') OR\n",
        "                            contains(lower(email),'help')\n",
        "                          '''))\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-14"
      },
      "source": [
        "**Instructor Notes:** String pattern matching with regex. Tests rlike and case-insensitive pattern matching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-15"
      },
      "source": [
        "## Problem 15: Array Column Operations\n",
        "\n",
        "**Requirement:** Product team needs to analyze product tags for categorization.\n",
        "\n",
        "**Scenario:** Explode array column of product tags and count products per tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c11d3d3-9cb7-41b8-89d1-67fb7c34282d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+--------------------+\n",
            "|product_id|product_name|                tags|\n",
            "+----------+------------+--------------------+\n",
            "|      P001|      Laptop|[electronics, com...|\n",
            "|      P002|       Mouse|[electronics, acc...|\n",
            "|      P003|    Notebook|[stationery, office]|\n",
            "|      P004|     Monitor|[electronics, com...|\n",
            "|      P005|         Pen|[stationery, office]|\n",
            "+----------+------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "products_tags_data = [\n",
        "    (\"P001\", \"Laptop\", [\"electronics\", \"computing\", \"premium\"]),\n",
        "    (\"P002\", \"Mouse\", [\"electronics\", \"accessories\"]),\n",
        "    (\"P003\", \"Notebook\", [\"stationery\", \"office\"]),\n",
        "    (\"P004\", \"Monitor\", [\"electronics\", \"computing\"]),\n",
        "    (\"P005\", \"Pen\", [\"stationery\", \"office\"])\n",
        "]\n",
        "\n",
        "products_tags_df = spark.createDataFrame(products_tags_data, [\"product_id\", \"product_name\", \"tags\"])\n",
        "products_tags_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df38a25f-df67-4f14-d5ea-584ed1486233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|        tag|product_count|\n",
            "+-----------+-------------+\n",
            "|electronics|            3|\n",
            "|  computing|            2|\n",
            "| stationery|            2|\n",
            "|     office|            2|\n",
            "|    premium|            1|\n",
            "|accessories|            1|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "\n",
        "expected_data = [\n",
        "    (\"electronics\", 3),\n",
        "    (\"computing\", 2),\n",
        "    (\"stationery\", 2),\n",
        "    (\"office\", 2),\n",
        "    (\"premium\", 1),\n",
        "    (\"accessories\", 1)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"tag\", \"product_count\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434d5aee-3173-44a3-b115-67a9f6558e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|        tag|product_count|\n",
            "+-----------+-------------+\n",
            "|electronics|            3|\n",
            "|  computing|            2|\n",
            "|     office|            2|\n",
            "| stationery|            2|\n",
            "|accessories|            1|\n",
            "|    premium|            1|\n",
            "+-----------+-------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "result_df = \\\n",
        "      products_tags_df\\\n",
        "            .select('tags')\\\n",
        "            .withColumn('tags', fn.explode(fn.col('tags')))\\\n",
        "            .groupBy('tags')\\\n",
        "            .agg(fn.expr('count(*) as product_count'))\\\n",
        "            .withColumnRenamed('tags','tag')\\\n",
        "            .orderBy(fn.col('product_count').desc())\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-15"
      },
      "source": [
        "**Instructor Notes:** Array operations with explode function. Tests handling of complex types and flattening arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-16"
      },
      "source": [
        "## Problem 16: Null Value Handling\n",
        "\n",
        "**Requirement:** Data quality team needs to handle missing customer phone numbers.\n",
        "\n",
        "**Scenario:** Replace null phone numbers with 'Not Provided' and count nulls by city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1512479a-d7e1-4064-cdf2-8ad367654018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+------------+\n",
            "|customer_id|customer_name|    city|       phone|\n",
            "+-----------+-------------+--------+------------+\n",
            "|          1|         John|New York|123-456-7890|\n",
            "|          2|         Jane| Chicago|        NULL|\n",
            "|          3|          Bob|New York|        NULL|\n",
            "|          4|        Alice| Chicago|987-654-3210|\n",
            "|          5|      Charlie|  Boston|        NULL|\n",
            "|          6|        Diana|New York|555-123-4567|\n",
            "+-----------+-------------+--------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customers_contact_data = [\n",
        "    (1, \"John\", \"New York\", \"123-456-7890\"),\n",
        "    (2, \"Jane\", \"Chicago\", None),\n",
        "    (3, \"Bob\", \"New York\", None),\n",
        "    (4, \"Alice\", \"Chicago\", \"987-654-3210\"),\n",
        "    (5, \"Charlie\", \"Boston\", None),\n",
        "    (6, \"Diana\", \"New York\", \"555-123-4567\")\n",
        "]\n",
        "\n",
        "customers_contact_df = spark.createDataFrame(customers_contact_data, [\"customer_id\", \"customer_name\", \"city\", \"phone\"])\n",
        "customers_contact_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50485b6e-f91f-45e6-8679-87b402975ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+------------+\n",
            "|customer_id|customer_name|    city|       phone|\n",
            "+-----------+-------------+--------+------------+\n",
            "|          1|         John|New York|123-456-7890|\n",
            "|          2|         Jane| Chicago|Not Provided|\n",
            "|          3|          Bob|New York|Not Provided|\n",
            "|          4|        Alice| Chicago|987-654-3210|\n",
            "|          5|      Charlie|  Boston|Not Provided|\n",
            "|          6|        Diana|New York|555-123-4567|\n",
            "+-----------+-------------+--------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John\", \"New York\", \"123-456-7890\"),\n",
        "    (2, \"Jane\", \"Chicago\", \"Not Provided\"),\n",
        "    (3, \"Bob\", \"New York\", \"Not Provided\"),\n",
        "    (4, \"Alice\", \"Chicago\", \"987-654-3210\"),\n",
        "    (5, \"Charlie\", \"Boston\", \"Not Provided\"),\n",
        "    (6, \"Diana\", \"New York\", \"555-123-4567\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"city\", \"phone\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7271450-a47f-4b29-8b6e-e99fd81fb18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+------------+\n",
            "|customer_id|customer_name|    city|       phone|\n",
            "+-----------+-------------+--------+------------+\n",
            "|          1|         John|New York|123-456-7890|\n",
            "|          2|         Jane| Chicago|Not Provided|\n",
            "|          3|          Bob|New York|Not Provided|\n",
            "|          4|        Alice| Chicago|987-654-3210|\n",
            "|          5|      Charlie|  Boston|Not Provided|\n",
            "|          6|        Diana|New York|555-123-4567|\n",
            "+-----------+-------------+--------+------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "result_df = \\\n",
        "      customers_contact_df\\\n",
        "          .withColumn('phone',fn.nvl(fn.col('phone'),\n",
        "                                     fn.lit('Not Provided')))\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-16"
      },
      "source": [
        "**Instructor Notes:** Null value handling with fillna. Tests data cleaning and missing value imputation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-17"
      },
      "source": [
        "## Problem 17: Column Renaming and Selection\n",
        "\n",
        "**Requirement:** Reporting team needs specific column names for their dashboard.\n",
        "\n",
        "**Scenario:** Select specific columns and rename them to business-friendly names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d81133-b37a-4611-b04d-b680a3bdf871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-----------+------+----------+\n",
            "|emp_id|   emp_name| department|salary| hire_date|\n",
            "+------+-----------+-----------+------+----------+\n",
            "|     1|   John Doe|Engineering| 80000|2020-01-15|\n",
            "|     2| Jane Smith|  Marketing| 75000|2019-03-20|\n",
            "|     3|Bob Johnson|Engineering| 90000|2018-06-10|\n",
            "|     4|Alice Brown|      Sales| 70000|2021-02-05|\n",
            "+------+-----------+-----------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "employee_details_data = [\n",
        "    (1, \"John Doe\", \"Engineering\", 80000, \"2020-01-15\"),\n",
        "    (2, \"Jane Smith\", \"Marketing\", 75000, \"2019-03-20\"),\n",
        "    (3, \"Bob Johnson\", \"Engineering\", 90000, \"2018-06-10\"),\n",
        "    (4, \"Alice Brown\", \"Sales\", 70000, \"2021-02-05\")\n",
        "]\n",
        "\n",
        "employee_details_df = spark.createDataFrame(employee_details_data, [\"emp_id\", \"emp_name\", \"department\", \"salary\", \"hire_date\"])\n",
        "employee_details_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626c722d-1f76-4003-9875-82ae206208f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+------------+\n",
            "|EmployeeID|   FullName| Department|AnnualSalary|\n",
            "+----------+-----------+-----------+------------+\n",
            "|         1|   John Doe|Engineering|       80000|\n",
            "|         2| Jane Smith|  Marketing|       75000|\n",
            "|         3|Bob Johnson|Engineering|       90000|\n",
            "|         4|Alice Brown|      Sales|       70000|\n",
            "+----------+-----------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John Doe\", \"Engineering\", 80000),\n",
        "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
        "    (3, \"Bob Johnson\", \"Engineering\", 90000),\n",
        "    (4, \"Alice Brown\", \"Sales\", 70000)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"EmployeeID\", \"FullName\", \"Department\", \"AnnualSalary\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac2a741-f14c-4e6f-ef69-c0fb773282f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+------------+\n",
            "|EmployeeID|   FullName| Department|AnnualSalary|\n",
            "+----------+-----------+-----------+------------+\n",
            "|         1|   John Doe|Engineering|       80000|\n",
            "|         2| Jane Smith|  Marketing|       75000|\n",
            "|         3|Bob Johnson|Engineering|       90000|\n",
            "|         4|Alice Brown|      Sales|       70000|\n",
            "+----------+-----------+-----------+------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "result_df = \\\n",
        "      employee_details_df\\\n",
        "        .select('emp_id','emp_name','department','salary')\\\n",
        "        .withColumnsRenamed({'emp_id':'EmployeeID',\n",
        "                            'emp_name':'FullName',\n",
        "                            'department':'Department',\n",
        "                            'salary':'AnnualSalary'})\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-17"
      },
      "source": [
        "**Instructor Notes:** Column selection and renaming. Tests alias usage and selective column operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-18"
      },
      "source": [
        "## Problem 18: Date Format Conversion\n",
        "\n",
        "**Requirement:** International team needs dates in specific format for reporting.\n",
        "\n",
        "**Scenario:** Convert date format from YYYY-MM-DD to DD/MM/YYYY for international standards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7bc1d0-1430-4547-db54-2ec84d28048d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|order_id|order_date|\n",
            "+--------+----------+\n",
            "|       1|2023-01-15|\n",
            "|       2|2023-02-20|\n",
            "|       3|2023-03-10|\n",
            "|       4|2023-04-05|\n",
            "|       5|2023-05-25|\n",
            "+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "orders_date_data = [\n",
        "    (1, \"2023-01-15\"),\n",
        "    (2, \"2023-02-20\"),\n",
        "    (3, \"2023-03-10\"),\n",
        "    (4, \"2023-04-05\"),\n",
        "    (5, \"2023-05-25\")\n",
        "]\n",
        "\n",
        "orders_date_df = spark.createDataFrame(orders_date_data, [\"order_id\", \"order_date\"])\n",
        "orders_date_df = orders_date_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
        "orders_date_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3ea4fd-c509-40a9-de36-d3106a301956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------+\n",
            "|order_id|formatted_date|\n",
            "+--------+--------------+\n",
            "|       1|    15/01/2023|\n",
            "|       2|    20/02/2023|\n",
            "|       3|    10/03/2023|\n",
            "|       4|    05/04/2023|\n",
            "|       5|    25/05/2023|\n",
            "+--------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"15/01/2023\"),\n",
        "    (2, \"20/02/2023\"),\n",
        "    (3, \"10/03/2023\"),\n",
        "    (4, \"05/04/2023\"),\n",
        "    (5, \"25/05/2023\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"formatted_date\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0acc376-4928-4063-836d-e8ee0737d9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------+\n",
            "|order_id|formatted_date|\n",
            "+--------+--------------+\n",
            "|       1|    15/01/2023|\n",
            "|       2|    20/02/2023|\n",
            "|       3|    10/03/2023|\n",
            "|       4|    05/04/2023|\n",
            "|       5|    25/05/2023|\n",
            "+--------+--------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "result_df = \\\n",
        "      orders_date_df\\\n",
        "          .withColumn('formatted_date',fn.date_format(fn.col('order_date'),'dd/MM/yyyy'))\\\n",
        "          .drop('order_date')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-18"
      },
      "source": [
        "**Instructor Notes:** Date formatting operations. Tests date_format function for international date standards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-19"
      },
      "source": [
        "## Problem 19: Simple Union Operation\n",
        "\n",
        "**Requirement:** Operations team needs to combine current and historical customer data.\n",
        "\n",
        "**Scenario:** Union two customer DataFrames with same schema into single dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29f0ace-492c-4d69-aa90-6fd82d6417a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Customers:\n",
            "+-----------+-------------+------+\n",
            "|customer_id|customer_name|status|\n",
            "+-----------+-------------+------+\n",
            "|          1|         John|Active|\n",
            "|          2|         Jane|Active|\n",
            "|          3|          Bob|Active|\n",
            "+-----------+-------------+------+\n",
            "\n",
            "Historical Customers:\n",
            "+-----------+-------------+--------+\n",
            "|customer_id|customer_name|  status|\n",
            "+-----------+-------------+--------+\n",
            "|          4|        Alice|Inactive|\n",
            "|          5|      Charlie|Inactive|\n",
            "+-----------+-------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "current_customers_data = [\n",
        "    (1, \"John\", \"Active\"),\n",
        "    (2, \"Jane\", \"Active\"),\n",
        "    (3, \"Bob\", \"Active\")\n",
        "]\n",
        "\n",
        "historical_customers_data = [\n",
        "    (4, \"Alice\", \"Inactive\"),\n",
        "    (5, \"Charlie\", \"Inactive\")\n",
        "]\n",
        "\n",
        "current_customers_df = spark.createDataFrame(current_customers_data, [\"customer_id\", \"customer_name\", \"status\"])\n",
        "historical_customers_df = spark.createDataFrame(historical_customers_data, [\"customer_id\", \"customer_name\", \"status\"])\n",
        "\n",
        "print(\"Current Customers:\")\n",
        "current_customers_df.show()\n",
        "print(\"Historical Customers:\")\n",
        "historical_customers_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b80245f-16a6-4366-9b42-175356623c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+\n",
            "|customer_id|customer_name|  status|\n",
            "+-----------+-------------+--------+\n",
            "|          1|         John|  Active|\n",
            "|          2|         Jane|  Active|\n",
            "|          3|          Bob|  Active|\n",
            "|          4|        Alice|Inactive|\n",
            "|          5|      Charlie|Inactive|\n",
            "+-----------+-------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John\", \"Active\"),\n",
        "    (2, \"Jane\", \"Active\"),\n",
        "    (3, \"Bob\", \"Active\"),\n",
        "    (4, \"Alice\", \"Inactive\"),\n",
        "    (5, \"Charlie\", \"Inactive\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"status\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310665d2-f6da-4a90-f72b-4d98495ecc07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+\n",
            "|customer_id|customer_name|  status|\n",
            "+-----------+-------------+--------+\n",
            "|          1|         John|  Active|\n",
            "|          2|         Jane|  Active|\n",
            "|          3|          Bob|  Active|\n",
            "|          4|        Alice|Inactive|\n",
            "|          5|      Charlie|Inactive|\n",
            "+-----------+-------------+--------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "result_df = \\\n",
        "      current_customers_df\\\n",
        "        .union(historical_customers_df)\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-19"
      },
      "source": [
        "**Instructor Notes:** Union operation for combining datasets. Tests union() with same schema DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-20"
      },
      "source": [
        "## Problem 20: Distinct Value Count\n",
        "\n",
        "**Requirement:** Data governance team needs to count distinct values for data quality assessment.\n",
        "\n",
        "**Scenario:** Count distinct departments and distinct job titles in employee data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2504ff-1761-4dac-f046-4661ea01f3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+-----------------+\n",
            "|emp_id|emp_name| department|        job_title|\n",
            "+------+--------+-----------+-----------------+\n",
            "|     1|    John|Engineering|Software Engineer|\n",
            "|     2|    Jane|Engineering|   Data Scientist|\n",
            "|     3|     Bob|  Marketing|Marketing Manager|\n",
            "|     4|   Alice|  Marketing|   Content Writer|\n",
            "|     5| Charlie|Engineering|Software Engineer|\n",
            "|     6|   Diana|      Sales|  Sales Executive|\n",
            "|     7|     Eve|      Sales|  Sales Executive|\n",
            "+------+--------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "employees_diverse_data = [\n",
        "    (1, \"John\", \"Engineering\", \"Software Engineer\"),\n",
        "    (2, \"Jane\", \"Engineering\", \"Data Scientist\"),\n",
        "    (3, \"Bob\", \"Marketing\", \"Marketing Manager\"),\n",
        "    (4, \"Alice\", \"Marketing\", \"Content Writer\"),\n",
        "    (5, \"Charlie\", \"Engineering\", \"Software Engineer\"),\n",
        "    (6, \"Diana\", \"Sales\", \"Sales Executive\"),\n",
        "    (7, \"Eve\", \"Sales\", \"Sales Executive\")\n",
        "]\n",
        "\n",
        "employees_diverse_df = spark.createDataFrame(employees_diverse_data, [\"emp_id\", \"emp_name\", \"department\", \"job_title\"])\n",
        "employees_diverse_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e40a19-d0d0-4350-b0f4-30c31706fb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+\n",
            "|distinct_departments|distinct_job_titles|\n",
            "+--------------------+-------------------+\n",
            "|                   3|                  5|\n",
            "+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (3, 5)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"distinct_departments\", \"distinct_job_titles\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648909af-177a-4030-a51c-0cdbc7adb358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+\n",
            "|distinct_departments|distinct_job_titles|\n",
            "+--------------------+-------------------+\n",
            "|                   3|                  5|\n",
            "+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "result_df = \\\n",
        "          employees_diverse_df\\\n",
        "            .select(\n",
        "                    fn.countDistinct(col('department')).alias('distinct_departments'),\n",
        "                    fn.countDistinct(col('job_title')).alias('distinct_job_titles')\n",
        "                    )\n",
        "result_df.show()\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-20"
      },
      "source": [
        "**Instructor Notes:** Distinct counting with countDistinct. Tests aggregation without grouping for overall distinct counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-21"
      },
      "source": [
        "## Problem 21: Column Concatenation\n",
        "\n",
        "**Requirement:** Reporting team needs full names from separate first and last name columns.\n",
        "\n",
        "**Scenario:** Concatenate first and last name columns with space separator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0dca39-9b5b-4b68-8580-5db726a01fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+---------+\n",
            "|emp_id|first_name|last_name|\n",
            "+------+----------+---------+\n",
            "|     1|      John|      Doe|\n",
            "|     2|      Jane|    Smith|\n",
            "|     3|       Bob|  Johnson|\n",
            "|     4|     Alice|    Brown|\n",
            "|     5|   Charlie|   Wilson|\n",
            "+------+----------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "employees_names_data = [\n",
        "    (1, \"John\", \"Doe\"),\n",
        "    (2, \"Jane\", \"Smith\"),\n",
        "    (3, \"Bob\", \"Johnson\"),\n",
        "    (4, \"Alice\", \"Brown\"),\n",
        "    (5, \"Charlie\", \"Wilson\")\n",
        "]\n",
        "\n",
        "employees_names_df = spark.createDataFrame(employees_names_data, [\"emp_id\", \"first_name\", \"last_name\"])\n",
        "employees_names_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd78906-04a1-4fdc-96da-da12703ae981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+\n",
            "|emp_id|     full_name|\n",
            "+------+--------------+\n",
            "|     1|      John Doe|\n",
            "|     2|    Jane Smith|\n",
            "|     3|   Bob Johnson|\n",
            "|     4|   Alice Brown|\n",
            "|     5|Charlie Wilson|\n",
            "+------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John Doe\"),\n",
        "    (2, \"Jane Smith\"),\n",
        "    (3, \"Bob Johnson\"),\n",
        "    (4, \"Alice Brown\"),\n",
        "    (5, \"Charlie Wilson\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"full_name\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e50939-d915-4dc3-93f8-cd0c50b25d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+\n",
            "|emp_id|     full_name|\n",
            "+------+--------------+\n",
            "|     1|      John Doe|\n",
            "|     2|    Jane Smith|\n",
            "|     3|   Bob Johnson|\n",
            "|     4|   Alice Brown|\n",
            "|     5|Charlie Wilson|\n",
            "+------+--------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    employees_names_df\\\n",
        "        .select('emp_id', fn.concat_ws(' ',fn.col('first_name'), fn.col('last_name')).alias('full_name'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-21"
      },
      "source": [
        "**Instructor Notes:** String concatenation with concat function. Tests string manipulation and literal usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-22"
      },
      "source": [
        "## Problem 22: Row Number Generation\n",
        "\n",
        "**Requirement:** Analytics team needs sequential row numbers for data processing.\n",
        "\n",
        "**Scenario:** Add sequential row numbers to customer data ordered by customer_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5b239b-476f-49f5-d2b1-230c60eb130f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|customer_id|customer_name|\n",
            "+-----------+-------------+\n",
            "|        105|         John|\n",
            "|        102|         Jane|\n",
            "|        108|          Bob|\n",
            "|        101|        Alice|\n",
            "|        107|      Charlie|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customers_sequential_data = [\n",
        "    (105, \"John\"),\n",
        "    (102, \"Jane\"),\n",
        "    (108, \"Bob\"),\n",
        "    (101, \"Alice\"),\n",
        "    (107, \"Charlie\")\n",
        "]\n",
        "\n",
        "customers_sequential_df = spark.createDataFrame(customers_sequential_data, [\"customer_id\", \"customer_name\"])\n",
        "customers_sequential_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf99976-1ad3-4db2-a580-6d4bdd208421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+\n",
            "|customer_id|customer_name|row_number|\n",
            "+-----------+-------------+----------+\n",
            "|        101|        Alice|         1|\n",
            "|        102|         Jane|         2|\n",
            "|        105|         John|         3|\n",
            "|        107|      Charlie|         4|\n",
            "|        108|          Bob|         5|\n",
            "+-----------+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (101, \"Alice\", 1),\n",
        "    (102, \"Jane\", 2),\n",
        "    (105, \"John\", 3),\n",
        "    (107, \"Charlie\", 4),\n",
        "    (108, \"Bob\", 5)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"row_number\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c966ecb9-1f9f-49f7-dd77-d9352d1d920e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+\n",
            "|customer_id|customer_name|row_number|\n",
            "+-----------+-------------+----------+\n",
            "|        101|        Alice|         1|\n",
            "|        102|         Jane|         2|\n",
            "|        105|         John|         3|\n",
            "|        107|      Charlie|         4|\n",
            "|        108|          Bob|         5|\n",
            "+-----------+-------------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "win = Window.orderBy(col('customer_id').asc_nulls_last())\n",
        "\n",
        "result_df = \\\n",
        "      customers_sequential_df\\\n",
        "        .withColumn('row_number',fn.row_number().over(win))\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-22"
      },
      "source": [
        "**Instructor Notes:** Row number generation with window functions. Tests row_number() for sequential numbering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-23"
      },
      "source": [
        "## Problem 23: Simple Conditional Aggregation\n",
        "\n",
        "**Requirement:** Finance team needs separate totals for domestic and international sales.\n",
        "\n",
        "**Scenario:** Calculate total sales amount for domestic vs international orders using conditional sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed647719-8a6f-494f-b092-ec406f0f43d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+------+\n",
            "|order_id|   order_type|amount|\n",
            "+--------+-------------+------+\n",
            "|       1|     Domestic|1000.0|\n",
            "|       2|International|1500.0|\n",
            "|       3|     Domestic| 800.0|\n",
            "|       4|International|2000.0|\n",
            "|       5|     Domestic|1200.0|\n",
            "|       6|International|1800.0|\n",
            "+--------+-------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "orders_international_data = [\n",
        "    (1, \"Domestic\", 1000.0),\n",
        "    (2, \"International\", 1500.0),\n",
        "    (3, \"Domestic\", 800.0),\n",
        "    (4, \"International\", 2000.0),\n",
        "    (5, \"Domestic\", 1200.0),\n",
        "    (6, \"International\", 1800.0)\n",
        "]\n",
        "\n",
        "orders_international_df = spark.createDataFrame(orders_international_data, [\"order_id\", \"order_type\", \"amount\"])\n",
        "orders_international_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e2e2ba-e51d-48b3-ef0e-757ff437e8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------------+\n",
            "|domestic_sales|international_sales|\n",
            "+--------------+-------------------+\n",
            "|        3000.0|             5300.0|\n",
            "+--------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (3000.0, 5300.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"domestic_sales\", \"international_sales\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41021ffa-9dc6-48a9-d4eb-e63c6c50bb9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------------+\n",
            "|domestic_sales|international_sales|\n",
            "+--------------+-------------------+\n",
            "|        3000.0|             5300.0|\n",
            "+--------------+-------------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      orders_international_df\\\n",
        "        .select(\n",
        "                fn.expr('''sum(case when order_type = 'Domestic' then amount end) as domestic_sales '''),\n",
        "                fn.expr('''sum(case when order_type = 'International' then amount end) as international_sales ''')\n",
        "                )\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-23"
      },
      "source": [
        "**Instructor Notes:** Conditional aggregation with when().otherwise(). Tests conditional sum operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-24"
      },
      "source": [
        "## Problem 24: Data Type Conversion\n",
        "\n",
        "**Requirement:** Data engineering team needs to convert string columns to proper data types.\n",
        "\n",
        "**Scenario:** Convert string representations of numbers and dates to appropriate data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63249fed-67fb-44d8-c97c-4439b7bcb3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----------+\n",
            "|id_str|amount_str|  date_str|\n",
            "+------+----------+----------+\n",
            "|     1|   1000.50|2023-01-15|\n",
            "|     2|   2000.75|2023-02-20|\n",
            "|     3|   1500.25|2023-03-10|\n",
            "+------+----------+----------+\n",
            "\n",
            "root\n",
            " |-- id_str: string (nullable = true)\n",
            " |-- amount_str: string (nullable = true)\n",
            " |-- date_str: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "raw_data_data = [\n",
        "    (\"1\", \"1000.50\", \"2023-01-15\"),\n",
        "    (\"2\", \"2000.75\", \"2023-02-20\"),\n",
        "    (\"3\", \"1500.25\", \"2023-03-10\")\n",
        "]\n",
        "\n",
        "raw_data_df = spark.createDataFrame(raw_data_data, [\"id_str\", \"amount_str\", \"date_str\"])\n",
        "raw_data_df.show()\n",
        "raw_data_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ecb4d8-fcad-4568-8c23-f47fc43f0e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+\n",
            "| id| amount|      date|\n",
            "+---+-------+----------+\n",
            "|  1| 1000.5|2023-01-15|\n",
            "|  2|2000.75|2023-02-20|\n",
            "|  3|1500.25|2023-03-10|\n",
            "+---+-------+----------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, 1000.5, \"2023-01-15\"),\n",
        "    (2, 2000.75, \"2023-02-20\"),\n",
        "    (3, 1500.25, \"2023-03-10\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"id\", \"amount\", \"date\"])\n",
        "expected_df = expected_df.withColumn(\"id\", col(\"id\").cast(\"integer\"))\\\n",
        "                       .withColumn(\"amount\", col(\"amount\").cast(\"double\"))\\\n",
        "                       .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()\n",
        "expected_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ce261b-6916-41bd-9e75-92b905d9db69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+\n",
            "| id| amount|      date|\n",
            "+---+-------+----------+\n",
            "|  1| 1000.5|2023-01-15|\n",
            "|  2|2000.75|2023-02-20|\n",
            "|  3|1500.25|2023-03-10|\n",
            "+---+-------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    raw_data_df\\\n",
        "      .withColumn('id', fn.col('id_str').cast('int'))\\\n",
        "      .withColumn('amount', fn.col('amount_str').cast('double'))\\\n",
        "      .withColumn('date', fn.to_date(col('date_str'),'yyyy-MM-dd'))\\\n",
        "      .drop('id_str','amount_str','date_str')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-24"
      },
      "source": [
        "**Instructor Notes:** Data type casting operations. Tests cast() method for type conversions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-25"
      },
      "source": [
        "## Problem 25: Simple Left Join\n",
        "\n",
        "**Requirement:** Customer service needs order details with customer information, including customers without orders.\n",
        "\n",
        "**Scenario:** Perform left join between customers and orders to include all customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d473842-5231-47ae-d5b5-6a0adeaebf4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers:\n",
            "+-----------+-------------+\n",
            "|customer_id|customer_name|\n",
            "+-----------+-------------+\n",
            "|          1|         John|\n",
            "|          2|         Jane|\n",
            "|          3|          Bob|\n",
            "|          4|        Alice|\n",
            "+-----------+-------------+\n",
            "\n",
            "Orders:\n",
            "+--------+-----------+------+\n",
            "|order_id|customer_id|amount|\n",
            "+--------+-----------+------+\n",
            "|     101|          1| 100.0|\n",
            "|     102|          1| 150.0|\n",
            "|     103|          2| 200.0|\n",
            "|     104|          3|  75.0|\n",
            "+--------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "customers_left_data = [\n",
        "    (1, \"John\"),\n",
        "    (2, \"Jane\"),\n",
        "    (3, \"Bob\"),\n",
        "    (4, \"Alice\")\n",
        "]\n",
        "\n",
        "orders_left_data = [\n",
        "    (101, 1, 100.0),\n",
        "    (102, 1, 150.0),\n",
        "    (103, 2, 200.0),\n",
        "    (104, 3, 75.0)\n",
        "]\n",
        "\n",
        "customers_left_df = spark.createDataFrame(customers_left_data, [\"customer_id\", \"customer_name\"])\n",
        "orders_left_df = spark.createDataFrame(orders_left_data, [\"order_id\", \"customer_id\", \"amount\"])\n",
        "\n",
        "print(\"Customers:\")\n",
        "customers_left_df.show()\n",
        "print(\"Orders:\")\n",
        "orders_left_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b8f3e5-2282-4e22-907f-77b3bff1200f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+------+\n",
            "|customer_id|customer_name|order_id|amount|\n",
            "+-----------+-------------+--------+------+\n",
            "|          1|         John|     101| 100.0|\n",
            "|          1|         John|     102| 150.0|\n",
            "|          2|         Jane|     103| 200.0|\n",
            "|          3|          Bob|     104|  75.0|\n",
            "|          4|        Alice|    NULL|  NULL|\n",
            "+-----------+-------------+--------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John\", 101, 100.0),\n",
        "    (1, \"John\", 102, 150.0),\n",
        "    (2, \"Jane\", 103, 200.0),\n",
        "    (3, \"Bob\", 104, 75.0),\n",
        "    (4, \"Alice\", None, None)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"order_id\", \"amount\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e309616-fd53-4df3-ee7f-a6dfacd1a96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------+------+\n",
            "|customer_id|customer_name|order_id|amount|\n",
            "+-----------+-------------+--------+------+\n",
            "|          1|         John|     102| 150.0|\n",
            "|          1|         John|     101| 100.0|\n",
            "|          2|         Jane|     103| 200.0|\n",
            "|          3|          Bob|     104|  75.0|\n",
            "|          4|        Alice|    NULL|  NULL|\n",
            "+-----------+-------------+--------+------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "join_on = customers_left_df.customer_id == orders_left_df.customer_id\n",
        "\n",
        "result_df = \\\n",
        "      customers_left_df\\\n",
        "        .join(orders_left_df,join_on, 'left')\\\n",
        "        .drop(orders_left_df.customer_id)\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-25"
      },
      "source": [
        "**Instructor Notes:** Left join operation. Tests different join types and handling of null values from unmatched records."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-26"
      },
      "source": [
        "## Problem 26: Basic Statistical Aggregations\n",
        "\n",
        "**Requirement:** Analytics team needs basic statistics for sales data analysis.\n",
        "\n",
        "**Scenario:** Calculate count, mean, standard deviation, min, and max of sales amounts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5903c3-0b00-42a4-9372-6f5721521e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|amount|\n",
            "+------+\n",
            "| 100.0|\n",
            "| 150.0|\n",
            "| 200.0|\n",
            "|  75.0|\n",
            "| 300.0|\n",
            "| 250.0|\n",
            "| 100.0|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "sales_stats_data = [\n",
        "    (100.0,),\n",
        "    (150.0,),\n",
        "    (200.0,),\n",
        "    (75.0,),\n",
        "    (300.0,),\n",
        "    (250.0,),\n",
        "    (100.0,)\n",
        "]\n",
        "\n",
        "sales_stats_df = spark.createDataFrame(sales_stats_data, [\"amount\"])\n",
        "sales_stats_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a43f69-7b3b-4e84-c1e8-50b75df4668c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+----+-----+\n",
            "|count|  mean|stddev| min|  max|\n",
            "+-----+------+------+----+-----+\n",
            "|    7|167.86| 87.88|75.0|300.0|\n",
            "+-----+------+------+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (7, 167.86, 87.88, 75.0, 300.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"count\", \"mean\", \"stddev\", \"min\", \"max\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77c079b-5005-4665-d17e-2e73a528628c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+----+-----+\n",
            "|count|  mean|stddev| min|  max|\n",
            "+-----+------+------+----+-----+\n",
            "|    7|167.86| 85.04|75.0|300.0|\n",
            "+-----+------+------+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "        sales_stats_df\\\n",
        "            .select(\n",
        "                    fn.count(fn.col('amount')).alias('count'),\n",
        "                    fn.round(fn.mean(fn.col('amount')),2).alias('mean'),\n",
        "                    fn.round(fn.stddev(fn.col('amount')),2).alias('stddev'),\n",
        "                    fn.round(fn.min(fn.col('amount')),2).alias('min'),\n",
        "                    fn.round(fn.max(fn.col('amount')),2).alias('max')\n",
        "                )\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-26"
      },
      "source": [
        "**Instructor Notes:** Multiple statistical aggregations. Tests various aggregation functions together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-27"
      },
      "source": [
        "## Problem 27: Column Dropping\n",
        "\n",
        "**Requirement:** Data privacy team requires removal of sensitive columns from dataset.\n",
        "\n",
        "**Scenario:** Drop sensitive columns (SSN, salary) from employee data for external sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26241749-b0db-469a-8edc-3afb12687f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+-----------+------+\n",
            "|emp_id|emp_name|        ssn| department|salary|\n",
            "+------+--------+-----------+-----------+------+\n",
            "|     1|    John|123-45-6789|Engineering| 80000|\n",
            "|     2|    Jane|987-65-4321|  Marketing| 75000|\n",
            "|     3|     Bob|456-78-9123|Engineering| 90000|\n",
            "+------+--------+-----------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "employees_sensitive_data = [\n",
        "    (1, \"John\", \"123-45-6789\", \"Engineering\", 80000),\n",
        "    (2, \"Jane\", \"987-65-4321\", \"Marketing\", 75000),\n",
        "    (3, \"Bob\", \"456-78-9123\", \"Engineering\", 90000)\n",
        "]\n",
        "\n",
        "employees_sensitive_df = spark.createDataFrame(employees_sensitive_data, [\"emp_id\", \"emp_name\", \"ssn\", \"department\", \"salary\"])\n",
        "employees_sensitive_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7e7bd5-9b97-4c29-b78d-2881c0c586fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+\n",
            "|emp_id|emp_name| department|\n",
            "+------+--------+-----------+\n",
            "|     1|    John|Engineering|\n",
            "|     2|    Jane|  Marketing|\n",
            "|     3|     Bob|Engineering|\n",
            "+------+--------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John\", \"Engineering\"),\n",
        "    (2, \"Jane\", \"Marketing\"),\n",
        "    (3, \"Bob\", \"Engineering\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"department\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30733720-a1a3-4642-9125-baaf4450c7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+\n",
            "|emp_id|emp_name| department|\n",
            "+------+--------+-----------+\n",
            "|     1|    John|Engineering|\n",
            "|     2|    Jane|  Marketing|\n",
            "|     3|     Bob|Engineering|\n",
            "+------+--------+-----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    employees_sensitive_df\\\n",
        "        .drop('ssn','salary')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "#Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-27"
      },
      "source": [
        "**Instructor Notes:** Column dropping operation. Tests drop() method for removing specific columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-28"
      },
      "source": [
        "## Problem 28: Simple Sort Operation\n",
        "\n",
        "**Requirement:** Reporting team needs customer data sorted for consistent presentation.\n",
        "\n",
        "**Scenario:** Sort customers by name in alphabetical order and by customer_id descending."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e186a286-fc2f-429a-aefe-54ee754f4f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|customer_id|customer_name|\n",
            "+-----------+-------------+\n",
            "|          3|      Charlie|\n",
            "|          1|        Alice|\n",
            "|          4|        Diana|\n",
            "|          2|          Bob|\n",
            "|          5|          Eve|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customers_sort_data = [\n",
        "    (3, \"Charlie\"),\n",
        "    (1, \"Alice\"),\n",
        "    (4, \"Diana\"),\n",
        "    (2, \"Bob\"),\n",
        "    (5, \"Eve\")\n",
        "]\n",
        "\n",
        "customers_sort_df = spark.createDataFrame(customers_sort_data, [\"customer_id\", \"customer_name\"])\n",
        "customers_sort_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed06ad52-e752-48e7-ad33-13d2f551b1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|customer_id|customer_name|\n",
            "+-----------+-------------+\n",
            "|          1|        Alice|\n",
            "|          2|          Bob|\n",
            "|          3|      Charlie|\n",
            "|          4|        Diana|\n",
            "|          5|          Eve|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"Alice\"),\n",
        "    (2, \"Bob\"),\n",
        "    (3, \"Charlie\"),\n",
        "    (4, \"Diana\"),\n",
        "    (5, \"Eve\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b326e0c-eee3-4392-a4e7-1f6629ca16ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|customer_id|customer_name|\n",
            "+-----------+-------------+\n",
            "|          1|        Alice|\n",
            "|          2|          Bob|\n",
            "|          3|      Charlie|\n",
            "|          4|        Diana|\n",
            "|          5|          Eve|\n",
            "+-----------+-------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      customers_sort_df\\\n",
        "            .orderBy(fn.col('customer_name').asc_nulls_last(),\n",
        "                    fn.col('customer_id').desc_nulls_last())\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-28"
      },
      "source": [
        "**Instructor Notes:** Sorting operation. Tests orderBy for data ordering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-29"
      },
      "source": [
        "## Problem 29: Basic Mathematical Operations\n",
        "\n",
        "**Requirement:** Finance team needs calculated fields for financial reporting.\n",
        "\n",
        "**Scenario:** Calculate tax (15%) and total amount after tax for each sale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e5953a-18a9-425b-ea8f-b854f1463b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|sale_id|amount|\n",
            "+-------+------+\n",
            "|      1| 100.0|\n",
            "|      2| 200.0|\n",
            "|      3| 150.0|\n",
            "|      4| 300.0|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "sales_tax_data = [\n",
        "    (1, 100.0),\n",
        "    (2, 200.0),\n",
        "    (3, 150.0),\n",
        "    (4, 300.0)\n",
        "]\n",
        "\n",
        "sales_tax_df = spark.createDataFrame(sales_tax_data, [\"sale_id\", \"amount\"])\n",
        "sales_tax_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6c07cb-8cc4-48e1-ed27-745420d00247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+----+------------+\n",
            "|sale_id|amount| tax|total_amount|\n",
            "+-------+------+----+------------+\n",
            "|      1| 100.0|15.0|       115.0|\n",
            "|      2| 200.0|30.0|       230.0|\n",
            "|      3| 150.0|22.5|       172.5|\n",
            "|      4| 300.0|45.0|       345.0|\n",
            "+-------+------+----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, 100.0, 15.0, 115.0),\n",
        "    (2, 200.0, 30.0, 230.0),\n",
        "    (3, 150.0, 22.5, 172.5),\n",
        "    (4, 300.0, 45.0, 345.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"sale_id\", \"amount\", \"tax\", \"total_amount\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e6567a-f522-4d84-a568-179e0d3be8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+----+------------+\n",
            "|sale_id|amount| tax|total_amount|\n",
            "+-------+------+----+------------+\n",
            "|      1| 100.0|15.0|       115.0|\n",
            "|      2| 200.0|30.0|       230.0|\n",
            "|      3| 150.0|22.5|       172.5|\n",
            "|      4| 300.0|45.0|       345.0|\n",
            "+-------+------+----+------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      sales_tax_df\\\n",
        "        .withColumn('tax', fn.expr('amount * 0.15'))\\\n",
        "        .withColumn('total_amount', col('amount') + col('tax'))\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-29"
      },
      "source": [
        "**Instructor Notes:** Mathematical operations on columns. Tests arithmetic operations and column references."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-30"
      },
      "source": [
        "## Problem 30: Simple Filter with Multiple Conditions\n",
        "\n",
        "**Requirement:** Sales team needs to identify high-value recent customers.\n",
        "\n",
        "**Scenario:** Filter customers who joined in 2023 and have spent more than $1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f520ec47-0510-4d8f-8f35-3aaa905a9e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+-----------+\n",
            "|customer_id|customer_name| join_date|total_spent|\n",
            "+-----------+-------------+----------+-----------+\n",
            "|          1|         John|2023-01-15|      800.0|\n",
            "|          2|         Jane|2023-02-20|     1500.0|\n",
            "|          3|          Bob|2022-12-10|     1200.0|\n",
            "|          4|        Alice|2023-03-05|     2000.0|\n",
            "|          5|      Charlie|2022-11-20|      900.0|\n",
            "+-----------+-------------+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customers_high_value_data = [\n",
        "    (1, \"John\", \"2023-01-15\", 800.0),\n",
        "    (2, \"Jane\", \"2023-02-20\", 1500.0),\n",
        "    (3, \"Bob\", \"2022-12-10\", 1200.0),\n",
        "    (4, \"Alice\", \"2023-03-05\", 2000.0),\n",
        "    (5, \"Charlie\", \"2022-11-20\", 900.0)\n",
        "]\n",
        "\n",
        "customers_high_value_df = spark.createDataFrame(customers_high_value_data, [\"customer_id\", \"customer_name\", \"join_date\", \"total_spent\"])\n",
        "customers_high_value_df = customers_high_value_df.withColumn(\"join_date\", col(\"join_date\").cast(\"date\"))\n",
        "customers_high_value_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30998f26-5443-4b78-a4d4-5a53808285e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+-----------+\n",
            "|customer_id|customer_name| join_date|total_spent|\n",
            "+-----------+-------------+----------+-----------+\n",
            "|          2|         Jane|2023-02-20|     1500.0|\n",
            "|          4|        Alice|2023-03-05|     2000.0|\n",
            "+-----------+-------------+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (2, \"Jane\", \"2023-02-20\", 1500.0),\n",
        "    (4, \"Alice\", \"2023-03-05\", 2000.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"join_date\", \"total_spent\"])\n",
        "expected_df = expected_df.withColumn(\"join_date\", col(\"join_date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9966ed-1ac5-4a01-f20e-a0498b071a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+-----------+\n",
            "|customer_id|customer_name| join_date|total_spent|\n",
            "+-----------+-------------+----------+-----------+\n",
            "|          2|         Jane|2023-02-20|     1500.0|\n",
            "|          4|        Alice|2023-03-05|     2000.0|\n",
            "+-----------+-------------+----------+-----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    customers_high_value_df\\\n",
        "      .filter('''\n",
        "              (join_date >= cast('2023-01-01' as date))\n",
        "              and\n",
        "              (total_spent > 1000)\n",
        "              ''')\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "#Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-30"
      },
      "source": [
        "**Instructor Notes:** Multiple condition filtering with date functions. Tests compound conditions and date extraction functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "completion-note"
      },
      "source": [
        "# Set 1 Complete!\n",
        "\n",
        "You've completed all 30 Easy problems in Set 1. These problems cover:\n",
        "- Basic filtering and selection\n",
        "- Simple aggregations\n",
        "- Basic joins\n",
        "- Window functions\n",
        "- String and date operations\n",
        "- UDFs\n",
        "- Data type conversions\n",
        "- And other fundamental PySpark operations\n",
        "\n",
        "Ready for Set 2 with Easy/Medium difficulty problems?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}