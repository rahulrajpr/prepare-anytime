{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview-section"
   },
   "source": [
    "# PySpark Interview Preparation - Set 1 (Easy)\n",
    "\n",
    "## Overview & Instructions\n",
    "\n",
    "### How to run this notebook in Google Colab:\n",
    "1. Upload this .ipynb file to Google Colab\n",
    "2. Run the installation cells below\n",
    "3. Execute each problem cell sequentially\n",
    "\n",
    "### Installation Commands:\n",
    "The following cell installs Java and PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "installation-cell"
   },
   "outputs": [],
   "source": [
    "# Install Java and PySpark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!pip install -q pyspark\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sparksession-section"
   },
   "source": [
    "### SparkSession Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sparksession-cell"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"PySparkInterviewSet1\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assert-function-section"
   },
   "source": [
    "### DataFrame Assertion Function:\n",
    "\n",
    "This function compares DataFrames ignoring order and with floating-point tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "assert-function-cell"
   },
   "outputs": [],
   "source": [
    "def assert_dataframe_equal(df_actual, df_expected, epsilon=1e-6):\n",
    "    \"\"\"Compare two DataFrames ignoring order and with floating-point tolerance\"\"\"\n",
    "    \n",
    "    # Check schema first\n",
    "    if df_actual.schema != df_expected.schema:\n",
    "        print(\"Schema mismatch!\")\n",
    "        print(\"Actual schema:\", df_actual.schema)\n",
    "        print(\"Expected schema:\", df_expected.schema)\n",
    "        raise AssertionError(\"Schema mismatch\")\n",
    "    \n",
    "    # Collect data\n",
    "    actual_data = df_actual.collect()\n",
    "    expected_data = df_expected.collect()\n",
    "    \n",
    "    if len(actual_data) != len(expected_data):\n",
    "        print(f\"Row count mismatch! Actual: {len(actual_data)}, Expected: {len(expected_data)}\")\n",
    "        raise AssertionError(\"Row count mismatch\")\n",
    "    \n",
    "    # Convert to sets of tuples for order-insensitive comparison\n",
    "    def row_to_comparable(row):\n",
    "        values = []\n",
    "        for field in row:\n",
    "            if isinstance(field, float):\n",
    "                # Handle floating point comparison\n",
    "                values.append(round(field / epsilon) * epsilon)\n",
    "            elif isinstance(field, list):\n",
    "                # Handle arrays\n",
    "                values.append(tuple(sorted(field)) if field else tuple())\n",
    "            elif isinstance(field, dict):\n",
    "                # Handle structs\n",
    "                values.append(tuple(sorted(field.items())))\n",
    "            else:\n",
    "                values.append(field)\n",
    "        return tuple(values)\n",
    "    \n",
    "    actual_set = set(row_to_comparable(row) for row in actual_data)\n",
    "    expected_set = set(row_to_comparable(row) for row in expected_data)\n",
    "    \n",
    "    if actual_set != expected_set:\n",
    "        print(\"Data mismatch!\")\n",
    "        print(\"Actual data:\", actual_set)\n",
    "        print(\"Expected data:\", expected_set)\n",
    "        raise AssertionError(\"Data content mismatch\")\n",
    "    \n",
    "    print(\"âœ“ DataFrames are equal!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toc-section"
   },
   "source": [
    "## Table of Contents - Set 1 (Easy)\n",
    "\n",
    "**Difficulty Distribution:** 30 Easy Problems\n",
    "\n",
    "**Topics Covered:**\n",
    "- Basic Filtering & Selection (6 problems)\n",
    "- Simple Aggregations (6 problems) \n",
    "- Basic Joins (6 problems)\n",
    "- Window Functions (4 problems)\n",
    "- String & Date Operations (4 problems)\n",
    "- UDFs (4 problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-1"
   },
   "source": [
    "## Problem 1: Active Customer Filter\n",
    "\n",
    "**Requirement:** The marketing team needs a list of all active customers for a promotional campaign.\n",
    "\n",
    "**Scenario:** Filter customers where status is 'Active' from the customer database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-1"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_data = [\n",
    "    (1, \"John Doe\", \"Active\", \"2023-01-15\"),\n",
    "    (2, \"Jane Smith\", \"Inactive\", \"2023-02-20\"),\n",
    "    (3, \"Bob Johnson\", \"Active\", \"2023-03-10\"),\n",
    "    (4, \"Alice Brown\", \"Active\", \"2023-01-05\"),\n",
    "    (5, \"Charlie Wilson\", \"Inactive\", \"2023-04-01\")\n",
    "]\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data, [\"customer_id\", \"customer_name\", \"status\", \"join_date\"])\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-1"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John Doe\", \"Active\", \"2023-01-15\"),\n",
    "    (3, \"Bob Johnson\", \"Active\", \"2023-03-10\"),\n",
    "    (4, \"Alice Brown\", \"Active\", \"2023-01-05\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"status\", \"join_date\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-1"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-1"
   },
   "source": [
    "**Instructor Notes:** Basic filtering operation. Tests understanding of filter() or where() methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-2"
   },
   "source": [
    "## Problem 2: Total Sales by Product\n",
    "\n",
    "**Requirement:** Sales department wants total revenue by product for quarterly reporting.\n",
    "\n",
    "**Scenario:** Calculate sum of sales amount grouped by product_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-2"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "sales_data = [\n",
    "    (1, \"A\", 100.0),\n",
    "    (1, \"A\", 150.0),\n",
    "    (2, \"B\", 200.0),\n",
    "    (1, \"A\", 75.0),\n",
    "    (3, \"C\", 300.0),\n",
    "    (2, \"B\", 250.0),\n",
    "    (3, \"C\", 100.0)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"product_id\", \"product_name\", \"amount\"])\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-2"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (3, \"C\", 400.0),\n",
    "    (1, \"A\", 325.0),\n",
    "    (2, \"B\", 450.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"total_sales\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-2"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-2"
   },
   "source": [
    "**Instructor Notes:** Basic groupBy and aggregation. Tests sum() function and alias usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-3"
   },
   "source": [
    "## Problem 3: Employee Department Join\n",
    "\n",
    "**Requirement:** HR needs a combined view of employees with their department names.\n",
    "\n",
    "**Scenario:** Join employees DataFrame with departments DataFrame on department_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-3"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\n",
    "employees_data = [\n",
    "    (1, \"John\", 101),\n",
    "    (2, \"Jane\", 102),\n",
    "    (3, \"Bob\", 101),\n",
    "    (4, \"Alice\", 103),\n",
    "    (5, \"Charlie\", 102)\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (101, \"Engineering\"),\n",
    "    (102, \"Marketing\"),\n",
    "    (103, \"Sales\")\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"emp_name\", \"dept_id\"])\n",
    "departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "print(\"Employees:\")\n",
    "employees_df.show()\n",
    "print(\"Departments:\")\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-3"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John\", 101, \"Engineering\"),\n",
    "    (2, \"Jane\", 102, \"Marketing\"),\n",
    "    (3, \"Bob\", 101, \"Engineering\"),\n",
    "    (4, \"Alice\", 103, \"Sales\"),\n",
    "    (5, \"Charlie\", 102, \"Marketing\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"dept_id\", \"dept_name\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-3"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-3"
   },
   "source": [
    "**Instructor Notes:** Basic inner join operation. Tests join syntax and column handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-4"
   },
   "source": [
    "## Problem 4: Top N Products by Sales\n",
    "\n",
    "**Requirement:** Business stakeholders want to identify top 3 best-selling products.\n",
    "\n",
    "**Scenario:** Use window functions to rank products by total sales and select top 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-4"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "product_sales_data = [\n",
    "    (\"P001\", \"Laptop\", 50000.0),\n",
    "    (\"P002\", \"Mouse\", 15000.0),\n",
    "    (\"P003\", \"Keyboard\", 25000.0),\n",
    "    (\"P004\", \"Monitor\", 45000.0),\n",
    "    (\"P005\", \"Headphones\", 18000.0),\n",
    "    (\"P006\", \"Tablet\", 35000.0)\n",
    "]\n",
    "\n",
    "product_sales_df = spark.createDataFrame(product_sales_data, [\"product_id\", \"product_name\", \"total_sales\"])\n",
    "product_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-4"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"P001\", \"Laptop\", 50000.0, 1),\n",
    "    (\"P004\", \"Monitor\", 45000.0, 2),\n",
    "    (\"P006\", \"Tablet\", 35000.0, 3)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"total_sales\", \"rank\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-4"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-4"
   },
   "source": [
    "**Instructor Notes:** Basic window function with ranking. Tests Window specification and rank() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-5"
   },
   "source": [
    "## Problem 5: Customer Email Domain Extraction\n",
    "\n",
    "**Requirement:** Marketing team wants to analyze customer distribution by email domain.\n",
    "\n",
    "**Scenario:** Extract domain from email addresses and count customers by domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-5"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customers_data = [\n",
    "    (1, \"john@gmail.com\"),\n",
    "    (2, \"jane@yahoo.com\"),\n",
    "    (3, \"bob@gmail.com\"),\n",
    "    (4, \"alice@company.com\"),\n",
    "    (5, \"charlie@gmail.com\"),\n",
    "    (6, \"diana@yahoo.com\")\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"email\"])\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-5"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"gmail.com\", 3),\n",
    "    (\"yahoo.com\", 2),\n",
    "    (\"company.com\", 1)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"domain\", \"customer_count\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-5"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-5"
   },
   "source": [
    "**Instructor Notes:** String manipulation with split function. Tests string operations and array element access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-6"
   },
   "source": [
    "## Problem 6: Age Category UDF\n",
    "\n",
    "**Requirement:** Analytics team needs to categorize customers by age groups for segmentation.\n",
    "\n",
    "**Scenario:** Create a UDF to categorize ages and apply it to customer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-6"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_ages_data = [\n",
    "    (1, \"John\", 25),\n",
    "    (2, \"Jane\", 35),\n",
    "    (3, \"Bob\", 17),\n",
    "    (4, \"Alice\", 45),\n",
    "    (5, \"Charlie\", 60),\n",
    "    (6, \"Diana\", 15)\n",
    "]\n",
    "\n",
    "customer_ages_df = spark.createDataFrame(customer_ages_data, [\"customer_id\", \"name\", \"age\"])\n",
    "customer_ages_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-6"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John\", 25, \"Adult\"),\n",
    "    (2, \"Jane\", 35, \"Adult\"),\n",
    "    (3, \"Bob\", 17, \"Teen\"),\n",
    "    (4, \"Alice\", 45, \"Adult\"),\n",
    "    (5, \"Charlie\", 60, \"Senior\"),\n",
    "    (6, \"Diana\", 15, \"Teen\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"name\", \"age\", \"age_category\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-6"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-6"
   },
   "source": [
    "**Instructor Notes:** Basic UDF creation and application. Tests UDF registration and usage with column operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-7"
   },
   "source": [
    "## Problem 7: Monthly Sales Growth\n",
    "\n",
    "**Requirement:** Finance team needs month-over-month sales growth percentage.\n",
    "\n",
    "**Scenario:** Calculate percentage growth compared to previous month using window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-7"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "monthly_sales_data = [\n",
    "    (\"2023-01\", 100000.0),\n",
    "    (\"2023-02\", 120000.0),\n",
    "    (\"2023-03\", 110000.0),\n",
    "    (\"2023-04\", 130000.0),\n",
    "    (\"2023-05\", 150000.0)\n",
    "]\n",
    "\n",
    "monthly_sales_df = spark.createDataFrame(monthly_sales_data, [\"month\", \"sales\"])\n",
    "monthly_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-7"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"2023-01\", 100000.0, None),\n",
    "    (\"2023-02\", 120000.0, 20.0),\n",
    "    (\"2023-03\", 110000.0, -8.33),\n",
    "    (\"2023-04\", 130000.0, 18.18),\n",
    "    (\"2023-05\", 150000.0, 15.38)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"month\", \"sales\", \"growth_pct\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-7"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-7"
   },
   "source": [
    "**Instructor Notes:** Window function with lag for time-series analysis. Tests lag() and percentage calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-8"
   },
   "source": [
    "## Problem 8: Duplicate Order Detection\n",
    "\n",
    "**Requirement:** Operations team needs to identify duplicate orders for fraud detection.\n",
    "\n",
    "**Scenario:** Find orders with same customer_id, product_id, and order_date within 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-8"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "orders_data = [\n",
    "    (1, 101, \"2023-01-01 10:00:00\", 2, 100.0),\n",
    "    (2, 101, \"2023-01-01 10:30:00\", 1, 50.0),\n",
    "    (3, 102, \"2023-01-01 11:00:00\", 1, 75.0),\n",
    "    (4, 101, \"2023-01-01 10:45:00\", 1, 50.0),  # Duplicate\n",
    "    (5, 103, \"2023-01-01 12:00:00\", 3, 200.0),\n",
    "    (6, 102, \"2023-01-01 13:00:00\", 2, 150.0)\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"order_time\", \"product_id\", \"amount\"])\n",
    "orders_df = orders_df.withColumn(\"order_time\", col(\"order_time\").cast(\"timestamp\"))\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-8"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (2, 101, \"2023-01-01 10:30:00\", 1, 50.0),\n",
    "    (4, 101, \"2023-01-01 10:45:00\", 1, 50.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"customer_id\", \"order_time\", \"product_id\", \"amount\"])\n",
    "expected_df = expected_df.withColumn(\"order_time\", col(\"order_time\").cast(\"timestamp\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-8"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-8"
   },
   "source": [
    "**Instructor Notes:** Window functions with time-based duplicate detection. Tests timestamp operations and conditional filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-9"
   },
   "source": [
    "## Problem 9: Product Price Range Categorization\n",
    "\n",
    "**Requirement:** Pricing team wants to categorize products into price ranges for analysis.\n",
    "\n",
    "**Scenario:** Use CASE WHEN statements to categorize products by price ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-9"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "products_data = [\n",
    "    (\"P001\", \"Laptop\", 999.99),\n",
    "    (\"P002\", \"Mouse\", 25.50),\n",
    "    (\"P003\", \"Keyboard\", 75.00),\n",
    "    (\"P004\", \"Monitor\", 299.99),\n",
    "    (\"P005\", \"Headphones\", 150.00),\n",
    "    (\"P006\", \"Tablet\", 450.00)\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"price\"])\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-9"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"P001\", \"Laptop\", 999.99, \"Premium\"),\n",
    "    (\"P002\", \"Mouse\", 25.50, \"Budget\"),\n",
    "    (\"P003\", \"Keyboard\", 75.00, \"Standard\"),\n",
    "    (\"P004\", \"Monitor\", 299.99, \"Standard\"),\n",
    "    (\"P005\", \"Headphones\", 150.00, \"Standard\"),\n",
    "    (\"P006\", \"Tablet\", 450.00, \"Premium\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"product_name\", \"price\", \"price_category\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-9"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-9"
   },
   "source": [
    "**Instructor Notes:** Conditional logic with CASE WHEN. Tests when().otherwise() pattern for categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-10"
   },
   "source": [
    "## Problem 10: Customer Order Summary\n",
    "\n",
    "**Requirement:** Sales team wants a summary of each customer's order history.\n",
    "\n",
    "**Scenario:** For each customer, calculate total orders, total amount, and average order value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-10"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_orders_data = [\n",
    "    (1, 101, 100.0),\n",
    "    (2, 101, 150.0),\n",
    "    (3, 102, 200.0),\n",
    "    (4, 101, 75.0),\n",
    "    (5, 103, 300.0),\n",
    "    (6, 102, 250.0),\n",
    "    (7, 103, 100.0),\n",
    "    (8, 104, 500.0)\n",
    "]\n",
    "\n",
    "customer_orders_df = spark.createDataFrame(customer_orders_data, [\"order_id\", \"customer_id\", \"amount\"])\n",
    "customer_orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-10"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (104, 1, 500.0, 500.0),\n",
    "    (103, 2, 400.0, 200.0),\n",
    "    (101, 3, 325.0, 108.33),\n",
    "    (102, 2, 450.0, 225.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"total_orders\", \"total_amount\", \"avg_order_value\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-10"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-10"
   },
   "source": [
    "**Instructor Notes:** Multiple aggregations in single groupBy. Tests count, sum, avg functions together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-11"
   },
   "source": [
    "## Problem 11: Employee Salary Percentile\n",
    "\n",
    "**Requirement:** HR analytics needs to calculate salary percentiles by department.\n",
    "\n",
    "**Scenario:** Use window functions to calculate percentile rank of salaries within each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-11"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "employees_salary_data = [\n",
    "    (1, \"John\", \"Engineering\", 80000),\n",
    "    (2, \"Jane\", \"Engineering\", 95000),\n",
    "    (3, \"Bob\", \"Engineering\", 70000),\n",
    "    (4, \"Alice\", \"Marketing\", 60000),\n",
    "    (5, \"Charlie\", \"Marketing\", 75000),\n",
    "    (6, \"Diana\", \"Sales\", 65000),\n",
    "    (7, \"Eve\", \"Sales\", 85000)\n",
    "]\n",
    "\n",
    "employees_salary_df = spark.createDataFrame(employees_salary_data, [\"emp_id\", \"emp_name\", \"department\", \"salary\"])\n",
    "employees_salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-11"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John\", \"Engineering\", 80000, 0.5),\n",
    "    (2, \"Jane\", \"Engineering\", 95000, 1.0),\n",
    "    (3, \"Bob\", \"Engineering\", 70000, 0.0),\n",
    "    (4, \"Alice\", \"Marketing\", 60000, 0.0),\n",
    "    (5, \"Charlie\", \"Marketing\", 75000, 1.0),\n",
    "    (6, \"Diana\", \"Sales\", 65000, 0.0),\n",
    "    (7, \"Eve\", \"Sales\", 85000, 1.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"department\", \"salary\", \"percentile\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-11"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-11"
   },
   "source": [
    "**Instructor Notes:** Window function with percent_rank for percentile calculations. Tests partitioning and ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-12"
   },
   "source": [
    "## Problem 12: Product Sales Pivot\n",
    "\n",
    "**Requirement:** Business intelligence needs monthly sales data in pivot table format.\n",
    "\n",
    "**Scenario:** Pivot sales data to show product sales by month as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-12"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "product_monthly_sales_data = [\n",
    "    (\"P001\", \"2023-01\", 1000),\n",
    "    (\"P001\", \"2023-02\", 1200),\n",
    "    (\"P001\", \"2023-03\", 1100),\n",
    "    (\"P002\", \"2023-01\", 500),\n",
    "    (\"P002\", \"2023-02\", 600),\n",
    "    (\"P002\", \"2023-03\", 550),\n",
    "    (\"P003\", \"2023-01\", 800),\n",
    "    (\"P003\", \"2023-02\", 900),\n",
    "    (\"P003\", \"2023-03\", 850)\n",
    "]\n",
    "\n",
    "product_monthly_sales_df = spark.createDataFrame(product_monthly_sales_data, [\"product_id\", \"month\", \"sales\"])\n",
    "product_monthly_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-12"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"P001\", 1000, 1200, 1100),\n",
    "    (\"P002\", 500, 600, 550),\n",
    "    (\"P003\", 800, 900, 850)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"product_id\", \"2023-01\", \"2023-02\", \"2023-03\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-12"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-12"
   },
   "source": [
    "**Instructor Notes:** Pivot table operation. Tests pivot() method with aggregation for data reshaping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-13"
   },
   "source": [
    "## Problem 13: Customer Purchase Intervals\n",
    "\n",
    "**Requirement:** Marketing team wants to analyze time between customer purchases for retention.\n",
    "\n",
    "**Scenario:** Calculate days between consecutive purchases for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-13"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_purchases_data = [\n",
    "    (1, 101, \"2023-01-01\"),\n",
    "    (2, 101, \"2023-01-05\"),\n",
    "    (3, 101, \"2023-01-12\"),\n",
    "    (4, 102, \"2023-01-02\"),\n",
    "    (5, 102, \"2023-01-15\"),\n",
    "    (6, 103, \"2023-01-03\")\n",
    "]\n",
    "\n",
    "customer_purchases_df = spark.createDataFrame(customer_purchases_data, [\"order_id\", \"customer_id\", \"order_date\"])\n",
    "customer_purchases_df = customer_purchases_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "customer_purchases_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-13"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (2, 101, \"2023-01-05\", 4),\n",
    "    (3, 101, \"2023-01-12\", 7),\n",
    "    (5, 102, \"2023-01-15\", 13)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"customer_id\", \"order_date\", \"days_since_last_purchase\"])\n",
    "expected_df = expected_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-13"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-13"
   },
   "source": [
    "**Instructor Notes:** Date operations with window functions. Tests datediff and lag for time interval calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-14"
   },
   "source": [
    "## Problem 14: String Pattern Matching\n",
    "\n",
    "**Requirement:** Support team needs to find customers with specific email patterns for outreach.\n",
    "\n",
    "**Scenario:** Filter customers whose email contains 'support' or 'help' in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-14"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_emails_data = [\n",
    "    (1, \"john@gmail.com\"),\n",
    "    (2, \"support@company.com\"),\n",
    "    (3, \"jane@yahoo.com\"),\n",
    "    (4, \"HELPdesk@business.com\"),\n",
    "    (5, \"bob@gmail.com\"),\n",
    "    (6, \"info@company.com\")\n",
    "]\n",
    "\n",
    "customer_emails_df = spark.createDataFrame(customer_emails_data, [\"customer_id\", \"email\"])\n",
    "customer_emails_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-14"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (2, \"support@company.com\"),\n",
    "    (4, \"HELPdesk@business.com\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"email\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-14"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-14"
   },
   "source": [
    "**Instructor Notes:** String pattern matching with regex. Tests rlike and case-insensitive pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-15"
   },
   "source": [
    "## Problem 15: Array Column Operations\n",
    "\n",
    "**Requirement:** Product team needs to analyze product tags for categorization.\n",
    "\n",
    "**Scenario:** Explode array column of product tags and count products per tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-15"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "products_tags_data = [\n",
    "    (\"P001\", \"Laptop\", [\"electronics\", \"computing\", \"premium\"]),\n",
    "    (\"P002\", \"Mouse\", [\"electronics\", \"accessories\"]),\n",
    "    (\"P003\", \"Notebook\", [\"stationery\", \"office\"]),\n",
    "    (\"P004\", \"Monitor\", [\"electronics\", \"computing\"]),\n",
    "    (\"P005\", \"Pen\", [\"stationery\", \"office\"])\n",
    "]\n",
    "\n",
    "products_tags_df = spark.createDataFrame(products_tags_data, [\"product_id\", \"product_name\", \"tags\"])\n",
    "products_tags_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-15"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"electronics\", 3),\n",
    "    (\"computing\", 2),\n",
    "    (\"stationery\", 2),\n",
    "    (\"office\", 2),\n",
    "    (\"premium\", 1),\n",
    "    (\"accessories\", 1)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"tag\", \"product_count\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-15"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-15"
   },
   "source": [
    "**Instructor Notes:** Array operations with explode function. Tests handling of complex types and flattening arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-16"
   },
   "source": [
    "## Problem 16: Null Value Handling\n",
    "\n",
    "**Requirement:** Data quality team needs to handle missing customer phone numbers.\n",
    "\n",
    "**Scenario:** Replace null phone numbers with 'Not Provided' and count nulls by city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-16"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customers_contact_data = [\n",
    "    (1, \"John\", \"New York\", \"123-456-7890\"),\n",
    "    (2, \"Jane\", \"Chicago\", None),\n",
    "    (3, \"Bob\", \"New York\", None),\n",
    "    (4, \"Alice\", \"Chicago\", \"987-654-3210\"),\n",
    "    (5, \"Charlie\", \"Boston\", None),\n",
    "    (6, \"Diana\", \"New York\", \"555-123-4567\")\n",
    "]\n",
    "\n",
    "customers_contact_df = spark.createDataFrame(customers_contact_data, [\"customer_id\", \"customer_name\", \"city\", \"phone\"])\n",
    "customers_contact_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-16"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John\", \"New York\", \"123-456-7890\"),\n",
    "    (2, \"Jane\", \"Chicago\", \"Not Provided\"),\n",
    "    (3, \"Bob\", \"New York\", \"Not Provided\"),\n",
    "    (4, \"Alice\", \"Chicago\", \"987-654-3210\"),\n",
    "    (5, \"Charlie\", \"Boston\", \"Not Provided\"),\n",
    "    (6, \"Diana\", \"New York\", \"555-123-4567\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"city\", \"phone\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-16"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-16"
   },
   "source": [
    "**Instructor Notes:** Null value handling with fillna. Tests data cleaning and missing value imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-17"
   },
   "source": [
    "## Problem 17: Column Renaming and Selection\n",
    "\n",
    "**Requirement:** Reporting team needs specific column names for their dashboard.\n",
    "\n",
    "**Scenario:** Select specific columns and rename them to business-friendly names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-17"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "employee_details_data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 80000, \"2020-01-15\"),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000, \"2019-03-20\"),\n",
    "    (3, \"Bob Johnson\", \"Engineering\", 90000, \"2018-06-10\"),\n",
    "    (4, \"Alice Brown\", \"Sales\", 70000, \"2021-02-05\")\n",
    "]\n",
    "\n",
    "employee_details_df = spark.createDataFrame(employee_details_data, [\"emp_id\", \"emp_name\", \"department\", \"salary\", \"hire_date\"])\n",
    "employee_details_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-17"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 80000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Bob Johnson\", \"Engineering\", 90000),\n",
    "    (4, \"Alice Brown\", \"Sales\", 70000)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"EmployeeID\", \"FullName\", \"Department\", \"AnnualSalary\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-17"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-17"
   },
   "source": [
    "**Instructor Notes:** Column selection and renaming. Tests alias usage and selective column operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-18"
   },
   "source": [
    "## Problem 18: Date Format Conversion\n",
    "\n",
    "**Requirement:** International team needs dates in specific format for reporting.\n",
    "\n",
    "**Scenario:** Convert date format from YYYY-MM-DD to DD/MM/YYYY for international standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-18"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "orders_date_data = [\n",
    "    (1, \"2023-01-15\"),\n",
    "    (2, \"2023-02-20\"),\n",
    "    (3, \"2023-03-10\"),\n",
    "    (4, \"2023-04-05\"),\n",
    "    (5, \"2023-05-25\")\n",
    "]\n",
    "\n",
    "orders_date_df = spark.createDataFrame(orders_date_data, [\"order_id\", \"order_date\"])\n",
    "orders_date_df = orders_date_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "orders_date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-18"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"15/01/2023\"),\n",
    "    (2, \"20/02/2023\"),\n",
    "    (3, \"10/03/2023\"),\n",
    "    (4, \"05/04/2023\"),\n",
    "    (5, \"25/05/2023\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"formatted_date\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-18"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-18"
   },
   "source": [
    "**Instructor Notes:** Date formatting operations. Tests date_format function for international date standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-19"
   },
   "source": [
    "## Problem 19: Simple Union Operation\n",
    "\n",
    "**Requirement:** Operations team needs to combine current and historical customer data.\n",
    "\n",
    "**Scenario:** Union two customer DataFrames with same schema into single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-19"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\n",
    "current_customers_data = [\n",
    "    (1, \"John\", \"Active\"),\n",
    "    (2, \"Jane\", \"Active\"),\n",
    "    (3, \"Bob\", \"Active\")\n",
    "]\n",
    "\n",
    "historical_customers_data = [\n",
    "    (4, \"Alice\", \"Inactive\"),\n",
    "    (5, \"Charlie\", \"Inactive\")\n",
    "]\n",
    "\n",
    "current_customers_df = spark.createDataFrame(current_customers_data, [\"customer_id\", \"customer_name\", \"status\"])\n",
    "historical_customers_df = spark.createDataFrame(historical_customers_data, [\"customer_id\", \"customer_name\", \"status\"])\n",
    "\n",
    "print(\"Current Customers:\")\n",
    "current_customers_df.show()\n",
    "print(\"Historical Customers:\")\n",
    "historical_customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-19"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John\", \"Active\"),\n",
    "    (2, \"Jane\", \"Active\"),\n",
    "    (3, \"Bob\", \"Active\"),\n",
    "    (4, \"Alice\", \"Inactive\"),\n",
    "    (5, \"Charlie\", \"Inactive\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"status\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-19"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-19"
   },
   "source": [
    "**Instructor Notes:** Union operation for combining datasets. Tests union() with same schema DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-20"
   },
   "source": [
    "## Problem 20: Distinct Value Count\n",
    "\n",
    "**Requirement:** Data governance team needs to count distinct values for data quality assessment.\n",
    "\n",
    "**Scenario:** Count distinct departments and distinct job titles in employee data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-20"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "employees_diverse_data = [\n",
    "    (1, \"John\", \"Engineering\", \"Software Engineer\"),\n",
    "    (2, \"Jane\", \"Engineering\", \"Data Scientist\"),\n",
    "    (3, \"Bob\", \"Marketing\", \"Marketing Manager\"),\n",
    "    (4, \"Alice\", \"Marketing\", \"Content Writer\"),\n",
    "    (5, \"Charlie\", \"Engineering\", \"Software Engineer\"),\n",
    "    (6, \"Diana\", \"Sales\", \"Sales Executive\"),\n",
    "    (7, \"Eve\", \"Sales\", \"Sales Executive\")\n",
    "]\n",
    "\n",
    "employees_diverse_df = spark.createDataFrame(employees_diverse_data, [\"emp_id\", \"emp_name\", \"department\", \"job_title\"])\n",
    "employees_diverse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-20"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (3, 5)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"distinct_departments\", \"distinct_job_titles\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-20"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-20"
   },
   "source": [
    "**Instructor Notes:** Distinct counting with countDistinct. Tests aggregation without grouping for overall distinct counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-21"
   },
   "source": [
    "## Problem 21: Column Concatenation\n",
    "\n",
    "**Requirement:** Reporting team needs full names from separate first and last name columns.\n",
    "\n",
    "**Scenario:** Concatenate first and last name columns with space separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-21"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "employees_names_data = [\n",
    "    (1, \"John\", \"Doe\"),\n",
    "    (2, \"Jane\", \"Smith\"),\n",
    "    (3, \"Bob\", \"Johnson\"),\n",
    "    (4, \"Alice\", \"Brown\"),\n",
    "    (5, \"Charlie\", \"Wilson\")\n",
    "]\n",
    "\n",
    "employees_names_df = spark.createDataFrame(employees_names_data, [\"emp_id\", \"first_name\", \"last_name\"])\n",
    "employees_names_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-21"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John Doe\"),\n",
    "    (2, \"Jane Smith\"),\n",
    "    (3, \"Bob Johnson\"),\n",
    "    (4, \"Alice Brown\"),\n",
    "    (5, \"Charlie Wilson\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"full_name\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-21"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-21"
   },
   "source": [
    "**Instructor Notes:** String concatenation with concat function. Tests string manipulation and literal usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-22"
   },
   "source": [
    "## Problem 22: Row Number Generation\n",
    "\n",
    "**Requirement:** Analytics team needs sequential row numbers for data processing.\n",
    "\n",
    "**Scenario:** Add sequential row numbers to customer data ordered by customer_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-22"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customers_sequential_data = [\n",
    "    (105, \"John\"),\n",
    "    (102, \"Jane\"),\n",
    "    (108, \"Bob\"),\n",
    "    (101, \"Alice\"),\n",
    "    (107, \"Charlie\")\n",
    "]\n",
    "\n",
    "customers_sequential_df = spark.createDataFrame(customers_sequential_data, [\"customer_id\", \"customer_name\"])\n",
    "customers_sequential_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-22"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (101, \"Alice\", 1),\n",
    "    (102, \"Jane\", 2),\n",
    "    (105, \"John\", 3),\n",
    "    (107, \"Charlie\", 4),\n",
    "    (108, \"Bob\", 5)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"row_number\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-22"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-22"
   },
   "source": [
    "**Instructor Notes:** Row number generation with window functions. Tests row_number() for sequential numbering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-23"
   },
   "source": [
    "## Problem 23: Simple Conditional Aggregation\n",
    "\n",
    "**Requirement:** Finance team needs separate totals for domestic and international sales.\n",
    "\n",
    "**Scenario:** Calculate total sales amount for domestic vs international orders using conditional sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-23"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "orders_international_data = [\n",
    "    (1, \"Domestic\", 1000.0),\n",
    "    (2, \"International\", 1500.0),\n",
    "    (3, \"Domestic\", 800.0),\n",
    "    (4, \"International\", 2000.0),\n",
    "    (5, \"Domestic\", 1200.0),\n",
    "    (6, \"International\", 1800.0)\n",
    "]\n",
    "\n",
    "orders_international_df = spark.createDataFrame(orders_international_data, [\"order_id\", \"order_type\", \"amount\"])\n",
    "orders_international_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-23"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (3000.0, 5300.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"domestic_sales\", \"international_sales\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-23"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-23"
   },
   "source": [
    "**Instructor Notes:** Conditional aggregation with when().otherwise(). Tests conditional sum operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-24"
   },
   "source": [
    "## Problem 24: Data Type Conversion\n",
    "\n",
    "**Requirement:** Data engineering team needs to convert string columns to proper data types.\n",
    "\n",
    "**Scenario:** Convert string representations of numbers and dates to appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-24"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "raw_data_data = [\n",
    "    (\"1\", \"1000.50\", \"2023-01-15\"),\n",
    "    (\"2\", \"2000.75\", \"2023-02-20\"),\n",
    "    (\"3\", \"1500.25\", \"2023-03-10\")\n",
    "]\n",
    "\n",
    "raw_data_df = spark.createDataFrame(raw_data_data, [\"id_str\", \"amount_str\", \"date_str\"])\n",
    "raw_data_df.show()\n",
    "raw_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-24"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, 1000.5, \"2023-01-15\"),\n",
    "    (2, 2000.75, \"2023-02-20\"),\n",
    "    (3, 1500.25, \"2023-03-10\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"id\", \"amount\", \"date\"])\n",
    "expected_df = expected_df.withColumn(\"id\", col(\"id\").cast(\"integer\"))\\\n",
    "                       .withColumn(\"amount\", col(\"amount\").cast(\"double\"))\\\n",
    "                       .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "expected_df.show()\n",
    "expected_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-24"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-24"
   },
   "source": [
    "**Instructor Notes:** Data type casting operations. Tests cast() method for type conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-25"
   },
   "source": [
    "## Problem 25: Simple Left Join\n",
    "\n",
    "**Requirement:** Customer service needs order details with customer information, including customers without orders.\n",
    "\n",
    "**Scenario:** Perform left join between customers and orders to include all customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-25"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\n",
    "customers_left_data = [\n",
    "    (1, \"John\"),\n",
    "    (2, \"Jane\"),\n",
    "    (3, \"Bob\"),\n",
    "    (4, \"Alice\")\n",
    "]\n",
    "\n",
    "orders_left_data = [\n",
    "    (101, 1, 100.0),\n",
    "    (102, 1, 150.0),\n",
    "    (103, 2, 200.0),\n",
    "    (104, 3, 75.0)\n",
    "]\n",
    "\n",
    "customers_left_df = spark.createDataFrame(customers_left_data, [\"customer_id\", \"customer_name\"])\n",
    "orders_left_df = spark.createDataFrame(orders_left_data, [\"order_id\", \"customer_id\", \"amount\"])\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers_left_df.show()\n",
    "print(\"Orders:\")\n",
    "orders_left_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-25"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John\", 101, 100.0),\n",
    "    (1, \"John\", 102, 150.0),\n",
    "    (2, \"Jane\", 103, 200.0),\n",
    "    (3, \"Bob\", 104, 75.0),\n",
    "    (4, \"Alice\", None, None)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"order_id\", \"amount\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-25"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-25"
   },
   "source": [
    "**Instructor Notes:** Left join operation. Tests different join types and handling of null values from unmatched records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-26"
   },
   "source": [
    "## Problem 26: Basic Statistical Aggregations\n",
    "\n",
    "**Requirement:** Analytics team needs basic statistics for sales data analysis.\n",
    "\n",
    "**Scenario:** Calculate count, mean, standard deviation, min, and max of sales amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-26"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "sales_stats_data = [\n",
    "    (100.0,),\n",
    "    (150.0,),\n",
    "    (200.0,),\n",
    "    (75.0,),\n",
    "    (300.0,),\n",
    "    (250.0,),\n",
    "    (100.0,)\n",
    "]\n",
    "\n",
    "sales_stats_df = spark.createDataFrame(sales_stats_data, [\"amount\"])\n",
    "sales_stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-26"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (7, 167.86, 87.88, 75.0, 300.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"count\", \"mean\", \"stddev\", \"min\", \"max\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-26"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-26"
   },
   "source": [
    "**Instructor Notes:** Multiple statistical aggregations. Tests various aggregation functions together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-27"
   },
   "source": [
    "## Problem 27: Column Dropping\n",
    "\n",
    "**Requirement:** Data privacy team requires removal of sensitive columns from dataset.\n",
    "\n",
    "**Scenario:** Drop sensitive columns (SSN, salary) from employee data for external sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-27"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "employees_sensitive_data = [\n",
    "    (1, \"John\", \"123-45-6789\", \"Engineering\", 80000),\n",
    "    (2, \"Jane\", \"987-65-4321\", \"Marketing\", 75000),\n",
    "    (3, \"Bob\", \"456-78-9123\", \"Engineering\", 90000)\n",
    "]\n",
    "\n",
    "employees_sensitive_df = spark.createDataFrame(employees_sensitive_data, [\"emp_id\", \"emp_name\", \"ssn\", \"department\", \"salary\"])\n",
    "employees_sensitive_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-27"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John\", \"Engineering\"),\n",
    "    (2, \"Jane\", \"Marketing\"),\n",
    "    (3, \"Bob\", \"Engineering\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"department\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-27"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-27"
   },
   "source": [
    "**Instructor Notes:** Column dropping operation. Tests drop() method for removing specific columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-28"
   },
   "source": [
    "## Problem 28: Simple Sort Operation\n",
    "\n",
    "**Requirement:** Reporting team needs customer data sorted for consistent presentation.\n",
    "\n",
    "**Scenario:** Sort customers by name in alphabetical order and by customer_id descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-28"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customers_sort_data = [\n",
    "    (3, \"Charlie\"),\n",
    "    (1, \"Alice\"),\n",
    "    (4, \"Diana\"),\n",
    "    (2, \"Bob\"),\n",
    "    (5, \"Eve\")\n",
    "]\n",
    "\n",
    "customers_sort_df = spark.createDataFrame(customers_sort_data, [\"customer_id\", \"customer_name\"])\n",
    "customers_sort_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-28"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"Diana\"),\n",
    "    (5, \"Eve\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-28"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-28"
   },
   "source": [
    "**Instructor Notes:** Sorting operation. Tests orderBy for data ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-29"
   },
   "source": [
    "## Problem 29: Basic Mathematical Operations\n",
    "\n",
    "**Requirement:** Finance team needs calculated fields for financial reporting.\n",
    "\n",
    "**Scenario:** Calculate tax (15%) and total amount after tax for each sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-29"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "sales_tax_data = [\n",
    "    (1, 100.0),\n",
    "    (2, 200.0),\n",
    "    (3, 150.0),\n",
    "    (4, 300.0)\n",
    "]\n",
    "\n",
    "sales_tax_df = spark.createDataFrame(sales_tax_data, [\"sale_id\", \"amount\"])\n",
    "sales_tax_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-29"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, 100.0, 15.0, 115.0),\n",
    "    (2, 200.0, 30.0, 230.0),\n",
    "    (3, 150.0, 22.5, 172.5),\n",
    "    (4, 300.0, 45.0, 345.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"sale_id\", \"amount\", \"tax\", \"total_amount\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-29"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-29"
   },
   "source": [
    "**Instructor Notes:** Mathematical operations on columns. Tests arithmetic operations and column references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-30"
   },
   "source": [
    "## Problem 30: Simple Filter with Multiple Conditions\n",
    "\n",
    "**Requirement:** Sales team needs to identify high-value recent customers.\n",
    "\n",
    "**Scenario:** Filter customers who joined in 2023 and have spent more than $1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-30"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customers_high_value_data = [\n",
    "    (1, \"John\", \"2023-01-15\", 800.0),\n",
    "    (2, \"Jane\", \"2023-02-20\", 1500.0),\n",
    "    (3, \"Bob\", \"2022-12-10\", 1200.0),\n",
    "    (4, \"Alice\", \"2023-03-05\", 2000.0),\n",
    "    (5, \"Charlie\", \"2022-11-20\", 900.0)\n",
    "]\n",
    "\n",
    "customers_high_value_df = spark.createDataFrame(customers_high_value_data, [\"customer_id\", \"customer_name\", \"join_date\", \"total_spent\"])\n",
    "customers_high_value_df = customers_high_value_df.withColumn(\"join_date\", col(\"join_date\").cast(\"date\"))\n",
    "customers_high_value_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-30"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (2, \"Jane\", \"2023-02-20\", 1500.0),\n",
    "    (4, \"Alice\", \"2023-03-05\", 2000.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"join_date\", \"total_spent\"])\n",
    "expected_df = expected_df.withColumn(\"join_date\", col(\"join_date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-30"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-30"
   },
   "source": [
    "**Instructor Notes:** Multiple condition filtering with date functions. Tests compound conditions and date extraction functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "completion-note"
   },
   "source": [
    "# Set 1 Complete!\n",
    "\n",
    "You've completed all 30 Easy problems in Set 1. These problems cover:\n",
    "- Basic filtering and selection\n",
    "- Simple aggregations\n",
    "- Basic joins\n",
    "- Window functions\n",
    "- String and date operations\n",
    "- UDFs\n",
    "- Data type conversions\n",
    "- And other fundamental PySpark operations\n",
    "\n",
    "Ready for Set 2 with Easy/Medium difficulty problems?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}