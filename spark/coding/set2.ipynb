{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview-section"
   },
   "source": [
    "# PySpark Interview Preparation - Set 2 (Easy/Medium)\n",
    "\n",
    "## Overview & Instructions\n",
    "\n",
    "### How to run this notebook in Google Colab:\n",
    "1. Upload this .ipynb file to Google Colab\n",
    "2. Run the installation cells below\n",
    "3. Execute each problem cell sequentially\n",
    "\n",
    "### Installation Commands:\n",
    "The following cell installs Java and PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "installation-cell"
   },
   "outputs": [],
   "source": [
    "# Install Java and PySpark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!pip install -q pyspark\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sparksession-section"
   },
   "source": [
    "### SparkSession Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sparksession-cell"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"PySparkInterviewSet2\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assert-function-section"
   },
   "source": [
    "### DataFrame Assertion Function:\n",
    "\n",
    "This function compares DataFrames ignoring order and with floating-point tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "assert-function-cell"
   },
   "outputs": [],
   "source": [
    "def assert_dataframe_equal(df_actual, df_expected, epsilon=1e-6):\n",
    "    \"\"\"Compare two DataFrames ignoring order and with floating-point tolerance\"\"\"\n",
    "    \n",
    "    # Check schema first\n",
    "    if df_actual.schema != df_expected.schema:\n",
    "        print(\"Schema mismatch!\")\n",
    "        print(\"Actual schema:\", df_actual.schema)\n",
    "        print(\"Expected schema:\", df_expected.schema)\n",
    "        raise AssertionError(\"Schema mismatch\")\n",
    "    \n",
    "    # Collect data\n",
    "    actual_data = df_actual.collect()\n",
    "    expected_data = df_expected.collect()\n",
    "    \n",
    "    if len(actual_data) != len(expected_data):\n",
    "        print(f\"Row count mismatch! Actual: {len(actual_data)}, Expected: {len(expected_data)}\")\n",
    "        raise AssertionError(\"Row count mismatch\")\n",
    "    \n",
    "    # Convert to sets of tuples for order-insensitive comparison\n",
    "    def row_to_comparable(row):\n",
    "        values = []\n",
    "        for field in row:\n",
    "            if isinstance(field, float):\n",
    "                # Handle floating point comparison\n",
    "                values.append(round(field / epsilon) * epsilon)\n",
    "            elif isinstance(field, list):\n",
    "                # Handle arrays\n",
    "                values.append(tuple(sorted(field)) if field else tuple())\n",
    "            elif isinstance(field, dict):\n",
    "                # Handle structs\n",
    "                values.append(tuple(sorted(field.items())))\n",
    "            else:\n",
    "                values.append(field)\n",
    "        return tuple(values)\n",
    "    \n",
    "    actual_set = set(row_to_comparable(row) for row in actual_data)\n",
    "    expected_set = set(row_to_comparable(row) for row in expected_data)\n",
    "    \n",
    "    if actual_set != expected_set:\n",
    "        print(\"Data mismatch!\")\n",
    "        print(\"Actual data:\", actual_set)\n",
    "        print(\"Expected data:\", expected_set)\n",
    "        raise AssertionError(\"Data content mismatch\")\n",
    "    \n",
    "    print(\"âœ“ DataFrames are equal!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toc-section"
   },
   "source": [
    "## Table of Contents - Set 2 (Easy/Medium)\n",
    "\n",
    "**Difficulty Distribution:** 30 Easy/Medium Problems\n",
    "\n",
    "**Topics Covered:**\n",
    "- Advanced Joins & Deduplication (8 problems)\n",
    "- Complex Window Functions (6 problems) \n",
    "- Multi-level Aggregations (6 problems)\n",
    "- Advanced UDFs & Pandas UDFs (4 problems)\n",
    "- Nested Data Operations (3 problems)\n",
    "- Performance & Partitioning (3 problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-1"
   },
   "source": [
    "## Problem 1: Customer Lifetime Value Calculation\n",
    "\n",
    "**Requirement:** Marketing analytics needs to calculate Customer Lifetime Value (CLV) for segmentation.\n",
    "\n",
    "**Scenario:** Calculate total revenue, average order value, and purchase frequency for each customer over their lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-1"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_orders_data = [\n",
    "    (1, \"C001\", \"2023-01-15\", 100.0),\n",
    "    (2, \"C001\", \"2023-02-20\", 150.0),\n",
    "    (3, \"C002\", \"2023-01-10\", 200.0),\n",
    "    (4, \"C001\", \"2023-03-05\", 75.0),\n",
    "    (5, \"C003\", \"2023-02-01\", 300.0),\n",
    "    (6, \"C002\", \"2023-03-15\", 250.0),\n",
    "    (7, \"C003\", \"2023-03-20\", 100.0),\n",
    "    (8, \"C004\", \"2023-01-25\", 500.0),\n",
    "    (9, \"C001\", \"2023-04-10\", 125.0)\n",
    "]\n",
    "\n",
    "customer_orders_df = spark.createDataFrame(customer_orders_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])\n",
    "customer_orders_df = customer_orders_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "customer_orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-1"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"C004\", 1, 500.0, 500.0, 500.0),\n",
    "    (\"C003\", 2, 400.0, 200.0, 200.0),\n",
    "    (\"C002\", 2, 450.0, 225.0, 225.0),\n",
    "    (\"C001\", 4, 450.0, 112.5, 112.5)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"total_orders\", \"total_revenue\", \"avg_order_value\", \"clv\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-1"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-1"
   },
   "source": [
    "**Instructor Notes:** Multi-column aggregation with customer metrics. Tests complex business metric calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-2"
   },
   "source": [
    "## Problem 2: Employee Department Hierarchy\n",
    "\n",
    "**Requirement:** HR needs to identify employees with their managers for organizational reporting.\n",
    "\n",
    "**Scenario:** Perform self-join on employee table to get manager names for each employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-2"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "employees_hierarchy_data = [\n",
    "    (1, \"John CEO\", None, \"CEO\"),\n",
    "    (2, \"Alice VP\", 1, \"VP Engineering\"),\n",
    "    (3, \"Bob Manager\", 2, \"Engineering Manager\"),\n",
    "    (4, \"Charlie Developer\", 3, \"Senior Developer\"),\n",
    "    (5, \"Diana VP\", 1, \"VP Marketing\"),\n",
    "    (6, \"Eve Specialist\", 5, \"Marketing Specialist\"),\n",
    "    (7, \"Frank Manager\", 2, \"QA Manager\")\n",
    "]\n",
    "\n",
    "employees_hierarchy_df = spark.createDataFrame(employees_hierarchy_data, [\"emp_id\", \"emp_name\", \"manager_id\", \"title\"])\n",
    "employees_hierarchy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-2"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (2, \"Alice VP\", \"John CEO\", \"VP Engineering\"),\n",
    "    (3, \"Bob Manager\", \"Alice VP\", \"Engineering Manager\"),\n",
    "    (4, \"Charlie Developer\", \"Bob Manager\", \"Senior Developer\"),\n",
    "    (5, \"Diana VP\", \"John CEO\", \"VP Marketing\"),\n",
    "    (6, \"Eve Specialist\", \"Diana VP\", \"Marketing Specialist\"),\n",
    "    (7, \"Frank Manager\", \"Alice VP\", \"QA Manager\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"manager_name\", \"title\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-2"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-2"
   },
   "source": [
    "**Instructor Notes:** Self-join operation. Tests joining a table with itself on different conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-3"
   },
   "source": [
    "## Problem 3: Running Total with Window Functions\n",
    "\n",
    "**Requirement:** Finance team needs running total of daily sales for cash flow analysis.\n",
    "\n",
    "**Scenario:** Calculate cumulative sum of sales ordered by date using window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-3"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "daily_sales_data = [\n",
    "    (\"2023-01-01\", 1000.0),\n",
    "    (\"2023-01-02\", 1500.0),\n",
    "    (\"2023-01-03\", 800.0),\n",
    "    (\"2023-01-04\", 2000.0),\n",
    "    (\"2023-01-05\", 1200.0),\n",
    "    (\"2023-01-06\", 1800.0)\n",
    "]\n",
    "\n",
    "daily_sales_df = spark.createDataFrame(daily_sales_data, [\"date\", \"daily_sales\"])\n",
    "daily_sales_df = daily_sales_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "daily_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-3"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"2023-01-01\", 1000.0, 1000.0),\n",
    "    (\"2023-01-02\", 1500.0, 2500.0),\n",
    "    (\"2023-01-03\", 800.0, 3300.0),\n",
    "    (\"2023-01-04\", 2000.0, 5300.0),\n",
    "    (\"2023-01-05\", 1200.0, 6500.0),\n",
    "    (\"2023-01-06\", 1800.0, 8300.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"date\", \"daily_sales\", \"running_total\"])\n",
    "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-3"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-3"
   },
   "source": [
    "**Instructor Notes:** Window function with cumulative sum. Tests unbounded window for running totals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-4"
   },
   "source": [
    "## Problem 4: Product Recommendation Engine\n",
    "\n",
    "**Requirement:** E-commerce team wants to recommend products frequently bought together.\n",
    "\n",
    "**Scenario:** Find product pairs that are frequently purchased in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-4"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "order_items_data = [\n",
    "    (1, \"P001\", \"Laptop\"),\n",
    "    (1, \"P002\", \"Mouse\"),\n",
    "    (1, \"P003\", \"Laptop Bag\"),\n",
    "    (2, \"P001\", \"Laptop\"),\n",
    "    (2, \"P002\", \"Mouse\"),\n",
    "    (3, \"P004\", \"Monitor\"),\n",
    "    (3, \"P002\", \"Mouse\"),\n",
    "    (4, \"P001\", \"Laptop\"),\n",
    "    (4, \"P005\", \"Keyboard\"),\n",
    "    (5, \"P002\", \"Mouse\"),\n",
    "    (5, \"P005\", \"Keyboard\")\n",
    "]\n",
    "\n",
    "order_items_df = spark.createDataFrame(order_items_data, [\"order_id\", \"product_id\", \"product_name\"])\n",
    "order_items_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-4"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"P001\", \"Laptop\", \"P002\", \"Mouse\", 2),\n",
    "    (\"P002\", \"Mouse\", \"P005\", \"Keyboard\", 2),\n",
    "    (\"P001\", \"Laptop\", \"P003\", \"Laptop Bag\", 1),\n",
    "    (\"P004\", \"Monitor\", \"P002\", \"Mouse\", 1),\n",
    "    (\"P001\", \"Laptop\", \"P005\", \"Keyboard\", 1)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"product1_id\", \"product1_name\", \"product2_id\", \"product2_name\", \"pair_count\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-4"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-4"
   },
   "source": [
    "**Instructor Notes:** Self-join for co-occurrence analysis. Tests complex join conditions and pair counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-5"
   },
   "source": [
    "## Problem 5: Time-Based Sessionization\n",
    "\n",
    "**Requirement:** Analytics team needs to group user activities into sessions based on time gaps.\n",
    "\n",
    "**Scenario:** Group user activities into sessions where gaps between activities are > 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-5"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "user_activities_data = [\n",
    "    (\"U001\", \"2023-01-01 10:00:00\", \"login\"),\n",
    "    (\"U001\", \"2023-01-01 10:05:00\", \"browse\"),\n",
    "    (\"U001\", \"2023-01-01 10:10:00\", \"click\"),\n",
    "    (\"U001\", \"2023-01-01 10:45:00\", \"purchase\"),  # New session (35 min gap)\n",
    "    (\"U001\", \"2023-01-01 10:50:00\", \"logout\"),\n",
    "    (\"U002\", \"2023-01-01 11:00:00\", \"login\"),\n",
    "    (\"U002\", \"2023-01-01 11:15:00\", \"browse\"),\n",
    "    (\"U002\", \"2023-01-01 11:20:00\", \"click\")\n",
    "]\n",
    "\n",
    "user_activities_df = spark.createDataFrame(user_activities_data, [\"user_id\", \"timestamp\", \"action\"])\n",
    "user_activities_df = user_activities_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "user_activities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-5"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"U001\", \"2023-01-01 10:00:00\", \"login\", 1),\n",
    "    (\"U001\", \"2023-01-01 10:05:00\", \"browse\", 1),\n",
    "    (\"U001\", \"2023-01-01 10:10:00\", \"click\", 1),\n",
    "    (\"U001\", \"2023-01-01 10:45:00\", \"purchase\", 2),\n",
    "    (\"U001\", \"2023-01-01 10:50:00\", \"logout\", 2),\n",
    "    (\"U002\", \"2023-01-01 11:00:00\", \"login\", 1),\n",
    "    (\"U002\", \"2023-01-01 11:15:00\", \"browse\", 1),\n",
    "    (\"U002\", \"2023-01-01 11:20:00\", \"click\", 1)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"user_id\", \"timestamp\", \"action\", \"session_id\"])\n",
    "expected_df = expected_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-5"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-5"
   },
   "source": [
    "**Instructor Notes:** Advanced window functions for sessionization. Tests time gap analysis and conditional session creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-6"
   },
   "source": [
    "## Problem 6: Complex UDF for Text Analysis\n",
    "\n",
    "**Requirement:** Customer service needs to categorize support tickets based on sentiment and urgency.\n",
    "\n",
    "**Scenario:** Create a UDF that analyzes ticket text and returns priority level based on keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-6"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "support_tickets_data = [\n",
    "    (1, \"My login is not working, need immediate help\", \"John\"),\n",
    "    (2, \"Feature request for dark mode\", \"Jane\"),\n",
    "    (3, \"URGENT: Payment failed but money deducted\", \"Bob\"),\n",
    "    (4, \"Bug report: button color issue\", \"Alice\"),\n",
    "    (5, \"CRITICAL: System down, cannot access anything\", \"Charlie\")\n",
    "]\n",
    "\n",
    "support_tickets_df = spark.createDataFrame(support_tickets_data, [\"ticket_id\", \"description\", \"reporter\"])\n",
    "support_tickets_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-6"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"My login is not working, need immediate help\", \"John\", \"High\"),\n",
    "    (2, \"Feature request for dark mode\", \"Jane\", \"Low\"),\n",
    "    (3, \"URGENT: Payment failed but money deducted\", \"Bob\", \"Critical\"),\n",
    "    (4, \"Bug report: button color issue\", \"Alice\", \"Medium\"),\n",
    "    (5, \"CRITICAL: System down, cannot access anything\", \"Charlie\", \"Critical\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"ticket_id\", \"description\", \"reporter\", \"priority\"])\n",
    "expected_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-6"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-6"
   },
   "source": [
    "**Instructor Notes:** Complex UDF with string analysis. Tests text processing and conditional logic in UDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-7"
   },
   "source": [
    "## Problem 7: Multiple Column Pivot\n",
    "\n",
    "**Requirement:** Sales analytics needs quarterly sales data pivoted by both product and region.\n",
    "\n",
    "**Scenario:** Create a pivot table showing sales by product category and quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-7"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "regional_sales_data = [\n",
    "    (\"Electronics\", \"Q1\", \"North\", 50000),\n",
    "    (\"Electronics\", \"Q1\", \"South\", 45000),\n",
    "    (\"Electronics\", \"Q2\", \"North\", 60000),\n",
    "    (\"Electronics\", \"Q2\", \"South\", 55000),\n",
    "    (\"Clothing\", \"Q1\", \"North\", 30000),\n",
    "    (\"Clothing\", \"Q1\", \"South\", 35000),\n",
    "    (\"Clothing\", \"Q2\", \"North\", 40000),\n",
    "    (\"Clothing\", \"Q2\", \"South\", 45000)\n",
    "]\n",
    "\n",
    "regional_sales_df = spark.createDataFrame(regional_sales_data, [\"category\", \"quarter\", \"region\", \"sales\"])\n",
    "regional_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-7"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"Electronics\", 95000, 115000),\n",
    "    (\"Clothing\", 65000, 85000)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"category\", \"Q1_sales\", \"Q2_sales\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-7"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-7"
   },
   "source": [
    "**Instructor Notes:** Multi-level pivot with aggregation. Tests complex pivot operations with multiple grouping columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-8"
   },
   "source": [
    "## Problem 8: Advanced Deduplication with Multiple Criteria\n",
    "\n",
    "**Requirement:** Data quality team needs to identify and remove duplicate customer records.\n",
    "\n",
    "**Scenario:** Find duplicate customers based on name, email, or phone with different criteria weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-8"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_duplicates_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\"),\n",
    "    (2, \"John Doe\", \"john.doe@email.com\", \"123-456-7890\"),\n",
    "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n",
    "    (4, \"Jane Smith\", \"jane@email.com\", \"555-123-4567\"),\n",
    "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\"),\n",
    "    (6, \"Robert Johnson\", \"bob@email.com\", \"111-222-3333\")\n",
    "]\n",
    "\n",
    "customer_duplicates_df = spark.createDataFrame(customer_duplicates_data, [\"cust_id\", \"name\", \"email\", \"phone\"])\n",
    "customer_duplicates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-8"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\"),\n",
    "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n",
    "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"name\", \"email\", \"phone\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-8"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-8"
   },
   "source": [
    "**Instructor Notes:** Advanced deduplication with multiple matching criteria. Tests window functions and complex duplicate identification logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-9"
   },
   "source": [
    "## Problem 9: Nested JSON Data Processing\n",
    "\n",
    "**Requirement:** Analytics team needs to flatten nested JSON data from API responses.\n",
    "\n",
    "**Scenario:** Extract and flatten nested customer order data with array of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-9"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame with nested structure\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"items\", ArrayType(StructType([\n",
    "        StructField(\"product\", StringType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"price\", IntegerType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "nested_data = [\n",
    "    (\"O001\", (\"John Doe\", \"john@email.com\"), [(\"Laptop\", 1, 1000), (\"Mouse\", 2, 50)]),\n",
    "    (\"O002\", (\"Jane Smith\", \"jane@email.com\"), [(\"Monitor\", 1, 300), (\"Keyboard\", 1, 100)])\n",
    "]\n",
    "\n",
    "nested_df = spark.createDataFrame(nested_data, schema)\n",
    "nested_df.show(truncate=False)\n",
    "nested_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-9"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"O001\", \"John Doe\", \"john@email.com\", \"Laptop\", 1, 1000),\n",
    "    (\"O001\", \"John Doe\", \"john@email.com\", \"Mouse\", 2, 50),\n",
    "    (\"O002\", \"Jane Smith\", \"jane@email.com\", \"Monitor\", 1, 300),\n",
    "    (\"O002\", \"Jane Smith\", \"jane@email.com\", \"Keyboard\", 1, 100)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"customer_name\", \"customer_email\", \"product\", \"quantity\", \"price\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-9"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-9"
   },
   "source": [
    "**Instructor Notes:** Complex nested data flattening. Tests struct and array operations with explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-10"
   },
   "source": [
    "## Problem 10: Time-Series Gap Filling\n",
    "\n",
    "**Requirement:** Finance team needs complete time series data with missing dates filled.\n",
    "\n",
    "**Scenario:** Fill missing dates in stock price data and forward-fill the last known prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-10"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "stock_prices_data = [\n",
    "    (\"2023-01-01\", \"AAPL\", 150.0),\n",
    "    (\"2023-01-03\", \"AAPL\", 152.0),\n",
    "    (\"2023-01-04\", \"AAPL\", 151.5),\n",
    "    (\"2023-01-06\", \"AAPL\", 153.0),\n",
    "    (\"2023-01-01\", \"GOOG\", 2800.0),\n",
    "    (\"2023-01-02\", \"GOOG\", 2810.0),\n",
    "    (\"2023-01-05\", \"GOOG\", 2820.0)\n",
    "]\n",
    "\n",
    "stock_prices_df = spark.createDataFrame(stock_prices_data, [\"date\", \"symbol\", \"price\"])\n",
    "stock_prices_df = stock_prices_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "stock_prices_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-10"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"2023-01-01\", \"AAPL\", 150.0),\n",
    "    (\"2023-01-02\", \"AAPL\", 150.0),\n",
    "    (\"2023-01-03\", \"AAPL\", 152.0),\n",
    "    (\"2023-01-04\", \"AAPL\", 151.5),\n",
    "    (\"2023-01-05\", \"AAPL\", 151.5),\n",
    "    (\"2023-01-06\", \"AAPL\", 153.0),\n",
    "    (\"2023-01-01\", \"GOOG\", 2800.0),\n",
    "    (\"2023-01-02\", \"GOOG\", 2810.0),\n",
    "    (\"2023-01-03\", \"GOOG\", 2810.0),\n",
    "    (\"2023-01-04\", \"GOOG\", 2810.0),\n",
    "    (\"2023-01-05\", \"GOOG\", 2820.0),\n",
    "    (\"2023-01-06\", \"GOOG\", 2820.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\"])\n",
    "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-10"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-10"
   },
   "source": [
    "**Instructor Notes:** Time-series gap filling with last observation carried forward. Tests complex window functions and date generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-11"
   },
   "source": [
    "## Problem 11: Multi-Table Relationship Analysis\n",
    "\n",
    "**Requirement:** Business intelligence needs customer journey analysis across multiple touchpoints.\n",
    "\n",
    "**Scenario:** Join customer, orders, and payments tables to analyze complete customer journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-11"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\n",
    "customers_multi_data = [\n",
    "    (\"C001\", \"John Doe\", \"Premium\"),\n",
    "    (\"C002\", \"Jane Smith\", \"Standard\"),\n",
    "    (\"C003\", \"Bob Johnson\", \"Premium\")\n",
    "]\n",
    "\n",
    "orders_multi_data = [\n",
    "    (\"O001\", \"C001\", \"2023-01-15\", 1000.0),\n",
    "    (\"O002\", \"C001\", \"2023-02-20\", 1500.0),\n",
    "    (\"O003\", \"C002\", \"2023-01-10\", 800.0),\n",
    "    (\"O004\", \"C003\", \"2023-03-05\", 2000.0)\n",
    "]\n",
    "\n",
    "payments_multi_data = [\n",
    "    (\"P001\", \"O001\", \"2023-01-16\", \"Credit Card\"),\n",
    "    (\"P002\", \"O002\", \"2023-02-21\", \"PayPal\"),\n",
    "    (\"P003\", \"O003\", \"2023-01-11\", \"Credit Card\"),\n",
    "    (\"P004\", \"O004\", \"2023-03-06\", \"Bank Transfer\")\n",
    "]\n",
    "\n",
    "customers_multi_df = spark.createDataFrame(customers_multi_data, [\"customer_id\", \"customer_name\", \"membership\"])\n",
    "orders_multi_df = spark.createDataFrame(orders_multi_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])\n",
    "payments_multi_df = spark.createDataFrame(payments_multi_data, [\"payment_id\", \"order_id\", \"payment_date\", \"method\"])\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers_multi_df.show()\n",
    "print(\"Orders:\")\n",
    "orders_multi_df.show()\n",
    "print(\"Payments:\")\n",
    "payments_multi_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-11"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"C001\", \"John Doe\", \"Premium\", \"O001\", 1000.0, \"P001\", \"Credit Card\"),\n",
    "    (\"C001\", \"John Doe\", \"Premium\", \"O002\", 1500.0, \"P002\", \"PayPal\"),\n",
    "    (\"C002\", \"Jane Smith\", \"Standard\", \"O003\", 800.0, \"P003\", \"Credit Card\"),\n",
    "    (\"C003\", \"Bob Johnson\", \"Premium\", \"O004\", 2000.0, \"P004\", \"Bank Transfer\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"membership\", \"order_id\", \"amount\", \"payment_id\", \"payment_method\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-11"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-11"
   },
   "source": [
    "**Instructor Notes:** Multiple table joins with complex relationships. Tests chaining multiple join operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-12"
   },
   "source": [
    "## Problem 12: Advanced Window Functions with Multiple Partitions\n",
    "\n",
    "**Requirement:** Sales team needs ranking of products within each category and region.\n",
    "\n",
    "**Scenario:** Calculate product rankings within each category and region based on sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-12"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "product_region_sales_data = [\n",
    "    (\"Electronics\", \"North\", \"Laptop\", 50000),\n",
    "    (\"Electronics\", \"North\", \"Smartphone\", 75000),\n",
    "    (\"Electronics\", \"North\", \"Tablet\", 30000),\n",
    "    (\"Electronics\", \"South\", \"Laptop\", 45000),\n",
    "    (\"Electronics\", \"South\", \"Smartphone\", 60000),\n",
    "    (\"Electronics\", \"South\", \"Tablet\", 25000),\n",
    "    (\"Clothing\", \"North\", \"Shirt\", 20000),\n",
    "    (\"Clothing\", \"North\", \"Pants\", 30000),\n",
    "    (\"Clothing\", \"South\", \"Shirt\", 25000),\n",
    "    (\"Clothing\", \"South\", \"Pants\", 35000)\n",
    "]\n",
    "\n",
    "product_region_sales_df = spark.createDataFrame(product_region_sales_data, [\"category\", \"region\", \"product\", \"sales\"])\n",
    "product_region_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-12"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"Electronics\", \"North\", \"Smartphone\", 75000, 1),\n",
    "    (\"Electronics\", \"North\", \"Laptop\", 50000, 2),\n",
    "    (\"Electronics\", \"North\", \"Tablet\", 30000, 3),\n",
    "    (\"Electronics\", \"South\", \"Smartphone\", 60000, 1),\n",
    "    (\"Electronics\", \"South\", \"Laptop\", 45000, 2),\n",
    "    (\"Electronics\", \"South\", \"Tablet\", 25000, 3),\n",
    "    (\"Clothing\", \"North\", \"Pants\", 30000, 1),\n",
    "    (\"Clothing\", \"North\", \"Shirt\", 20000, 2),\n",
    "    (\"Clothing\", \"South\", \"Pants\", 35000, 1),\n",
    "    (\"Clothing\", \"South\", \"Shirt\", 25000, 2)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"category\", \"region\", \"product\", \"sales\", \"rank\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-12"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-12"
   },
   "source": [
    "**Instructor Notes:** Multi-partition window functions. Tests complex window specifications with multiple partition keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-13"
   },
   "source": [
    "## Problem 13: Data Quality Validation UDF\n",
    "\n",
    "**Requirement:** Data governance team needs comprehensive data quality checks.\n",
    "\n",
    "**Scenario:** Create UDFs to validate email format, phone numbers, and age ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-13"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_validation_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\", 25),\n",
    "    (2, \"Jane Smith\", \"invalid-email\", \"987-654-3210\", 35),\n",
    "    (3, \"Bob Johnson\", \"bob@company.com\", \"555-1234\", 17),\n",
    "    (4, \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\", 150),\n",
    "    (5, \"Charlie Wilson\", \"charlie@email.com\", \"444-555-6666\", 45)\n",
    "]\n",
    "\n",
    "customer_validation_df = spark.createDataFrame(customer_validation_data, [\"cust_id\", \"name\", \"email\", \"phone\", \"age\"])\n",
    "customer_validation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-13"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\", 25, \"Valid\", \"Valid\", \"Valid\"),\n",
    "    (2, \"Jane Smith\", \"invalid-email\", \"987-654-3210\", 35, \"Invalid\", \"Valid\", \"Valid\"),\n",
    "    (3, \"Bob Johnson\", \"bob@company.com\", \"555-1234\", 17, \"Valid\", \"Invalid\", \"Valid\"),\n",
    "    (4, \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\", 150, \"Valid\", \"Valid\", \"Invalid\"),\n",
    "    (5, \"Charlie Wilson\", \"charlie@email.com\", \"444-555-6666\", 45, \"Valid\", \"Valid\", \"Valid\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"name\", \"email\", \"phone\", \"age\", \"email_status\", \"phone_status\", \"age_status\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-13"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-13"
   },
   "source": [
    "**Instructor Notes:** Multiple UDFs for data validation. Tests regex patterns and complex validation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-14"
   },
   "source": [
    "## Problem 14: Complex Conditional Aggregation\n",
    "\n",
    "**Requirement:** Business intelligence needs segmented revenue analysis.\n",
    "\n",
    "**Scenario:** Calculate revenue by multiple customer segments and product categories simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-14"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "segmented_sales_data = [\n",
    "    (\"Premium\", \"Electronics\", 1000.0),\n",
    "    (\"Premium\", \"Clothing\", 500.0),\n",
    "    (\"Standard\", \"Electronics\", 800.0),\n",
    "    (\"Standard\", \"Clothing\", 300.0),\n",
    "    (\"Premium\", \"Electronics\", 1200.0),\n",
    "    (\"Standard\", \"Electronics\", 600.0),\n",
    "    (\"Premium\", \"Clothing\", 400.0),\n",
    "    (\"Standard\", \"Clothing\", 200.0)\n",
    "]\n",
    "\n",
    "segmented_sales_df = spark.createDataFrame(segmented_sales_data, [\"membership\", \"category\", \"amount\"])\n",
    "segmented_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-14"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"Electronics\", 2200.0, 1400.0),\n",
    "    (\"Clothing\", 900.0, 500.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"category\", \"premium_revenue\", \"standard_revenue\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-14"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-14"
   },
   "source": [
    "**Instructor Notes:** Complex conditional aggregation with multiple sum conditions. Tests advanced aggregation patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-15"
   },
   "source": [
    "## Problem 15: Array and Map Operations\n",
    "\n",
    "**Requirement:** Product analytics needs to analyze product feature usage patterns.\n",
    "\n",
    "**Scenario:** Process arrays and maps to analyze which features are used together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-15"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame with complex types\n",
    "from pyspark.sql.types import MapType\n",
    "\n",
    "product_features_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"features\", ArrayType(StringType()), True),\n",
    "    StructField(\"usage_stats\", MapType(StringType(), IntegerType()), True)\n",
    "])\n",
    "\n",
    "product_features_data = [\n",
    "    (\"P001\", [\"search\", \"filter\", \"sort\"], {\"search\": 150, \"filter\": 75, \"sort\": 50}),\n",
    "    (\"P002\", [\"search\", \"export\"], {\"search\": 200, \"export\": 30}),\n",
    "    (\"P003\", [\"filter\", \"sort\", \"import\"], {\"filter\": 100, \"sort\": 60, \"import\": 20}),\n",
    "    (\"P004\", [\"search\", \"filter\"], {\"search\": 180, \"filter\": 90})\n",
    "]\n",
    "\n",
    "product_features_df = spark.createDataFrame(product_features_data, product_features_schema)\n",
    "product_features_df.show(truncate=False)\n",
    "product_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-15"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"search\", 3, 530),\n",
    "    (\"filter\", 3, 265),\n",
    "    (\"sort\", 2, 110),\n",
    "    (\"export\", 1, 30),\n",
    "    (\"import\", 1, 20)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"feature\", \"product_count\", \"total_usage\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-15"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-15"
   },
   "source": [
    "**Instructor Notes:** Complex type operations with arrays and maps. Tests explode and map value extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-16"
   },
   "source": [
    "## Problem 16: Advanced Date/Time Operations\n",
    "\n",
    "**Requirement:** Operations team needs business day calculations excluding weekends/holidays.\n",
    "\n",
    "**Scenario:** Calculate business days between dates and adjust for weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-16"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "business_dates_data = [\n",
    "    (1, \"2023-01-02\", \"2023-01-05\"),  # Mon to Thu (4 days, 3 business days)\n",
    "    (2, \"2023-01-06\", \"2023-01-09\"),  # Fri to Mon (4 days, 1 business day)\n",
    "    (3, \"2023-01-09\", \"2023-01-13\"),  # Mon to Fri (5 days, 5 business days)\n",
    "    (4, \"2023-01-13\", \"2023-01-17\")   # Fri to Tue (5 days, 2 business days)\n",
    "]\n",
    "\n",
    "business_dates_df = spark.createDataFrame(business_dates_data, [\"task_id\", \"start_date\", \"end_date\"])\n",
    "business_dates_df = business_dates_df.withColumn(\"start_date\", col(\"start_date\").cast(\"date\"))\\\n",
    "                                   .withColumn(\"end_date\", col(\"end_date\").cast(\"date\"))\n",
    "business_dates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-16"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"2023-01-02\", \"2023-01-05\", 3),\n",
    "    (2, \"2023-01-06\", \"2023-01-09\", 1),\n",
    "    (3, \"2023-01-09\", \"2023-01-13\", 5),\n",
    "    (4, \"2023-01-13\", \"2023-01-17\", 2)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"task_id\", \"start_date\", \"end_date\", \"business_days\"])\n",
    "expected_df = expected_df.withColumn(\"start_date\", col(\"start_date\").cast(\"date\"))\\\n",
    "                       .withColumn(\"end_date\", col(\"end_date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-16"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-16"
   },
   "source": [
    "**Instructor Notes:** Advanced date operations with business logic. Tests date sequence generation and conditional counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-17"
   },
   "source": [
    "## Problem 17: Hierarchical Data Processing\n",
    "\n",
    "**Requirement:** HR analytics needs organizational hierarchy reporting.\n",
    "\n",
    "**Scenario:** Process employee-manager relationships to build organizational trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-17"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "org_hierarchy_data = [\n",
    "    (1, \"CEO\", None),\n",
    "    (2, \"VP Engineering\", 1),\n",
    "    (3, \"Engineering Manager\", 2),\n",
    "    (4, \"Senior Developer\", 3),\n",
    "    (5, \"Junior Developer\", 3),\n",
    "    (6, \"VP Marketing\", 1),\n",
    "    (7, \"Marketing Manager\", 6),\n",
    "    (8, \"Marketing Specialist\", 7)\n",
    "]\n",
    "\n",
    "org_hierarchy_df = spark.createDataFrame(org_hierarchy_data, [\"emp_id\", \"title\", \"manager_id\"])\n",
    "org_hierarchy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-17"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"CEO\", 0),\n",
    "    (2, \"VP Engineering\", 1),\n",
    "    (3, \"Engineering Manager\", 2),\n",
    "    (4, \"Senior Developer\", 3),\n",
    "    (5, \"Junior Developer\", 3),\n",
    "    (6, \"VP Marketing\", 1),\n",
    "    (7, \"Marketing Manager\", 6),\n",
    "    (8, \"Marketing Specialist\", 7)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"title\", \"hierarchy_level\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-17"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-17"
   },
   "source": [
    "**Instructor Notes:** Hierarchical data processing with iterative logic. Tests complex self-joins and level calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-18"
   },
   "source": [
    "## Problem 18: Advanced String Manipulation\n",
    "\n",
    "**Requirement:** Data engineering needs to parse and standardize address data.\n",
    "\n",
    "**Scenario:** Extract and standardize address components from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-18"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "customer_addresses_data = [\n",
    "    (1, \"123 MAIN ST, NEW YORK, NY 10001\"),\n",
    "    (2, \"456 oak avenue, Los Angeles, CA 90001\"),\n",
    "    (3, \"789 Pine Rd, Suite 100, Chicago, IL 60601\"),\n",
    "    (4, \"321 ELM STREET BOSTON MA 02101\"),\n",
    "    (5, \"555 Cedar Ln, Apt 2B, Miami, FL 33101\")\n",
    "]\n",
    "\n",
    "customer_addresses_df = spark.createDataFrame(customer_addresses_data, [\"cust_id\", \"full_address\"])\n",
    "customer_addresses_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-18"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"123 Main St\", \"New York\", \"NY\", \"10001\"),\n",
    "    (2, \"456 Oak Avenue\", \"Los Angeles\", \"CA\", \"90001\"),\n",
    "    (3, \"789 Pine Rd\", \"Chicago\", \"IL\", \"60601\"),\n",
    "    (4, \"321 Elm Street\", \"Boston\", \"MA\", \"02101\"),\n",
    "    (5, \"555 Cedar Ln\", \"Miami\", \"FL\", \"33101\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"street\", \"city\", \"state\", \"zipcode\"])\n",
    "expected_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-18"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-18"
   },
   "source": [
    "**Instructor Notes:** Complex string parsing with regex and case normalization. Tests advanced string manipulation patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-19"
   },
   "source": [
    "## Problem 19: Multi-Conditional Window Functions\n",
    "\n",
    "**Requirement:** Financial analytics needs moving averages with different conditions.\n",
    "\n",
    "**Scenario:** Calculate different types of moving averages (simple, exponential) for stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-19"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "stock_ma_data = [\n",
    "    (\"2023-01-01\", \"AAPL\", 150.0),\n",
    "    (\"2023-01-02\", \"AAPL\", 152.0),\n",
    "    (\"2023-01-03\", \"AAPL\", 151.5),\n",
    "    (\"2023-01-04\", \"AAPL\", 153.0),\n",
    "    (\"2023-01-05\", \"AAPL\", 154.5),\n",
    "    (\"2023-01-06\", \"AAPL\", 153.5),\n",
    "    (\"2023-01-07\", \"AAPL\", 155.0)\n",
    "]\n",
    "\n",
    "stock_ma_df = spark.createDataFrame(stock_ma_data, [\"date\", \"symbol\", \"price\"])\n",
    "stock_ma_df = stock_ma_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "stock_ma_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-19"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"2023-01-01\", \"AAPL\", 150.0, None, None),\n",
    "    (\"2023-01-02\", \"AAPL\", 152.0, None, None),\n",
    "    (\"2023-01-03\", \"AAPL\", 151.5, 151.17, 151.17),\n",
    "    (\"2023-01-04\", \"AAPL\", 153.0, 152.17, 152.08),\n",
    "    (\"2023-01-05\", \"AAPL\", 154.5, 153.0, 153.29),\n",
    "    (\"2023-01-06\", \"AAPL\", 153.5, 153.67, 153.39),\n",
    "    (\"2023-01-07\", \"AAPL\", 155.0, 154.33, 154.19)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\", \"sma_3d\", \"ema_3d\"])\n",
    "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-19"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-19"
   },
   "source": [
    "**Instructor Notes:** Complex window functions with multiple moving averages. Tests financial calculations and window bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-20"
   },
   "source": [
    "## Problem 20: Data Skew Handling Strategy\n",
    "\n",
    "**Requirement:** Performance optimization for skewed customer order data.\n",
    "\n",
    "**Scenario:** Handle data skew in customer orders by implementing salting technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-20"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame (skewed data - one customer has most orders)\n",
    "skewed_orders_data = [\n",
    "    (1, \"C001\", 100.0),\n",
    "    (2, \"C001\", 150.0),\n",
    "    (3, \"C001\", 200.0),\n",
    "    (4, \"C001\", 175.0),\n",
    "    (5, \"C001\", 125.0),\n",
    "    (6, \"C002\", 300.0),\n",
    "    (7, \"C003\", 250.0),\n",
    "    (8, \"C004\", 400.0),\n",
    "    (9, \"C005\", 350.0)\n",
    "]\n",
    "\n",
    "skewed_orders_df = spark.createDataFrame(skewed_orders_data, [\"order_id\", \"customer_id\", \"amount\"])\n",
    "skewed_orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-20"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"C001\", 750.0),\n",
    "    (\"C002\", 300.0),\n",
    "    (\"C003\", 250.0),\n",
    "    (\"C004\", 400.0),\n",
    "    (\"C005\", 350.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"total_amount\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-20"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-20"
   },
   "source": [
    "**Instructor Notes:** Data skew handling with salting technique. Tests performance optimization strategies for skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-21"
   },
   "source": [
    "## Problem 21: Complex Filter with Multiple Joins\n",
    "\n",
    "**Requirement:** Customer service needs to identify high-value customers with recent issues.\n",
    "\n",
    "**Scenario:** Find customers with high lifetime value who have open support tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-21"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\n",
    "customers_high_value_data = [\n",
    "    (\"C001\", \"John Doe\", 5000.0),\n",
    "    (\"C002\", \"Jane Smith\", 3000.0),\n",
    "    (\"C003\", \"Bob Johnson\", 7500.0),\n",
    "    (\"C004\", \"Alice Brown\", 2000.0)\n",
    "]\n",
    "\n",
    "support_tickets_complex_data = [\n",
    "    (\"T001\", \"C001\", \"Open\", \"2023-01-15\"),\n",
    "    (\"T002\", \"C002\", \"Closed\", \"2023-01-10\"),\n",
    "    (\"T003\", \"C003\", \"Open\", \"2023-01-20\"),\n",
    "    (\"T004\", \"C001\", \"Open\", \"2023-01-18\")\n",
    "]\n",
    "\n",
    "customers_high_value_df = spark.createDataFrame(customers_high_value_data, [\"customer_id\", \"customer_name\", \"lifetime_value\"])\n",
    "support_tickets_complex_df = spark.createDataFrame(support_tickets_complex_data, [\"ticket_id\", \"customer_id\", \"status\", \"created_date\"])\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers_high_value_df.show()\n",
    "print(\"Support Tickets:\")\n",
    "support_tickets_complex_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-21"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"C001\", \"John Doe\", 5000.0, \"T001\", \"Open\"),\n",
    "    (\"C001\", \"John Doe\", 5000.0, \"T004\", \"Open\"),\n",
    "    (\"C003\", \"Bob Johnson\", 7500.0, \"T003\", \"Open\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"lifetime_value\", \"ticket_id\", \"status\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-21"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-21"
   },
   "source": [
    "**Instructor Notes:** Complex filtering with multiple join conditions. Tests business logic implementation with joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-22"
   },
   "source": [
    "## Problem 22: Advanced Grouping with Multiple Aggregates\n",
    "\n",
    "**Requirement:** Sales analytics needs comprehensive product performance metrics.\n",
    "\n",
    "**Scenario:** Calculate multiple statistics (count, sum, avg, stddev) for products across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-22"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "product_performance_data = [\n",
    "    (\"Electronics\", \"North\", \"Laptop\", 50000),\n",
    "    (\"Electronics\", \"North\", \"Laptop\", 55000),\n",
    "    (\"Electronics\", \"South\", \"Laptop\", 45000),\n",
    "    (\"Electronics\", \"South\", \"Laptop\", 48000),\n",
    "    (\"Electronics\", \"North\", \"Tablet\", 30000),\n",
    "    (\"Electronics\", \"South\", \"Tablet\", 25000),\n",
    "    (\"Clothing\", \"North\", \"Shirt\", 20000),\n",
    "    (\"Clothing\", \"South\", \"Shirt\", 22000)\n",
    "]\n",
    "\n",
    "product_performance_df = spark.createDataFrame(product_performance_data, [\"category\", \"region\", \"product\", \"sales\"])\n",
    "product_performance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-22"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"Electronics\", \"Laptop\", 4, 198000, 49500.0, 4419.41),\n",
    "    (\"Electronics\", \"Tablet\", 2, 55000, 27500.0, 3535.53),\n",
    "    (\"Clothing\", \"Shirt\", 2, 42000, 21000.0, 1414.21)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"category\", \"product\", \"transaction_count\", \"total_sales\", \"avg_sales\", \"std_sales\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-22"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-22"
   },
   "source": [
    "**Instructor Notes:** Multi-level aggregation with statistical functions. Tests complex grouping and multiple aggregate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-23"
   },
   "source": [
    "## Problem 23: Data Enrichment with External Reference\n",
    "\n",
    "**Requirement:** Marketing needs customer data enriched with geographic information.\n",
    "\n",
    "**Scenario:** Join customer data with postal code reference table to add city/state information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-23"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\n",
    "customers_geo_data = [\n",
    "    (\"C001\", \"John Doe\", \"10001\"),\n",
    "    (\"C002\", \"Jane Smith\", \"90001\"),\n",
    "    (\"C003\", \"Bob Johnson\", \"60601\"),\n",
    "    (\"C004\", \"Alice Brown\", \"02101\")\n",
    "]\n",
    "\n",
    "postal_codes_data = [\n",
    "    (\"10001\", \"New York\", \"NY\"),\n",
    "    (\"90001\", \"Los Angeles\", \"CA\"),\n",
    "    (\"60601\", \"Chicago\", \"IL\"),\n",
    "    (\"02101\", \"Boston\", \"MA\"),\n",
    "    (\"33101\", \"Miami\", \"FL\")\n",
    "]\n",
    "\n",
    "customers_geo_df = spark.createDataFrame(customers_geo_data, [\"customer_id\", \"customer_name\", \"postal_code\"])\n",
    "postal_codes_df = spark.createDataFrame(postal_codes_data, [\"postal_code\", \"city\", \"state\"])\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers_geo_df.show()\n",
    "print(\"Postal Codes:\")\n",
    "postal_codes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-23"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"C001\", \"John Doe\", \"10001\", \"New York\", \"NY\"),\n",
    "    (\"C002\", \"Jane Smith\", \"90001\", \"Los Angeles\", \"CA\"),\n",
    "    (\"C003\", \"Bob Johnson\", \"60601\", \"Chicago\", \"IL\"),\n",
    "    (\"C004\", \"Alice Brown\", \"02101\", \"Boston\", \"MA\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"postal_code\", \"city\", \"state\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-23"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-23"
   },
   "source": [
    "**Instructor Notes:** Data enrichment with reference table join. Tests lookup operations and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-24"
   },
   "source": [
    "## Problem 24: Conditional Window Functions\n",
    "\n",
    "**Requirement:** Analytics needs to calculate conditional running totals.\n",
    "\n",
    "**Scenario:** Calculate running total of sales, but reset when category changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-24"
   },
   "source": [
    "# Source DataFrame\n",
    "category_sales_data = [\n",
    "    (\"2023-01-01\", \"Electronics\", 1000.0),\n",
    "    (\"2023-01-02\", \"Electronics\", 1500.0),\n",
    "    (\"2023-01-03\", \"Clothing\", 800.0),\n",
    "    (\"2023-01-04\", \"Clothing\", 1200.0),\n",
    "    (\"2023-01-05\", \"Electronics\", 2000.0),\n",
    "    (\"2023-01-06\", \"Electronics\", 1800.0)\n",
    "]\n",
    "\n",
    "category_sales_df = spark.createDataFrame(category_sales_data, [\"date\", \"category\", \"sales\"])\n",
    "category_sales_df = category_sales_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "category_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-24"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"2023-01-01\", \"Electronics\", 1000.0, 1000.0),\n",
    "    (\"2023-01-02\", \"Electronics\", 1500.0, 2500.0),\n",
    "    (\"2023-01-03\", \"Clothing\", 800.0, 800.0),\n",
    "    (\"2023-01-04\", \"Clothing\", 1200.0, 2000.0),\n",
    "    (\"2023-01-05\", \"Electronics\", 2000.0, 2000.0),\n",
    "    (\"2023-01-06\", \"Electronics\", 1800.0, 3800.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"date\", \"category\", \"sales\", \"category_running_total\"])\n",
    "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-24"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-24"
   },
   "source": [
    "**Instructor Notes:** Conditional window functions with partition reset. Tests complex window specifications and ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-25"
   },
   "source": [
    "## Problem 25: Multi-Column Deduplication\n",
    "\n",
    "**Requirement:** Data quality needs advanced duplicate detection with fuzzy matching.\n",
    "\n",
    "**Scenario:** Identify potential duplicates based on name similarity and other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-25"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "fuzzy_duplicates_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\"),\n",
    "    (2, \"Jon Doe\", \"john.doe@email.com\", \"123-456-7890\"),\n",
    "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n",
    "    (4, \"Jane Smithe\", \"jane.smith@email.com\", \"987-654-3210\"),\n",
    "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\"),\n",
    "    (6, \"Robert Johnson\", \"bob.johnson@email.com\", \"111-222-3333\")\n",
    "]\n",
    "\n",
    "fuzzy_duplicates_df = spark.createDataFrame(fuzzy_duplicates_data, [\"cust_id\", \"name\", \"email\", \"phone\"])\n",
    "fuzzy_duplicates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-25"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\", 1),\n",
    "    (2, \"Jon Doe\", \"john.doe@email.com\", \"123-456-7890\", 1),\n",
    "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\", 2),\n",
    "    (4, \"Jane Smithe\", \"jane.smith@email.com\", \"987-654-3210\", 2),\n",
    "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\", 3),\n",
    "    (6, \"Robert Johnson\", \"bob.johnson@email.com\", \"111-222-3333\", 3)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"name\", \"email\", \"phone\", \"duplicate_group\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-25"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-25"
   },
   "source": [
    "**Instructor Notes:** Advanced deduplication with grouping logic. Tests complex duplicate identification strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-26"
   },
   "source": [
    "## Problem 26: Complex Data Type Transformations\n",
    "\n",
    "**Requirement:** Data engineering needs to transform nested JSON structures.\n",
    "\n",
    "**Scenario:** Convert array of structs to map and vice versa for different processing needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-26"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "user_preferences_data = [\n",
    "    (\"U001\", [(\"theme\", \"dark\"), (\"language\", \"en\"), (\"notifications\", \"on\")]),\n",
    "    (\"U002\", [(\"theme\", \"light\"), (\"language\", \"es\"), (\"notifications\", \"off\")]),\n",
    "    (\"U003\", [(\"theme\", \"dark\"), (\"language\", \"fr\"), (\"notifications\", \"on\")])\n",
    "]\n",
    "\n",
    "user_preferences_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"preferences\", ArrayType(StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "user_preferences_df = spark.createDataFrame(user_preferences_data, user_preferences_schema)\n",
    "user_preferences_df.show(truncate=False)\n",
    "user_preferences_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-26"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"U001\", \"dark\", \"en\", \"on\"),\n",
    "    (\"U002\", \"light\", \"es\", \"off\"),\n",
    "    (\"U003\", \"dark\", \"fr\", \"on\")\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"user_id\", \"theme\", \"language\", \"notifications\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-26"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-26"
   },
   "source": [
    "**Instructor Notes:** Complex data type transformations. Tests array and struct manipulation for data reshaping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-27"
   },
   "source": [
    "## Problem 27: Advanced Partitioning Strategy\n",
    "\n",
    "**Requirement:** Performance optimization for large-scale time-series data.\n",
    "\n",
    "**Scenario:** Implement partitioning strategy for efficient querying of time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-27"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "time_series_large_data = [\n",
    "    (\"2023-01-01 10:00:00\", \"Sensor_A\", 25.5),\n",
    "    (\"2023-01-01 10:00:00\", \"Sensor_B\", 30.2),\n",
    "    (\"2023-01-01 11:00:00\", \"Sensor_A\", 26.1),\n",
    "    (\"2023-01-01 11:00:00\", \"Sensor_B\", 31.0),\n",
    "    (\"2023-01-02 10:00:00\", \"Sensor_A\", 24.8),\n",
    "    (\"2023-01-02 10:00:00\", \"Sensor_B\", 29.5),\n",
    "    (\"2023-01-02 11:00:00\", \"Sensor_A\", 25.3),\n",
    "    (\"2023-01-02 11:00:00\", \"Sensor_B\", 30.1)\n",
    "]\n",
    "\n",
    "time_series_large_df = spark.createDataFrame(time_series_large_data, [\"timestamp\", \"sensor_id\", \"value\"])\n",
    "time_series_large_df = time_series_large_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "time_series_large_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-27"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"2023-01-01\", \"Sensor_A\", 25.5, 26.1),\n",
    "    (\"2023-01-01\", \"Sensor_B\", 30.2, 31.0),\n",
    "    (\"2023-01-02\", \"Sensor_A\", 24.8, 25.3),\n",
    "    (\"2023-01-02\", \"Sensor_B\", 29.5, 30.1)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"date\", \"sensor_id\", \"min_value\", \"max_value\"])\n",
    "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-27"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-27"
   },
   "source": [
    "**Instructor Notes:** Partitioning strategy for performance. Tests date extraction and efficient aggregation patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-28"
   },
   "source": [
    "## Problem 28: Complex Business Logic Implementation\n",
    "\n",
    "**Requirement:** Finance needs commission calculation with tiered rates.\n",
    "\n",
    "**Scenario:** Calculate sales commissions with different rates based on sales tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-28"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "sales_commissions_data = [\n",
    "    (\"S001\", \"John\", 5000.0),\n",
    "    (\"S002\", \"Jane\", 15000.0),\n",
    "    (\"S003\", \"Bob\", 8000.0),\n",
    "    (\"S004\", \"Alice\", 25000.0),\n",
    "    (\"S005\", \"Charlie\", 12000.0)\n",
    "]\n",
    "\n",
    "sales_commissions_df = spark.createDataFrame(sales_commissions_data, [\"sales_id\", \"salesperson\", \"sales_amount\"])\n",
    "sales_commissions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-28"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"S001\", \"John\", 5000.0, 250.0),\n",
    "    (\"S002\", \"Jane\", 15000.0, 1050.0),\n",
    "    (\"S003\", \"Bob\", 8000.0, 480.0),\n",
    "    (\"S004\", \"Alice\", 25000.0, 2150.0),\n",
    "    (\"S005\", \"Charlie\", 12000.0, 780.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"sales_id\", \"salesperson\", \"sales_amount\", \"commission\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-28"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-28"
   },
   "source": [
    "**Instructor Notes:** Complex business logic with tiered calculations. Tests conditional logic and mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-29"
   },
   "source": [
    "## Problem 29: Multi-Step Data Transformation Pipeline\n",
    "\n",
    "**Requirement:** ETL pipeline needs complex multi-step data transformation.\n",
    "\n",
    "**Scenario:** Implement a multi-step transformation: clean, enrich, aggregate, and pivot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-29"
   },
   "outputs": [],
   "source": [
    "# Source DataFrame\n",
    "raw_sales_data = [\n",
    "    (\"  john  \", \"Electronics\", \"2023-01-15\", \"1000.50\"),\n",
    "    (\"Jane\", \"Clothing\", \"2023-01-16\", \"800.75\"),\n",
    "    (\"bob\", \"Electronics\", \"2023-01-17\", \"1200.25\"),\n",
    "    (\"Alice\", \"Clothing\", \"2023-01-18\", \"950.00\")\n",
    "]\n",
    "\n",
    "raw_sales_df = spark.createDataFrame(raw_sales_data, [\"salesperson\", \"category\", \"sale_date\", \"amount\"])\n",
    "raw_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-29"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"Electronics\", 2200.75),\n",
    "    (\"Clothing\", 1750.75)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"category\", \"total_sales\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-29"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-29"
   },
   "source": [
    "**Instructor Notes:** Multi-step transformation pipeline. Tests data cleaning, type conversion, and aggregation in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem-30"
   },
   "source": [
    "## Problem 30: Complex Join with Aggregation\n",
    "\n",
    "**Requirement:** Business intelligence needs customer behavior analysis with purchase patterns.\n",
    "\n",
    "**Scenario:** Join customer data with orders and calculate complex behavioral metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "source-30"
   },
   "outputs": [],
   "source": [
    "# Source DataFrames\n",
    "customers_behavior_data = [\n",
    "    (\"C001\", \"John\", \"2023-01-01\"),\n",
    "    (\"C002\", \"Jane\", \"2023-01-05\"),\n",
    "    (\"C003\", \"Bob\", \"2023-01-10\")\n",
    "]\n",
    "\n",
    "orders_behavior_data = [\n",
    "    (\"O001\", \"C001\", \"2023-01-15\", 100.0),\n",
    "    (\"O002\", \"C001\", \"2023-01-20\", 150.0),\n",
    "    (\"O003\", \"C001\", \"2023-02-01\", 200.0),\n",
    "    (\"O004\", \"C002\", \"2023-01-25\", 300.0),\n",
    "    (\"O005\", \"C003\", \"2023-02-05\", 250.0)\n",
    "]\n",
    "\n",
    "customers_behavior_df = spark.createDataFrame(customers_behavior_data, [\"customer_id\", \"customer_name\", \"signup_date\"])\n",
    "orders_behavior_df = spark.createDataFrame(orders_behavior_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers_behavior_df.show()\n",
    "print(\"Orders:\")\n",
    "orders_behavior_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expected-30"
   },
   "outputs": [],
   "source": [
    "# Expected Output\n",
    "expected_data = [\n",
    "    (\"C001\", \"John\", 3, 450.0, 150.0, 14.5),\n",
    "    (\"C002\", \"Jane\", 1, 300.0, 300.0, 20.0),\n",
    "    (\"C003\", \"Bob\", 1, 250.0, 250.0, 26.0)\n",
    "]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"order_count\", \"total_spent\", \"avg_order_value\", \"days_to_first_order\"])\n",
    "expected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "solution-30"
   },
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "\n",
    "# Test your solution\n",
    "assert_dataframe_equal(result_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-note-30"
   },
   "source": [
    "**Instructor Notes:** Complex join with multiple aggregations and date calculations. Tests comprehensive data analysis patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "completion-note"
   },
   "source": [
    "# Set 2 Complete!\n",
    "\n",
    "You've completed all 30 Easy/Medium problems in Set 2. These problems cover:\n",
    "- Advanced joins and deduplication\n",
    "- Complex window functions\n",
    "- Multi-level aggregations\n",
    "- Advanced UDFs and data validation\n",
    "- Nested data operations\n",
    "- Performance optimization strategies\n",
    "- Complex business logic implementation\n",
    "\n",
    "Ready for Set 3 with Medium difficulty problems?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}