{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajpr/prepare-anytime/blob/main/spark/coding/set2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview-section"
      },
      "source": [
        "# PySpark Interview Preparation - Set 2 (Easy/Medium)\n",
        "\n",
        "## Overview & Instructions\n",
        "\n",
        "### How to run this notebook in Google Colab:\n",
        "1. Upload this .ipynb file to Google Colab\n",
        "2. Run the installation cells below\n",
        "3. Execute each problem cell sequentially\n",
        "\n",
        "### Installation Commands:\n",
        "The following cell installs Java and PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "installation-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ad46c1-7619-484b-c6d1-0d0da85a0705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 1.8.0_462\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ],
      "source": [
        "# Install Java and PySpark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!pyspark --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sparksession-section"
      },
      "source": [
        "### SparkSession Initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sparksession-cell"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "    .appName(\"PySparkInterviewSet2\")\\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assert-function-section"
      },
      "source": [
        "### DataFrame Assertion Function:\n",
        "\n",
        "This function compares DataFrames ignoring order and with floating-point tolerance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "assert-function-cell"
      },
      "outputs": [],
      "source": [
        "def assert_dataframe_equal(df_actual, df_expected, epsilon=1e-6, check_schema_strict=False):\n",
        "    \"\"\"Compare two DataFrames using PySpark operations\"\"\"\n",
        "\n",
        "    if check_schema_strict:\n",
        "        # Check schema exactly\n",
        "        if df_actual.schema != df_expected.schema:\n",
        "            print(\"Schema mismatch!\")\n",
        "            print(\"Actual schema:\", df_actual.schema)\n",
        "            print(\"Expected schema:\", df_expected.schema)\n",
        "            raise AssertionError(\"Schema mismatch\")\n",
        "    else:\n",
        "        # Check column names and basic types\n",
        "        actual_fields = df_actual.schema\n",
        "        expected_fields = df_expected.schema\n",
        "\n",
        "        if len(actual_fields) != len(expected_fields):\n",
        "            print(\"Column count mismatch!\")\n",
        "            raise AssertionError(\"Column count mismatch\")\n",
        "\n",
        "        for i, (actual_field, expected_field) in enumerate(zip(actual_fields, expected_fields)):\n",
        "            if actual_field.name != expected_field.name:\n",
        "                print(f\"Column name mismatch at position {i}: {actual_field.name} vs {expected_field.name}\")\n",
        "                raise AssertionError(\"Column name mismatch\")\n",
        "\n",
        "    # Rest of your comparison logic remains the same\n",
        "    if df_actual.count() != df_expected.count():\n",
        "        print(f\"Row count mismatch! Actual: {df_actual.count()}, Expected: {df_expected.count()}\")\n",
        "        raise AssertionError(\"Row count mismatch\")\n",
        "\n",
        "    diff_actual = df_actual.exceptAll(df_expected)\n",
        "    diff_expected = df_expected.exceptAll(df_actual)\n",
        "\n",
        "    if diff_actual.count() > 0 or diff_expected.count() > 0:\n",
        "        print(\"Data mismatch!\")\n",
        "        print(\"Rows in actual but not in expected:\")\n",
        "        diff_actual.show()\n",
        "        print(\"Rows in expected but not in actual:\")\n",
        "        diff_expected.show()\n",
        "        raise AssertionError(\"Data content mismatch\")\n",
        "\n",
        "    print(\"✓ DataFrames are equal!\\n\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toc-section"
      },
      "source": [
        "## Table of Contents - Set 2 (Easy/Medium)\n",
        "\n",
        "**Difficulty Distribution:** 30 Easy/Medium Problems\n",
        "\n",
        "**Topics Covered:**\n",
        "- Advanced Joins & Deduplication (8 problems)\n",
        "- Complex Window Functions (6 problems)\n",
        "- Multi-level Aggregations (6 problems)\n",
        "- Advanced UDFs & Pandas UDFs (4 problems)\n",
        "- Nested Data Operations (3 problems)\n",
        "- Performance & Partitioning (3 problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-1"
      },
      "source": [
        "## Problem 1: Customer Lifetime Value Calculation\n",
        "\n",
        "**Requirement:** Marketing analytics needs to calculate Customer Lifetime Value (CLV) for segmentation.\n",
        "\n",
        "**Scenario:** Calculate total revenue, average order value, and purchase frequency for each customer over their lifetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a271d806-7a58-4363-93ae-27114a5cab65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+------+\n",
            "|order_id|customer_id|order_date|amount|\n",
            "+--------+-----------+----------+------+\n",
            "|       1|       C001|2023-01-15| 100.0|\n",
            "|       2|       C001|2023-02-20| 150.0|\n",
            "|       3|       C002|2023-01-10| 200.0|\n",
            "|       4|       C001|2023-03-05|  75.0|\n",
            "|       5|       C003|2023-02-01| 300.0|\n",
            "|       6|       C002|2023-03-15| 250.0|\n",
            "|       7|       C003|2023-03-20| 100.0|\n",
            "|       8|       C004|2023-01-25| 500.0|\n",
            "|       9|       C001|2023-04-10| 125.0|\n",
            "+--------+-----------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_orders_data = [\n",
        "    (1, \"C001\", \"2023-01-15\", 100.0),\n",
        "    (2, \"C001\", \"2023-02-20\", 150.0),\n",
        "    (3, \"C002\", \"2023-01-10\", 200.0),\n",
        "    (4, \"C001\", \"2023-03-05\", 75.0),\n",
        "    (5, \"C003\", \"2023-02-01\", 300.0),\n",
        "    (6, \"C002\", \"2023-03-15\", 250.0),\n",
        "    (7, \"C003\", \"2023-03-20\", 100.0),\n",
        "    (8, \"C004\", \"2023-01-25\", 500.0),\n",
        "    (9, \"C001\", \"2023-04-10\", 125.0)\n",
        "]\n",
        "\n",
        "customer_orders_df = spark.createDataFrame(customer_orders_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])\n",
        "customer_orders_df = customer_orders_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
        "customer_orders_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ea0e8-fef6-440a-8323-259a0f486f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+-------------+---------------+-----+\n",
            "|customer_id|total_orders|total_revenue|avg_order_value|  clv|\n",
            "+-----------+------------+-------------+---------------+-----+\n",
            "|       C004|           1|        500.0|          500.0|500.0|\n",
            "|       C003|           2|        400.0|          200.0|200.0|\n",
            "|       C002|           2|        450.0|          225.0|225.0|\n",
            "|       C001|           4|        450.0|          112.5|112.5|\n",
            "+-----------+------------+-------------+---------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C004\", 1, 500.0, 500.0, 500.0),\n",
        "    (\"C003\", 2, 400.0, 200.0, 200.0),\n",
        "    (\"C002\", 2, 450.0, 225.0, 225.0),\n",
        "    (\"C001\", 4, 450.0, 112.5, 112.5)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"total_orders\", \"total_revenue\", \"avg_order_value\", \"clv\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0174108c-7fa0-482e-8901-f709cfbc9d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+-------------+---------------+-----+\n",
            "|customer_id|total_orders|total_revenue|avg_order_value|  clv|\n",
            "+-----------+------------+-------------+---------------+-----+\n",
            "|       C001|           4|        450.0|          112.5|112.5|\n",
            "|       C002|           2|        450.0|          225.0|225.0|\n",
            "|       C003|           2|        400.0|          200.0|200.0|\n",
            "|       C004|           1|        500.0|          500.0|500.0|\n",
            "+-----------+------------+-------------+---------------+-----+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql import types as tp\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "##-----------\n",
        "\n",
        "result_df = \\\n",
        "      customer_orders_df\\\n",
        "          .groupBy('customer_id')\\\n",
        "          .agg(\n",
        "              fn.count(fn.col('order_id')).alias('total_orders'),\n",
        "              fn.sum(fn.col('amount')).alias('total_revenue'),\n",
        "              fn.avg(fn.col('amount')).alias('avg_order_value'),\n",
        "              fn.avg(fn.col('amount')).alias('clv'),\n",
        "              )\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-1"
      },
      "source": [
        "**Instructor Notes:** Multi-column aggregation with customer metrics. Tests complex business metric calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-2"
      },
      "source": [
        "## Problem 2: Employee Department Hierarchy\n",
        "\n",
        "**Requirement:** HR needs to identify employees with their managers for organizational reporting.\n",
        "\n",
        "**Scenario:** Perform self-join on employee table to get manager names for each employee."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6533e55c-8579-4a31-a327-ca4263c2d82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+----------+--------------------+\n",
            "|emp_id|         emp_name|manager_id|               title|\n",
            "+------+-----------------+----------+--------------------+\n",
            "|     1|         John CEO|      NULL|                 CEO|\n",
            "|     2|         Alice VP|         1|      VP Engineering|\n",
            "|     3|      Bob Manager|         2| Engineering Manager|\n",
            "|     4|Charlie Developer|         3|    Senior Developer|\n",
            "|     5|         Diana VP|         1|        VP Marketing|\n",
            "|     6|   Eve Specialist|         5|Marketing Specialist|\n",
            "|     7|    Frank Manager|         2|          QA Manager|\n",
            "+------+-----------------+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "employees_hierarchy_data = [\n",
        "    (1, \"John CEO\", None, \"CEO\"),\n",
        "    (2, \"Alice VP\", 1, \"VP Engineering\"),\n",
        "    (3, \"Bob Manager\", 2, \"Engineering Manager\"),\n",
        "    (4, \"Charlie Developer\", 3, \"Senior Developer\"),\n",
        "    (5, \"Diana VP\", 1, \"VP Marketing\"),\n",
        "    (6, \"Eve Specialist\", 5, \"Marketing Specialist\"),\n",
        "    (7, \"Frank Manager\", 2, \"QA Manager\")\n",
        "]\n",
        "\n",
        "employees_hierarchy_df = spark.createDataFrame(employees_hierarchy_data, [\"emp_id\", \"emp_name\", \"manager_id\", \"title\"])\n",
        "employees_hierarchy_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70dd6701-2cbf-4f26-f04d-a549d97e1fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+------------+--------------------+\n",
            "|emp_id|         emp_name|manager_name|               title|\n",
            "+------+-----------------+------------+--------------------+\n",
            "|     2|         Alice VP|    John CEO|      VP Engineering|\n",
            "|     3|      Bob Manager|    Alice VP| Engineering Manager|\n",
            "|     4|Charlie Developer| Bob Manager|    Senior Developer|\n",
            "|     5|         Diana VP|    John CEO|        VP Marketing|\n",
            "|     6|   Eve Specialist|    Diana VP|Marketing Specialist|\n",
            "|     7|    Frank Manager|    Alice VP|          QA Manager|\n",
            "+------+-----------------+------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (2, \"Alice VP\", \"John CEO\", \"VP Engineering\"),\n",
        "    (3, \"Bob Manager\", \"Alice VP\", \"Engineering Manager\"),\n",
        "    (4, \"Charlie Developer\", \"Bob Manager\", \"Senior Developer\"),\n",
        "    (5, \"Diana VP\", \"John CEO\", \"VP Marketing\"),\n",
        "    (6, \"Eve Specialist\", \"Diana VP\", \"Marketing Specialist\"),\n",
        "    (7, \"Frank Manager\", \"Alice VP\", \"QA Manager\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"emp_name\", \"manager_name\", \"title\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b5dedc-539b-44b5-90c9-1089967faf8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+------------+--------------------+\n",
            "|emp_id|         emp_name|manager_name|               title|\n",
            "+------+-----------------+------------+--------------------+\n",
            "|     2|         Alice VP|    John CEO|      VP Engineering|\n",
            "|     5|         Diana VP|    John CEO|        VP Marketing|\n",
            "|     3|      Bob Manager|    Alice VP| Engineering Manager|\n",
            "|     7|    Frank Manager|    Alice VP|          QA Manager|\n",
            "|     4|Charlie Developer| Bob Manager|    Senior Developer|\n",
            "|     6|   Eve Specialist|    Diana VP|Marketing Specialist|\n",
            "+------+-----------------+------------+--------------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "join_on = expr('emp.manager_id == man.emp_id')\n",
        "\n",
        "result_df = \\\n",
        "      employees_hierarchy_df.alias('emp')\\\n",
        "          .join(employees_hierarchy_df.alias('man'),\n",
        "                join_on,\n",
        "                'inner')\\\n",
        "          .select(fn.col('emp.emp_id'),\n",
        "                  fn.col('emp.emp_name'),\n",
        "                  fn.col('man.emp_name').alias('manager_name'),\n",
        "                  fn.col('emp.title')\n",
        "                  )\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-2"
      },
      "source": [
        "**Instructor Notes:** Self-join operation. Tests joining a table with itself on different conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-3"
      },
      "source": [
        "## Problem 3: Running Total with Window Functions\n",
        "\n",
        "**Requirement:** Finance team needs running total of daily sales for cash flow analysis.\n",
        "\n",
        "**Scenario:** Calculate cumulative sum of sales ordered by date using window functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2467d9ed-e683-4ba0-d0e6-8fe34ac899cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|      date|daily_sales|\n",
            "+----------+-----------+\n",
            "|2023-01-01|     1000.0|\n",
            "|2023-01-02|     1500.0|\n",
            "|2023-01-03|      800.0|\n",
            "|2023-01-04|     2000.0|\n",
            "|2023-01-05|     1200.0|\n",
            "|2023-01-06|     1800.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "daily_sales_data = [\n",
        "    (\"2023-01-01\", 1000.0),\n",
        "    (\"2023-01-02\", 1500.0),\n",
        "    (\"2023-01-03\", 800.0),\n",
        "    (\"2023-01-04\", 2000.0),\n",
        "    (\"2023-01-05\", 1200.0),\n",
        "    (\"2023-01-06\", 1800.0)\n",
        "]\n",
        "\n",
        "daily_sales_df = spark.createDataFrame(daily_sales_data, [\"date\", \"daily_sales\"])\n",
        "daily_sales_df = daily_sales_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "daily_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb62c0f0-3acf-4141-f022-a4bbb5071e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-------------+\n",
            "|      date|daily_sales|running_total|\n",
            "+----------+-----------+-------------+\n",
            "|2023-01-01|     1000.0|       1000.0|\n",
            "|2023-01-02|     1500.0|       2500.0|\n",
            "|2023-01-03|      800.0|       3300.0|\n",
            "|2023-01-04|     2000.0|       5300.0|\n",
            "|2023-01-05|     1200.0|       6500.0|\n",
            "|2023-01-06|     1800.0|       8300.0|\n",
            "+----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", 1000.0, 1000.0),\n",
        "    (\"2023-01-02\", 1500.0, 2500.0),\n",
        "    (\"2023-01-03\", 800.0, 3300.0),\n",
        "    (\"2023-01-04\", 2000.0, 5300.0),\n",
        "    (\"2023-01-05\", 1200.0, 6500.0),\n",
        "    (\"2023-01-06\", 1800.0, 8300.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"daily_sales\", \"running_total\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54208be-61bf-475b-c506-c02af9cd333f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-------------+\n",
            "|      date|daily_sales|running_total|\n",
            "+----------+-----------+-------------+\n",
            "|2023-01-01|     1000.0|       1000.0|\n",
            "|2023-01-02|     1500.0|       2500.0|\n",
            "|2023-01-03|      800.0|       3300.0|\n",
            "|2023-01-04|     2000.0|       5300.0|\n",
            "|2023-01-05|     1200.0|       6500.0|\n",
            "|2023-01-06|     1800.0|       8300.0|\n",
            "+----------+-----------+-------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "win = Window.orderBy(fn.col('date').asc_nulls_last())\\\n",
        "            .rowsBetween(Window.unboundedPreceding, Window.currentRow) #\n",
        "\n",
        "result_df = \\\n",
        "      daily_sales_df\\\n",
        "        .withColumn('running_total', sum(fn.col('daily_sales')).over(win))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-3"
      },
      "source": [
        "**Instructor Notes:** Window function with cumulative sum. Tests unbounded window for running totals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-4"
      },
      "source": [
        "## Problem 4: Product Recommendation Engine\n",
        "\n",
        "**Requirement:** E-commerce team wants to recommend products frequently bought together.\n",
        "\n",
        "**Scenario:** Find product pairs that are frequently purchased in the same order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57c4d33-1f11-4163-8721-189cfd83de35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------+\n",
            "|order_id|product_id|product_name|\n",
            "+--------+----------+------------+\n",
            "|       1|      P001|      Laptop|\n",
            "|       1|      P002|       Mouse|\n",
            "|       1|      P003|  Laptop Bag|\n",
            "|       2|      P001|      Laptop|\n",
            "|       2|      P002|       Mouse|\n",
            "|       3|      P004|     Monitor|\n",
            "|       3|      P002|       Mouse|\n",
            "|       4|      P001|      Laptop|\n",
            "|       4|      P005|    Keyboard|\n",
            "|       5|      P002|       Mouse|\n",
            "|       5|      P005|    Keyboard|\n",
            "+--------+----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "order_items_data = [\n",
        "    (1, \"P001\", \"Laptop\"),\n",
        "    (1, \"P002\", \"Mouse\"),\n",
        "    (1, \"P003\", \"Laptop Bag\"),\n",
        "    (2, \"P001\", \"Laptop\"),\n",
        "    (2, \"P002\", \"Mouse\"),\n",
        "    (3, \"P004\", \"Monitor\"),\n",
        "    (3, \"P002\", \"Mouse\"),\n",
        "    (4, \"P001\", \"Laptop\"),\n",
        "    (4, \"P005\", \"Keyboard\"),\n",
        "    (5, \"P002\", \"Mouse\"),\n",
        "    (5, \"P005\", \"Keyboard\")\n",
        "]\n",
        "\n",
        "order_items_df = spark.createDataFrame(order_items_data, [\"order_id\", \"product_id\", \"product_name\"])\n",
        "order_items_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0634ffc9-91e4-47ae-d192-7d586920f271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-------------+----------+\n",
            "|product1_id|product1_name|product2_id|product2_name|pair_count|\n",
            "+-----------+-------------+-----------+-------------+----------+\n",
            "|       P001|       Laptop|       P002|        Mouse|         2|\n",
            "|       P002|        Mouse|       P005|     Keyboard|         2|\n",
            "|       P001|       Laptop|       P003|   Laptop Bag|         1|\n",
            "|       P004|      Monitor|       P002|        Mouse|         1|\n",
            "|       P001|       Laptop|       P005|     Keyboard|         1|\n",
            "+-----------+-------------+-----------+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"P001\", \"Laptop\", \"P002\", \"Mouse\", 2),\n",
        "    (\"P002\", \"Mouse\", \"P005\", \"Keyboard\", 2),\n",
        "    (\"P001\", \"Laptop\", \"P003\", \"Laptop Bag\", 1),\n",
        "    (\"P004\", \"Monitor\", \"P002\", \"Mouse\", 1),\n",
        "    (\"P001\", \"Laptop\", \"P005\", \"Keyboard\", 1)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"product1_id\", \"product1_name\", \"product2_id\", \"product2_name\", \"pair_count\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4b0b9e-55cf-4f81-fa51-15d85ab71163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-------------+----------+\n",
            "|product1_id|product1_name|product2_id|product2_name|pair_count|\n",
            "+-----------+-------------+-----------+-------------+----------+\n",
            "|       P002|        Mouse|       P001|       Laptop|         2|\n",
            "|       P003|   Laptop Bag|       P001|       Laptop|         1|\n",
            "|       P003|   Laptop Bag|       P002|        Mouse|         1|\n",
            "|       P004|      Monitor|       P002|        Mouse|         1|\n",
            "|       P005|     Keyboard|       P001|       Laptop|         1|\n",
            "|       P005|     Keyboard|       P002|        Mouse|         1|\n",
            "+-----------+-------------+-----------+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "join_on = fn.expr('''\n",
        "                  table1.order_id = table2.order_id\n",
        "                  and\n",
        "                  table1.product_id > table2.product_id\n",
        "                  ''')\n",
        "\n",
        "result_df = \\\n",
        "        order_items_df.alias('table1')\\\n",
        "              .join(order_items_df.alias('table2'),\n",
        "                    join_on,\n",
        "                    'inner')\\\n",
        "              .select(\n",
        "                    fn.col('table1.product_id').alias('product1_id'),\n",
        "                    fn.col('table1.product_name').alias('product1_name'),\n",
        "                    fn.col('table2.product_id').alias('product2_id'),\n",
        "                    fn.col('table2.product_name').alias('product2_name'),\n",
        "                    fn.col('table1.order_id').alias('order_id')\n",
        "                     )\\\n",
        "              .groupBy('product1_id','product1_name','product2_id','product2_name')\\\n",
        "                  .agg(fn.count(fn.col('order_id')).alias('pair_count'))\\\n",
        "              .orderBy(fn.col('pair_count').desc_nulls_first(),\n",
        "                       fn.col('product1_id'),\n",
        "                       fn.col('product2_id'),\n",
        "                       )\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-4"
      },
      "source": [
        "**Instructor Notes:** Self-join for co-occurrence analysis. Tests complex join conditions and pair counting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-5"
      },
      "source": [
        "## Problem 5: Time-Based Sessionization\n",
        "\n",
        "**Requirement:** Analytics team needs to group user activities into sessions based on time gaps.\n",
        "\n",
        "**Scenario:** Group user activities into sessions where gaps between activities are > 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd964323-8ae9-4aa6-c41a-a0968d6c351b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+--------+\n",
            "|user_id|          timestamp|  action|\n",
            "+-------+-------------------+--------+\n",
            "|   U001|2023-01-01 10:00:00|   login|\n",
            "|   U001|2023-01-01 10:05:00|  browse|\n",
            "|   U001|2023-01-01 10:10:00|   click|\n",
            "|   U001|2023-01-01 10:45:00|purchase|\n",
            "|   U001|2023-01-01 10:50:00|  logout|\n",
            "|   U002|2023-01-01 11:00:00|   login|\n",
            "|   U002|2023-01-01 11:15:00|  browse|\n",
            "|   U002|2023-01-01 11:20:00|   click|\n",
            "+-------+-------------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "user_activities_data = [\n",
        "    (\"U001\", \"2023-01-01 10:00:00\", \"login\"),\n",
        "    (\"U001\", \"2023-01-01 10:05:00\", \"browse\"),\n",
        "    (\"U001\", \"2023-01-01 10:10:00\", \"click\"),\n",
        "    (\"U001\", \"2023-01-01 10:45:00\", \"purchase\"),  # New session (35 min gap)\n",
        "    (\"U001\", \"2023-01-01 10:50:00\", \"logout\"),\n",
        "    (\"U002\", \"2023-01-01 11:00:00\", \"login\"),\n",
        "    (\"U002\", \"2023-01-01 11:15:00\", \"browse\"),\n",
        "    (\"U002\", \"2023-01-01 11:20:00\", \"click\")\n",
        "]\n",
        "\n",
        "user_activities_df = spark.createDataFrame(user_activities_data, [\"user_id\", \"timestamp\", \"action\"])\n",
        "user_activities_df = user_activities_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "user_activities_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82273883-e7c2-49db-eeaf-80224b9c0f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+--------+----------+\n",
            "|user_id|          timestamp|  action|session_id|\n",
            "+-------+-------------------+--------+----------+\n",
            "|   U001|2023-01-01 10:00:00|   login|         1|\n",
            "|   U001|2023-01-01 10:05:00|  browse|         1|\n",
            "|   U001|2023-01-01 10:10:00|   click|         1|\n",
            "|   U001|2023-01-01 10:45:00|purchase|         2|\n",
            "|   U001|2023-01-01 10:50:00|  logout|         2|\n",
            "|   U002|2023-01-01 11:00:00|   login|         1|\n",
            "|   U002|2023-01-01 11:15:00|  browse|         1|\n",
            "|   U002|2023-01-01 11:20:00|   click|         1|\n",
            "+-------+-------------------+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"U001\", \"2023-01-01 10:00:00\", \"login\", 1),\n",
        "    (\"U001\", \"2023-01-01 10:05:00\", \"browse\", 1),\n",
        "    (\"U001\", \"2023-01-01 10:10:00\", \"click\", 1),\n",
        "    (\"U001\", \"2023-01-01 10:45:00\", \"purchase\", 2),\n",
        "    (\"U001\", \"2023-01-01 10:50:00\", \"logout\", 2),\n",
        "    (\"U002\", \"2023-01-01 11:00:00\", \"login\", 1),\n",
        "    (\"U002\", \"2023-01-01 11:15:00\", \"browse\", 1),\n",
        "    (\"U002\", \"2023-01-01 11:20:00\", \"click\", 1)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"user_id\", \"timestamp\", \"action\", \"session_id\"])\n",
        "expected_df = expected_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164d44be-f99e-449b-e666-dd8e7673c4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+--------+----------+\n",
            "|user_id|          timestamp|  action|session_id|\n",
            "+-------+-------------------+--------+----------+\n",
            "|   U001|2023-01-01 10:00:00|   login|         1|\n",
            "|   U001|2023-01-01 10:05:00|  browse|         1|\n",
            "|   U001|2023-01-01 10:10:00|   click|         1|\n",
            "|   U001|2023-01-01 10:45:00|purchase|         2|\n",
            "|   U001|2023-01-01 10:50:00|  logout|         2|\n",
            "|   U002|2023-01-01 11:00:00|   login|         1|\n",
            "|   U002|2023-01-01 11:15:00|  browse|         1|\n",
            "|   U002|2023-01-01 11:20:00|   click|         1|\n",
            "+-------+-------------------+--------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "win = Window.partitionBy('user_id').orderBy(fn.col('timestamp').asc_nulls_last())\n",
        "\n",
        "result_df = \\\n",
        "    user_activities_df\\\n",
        "      .withColumn('lastTimeStamp',fn.lag(fn.col('timestamp')).over(win))\\\n",
        "      .withColumn('TimeExceedFlag',fn.expr('CASE WHEN timestamp - lastTimeStamp <= INTERVAL 30 MINUTES THEN 0 ELSE 1 END'))\\\n",
        "      .withColumn('session_id', fn.sum(fn.col('TimeExceedFlag')).over(win))\\\n",
        "      .drop('lastTimeStamp','TimeExceedFlag')\\\n",
        "      .orderBy('user_id','timestamp')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-5"
      },
      "source": [
        "**Instructor Notes:** Advanced window functions for sessionization. Tests time gap analysis and conditional session creation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-6"
      },
      "source": [
        "## Problem 6: Complex UDF for Text Analysis\n",
        "\n",
        "**Requirement:** Customer service needs to categorize support tickets based on sentiment and urgency.\n",
        "\n",
        "**Scenario:** Create a UDF that analyzes ticket text and returns priority level based on keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5f8c1f-feff-4536-c96f-1db1f13ece9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------------------------------------+--------+\n",
            "|ticket_id|description                                  |reporter|\n",
            "+---------+---------------------------------------------+--------+\n",
            "|1        |My login is not working, need immediate help |John    |\n",
            "|2        |Feature request for dark mode                |Jane    |\n",
            "|3        |URGENT: Payment failed but money deducted    |Bob     |\n",
            "|4        |Bug report: button color issue               |Alice   |\n",
            "|5        |CRITICAL: System down, cannot access anything|Charlie |\n",
            "+---------+---------------------------------------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "support_tickets_data = [\n",
        "    (1, \"My login is not working, need immediate help\", \"John\"),\n",
        "    (2, \"Feature request for dark mode\", \"Jane\"),\n",
        "    (3, \"URGENT: Payment failed but money deducted\", \"Bob\"),\n",
        "    (4, \"Bug report: button color issue\", \"Alice\"),\n",
        "    (5, \"CRITICAL: System down, cannot access anything\", \"Charlie\")\n",
        "]\n",
        "\n",
        "support_tickets_df = spark.createDataFrame(support_tickets_data, [\"ticket_id\", \"description\", \"reporter\"])\n",
        "support_tickets_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8256ccaf-8a8e-4a02-b158-8106854f40ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------------------------------------+--------+--------+\n",
            "|ticket_id|description                                  |reporter|priority|\n",
            "+---------+---------------------------------------------+--------+--------+\n",
            "|1        |My login is not working, need immediate help |John    |High    |\n",
            "|2        |Feature request for dark mode                |Jane    |Low     |\n",
            "|3        |URGENT: Payment failed but money deducted    |Bob     |Critical|\n",
            "|4        |Bug report: button color issue               |Alice   |Medium  |\n",
            "|5        |CRITICAL: System down, cannot access anything|Charlie |Critical|\n",
            "+---------+---------------------------------------------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"My login is not working, need immediate help\", \"John\", \"High\"),\n",
        "    (2, \"Feature request for dark mode\", \"Jane\", \"Low\"),\n",
        "    (3, \"URGENT: Payment failed but money deducted\", \"Bob\", \"Critical\"),\n",
        "    (4, \"Bug report: button color issue\", \"Alice\", \"Medium\"),\n",
        "    (5, \"CRITICAL: System down, cannot access anything\", \"Charlie\", \"Critical\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"ticket_id\", \"description\", \"reporter\", \"priority\"])\n",
        "expected_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-6"
      },
      "outputs": [],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "# i will skip this question - because, for me this is not making much sense\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-6"
      },
      "source": [
        "**Instructor Notes:** Complex UDF with string analysis. Tests text processing and conditional logic in UDFs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-7"
      },
      "source": [
        "## Problem 7: Multiple Column Pivot\n",
        "\n",
        "**Requirement:** Sales analytics needs quarterly sales data pivoted by both product and region.\n",
        "\n",
        "**Scenario:** Create a pivot table showing sales by product category and quarter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c95977d-67cc-4398-a289-89a01af70900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+------+-----+\n",
            "|   category|quarter|region|sales|\n",
            "+-----------+-------+------+-----+\n",
            "|Electronics|     Q1| North|50000|\n",
            "|Electronics|     Q1| South|45000|\n",
            "|Electronics|     Q2| North|60000|\n",
            "|Electronics|     Q2| South|55000|\n",
            "|   Clothing|     Q1| North|30000|\n",
            "|   Clothing|     Q1| South|35000|\n",
            "|   Clothing|     Q2| North|40000|\n",
            "|   Clothing|     Q2| South|45000|\n",
            "+-----------+-------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "regional_sales_data = [\n",
        "    (\"Electronics\", \"Q1\", \"North\", 50000),\n",
        "    (\"Electronics\", \"Q1\", \"South\", 45000),\n",
        "    (\"Electronics\", \"Q2\", \"North\", 60000),\n",
        "    (\"Electronics\", \"Q2\", \"South\", 55000),\n",
        "    (\"Clothing\", \"Q1\", \"North\", 30000),\n",
        "    (\"Clothing\", \"Q1\", \"South\", 35000),\n",
        "    (\"Clothing\", \"Q2\", \"North\", 40000),\n",
        "    (\"Clothing\", \"Q2\", \"South\", 45000)\n",
        "]\n",
        "\n",
        "regional_sales_df = spark.createDataFrame(regional_sales_data, [\"category\", \"quarter\", \"region\", \"sales\"])\n",
        "regional_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f715c6c-1949-4a29-fa7c-a7f91f25ddfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+--------+\n",
            "|   category|Q1_sales|Q2_sales|\n",
            "+-----------+--------+--------+\n",
            "|Electronics|   95000|  115000|\n",
            "|   Clothing|   65000|   85000|\n",
            "+-----------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Electronics\", 95000, 115000),\n",
        "    (\"Clothing\", 65000, 85000)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"category\", \"Q1_sales\", \"Q2_sales\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6150ea2-680a-4d66-fa07-a1bed67a9f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+--------+\n",
            "|   category|Q1_sales|Q2_sales|\n",
            "+-----------+--------+--------+\n",
            "|Electronics|   95000|  115000|\n",
            "|   Clothing|   65000|   85000|\n",
            "+-----------+--------+--------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    regional_sales_df\\\n",
        "      .withColumn('quarter_naming',fn.expr('''quarter || '_sales' '''))\\\n",
        "      .groupBy('category')\\\n",
        "      .pivot('quarter_naming')\\\n",
        "      .agg(fn.expr('sum(sales)'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-7"
      },
      "source": [
        "**Instructor Notes:** Multi-level pivot with aggregation. Tests complex pivot operations with multiple grouping columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-8"
      },
      "source": [
        "## Problem 8: Advanced Deduplication with Multiple Criteria\n",
        "\n",
        "**Requirement:** Data quality team needs to identify and remove duplicate customer records.\n",
        "\n",
        "**Scenario:** Find duplicate customers based on name, email, or phone with different criteria weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bcea20-f4f0-4647-f092-453d4ac5f426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+------------------+------------+\n",
            "|cust_id|          name|             email|       phone|\n",
            "+-------+--------------+------------------+------------+\n",
            "|      1|      John Doe|    john@email.com|123-456-7890|\n",
            "|      2|      John Doe|john.doe@email.com|123-456-7890|\n",
            "|      3|    Jane Smith|    jane@email.com|987-654-3210|\n",
            "|      4|    Jane Smith|    jane@email.com|555-123-4567|\n",
            "|      5|   Bob Johnson|     bob@email.com|111-222-3333|\n",
            "|      6|Robert Johnson|     bob@email.com|111-222-3333|\n",
            "+-------+--------------+------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_duplicates_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\"),\n",
        "    (2, \"John Doe\", \"john.doe@email.com\", \"123-456-7890\"),\n",
        "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n",
        "    (4, \"Jane Smith\", \"jane@email.com\", \"555-123-4567\"),\n",
        "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\"),\n",
        "    (6, \"Robert Johnson\", \"bob@email.com\", \"111-222-3333\")\n",
        "]\n",
        "\n",
        "customer_duplicates_df = spark.createDataFrame(customer_duplicates_data, [\"cust_id\", \"name\", \"email\", \"phone\"])\n",
        "customer_duplicates_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b4bfb8-f21a-42e2-a020-7782610a7568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+--------------+------------+\n",
            "|cust_id|       name|         email|       phone|\n",
            "+-------+-----------+--------------+------------+\n",
            "|      1|   John Doe|john@email.com|123-456-7890|\n",
            "|      3| Jane Smith|jane@email.com|987-654-3210|\n",
            "|      5|Bob Johnson| bob@email.com|111-222-3333|\n",
            "+-------+-----------+--------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\"),\n",
        "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n",
        "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"name\", \"email\", \"phone\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d81f2d-e446-44cb-fc6d-d31b1eea3067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+------------------+------------+\n",
            "|cust_id|          name|             email|       phone|\n",
            "+-------+--------------+------------------+------------+\n",
            "|      2|      John Doe|john.doe@email.com|123-456-7890|\n",
            "|      1|      John Doe|    john@email.com|123-456-7890|\n",
            "|      3|    Jane Smith|    jane@email.com|987-654-3210|\n",
            "|      6|Robert Johnson|     bob@email.com|111-222-3333|\n",
            "|      5|   Bob Johnson|     bob@email.com|111-222-3333|\n",
            "|      4|    Jane Smith|    jane@email.com|555-123-4567|\n",
            "+-------+--------------+------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    customer_duplicates_df\\\n",
        "        .dropDuplicates(['name','email','phone'])\\\n",
        "        .select('cust_id','name','email','phone')\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# # Test your solution\n",
        "# assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-8"
      },
      "source": [
        "**Instructor Notes:** Advanced deduplication with multiple matching criteria. Tests window functions and complex duplicate identification logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-9"
      },
      "source": [
        "## Problem 9: Nested JSON Data Processing\n",
        "\n",
        "**Requirement:** Analytics team needs to flatten nested JSON data from API responses.\n",
        "\n",
        "**Scenario:** Extract and flatten nested customer order data with array of items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8a554e-ba60-4d6c-cf11-6ffb7ac8d54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------------------------+---------------------------------------+\n",
            "|order_id|customer                    |items                                  |\n",
            "+--------+----------------------------+---------------------------------------+\n",
            "|O001    |{John Doe, john@email.com}  |[{Laptop, 1, 1000}, {Mouse, 2, 50}]    |\n",
            "|O002    |{Jane Smith, jane@email.com}|[{Monitor, 1, 300}, {Keyboard, 1, 100}]|\n",
            "+--------+----------------------------+---------------------------------------+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer: struct (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- email: string (nullable = true)\n",
            " |-- items: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- product: string (nullable = true)\n",
            " |    |    |-- quantity: integer (nullable = true)\n",
            " |    |    |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame with nested structure\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"customer\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"email\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"items\", ArrayType(StructType([\n",
        "        StructField(\"product\", StringType(), True),\n",
        "        StructField(\"quantity\", IntegerType(), True),\n",
        "        StructField(\"price\", IntegerType(), True)\n",
        "    ])), True)\n",
        "])\n",
        "\n",
        "nested_data = [\n",
        "    (\"O001\", (\"John Doe\", \"john@email.com\"), [(\"Laptop\", 1, 1000), (\"Mouse\", 2, 50)]),\n",
        "    (\"O002\", (\"Jane Smith\", \"jane@email.com\"), [(\"Monitor\", 1, 300), (\"Keyboard\", 1, 100)])\n",
        "]\n",
        "\n",
        "nested_df = spark.createDataFrame(nested_data, schema)\n",
        "nested_df.show(truncate=False)\n",
        "nested_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab50341-d463-4f0e-b3f2-27c8d6220782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+--------------+--------+--------+-----+\n",
            "|order_id|customer_name|customer_email| product|quantity|price|\n",
            "+--------+-------------+--------------+--------+--------+-----+\n",
            "|    O001|     John Doe|john@email.com|  Laptop|       1| 1000|\n",
            "|    O001|     John Doe|john@email.com|   Mouse|       2|   50|\n",
            "|    O002|   Jane Smith|jane@email.com| Monitor|       1|  300|\n",
            "|    O002|   Jane Smith|jane@email.com|Keyboard|       1|  100|\n",
            "+--------+-------------+--------------+--------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"O001\", \"John Doe\", \"john@email.com\", \"Laptop\", 1, 1000),\n",
        "    (\"O001\", \"John Doe\", \"john@email.com\", \"Mouse\", 2, 50),\n",
        "    (\"O002\", \"Jane Smith\", \"jane@email.com\", \"Monitor\", 1, 300),\n",
        "    (\"O002\", \"Jane Smith\", \"jane@email.com\", \"Keyboard\", 1, 100)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"order_id\", \"customer_name\", \"customer_email\", \"product\", \"quantity\", \"price\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9176ae7d-64cb-4137-84bd-173ca53839c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+--------------+--------+--------+-----+\n",
            "|order_id|customer_name|customer_email| product|quantity|price|\n",
            "+--------+-------------+--------------+--------+--------+-----+\n",
            "|    O001|     John Doe|john@email.com|  Laptop|       1| 1000|\n",
            "|    O001|     John Doe|john@email.com|   Mouse|       2|   50|\n",
            "|    O002|   Jane Smith|jane@email.com| Monitor|       1|  300|\n",
            "|    O002|   Jane Smith|jane@email.com|Keyboard|       1|  100|\n",
            "+--------+-------------+--------------+--------+--------+-----+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      nested_df\\\n",
        "        .withColumn('customer_name',fn.col('customer')['name'])\\\n",
        "        .withColumn('customer_email',fn.col('customer')['email'])\\\n",
        "        .drop('customer')\\\n",
        "        .select('*', fn.inline_outer('items'))\\\n",
        "        .drop('items')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-9"
      },
      "source": [
        "**Instructor Notes:** Complex nested data flattening. Tests struct and array operations with explode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-10"
      },
      "source": [
        "## Problem 10: Time-Series Gap Filling\n",
        "\n",
        "**Requirement:** Finance team needs complete time series data with missing dates filled.\n",
        "\n",
        "**Scenario:** Fill missing dates in stock price data and forward-fill the last known prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e881c4e-5d21-4162-a7ff-47d06ab824d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+------+\n",
            "|      date|symbol| price|\n",
            "+----------+------+------+\n",
            "|2023-01-01|  AAPL| 150.0|\n",
            "|2023-01-03|  AAPL| 152.0|\n",
            "|2023-01-04|  AAPL| 151.5|\n",
            "|2023-01-06|  AAPL| 153.0|\n",
            "|2023-01-01|  GOOG|2800.0|\n",
            "|2023-01-02|  GOOG|2810.0|\n",
            "|2023-01-05|  GOOG|2820.0|\n",
            "+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "stock_prices_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0),\n",
        "    (\"2023-01-03\", \"AAPL\", 152.0),\n",
        "    (\"2023-01-04\", \"AAPL\", 151.5),\n",
        "    (\"2023-01-06\", \"AAPL\", 153.0),\n",
        "    (\"2023-01-01\", \"GOOG\", 2800.0),\n",
        "    (\"2023-01-02\", \"GOOG\", 2810.0),\n",
        "    (\"2023-01-05\", \"GOOG\", 2820.0)\n",
        "]\n",
        "\n",
        "stock_prices_df = spark.createDataFrame(stock_prices_data, [\"date\", \"symbol\", \"price\"])\n",
        "stock_prices_df = stock_prices_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "stock_prices_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca11c032-4269-4540-89bd-db1f4da41df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+------+\n",
            "|      date|symbol| price|\n",
            "+----------+------+------+\n",
            "|2023-01-01|  AAPL| 150.0|\n",
            "|2023-01-02|  AAPL| 150.0|\n",
            "|2023-01-03|  AAPL| 152.0|\n",
            "|2023-01-04|  AAPL| 151.5|\n",
            "|2023-01-05|  AAPL| 151.5|\n",
            "|2023-01-06|  AAPL| 153.0|\n",
            "|2023-01-01|  GOOG|2800.0|\n",
            "|2023-01-02|  GOOG|2810.0|\n",
            "|2023-01-03|  GOOG|2810.0|\n",
            "|2023-01-04|  GOOG|2810.0|\n",
            "|2023-01-05|  GOOG|2820.0|\n",
            "|2023-01-06|  GOOG|2820.0|\n",
            "+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0),\n",
        "    (\"2023-01-02\", \"AAPL\", 150.0),\n",
        "    (\"2023-01-03\", \"AAPL\", 152.0),\n",
        "    (\"2023-01-04\", \"AAPL\", 151.5),\n",
        "    (\"2023-01-05\", \"AAPL\", 151.5),\n",
        "    (\"2023-01-06\", \"AAPL\", 153.0),\n",
        "    (\"2023-01-01\", \"GOOG\", 2800.0),\n",
        "    (\"2023-01-02\", \"GOOG\", 2810.0),\n",
        "    (\"2023-01-03\", \"GOOG\", 2810.0),\n",
        "    (\"2023-01-04\", \"GOOG\", 2810.0),\n",
        "    (\"2023-01-05\", \"GOOG\", 2820.0),\n",
        "    (\"2023-01-06\", \"GOOG\", 2820.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d793df-83e7-4cdf-d218-d8b60a146005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|      date|\n",
            "+----------+\n",
            "|2023-01-01|\n",
            "|2023-01-02|\n",
            "|2023-01-03|\n",
            "|2023-01-04|\n",
            "|2023-01-05|\n",
            "|2023-01-06|\n",
            "+----------+\n",
            "\n",
            "+------+\n",
            "|symbol|\n",
            "+------+\n",
            "|  AAPL|\n",
            "|  GOOG|\n",
            "+------+\n",
            "\n",
            "+----------+------+\n",
            "|      date|symbol|\n",
            "+----------+------+\n",
            "|2023-01-01|  AAPL|\n",
            "|2023-01-02|  AAPL|\n",
            "|2023-01-03|  AAPL|\n",
            "|2023-01-04|  AAPL|\n",
            "|2023-01-05|  AAPL|\n",
            "|2023-01-06|  AAPL|\n",
            "|2023-01-01|  GOOG|\n",
            "|2023-01-02|  GOOG|\n",
            "|2023-01-03|  GOOG|\n",
            "|2023-01-04|  GOOG|\n",
            "|2023-01-05|  GOOG|\n",
            "|2023-01-06|  GOOG|\n",
            "+----------+------+\n",
            "\n",
            "+----------+------+------+\n",
            "|      date|symbol| price|\n",
            "+----------+------+------+\n",
            "|2023-01-01|  AAPL| 150.0|\n",
            "|2023-01-02|  AAPL| 150.0|\n",
            "|2023-01-03|  AAPL| 152.0|\n",
            "|2023-01-04|  AAPL| 151.5|\n",
            "|2023-01-05|  AAPL| 151.5|\n",
            "|2023-01-06|  AAPL| 153.0|\n",
            "|2023-01-01|  GOOG|2800.0|\n",
            "|2023-01-02|  GOOG|2810.0|\n",
            "|2023-01-03|  GOOG|2810.0|\n",
            "|2023-01-04|  GOOG|2810.0|\n",
            "|2023-01-05|  GOOG|2820.0|\n",
            "|2023-01-06|  GOOG|2820.0|\n",
            "+----------+------+------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "min_date = stock_prices_df.select(fn.expr('min(date) as minDate'), fn.expr('max(date) as maxDate')).collect()[0][0].strftime('%Y-%m-%d')\n",
        "max_date = stock_prices_df.select(fn.expr('min(date) as minDate'), fn.expr('max(date) as maxDate')).collect()[0][1].strftime('%Y-%m-%d')\n",
        "\n",
        "date_series = spark.sql(f'''select sequence(to_date('{min_date}'), to_date('{max_date}')) as dtArray''')\\\n",
        "                    .select(fn.explode(fn.col('dtArray')).alias('date')).orderBy('date')\n",
        "date_series.show()\n",
        "\n",
        "symbol_series = stock_prices_df.select('symbol').distinct().orderBy('symbol')\n",
        "\n",
        "symbol_series.show()\n",
        "\n",
        "allDateAllSymbol = date_series.crossJoin(symbol_series)\n",
        "allDateAllSymbol.show()\n",
        "\n",
        "join_on = fn.expr('''\n",
        "                  allindex.date = stock.date\n",
        "                  and\n",
        "                  allindex.symbol = stock.symbol\n",
        "                  ''')\n",
        "\n",
        "allData = \\\n",
        "        allDateAllSymbol.alias('allindex')\\\n",
        "                .join(stock_prices_df.alias('stock'),\n",
        "                      join_on,\n",
        "                      'left')\\\n",
        "                .drop(fn.col('stock.date'),fn.col('stock.symbol'))\n",
        "\n",
        "win = Window.partitionBy('symbol').orderBy('date').rowsBetween(Window.unboundedPreceding, -1)\n",
        "\n",
        "result_df = \\\n",
        "          allData\\\n",
        "              .withColumn('fillPrice', fn.last_value(fn.col('price'), ignoreNulls= True).over(win))\\\n",
        "              .withColumn('price', fn.nvl(fn.col('price'),fn.col('fillPrice')))\\\n",
        "              .drop('fillPrice')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-10"
      },
      "source": [
        "**Instructor Notes:** Time-series gap filling with last observation carried forward. Tests complex window functions and date generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-11"
      },
      "source": [
        "## Problem 11: Multi-Table Relationship Analysis\n",
        "\n",
        "**Requirement:** Business intelligence needs customer journey analysis across multiple touchpoints.\n",
        "\n",
        "**Scenario:** Join customer, orders, and payments tables to analyze complete customer journey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cf27d4-3e9f-47bd-a039-a7704f3c8a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers:\n",
            "+-----------+-------------+----------+\n",
            "|customer_id|customer_name|membership|\n",
            "+-----------+-------------+----------+\n",
            "|       C001|     John Doe|   Premium|\n",
            "|       C002|   Jane Smith|  Standard|\n",
            "|       C003|  Bob Johnson|   Premium|\n",
            "+-----------+-------------+----------+\n",
            "\n",
            "Orders:\n",
            "+--------+-----------+----------+------+\n",
            "|order_id|customer_id|order_date|amount|\n",
            "+--------+-----------+----------+------+\n",
            "|    O001|       C001|2023-01-15|1000.0|\n",
            "|    O002|       C001|2023-02-20|1500.0|\n",
            "|    O003|       C002|2023-01-10| 800.0|\n",
            "|    O004|       C003|2023-03-05|2000.0|\n",
            "+--------+-----------+----------+------+\n",
            "\n",
            "Payments:\n",
            "+----------+--------+------------+-------------+\n",
            "|payment_id|order_id|payment_date|       method|\n",
            "+----------+--------+------------+-------------+\n",
            "|      P001|    O001|  2023-01-16|  Credit Card|\n",
            "|      P002|    O002|  2023-02-21|       PayPal|\n",
            "|      P003|    O003|  2023-01-11|  Credit Card|\n",
            "|      P004|    O004|  2023-03-06|Bank Transfer|\n",
            "+----------+--------+------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "customers_multi_data = [\n",
        "    (\"C001\", \"John Doe\", \"Premium\"),\n",
        "    (\"C002\", \"Jane Smith\", \"Standard\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"Premium\")\n",
        "]\n",
        "\n",
        "orders_multi_data = [\n",
        "    (\"O001\", \"C001\", \"2023-01-15\", 1000.0),\n",
        "    (\"O002\", \"C001\", \"2023-02-20\", 1500.0),\n",
        "    (\"O003\", \"C002\", \"2023-01-10\", 800.0),\n",
        "    (\"O004\", \"C003\", \"2023-03-05\", 2000.0)\n",
        "]\n",
        "\n",
        "payments_multi_data = [\n",
        "    (\"P001\", \"O001\", \"2023-01-16\", \"Credit Card\"),\n",
        "    (\"P002\", \"O002\", \"2023-02-21\", \"PayPal\"),\n",
        "    (\"P003\", \"O003\", \"2023-01-11\", \"Credit Card\"),\n",
        "    (\"P004\", \"O004\", \"2023-03-06\", \"Bank Transfer\")\n",
        "]\n",
        "\n",
        "customers_multi_df = spark.createDataFrame(customers_multi_data, [\"customer_id\", \"customer_name\", \"membership\"])\n",
        "orders_multi_df = spark.createDataFrame(orders_multi_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])\n",
        "payments_multi_df = spark.createDataFrame(payments_multi_data, [\"payment_id\", \"order_id\", \"payment_date\", \"method\"])\n",
        "\n",
        "print(\"Customers:\")\n",
        "customers_multi_df.show()\n",
        "print(\"Orders:\")\n",
        "orders_multi_df.show()\n",
        "print(\"Payments:\")\n",
        "payments_multi_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48570f91-7822-45ec-a47e-a7c5364a0cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+--------+------+----------+--------------+\n",
            "|customer_id|customer_name|membership|order_id|amount|payment_id|payment_method|\n",
            "+-----------+-------------+----------+--------+------+----------+--------------+\n",
            "|       C001|     John Doe|   Premium|    O001|1000.0|      P001|   Credit Card|\n",
            "|       C001|     John Doe|   Premium|    O002|1500.0|      P002|        PayPal|\n",
            "|       C002|   Jane Smith|  Standard|    O003| 800.0|      P003|   Credit Card|\n",
            "|       C003|  Bob Johnson|   Premium|    O004|2000.0|      P004| Bank Transfer|\n",
            "+-----------+-------------+----------+--------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John Doe\", \"Premium\", \"O001\", 1000.0, \"P001\", \"Credit Card\"),\n",
        "    (\"C001\", \"John Doe\", \"Premium\", \"O002\", 1500.0, \"P002\", \"PayPal\"),\n",
        "    (\"C002\", \"Jane Smith\", \"Standard\", \"O003\", 800.0, \"P003\", \"Credit Card\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"Premium\", \"O004\", 2000.0, \"P004\", \"Bank Transfer\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"membership\", \"order_id\", \"amount\", \"payment_id\", \"payment_method\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3fca0b-7c8c-4bc1-fa31-9303a16a07d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------+--------+------+----------+--------------+\n",
            "|customer_id|customer_name|membership|order_id|amount|payment_id|payment_method|\n",
            "+-----------+-------------+----------+--------+------+----------+--------------+\n",
            "|       C001|     John Doe|   Premium|    O002|1500.0|      P002|        PayPal|\n",
            "|       C001|     John Doe|   Premium|    O001|1000.0|      P001|   Credit Card|\n",
            "|       C003|  Bob Johnson|   Premium|    O004|2000.0|      P004| Bank Transfer|\n",
            "|       C002|   Jane Smith|  Standard|    O003| 800.0|      P003|   Credit Card|\n",
            "+-----------+-------------+----------+--------+------+----------+--------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "join_on_1 = fn.expr('''customers.customer_id = orders.customer_id''')\n",
        "join_on_2 = fn.expr('''orders.order_id = payments.order_id''')\n",
        "\n",
        "result_df = \\\n",
        "        customers_multi_df.alias('customers')\\\n",
        "                  .join(orders_multi_df.alias('orders'),\n",
        "                        join_on_1,\n",
        "                        'inner')\\\n",
        "                  .join(payments_multi_df.alias('payments'),\n",
        "                        join_on_2,\n",
        "                        'inner')\\\n",
        "                  .drop(fn.col('orders.customer_id'), fn.col('payments.order_id'),fn.col('orders.order_date'),fn.col('payments.payment_date'))\\\n",
        "                  .withColumnRenamed('method','payment_method')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-11"
      },
      "source": [
        "**Instructor Notes:** Multiple table joins with complex relationships. Tests chaining multiple join operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-12"
      },
      "source": [
        "## Problem 12: Advanced Window Functions with Multiple Partitions\n",
        "\n",
        "**Requirement:** Sales team needs ranking of products within each category and region.\n",
        "\n",
        "**Scenario:** Calculate product rankings within each category and region based on sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2e2c8b-53ca-4c46-999b-2727d4e37367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+----------+-----+\n",
            "|   category|region|   product|sales|\n",
            "+-----------+------+----------+-----+\n",
            "|Electronics| North|    Laptop|50000|\n",
            "|Electronics| North|Smartphone|75000|\n",
            "|Electronics| North|    Tablet|30000|\n",
            "|Electronics| South|    Laptop|45000|\n",
            "|Electronics| South|Smartphone|60000|\n",
            "|Electronics| South|    Tablet|25000|\n",
            "|   Clothing| North|     Shirt|20000|\n",
            "|   Clothing| North|     Pants|30000|\n",
            "|   Clothing| South|     Shirt|25000|\n",
            "|   Clothing| South|     Pants|35000|\n",
            "+-----------+------+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "product_region_sales_data = [\n",
        "    (\"Electronics\", \"North\", \"Laptop\", 50000),\n",
        "    (\"Electronics\", \"North\", \"Smartphone\", 75000),\n",
        "    (\"Electronics\", \"North\", \"Tablet\", 30000),\n",
        "    (\"Electronics\", \"South\", \"Laptop\", 45000),\n",
        "    (\"Electronics\", \"South\", \"Smartphone\", 60000),\n",
        "    (\"Electronics\", \"South\", \"Tablet\", 25000),\n",
        "    (\"Clothing\", \"North\", \"Shirt\", 20000),\n",
        "    (\"Clothing\", \"North\", \"Pants\", 30000),\n",
        "    (\"Clothing\", \"South\", \"Shirt\", 25000),\n",
        "    (\"Clothing\", \"South\", \"Pants\", 35000)\n",
        "]\n",
        "\n",
        "product_region_sales_df = spark.createDataFrame(product_region_sales_data, [\"category\", \"region\", \"product\", \"sales\"])\n",
        "product_region_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f964f62-7320-4ce7-c0be-563b877fb361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+----------+-----+----+\n",
            "|   category|region|   product|sales|rank|\n",
            "+-----------+------+----------+-----+----+\n",
            "|Electronics| North|Smartphone|75000|   1|\n",
            "|Electronics| North|    Laptop|50000|   2|\n",
            "|Electronics| North|    Tablet|30000|   3|\n",
            "|Electronics| South|Smartphone|60000|   1|\n",
            "|Electronics| South|    Laptop|45000|   2|\n",
            "|Electronics| South|    Tablet|25000|   3|\n",
            "|   Clothing| North|     Pants|30000|   1|\n",
            "|   Clothing| North|     Shirt|20000|   2|\n",
            "|   Clothing| South|     Pants|35000|   1|\n",
            "|   Clothing| South|     Shirt|25000|   2|\n",
            "+-----------+------+----------+-----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Electronics\", \"North\", \"Smartphone\", 75000, 1),\n",
        "    (\"Electronics\", \"North\", \"Laptop\", 50000, 2),\n",
        "    (\"Electronics\", \"North\", \"Tablet\", 30000, 3),\n",
        "    (\"Electronics\", \"South\", \"Smartphone\", 60000, 1),\n",
        "    (\"Electronics\", \"South\", \"Laptop\", 45000, 2),\n",
        "    (\"Electronics\", \"South\", \"Tablet\", 25000, 3),\n",
        "    (\"Clothing\", \"North\", \"Pants\", 30000, 1),\n",
        "    (\"Clothing\", \"North\", \"Shirt\", 20000, 2),\n",
        "    (\"Clothing\", \"South\", \"Pants\", 35000, 1),\n",
        "    (\"Clothing\", \"South\", \"Shirt\", 25000, 2)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"category\", \"region\", \"product\", \"sales\", \"rank\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608febef-a5ef-4631-be86-a0bed6d09386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+----------+-----+----+\n",
            "|   category|region|   product|sales|rank|\n",
            "+-----------+------+----------+-----+----+\n",
            "|   Clothing| North|     Pants|30000|   1|\n",
            "|   Clothing| North|     Shirt|20000|   2|\n",
            "|   Clothing| South|     Pants|35000|   1|\n",
            "|   Clothing| South|     Shirt|25000|   2|\n",
            "|Electronics| North|Smartphone|75000|   1|\n",
            "|Electronics| North|    Laptop|50000|   2|\n",
            "|Electronics| North|    Tablet|30000|   3|\n",
            "|Electronics| South|Smartphone|60000|   1|\n",
            "|Electronics| South|    Laptop|45000|   2|\n",
            "|Electronics| South|    Tablet|25000|   3|\n",
            "+-----------+------+----------+-----+----+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "win = Window.partitionBy('category','region').orderBy(fn.col('sales').desc_nulls_last())\n",
        "\n",
        "result_df = \\\n",
        "      product_region_sales_df\\\n",
        "        .withColumn('rank', fn.dense_rank().over(win))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-12"
      },
      "source": [
        "**Instructor Notes:** Multi-partition window functions. Tests complex window specifications with multiple partition keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-13"
      },
      "source": [
        "## Problem 13: Data Quality Validation UDF\n",
        "\n",
        "**Requirement:** Data governance team needs comprehensive data quality checks.\n",
        "\n",
        "**Scenario:** Create UDFs to validate email format, phone numbers, and age ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae999e1-7ac5-47c3-c1fa-33621e2b2c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+-----------------+------------+---+\n",
            "|cust_id|          name|            email|       phone|age|\n",
            "+-------+--------------+-----------------+------------+---+\n",
            "|      1|      John Doe|   john@email.com|123-456-7890| 25|\n",
            "|      2|    Jane Smith|    invalid-email|987-654-3210| 35|\n",
            "|      3|   Bob Johnson|  bob@company.com|    555-1234| 17|\n",
            "|      4|   Alice Brown| alice@domain.com|111-222-3333|150|\n",
            "|      5|Charlie Wilson|charlie@email.com|444-555-6666| 45|\n",
            "+-------+--------------+-----------------+------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_validation_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\", 25),\n",
        "    (2, \"Jane Smith\", \"invalid-email\", \"987-654-3210\", 35),\n",
        "    (3, \"Bob Johnson\", \"bob@company.com\", \"555-1234\", 17),\n",
        "    (4, \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\", 150),\n",
        "    (5, \"Charlie Wilson\", \"charlie@email.com\", \"444-555-6666\", 45)\n",
        "]\n",
        "\n",
        "customer_validation_df = spark.createDataFrame(customer_validation_data, [\"cust_id\", \"name\", \"email\", \"phone\", \"age\"])\n",
        "customer_validation_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fb2940-9b24-4109-af96-599d7cd5c537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+-----------------+------------+---+------------+------------+----------+\n",
            "|cust_id|          name|            email|       phone|age|email_status|phone_status|age_status|\n",
            "+-------+--------------+-----------------+------------+---+------------+------------+----------+\n",
            "|      1|      John Doe|   john@email.com|123-456-7890| 25|       Valid|       Valid|     Valid|\n",
            "|      2|    Jane Smith|    invalid-email|987-654-3210| 35|     Invalid|       Valid|     Valid|\n",
            "|      3|   Bob Johnson|  bob@company.com|    555-1234| 17|       Valid|     Invalid|     Valid|\n",
            "|      4|   Alice Brown| alice@domain.com|111-222-3333|150|       Valid|       Valid|   Invalid|\n",
            "|      5|Charlie Wilson|charlie@email.com|444-555-6666| 45|       Valid|       Valid|     Valid|\n",
            "+-------+--------------+-----------------+------------+---+------------+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\", 25, \"Valid\", \"Valid\", \"Valid\"),\n",
        "    (2, \"Jane Smith\", \"invalid-email\", \"987-654-3210\", 35, \"Invalid\", \"Valid\", \"Valid\"),\n",
        "    (3, \"Bob Johnson\", \"bob@company.com\", \"555-1234\", 17, \"Valid\", \"Invalid\", \"Valid\"),\n",
        "    (4, \"Alice Brown\", \"alice@domain.com\", \"111-222-3333\", 150, \"Valid\", \"Valid\", \"Invalid\"),\n",
        "    (5, \"Charlie Wilson\", \"charlie@email.com\", \"444-555-6666\", 45, \"Valid\", \"Valid\", \"Valid\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"name\", \"email\", \"phone\", \"age\", \"email_status\", \"phone_status\", \"age_status\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1a5af1-0106-48f0-848c-7d30ee7540ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+-----------------+------------+---+------------+------------+----------+\n",
            "|cust_id|          name|            email|       phone|age|email_status|phone_status|age_status|\n",
            "+-------+--------------+-----------------+------------+---+------------+------------+----------+\n",
            "|      1|      John Doe|   john@email.com|123-456-7890| 25|       Valid|       Valid|     Valid|\n",
            "|      2|    Jane Smith|    invalid-email|987-654-3210| 35|     Invalid|       Valid|     Valid|\n",
            "|      3|   Bob Johnson|  bob@company.com|    555-1234| 17|       Valid|     Invalid|     Valid|\n",
            "|      4|   Alice Brown| alice@domain.com|111-222-3333|150|       Valid|       Valid|   Invalid|\n",
            "|      5|Charlie Wilson|charlie@email.com|444-555-6666| 45|       Valid|       Valid|     Valid|\n",
            "+-------+--------------+-----------------+------------+---+------------+------------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "import re\n",
        "\n",
        "def is_email_valid(email):\n",
        "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "    return 'Valid' if re.match(pattern, email) is not None else 'Invalid'\n",
        "\n",
        "def is_phone_valid(phone):\n",
        "    regex = r'^(\\+\\d{1,3}[-.\\s]?)?(\\(\\d{3}\\)|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}$'\n",
        "    return 'Valid' if re.fullmatch(regex, phone) is not None else 'Invalid'\n",
        "\n",
        "def is_age_valid(age):\n",
        "    try:\n",
        "        age_num = int(age)\n",
        "        if 0 <= age_num <= 120:\n",
        "          return 'Valid'\n",
        "        else:\n",
        "          return 'Invalid'\n",
        "    except (ValueError, TypeError):\n",
        "        return 'Invalid'\n",
        "\n",
        "is_email_valid_udf = fn.udf(is_email_valid,tp.StringType())\n",
        "is_phone_valid_udf = fn.udf(is_phone_valid,tp.StringType())\n",
        "is_age_valid_udf = fn.udf(is_age_valid,tp.StringType())\n",
        "\n",
        "result_df = \\\n",
        "      customer_validation_df\\\n",
        "        .withColumn('email_status',is_email_valid_udf(fn.col('email')))\\\n",
        "        .withColumn('phone_status',is_phone_valid_udf(fn.col('phone')))\\\n",
        "        .withColumn('age_status',is_age_valid_udf(fn.col('age')))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-13"
      },
      "source": [
        "**Instructor Notes:** Multiple UDFs for data validation. Tests regex patterns and complex validation logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-14"
      },
      "source": [
        "## Problem 14: Complex Conditional Aggregation\n",
        "\n",
        "**Requirement:** Business intelligence needs segmented revenue analysis.\n",
        "\n",
        "**Scenario:** Calculate revenue by multiple customer segments and product categories simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28487199-15d4-4761-9690-8e75ecf3e909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------+\n",
            "|membership|   category|amount|\n",
            "+----------+-----------+------+\n",
            "|   Premium|Electronics|1000.0|\n",
            "|   Premium|   Clothing| 500.0|\n",
            "|  Standard|Electronics| 800.0|\n",
            "|  Standard|   Clothing| 300.0|\n",
            "|   Premium|Electronics|1200.0|\n",
            "|  Standard|Electronics| 600.0|\n",
            "|   Premium|   Clothing| 400.0|\n",
            "|  Standard|   Clothing| 200.0|\n",
            "+----------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "segmented_sales_data = [\n",
        "    (\"Premium\", \"Electronics\", 1000.0),\n",
        "    (\"Premium\", \"Clothing\", 500.0),\n",
        "    (\"Standard\", \"Electronics\", 800.0),\n",
        "    (\"Standard\", \"Clothing\", 300.0),\n",
        "    (\"Premium\", \"Electronics\", 1200.0),\n",
        "    (\"Standard\", \"Electronics\", 600.0),\n",
        "    (\"Premium\", \"Clothing\", 400.0),\n",
        "    (\"Standard\", \"Clothing\", 200.0)\n",
        "]\n",
        "\n",
        "segmented_sales_df = spark.createDataFrame(segmented_sales_data, [\"membership\", \"category\", \"amount\"])\n",
        "segmented_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757c8187-2d50-4dbb-93e2-50ad45a0e44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+----------------+\n",
            "|   category|premium_revenue|standard_revenue|\n",
            "+-----------+---------------+----------------+\n",
            "|Electronics|         2200.0|          1400.0|\n",
            "|   Clothing|          900.0|           500.0|\n",
            "+-----------+---------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Electronics\", 2200.0, 1400.0),\n",
        "    (\"Clothing\", 900.0, 500.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"category\", \"premium_revenue\", \"standard_revenue\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e47bedc-79aa-4940-cbbf-de8ad1a7c3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+----------------+\n",
            "|   category|premium_revenue|standard_revenue|\n",
            "+-----------+---------------+----------------+\n",
            "|Electronics|         2200.0|          1400.0|\n",
            "|   Clothing|          900.0|           500.0|\n",
            "+-----------+---------------+----------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      segmented_sales_df\\\n",
        "        .withColumn('pivot_col_naming', fn.expr('''lower(membership)||'_revenue' '''))\\\n",
        "        .groupBy('category')\\\n",
        "        .pivot('pivot_col_naming')\\\n",
        "        .agg(fn.sum(fn.col('amount')))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-14"
      },
      "source": [
        "**Instructor Notes:** Complex conditional aggregation with multiple sum conditions. Tests advanced aggregation patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-15"
      },
      "source": [
        "## Problem 15: Array and Map Operations\n",
        "\n",
        "**Requirement:** Product analytics needs to analyze product feature usage patterns.\n",
        "\n",
        "**Scenario:** Process arrays and maps to analyze which features are used together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ad06de-eb4d-46bc-ce7d-fd27a1ba78be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------------+-----------------------------------------+\n",
            "|product_id|features              |usage_stats                              |\n",
            "+----------+----------------------+-----------------------------------------+\n",
            "|P001      |[search, filter, sort]|{filter -> 75, search -> 150, sort -> 50}|\n",
            "|P002      |[search, export]      |{export -> 30, search -> 200}            |\n",
            "|P003      |[filter, sort, import]|{filter -> 100, sort -> 60, import -> 20}|\n",
            "|P004      |[search, filter]      |{filter -> 90, search -> 180}            |\n",
            "+----------+----------------------+-----------------------------------------+\n",
            "\n",
            "root\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- features: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- usage_stats: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame with complex types\n",
        "from pyspark.sql.types import MapType\n",
        "\n",
        "product_features_schema = StructType([\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"features\", ArrayType(StringType()), True),\n",
        "    StructField(\"usage_stats\", MapType(StringType(), IntegerType()), True)\n",
        "])\n",
        "\n",
        "product_features_data = [\n",
        "    (\"P001\", [\"search\", \"filter\", \"sort\"], {\"search\": 150, \"filter\": 75, \"sort\": 50}),\n",
        "    (\"P002\", [\"search\", \"export\"], {\"search\": 200, \"export\": 30}),\n",
        "    (\"P003\", [\"filter\", \"sort\", \"import\"], {\"filter\": 100, \"sort\": 60, \"import\": 20}),\n",
        "    (\"P004\", [\"search\", \"filter\"], {\"search\": 180, \"filter\": 90})\n",
        "]\n",
        "\n",
        "product_features_df = spark.createDataFrame(product_features_data, product_features_schema)\n",
        "product_features_df.show(truncate=False)\n",
        "product_features_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3294a1e-1165-40db-8b49-8e0c47d37fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+-----------+\n",
            "|feature|product_count|total_usage|\n",
            "+-------+-------------+-----------+\n",
            "| search|            3|        530|\n",
            "| filter|            3|        265|\n",
            "|   sort|            2|        110|\n",
            "| export|            1|         30|\n",
            "| import|            1|         20|\n",
            "+-------+-------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"search\", 3, 530),\n",
        "    (\"filter\", 3, 265),\n",
        "    (\"sort\", 2, 110),\n",
        "    (\"export\", 1, 30),\n",
        "    (\"import\", 1, 20)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"feature\", \"product_count\", \"total_usage\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7265e700-5fac-485c-ce7a-4281f94abc68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+-----------+\n",
            "|feature|product_count|total_usage|\n",
            "+-------+-------------+-----------+\n",
            "| search|            3|        530|\n",
            "| filter|            3|        265|\n",
            "|   sort|            2|        110|\n",
            "| export|            1|         30|\n",
            "| import|            1|         20|\n",
            "+-------+-------------+-----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "    product_features_df\\\n",
        "        .select('product_id',fn.explode(fn.col('usage_stats')).alias('feature','stat'))\\\n",
        "        .groupBy('feature')\\\n",
        "        .agg(fn.count(fn.col('product_id')).alias('product_count'),\n",
        "            fn.sum(fn.col('stat')).alias('total_usage'))\\\n",
        "        .orderBy(fn.col('total_usage').desc())\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-15"
      },
      "source": [
        "**Instructor Notes:** Complex type operations with arrays and maps. Tests explode and map value extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-16"
      },
      "source": [
        "## Problem 16: Advanced Date/Time Operations\n",
        "\n",
        "**Requirement:** Operations team needs business day calculations excluding weekends/holidays.\n",
        "\n",
        "**Scenario:** Calculate business days between dates and adjust for weekends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad1098f-a1d3-4d49-9bf6-86038e3528dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+\n",
            "|task_id|start_date|  end_date|\n",
            "+-------+----------+----------+\n",
            "|      1|2023-01-02|2023-01-05|\n",
            "|      2|2023-01-06|2023-01-09|\n",
            "|      3|2023-01-09|2023-01-13|\n",
            "|      4|2023-01-13|2023-01-17|\n",
            "+-------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "business_dates_data = [\n",
        "    (1, \"2023-01-02\", \"2023-01-05\"),  # Mon to Thu (4 days, 3 business days)\n",
        "    (2, \"2023-01-06\", \"2023-01-09\"),  # Fri to Mon (4 days, 1 business day)\n",
        "    (3, \"2023-01-09\", \"2023-01-13\"),  # Mon to Fri (5 days, 5 business days)\n",
        "    (4, \"2023-01-13\", \"2023-01-17\")   # Fri to Tue (5 days, 2 business days)\n",
        "]\n",
        "\n",
        "business_dates_df = spark.createDataFrame(business_dates_data, [\"task_id\", \"start_date\", \"end_date\"])\n",
        "business_dates_df = business_dates_df.withColumn(\"start_date\", col(\"start_date\").cast(\"date\"))\\\n",
        "                                   .withColumn(\"end_date\", col(\"end_date\").cast(\"date\"))\n",
        "business_dates_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064cd012-12ae-4532-a355-b5f897245843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+-------------+\n",
            "|task_id|start_date|  end_date|business_days|\n",
            "+-------+----------+----------+-------------+\n",
            "|      1|2023-01-02|2023-01-05|            4|\n",
            "|      2|2023-01-06|2023-01-09|            2|\n",
            "|      3|2023-01-09|2023-01-13|            5|\n",
            "|      4|2023-01-13|2023-01-17|            3|\n",
            "+-------+----------+----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"2023-01-02\", \"2023-01-05\", 4),\n",
        "    (2, \"2023-01-06\", \"2023-01-09\", 2),\n",
        "    (3, \"2023-01-09\", \"2023-01-13\", 5),\n",
        "    (4, \"2023-01-13\", \"2023-01-17\", 3)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"task_id\", \"start_date\", \"end_date\", \"business_days\"])\n",
        "expected_df = expected_df.withColumn(\"start_date\", col(\"start_date\").cast(\"date\"))\\\n",
        "                       .withColumn(\"end_date\", col(\"end_date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ffeb15-49f7-4717-c31a-f9100d7b505c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+-------------+\n",
            "|task_id|start_date|  end_date|business_days|\n",
            "+-------+----------+----------+-------------+\n",
            "|      1|2023-01-02|2023-01-05|            4|\n",
            "|      2|2023-01-06|2023-01-09|            2|\n",
            "|      3|2023-01-09|2023-01-13|            5|\n",
            "|      4|2023-01-13|2023-01-17|            3|\n",
            "+-------+----------+----------+-------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      business_dates_df\\\n",
        "        .withColumn('dateArray', fn.expr(''' sequence(start_date, end_date) '''))\\\n",
        "        .withColumn('dayOfWeekArray', fn.expr(''' transform(dateArray, x->  dayofweek(x))'''))\\\n",
        "        .withColumn('businsesDaysArray', fn.expr(''' filter(dayOfWeekArray, x-> x not in (1,7))  '''))\\\n",
        "        .withColumn('business_days', fn.expr(''' size(businsesDaysArray) '''))\\\n",
        "        .drop('dateArray','dayOfWeekArray','businsesDaysArray')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-16"
      },
      "source": [
        "**Instructor Notes:** Advanced date operations with business logic. Tests date sequence generation and conditional counting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-17"
      },
      "source": [
        "## Problem 17: Hierarchical Data Processing\n",
        "\n",
        "**Requirement:** HR analytics needs organizational hierarchy reporting.\n",
        "\n",
        "**Scenario:** Process employee-manager relationships to build organizational trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4330559d-8699-4e79-d1b5-768108d95594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------+\n",
            "|emp_id|               title|manager_id|\n",
            "+------+--------------------+----------+\n",
            "|     1|                 CEO|      NULL|\n",
            "|     2|      VP Engineering|         1|\n",
            "|     3| Engineering Manager|         2|\n",
            "|     4|    Senior Developer|         3|\n",
            "|     5|    Junior Developer|         3|\n",
            "|     6|        VP Marketing|         1|\n",
            "|     7|   Marketing Manager|         6|\n",
            "|     8|Marketing Specialist|         7|\n",
            "+------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "org_hierarchy_data = [\n",
        "    (1, \"CEO\", None),\n",
        "    (2, \"VP Engineering\", 1),\n",
        "    (3, \"Engineering Manager\", 2),\n",
        "    (4, \"Senior Developer\", 3),\n",
        "    (5, \"Junior Developer\", 3),\n",
        "    (6, \"VP Marketing\", 1),\n",
        "    (7, \"Marketing Manager\", 6),\n",
        "    (8, \"Marketing Specialist\", 7)\n",
        "]\n",
        "\n",
        "org_hierarchy_df = spark.createDataFrame(org_hierarchy_data, [\"emp_id\", \"title\", \"manager_id\"])\n",
        "org_hierarchy_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aaf4f4a-f3b2-4604-a4d4-e76331400e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+---------------+\n",
            "|emp_id|               title|hierarchy_level|\n",
            "+------+--------------------+---------------+\n",
            "|     1|                 CEO|              0|\n",
            "|     2|      VP Engineering|              1|\n",
            "|     3| Engineering Manager|              2|\n",
            "|     4|    Senior Developer|              3|\n",
            "|     5|    Junior Developer|              3|\n",
            "|     6|        VP Marketing|              1|\n",
            "|     7|   Marketing Manager|              2|\n",
            "|     8|Marketing Specialist|              3|\n",
            "+------+--------------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "\n",
        "expected_data = [\n",
        "    (1, \"CEO\", 0),\n",
        "    (2, \"VP Engineering\", 1),\n",
        "    (3, \"Engineering Manager\", 2),\n",
        "    (4, \"Senior Developer\", 3),\n",
        "    (5, \"Junior Developer\", 3),\n",
        "    (6, \"VP Marketing\", 1),\n",
        "    (7, \"Marketing Manager\", 2),\n",
        "    (8, \"Marketing Specialist\", 3)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"emp_id\", \"title\", \"hierarchy_level\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56db2572-ca49-4e3f-d6f9-1653b67d470a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+---------------+\n",
            "|emp_id|               title|hierarchy_level|\n",
            "+------+--------------------+---------------+\n",
            "|     1|                 CEO|              0|\n",
            "|     2|      VP Engineering|              1|\n",
            "|     6|        VP Marketing|              1|\n",
            "|     3| Engineering Manager|              2|\n",
            "|     7|   Marketing Manager|              2|\n",
            "|     4|    Senior Developer|              3|\n",
            "|     8|Marketing Specialist|              3|\n",
            "|     5|    Junior Developer|              3|\n",
            "+------+--------------------+---------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "\n",
        "manager_df = org_hierarchy_df\\\n",
        "                .filter('manager_id is null')\\\n",
        "                .withColumn('hierarchy_level', fn.lit(0))\\\n",
        "                .select('emp_id','title','hierarchy_level')\n",
        "\n",
        "for level in range(1,11):\n",
        "\n",
        "  prev_level_df = manager_df.filter(fn.col('hierarchy_level') == (level - 1))\n",
        "\n",
        "  level_df = org_hierarchy_df.alias('emp')\\\n",
        "                             .join(prev_level_df.alias('man'),\n",
        "                                    fn.col(\"emp.manager_id\") == fn.col(\"man.emp_id\"),\n",
        "                                   'inner')\\\n",
        "                             .select('emp.emp_id','emp.title')\n",
        "\n",
        "  level_df = level_df.withColumn('hierarchy_level',fn.lit(level))\n",
        "\n",
        "  manager_df = manager_df.unionByName(level_df)\n",
        "\n",
        "manager_df.show()\n",
        "\n",
        "result_df = manager_df\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-17"
      },
      "source": [
        "**Instructor Notes:** Hierarchical data processing with iterative logic. Tests complex self-joins and level calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-18"
      },
      "source": [
        "## Problem 18: Advanced String Manipulation\n",
        "\n",
        "**Requirement:** Data engineering needs to parse and standardize address data.\n",
        "\n",
        "**Scenario:** Extract and standardize address components from unstructured text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5122d1-dbc9-4b59-8ff1-28396be33c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------------+\n",
            "|cust_id|full_address                             |\n",
            "+-------+-----------------------------------------+\n",
            "|1      |123 MAIN ST, NEW YORK, NY 10001          |\n",
            "|2      |456 oak avenue, Los Angeles, CA 90001    |\n",
            "|3      |789 Pine Rd, Suite 100, Chicago, IL 60601|\n",
            "|4      |321 ELM STREET BOSTON MA 02101           |\n",
            "|5      |555 Cedar Ln, Apt 2B, Miami, FL 33101    |\n",
            "+-------+-----------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "customer_addresses_data = [\n",
        "    (1, \"123 MAIN ST, NEW YORK, NY 10001\"),\n",
        "    (2, \"456 oak avenue, Los Angeles, CA 90001\"),\n",
        "    (3, \"789 Pine Rd, Suite 100, Chicago, IL 60601\"),\n",
        "    (4, \"321 ELM STREET BOSTON MA 02101\"),\n",
        "    (5, \"555 Cedar Ln, Apt 2B, Miami, FL 33101\")\n",
        "]\n",
        "\n",
        "customer_addresses_df = spark.createDataFrame(customer_addresses_data, [\"cust_id\", \"full_address\"])\n",
        "customer_addresses_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597f5ecc-4b50-4f2d-baae-6dd1277c1faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------+-----------+-----+-------+\n",
            "|cust_id|street               |city       |state|zipcode|\n",
            "+-------+---------------------+-----------+-----+-------+\n",
            "|1      |123 Main St          |New York   |NY   |10001  |\n",
            "|2      |456 Oak Avenue       |Los Angeles|CA   |90001  |\n",
            "|3      |789 Pine Rd Suite 100|Chicago    |IL   |60601  |\n",
            "|4      |321 Elm Street       |Boston     |MA   |02101  |\n",
            "|5      |555 Cedar Ln Apt 2b  |Miami      |FL   |33101  |\n",
            "+-------+---------------------+-----------+-----+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"123 Main St\", \"New York\", \"NY\", \"10001\"),\n",
        "    (2, \"456 Oak Avenue\", \"Los Angeles\", \"CA\", \"90001\"),\n",
        "    (3, \"789 Pine Rd Suite 100\", \"Chicago\", \"IL\", \"60601\"),\n",
        "    (4, \"321 Elm Street\", \"Boston\", \"MA\", \"02101\"),\n",
        "    (5, \"555 Cedar Ln Apt 2b\", \"Miami\", \"FL\", \"33101\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"street\", \"city\", \"state\", \"zipcode\"])\n",
        "expected_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decb5b90-0f48-4871-d938-ffa9ba635eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------+-----------+-----+-------+\n",
            "|cust_id|street               |city       |state|zipcode|\n",
            "+-------+---------------------+-----------+-----+-------+\n",
            "|1      |123 Main St          |New York   |NY   |10001  |\n",
            "|2      |456 Oak Avenue       |Los Angeles|CA   |90001  |\n",
            "|3      |789 Pine Rd Suite 100|Chicago    |IL   |60601  |\n",
            "|5      |555 Cedar Ln Apt 2b  |Miami      |FL   |33101  |\n",
            "+-------+---------------------+-----------+-----+-------+\n",
            "\n",
            "+-------+--------------+------+-----+-------+\n",
            "|cust_id|street        |city  |state|zipcode|\n",
            "+-------+--------------+------+-----+-------+\n",
            "|4      |321 Elm Street|Boston|MA   |02101  |\n",
            "+-------+--------------+------+-----+-------+\n",
            "\n",
            "+-------+---------------------+-----------+-----+-------+\n",
            "|cust_id|street               |city       |state|zipcode|\n",
            "+-------+---------------------+-----------+-----+-------+\n",
            "|1      |123 Main St          |New York   |NY   |10001  |\n",
            "|2      |456 Oak Avenue       |Los Angeles|CA   |90001  |\n",
            "|3      |789 Pine Rd Suite 100|Chicago    |IL   |60601  |\n",
            "|4      |321 Elm Street       |Boston     |MA   |02101  |\n",
            "|5      |555 Cedar Ln Apt 2b  |Miami      |FL   |33101  |\n",
            "+-------+---------------------+-----------+-----+-------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "df1 = \\\n",
        "    customer_addresses_df\\\n",
        "      .filter(''' contains(full_address,',') ''')\\\n",
        "      .withColumn('addressArray',fn.split(fn.col('full_address'),','))\\\n",
        "      .withColumn('state_zip', fn.split(fn.trim(fn.element_at(fn.col('addressArray'),-1)),' '))\\\n",
        "      .withColumn('state', fn.element_at(fn.col('state_zip'),1))\\\n",
        "      .withColumn('zipcode', fn.element_at(fn.col('state_zip'),2))\\\n",
        "      .withColumn('city', fn.trim(fn.element_at(fn.col('addressArray'),-2)))\\\n",
        "      .withColumn('street', fn.array_join(fn.slice(fn.col('addressArray'), 1, fn.size(fn.col('addressArray')) - 2),' '))\\\n",
        "      .withColumn('street',fn.expr(''' INITCAP(REPLACE(TRIM(street),'  ',' ')) '''))\\\n",
        "      .withColumn('city', fn.expr(''' INITCAP(REPLACE(TRIM(city),'  ',' ')) '''))\\\n",
        "      .withColumn('state', fn.expr(''' UPPER(REPLACE(TRIM(state),'  ',' ')) '''))\\\n",
        "      .withColumn('zipcode', fn.expr(''' TRIM(zipcode) '''))\\\n",
        "      .drop('full_address','addressArray','state_zip')\\\n",
        "      .select('cust_id','street','city','state','zipcode')\n",
        "\n",
        "df1.show(truncate = False)\n",
        "\n",
        "df2 = customer_addresses_df\\\n",
        "      .filter(''' not contains(full_address,',') ''')\\\n",
        "      .withColumn('addressArray',fn.split(fn.col('full_address'),' '))\\\n",
        "      .withColumn('zipcode', fn.element_at(fn.col('addressArray'),-1))\\\n",
        "      .withColumn('state', fn.element_at(fn.col('addressArray'),-2))\\\n",
        "      .withColumn('city', fn.element_at(fn.col('addressArray'),-3))\\\n",
        "      .withColumn('street', fn.array_join(fn.slice(fn.col('addressArray'), 1, fn.size(fn.col('addressArray')) - 3),' '))\\\n",
        "      .withColumn('street',fn.expr(''' INITCAP(REPLACE(TRIM(street),'  ',' ')) '''))\\\n",
        "      .withColumn('city', fn.expr(''' INITCAP(REPLACE(TRIM(city),'  ',' ')) '''))\\\n",
        "      .withColumn('state', fn.expr(''' UPPER(REPLACE(TRIM(state),'  ',' ')) '''))\\\n",
        "      .withColumn('zipcode', fn.expr(''' TRIM(zipcode) '''))\\\n",
        "      .drop('full_address','addressArray')\\\n",
        "      .select('cust_id','street','city','state','zipcode')\\\n",
        "\n",
        "df2.show(truncate = False)\n",
        "\n",
        "result_df = df1\\\n",
        "            .unionByName(df2)\\\n",
        "            .orderBy('cust_id')\n",
        "\n",
        "result_df.show(truncate = False)\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-18"
      },
      "source": [
        "**Instructor Notes:** Complex string parsing with regex and case normalization. Tests advanced string manipulation patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-19"
      },
      "source": [
        "## Problem 19: Multi-Conditional Window Functions\n",
        "\n",
        "**Requirement:** Financial analytics needs moving averages with different conditions.\n",
        "\n",
        "**Scenario:** Calculate different types of moving averages (simple, exponential) for stock prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1701005-46c6-41ad-ffe2-f60989f60dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+\n",
            "|      date|symbol|price|\n",
            "+----------+------+-----+\n",
            "|2023-01-01|  AAPL|150.0|\n",
            "|2023-01-02|  AAPL|152.0|\n",
            "|2023-01-03|  AAPL|151.5|\n",
            "|2023-01-04|  AAPL|153.0|\n",
            "|2023-01-05|  AAPL|154.5|\n",
            "|2023-01-06|  AAPL|153.5|\n",
            "|2023-01-07|  AAPL|155.0|\n",
            "+----------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "stock_ma_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0),\n",
        "    (\"2023-01-02\", \"AAPL\", 152.0),\n",
        "    (\"2023-01-03\", \"AAPL\", 151.5),\n",
        "    (\"2023-01-04\", \"AAPL\", 153.0),\n",
        "    (\"2023-01-05\", \"AAPL\", 154.5),\n",
        "    (\"2023-01-06\", \"AAPL\", 153.5),\n",
        "    (\"2023-01-07\", \"AAPL\", 155.0)\n",
        "]\n",
        "\n",
        "stock_ma_df = spark.createDataFrame(stock_ma_data, [\"date\", \"symbol\", \"price\"])\n",
        "stock_ma_df = stock_ma_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "stock_ma_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedaed48-843b-43ea-e9e6-7ffeeeae40a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+------+\n",
            "|      date|symbol|price|sma_3d|\n",
            "+----------+------+-----+------+\n",
            "|2023-01-01|  AAPL|150.0|  NULL|\n",
            "|2023-01-02|  AAPL|152.0|  NULL|\n",
            "|2023-01-03|  AAPL|151.5|151.17|\n",
            "|2023-01-04|  AAPL|153.0|152.17|\n",
            "|2023-01-05|  AAPL|154.5| 153.0|\n",
            "|2023-01-06|  AAPL|153.5|153.67|\n",
            "|2023-01-07|  AAPL|155.0|154.33|\n",
            "+----------+------+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", \"AAPL\", 150.0, None),\n",
        "    (\"2023-01-02\", \"AAPL\", 152.0, None),\n",
        "    (\"2023-01-03\", \"AAPL\", 151.5, 151.17),\n",
        "    (\"2023-01-04\", \"AAPL\", 153.0, 152.17),\n",
        "    (\"2023-01-05\", \"AAPL\", 154.5, 153.0),\n",
        "    (\"2023-01-06\", \"AAPL\", 153.5, 153.67),\n",
        "    (\"2023-01-07\", \"AAPL\", 155.0, 154.33)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"symbol\", \"price\", \"sma_3d\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63454c40-80e0-4ee9-b98c-93cc6cd3c06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+------+\n",
            "|      date|symbol|price|sma_3d|\n",
            "+----------+------+-----+------+\n",
            "|2023-01-01|  AAPL|150.0|  NULL|\n",
            "|2023-01-02|  AAPL|152.0|  NULL|\n",
            "|2023-01-03|  AAPL|151.5|151.17|\n",
            "|2023-01-04|  AAPL|153.0|152.17|\n",
            "|2023-01-05|  AAPL|154.5| 153.0|\n",
            "|2023-01-06|  AAPL|153.5|153.67|\n",
            "|2023-01-07|  AAPL|155.0|154.33|\n",
            "+----------+------+-----+------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "win = Window.partitionBy('symbol').orderBy(fn.col('date').asc()).rowsBetween(-2,0)\n",
        "\n",
        "winRowNum = Window.partitionBy('symbol').orderBy(fn.col('date').asc())\n",
        "\n",
        "result_df = \\\n",
        "      stock_ma_df\\\n",
        "          .withColumn('sma_3d', fn.avg(fn.col('price')).over(win))\\\n",
        "          .withColumn('sma_3d', fn.expr('ROUND(CAST(sma_3d as DOUBLE),2)'))\\\n",
        "          .withColumn('rwNum', fn.row_number().over(winRowNum))\\\n",
        "          .withColumn('sma_3d', fn.expr('case when rwNum < 3 then null else sma_3d end'))\\\n",
        "          .drop('rwNum')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-19"
      },
      "source": [
        "**Instructor Notes:** Complex window functions with multiple moving averages. Tests financial calculations and window bounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-20"
      },
      "source": [
        "## Problem 20: Data Skew Handling Strategy\n",
        "\n",
        "**Requirement:** Performance optimization for skewed customer order data.\n",
        "\n",
        "**Scenario:** Handle data skew in customer orders by implementing salting technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79359a6-8ac7-49f6-9d69-0b11cfc637f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+\n",
            "|order_id|customer_id|amount|\n",
            "+--------+-----------+------+\n",
            "|       1|       C001| 100.0|\n",
            "|       2|       C001| 150.0|\n",
            "|       3|       C001| 200.0|\n",
            "|       4|       C001| 175.0|\n",
            "|       5|       C001| 125.0|\n",
            "|       6|       C002| 300.0|\n",
            "|       7|       C003| 250.0|\n",
            "|       8|       C004| 400.0|\n",
            "|       9|       C005| 350.0|\n",
            "+--------+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame (skewed data - one customer has most orders)\n",
        "skewed_orders_data = [\n",
        "    (1, \"C001\", 100.0),\n",
        "    (2, \"C001\", 150.0),\n",
        "    (3, \"C001\", 200.0),\n",
        "    (4, \"C001\", 175.0),\n",
        "    (5, \"C001\", 125.0),\n",
        "    (6, \"C002\", 300.0),\n",
        "    (7, \"C003\", 250.0),\n",
        "    (8, \"C004\", 400.0),\n",
        "    (9, \"C005\", 350.0)\n",
        "]\n",
        "\n",
        "skewed_orders_df = spark.createDataFrame(skewed_orders_data, [\"order_id\", \"customer_id\", \"amount\"])\n",
        "skewed_orders_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79df817-821b-4ebf-f9f9-5ba51fad8d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|customer_id|total_amount|\n",
            "+-----------+------------+\n",
            "|       C001|       750.0|\n",
            "|       C002|       300.0|\n",
            "|       C003|       250.0|\n",
            "|       C004|       400.0|\n",
            "|       C005|       350.0|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", 750.0),\n",
        "    (\"C002\", 300.0),\n",
        "    (\"C003\", 250.0),\n",
        "    (\"C004\", 400.0),\n",
        "    (\"C005\", 350.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"total_amount\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f280649-9b11-4e3c-8c21-960fad9d3936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+------------------+\n",
            "|order_id|customer_id|amount|salted_customer_id|\n",
            "+--------+-----------+------+------------------+\n",
            "|       1|       C001| 100.0|            C001_1|\n",
            "|       2|       C001| 150.0|            C001_1|\n",
            "|       3|       C001| 200.0|            C001_0|\n",
            "|       4|       C001| 175.0|            C001_2|\n",
            "|       5|       C001| 125.0|            C001_2|\n",
            "|       6|       C002| 300.0|              C002|\n",
            "|       7|       C003| 250.0|              C003|\n",
            "|       8|       C004| 400.0|              C004|\n",
            "|       9|       C005| 350.0|              C005|\n",
            "+--------+-----------+------+------------------+\n",
            "\n",
            "+-----------+------------+\n",
            "|customer_id|total_amount|\n",
            "+-----------+------------+\n",
            "|       C003|       250.0|\n",
            "|       C004|       400.0|\n",
            "|       C005|       350.0|\n",
            "|       C001|       750.0|\n",
            "|       C002|       300.0|\n",
            "+-----------+------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "# Lets impliment the key salting here\n",
        "\n",
        "salted_dataframe = \\\n",
        "        skewed_orders_df\\\n",
        "          .withColumn('salted_customer_id',\n",
        "                          fn.expr(''' case when customer_id = 'C001'\n",
        "                                      then customer_id ||'_'|| abs(hash(order_id) % 3)\n",
        "                                      else customer_id end '''))\n",
        "salted_dataframe.show()\n",
        "\n",
        "result_df = \\\n",
        "        salted_dataframe\\\n",
        "          .groupBy('salted_customer_id')\\\n",
        "          .agg(fn.expr('sum(amount) as total_amount'))\\\n",
        "          .withColumn('customer_id', fn.expr('''split(salted_customer_id,'_')[0]'''))\\\n",
        "          .groupBy('customer_id')\\\n",
        "          .agg(fn.expr('sum(total_amount) as total_amount'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-20"
      },
      "source": [
        "**Instructor Notes:** Data skew handling with salting technique. Tests performance optimization strategies for skewed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-21"
      },
      "source": [
        "## Problem 21: Complex Filter with Multiple Joins\n",
        "\n",
        "**Requirement:** Customer service needs to identify high-value customers with recent issues.\n",
        "\n",
        "**Scenario:** Find customers with high lifetime value who have open support tickets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6085b372-3b84-42d8-8147-ee996502021c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers:\n",
            "+-----------+-------------+--------------+\n",
            "|customer_id|customer_name|lifetime_value|\n",
            "+-----------+-------------+--------------+\n",
            "|       C001|     John Doe|        5000.0|\n",
            "|       C002|   Jane Smith|        3000.0|\n",
            "|       C003|  Bob Johnson|        7500.0|\n",
            "|       C004|  Alice Brown|        2000.0|\n",
            "+-----------+-------------+--------------+\n",
            "\n",
            "Support Tickets:\n",
            "+---------+-----------+------+------------+\n",
            "|ticket_id|customer_id|status|created_date|\n",
            "+---------+-----------+------+------------+\n",
            "|     T001|       C001|  Open|  2023-01-15|\n",
            "|     T002|       C002|Closed|  2023-01-10|\n",
            "|     T003|       C003|  Open|  2023-01-20|\n",
            "|     T004|       C001|  Open|  2023-01-18|\n",
            "+---------+-----------+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "customers_high_value_data = [\n",
        "    (\"C001\", \"John Doe\", 5000.0),\n",
        "    (\"C002\", \"Jane Smith\", 3000.0),\n",
        "    (\"C003\", \"Bob Johnson\", 7500.0),\n",
        "    (\"C004\", \"Alice Brown\", 2000.0)\n",
        "]\n",
        "\n",
        "support_tickets_complex_data = [\n",
        "    (\"T001\", \"C001\", \"Open\", \"2023-01-15\"),\n",
        "    (\"T002\", \"C002\", \"Closed\", \"2023-01-10\"),\n",
        "    (\"T003\", \"C003\", \"Open\", \"2023-01-20\"),\n",
        "    (\"T004\", \"C001\", \"Open\", \"2023-01-18\")\n",
        "]\n",
        "\n",
        "customers_high_value_df = spark.createDataFrame(customers_high_value_data, [\"customer_id\", \"customer_name\", \"lifetime_value\"])\n",
        "support_tickets_complex_df = spark.createDataFrame(support_tickets_complex_data, [\"ticket_id\", \"customer_id\", \"status\", \"created_date\"])\n",
        "\n",
        "print(\"Customers:\")\n",
        "customers_high_value_df.show()\n",
        "print(\"Support Tickets:\")\n",
        "support_tickets_complex_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b849bb54-ecb2-4994-ecc5-55059a028f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------+---------+------+\n",
            "|customer_id|customer_name|lifetime_value|ticket_id|status|\n",
            "+-----------+-------------+--------------+---------+------+\n",
            "|       C001|     John Doe|        5000.0|     T001|  Open|\n",
            "|       C001|     John Doe|        5000.0|     T004|  Open|\n",
            "|       C003|  Bob Johnson|        7500.0|     T003|  Open|\n",
            "+-----------+-------------+--------------+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John Doe\", 5000.0, \"T001\", \"Open\"),\n",
        "    (\"C001\", \"John Doe\", 5000.0, \"T004\", \"Open\"),\n",
        "    (\"C003\", \"Bob Johnson\", 7500.0, \"T003\", \"Open\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"lifetime_value\", \"ticket_id\", \"status\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af540e7-0ffe-494d-c8bd-9e5b82f86200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------+---------+------+\n",
            "|customer_id|customer_name|lifetime_value|ticket_id|status|\n",
            "+-----------+-------------+--------------+---------+------+\n",
            "|       C001|     John Doe|        5000.0|     T001|  Open|\n",
            "|       C001|     John Doe|        5000.0|     T004|  Open|\n",
            "|       C003|  Bob Johnson|        7500.0|     T003|  Open|\n",
            "+-----------+-------------+--------------+---------+------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "join_on = fn.expr(''' cust.customer_id = tickets.customer_id ''')\n",
        "\n",
        "result_df = \\\n",
        "      customers_high_value_df.alias('cust')\\\n",
        "        .join(support_tickets_complex_df.alias('tickets'),\n",
        "              join_on,\n",
        "              'inner')\\\n",
        "        .where(''' tickets.status = 'Open' ''')\\\n",
        "        .select('cust.customer_id','cust.customer_name',\n",
        "                'cust.lifetime_value','tickets.ticket_id','tickets.status')\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-21"
      },
      "source": [
        "**Instructor Notes:** Complex filtering with multiple join conditions. Tests business logic implementation with joins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-22"
      },
      "source": [
        "## Problem 22: Advanced Grouping with Multiple Aggregates\n",
        "\n",
        "**Requirement:** Sales analytics needs comprehensive product performance metrics.\n",
        "\n",
        "**Scenario:** Calculate multiple statistics (count, sum, avg, stddev) for products across regions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c03e68b-9f36-4b36-ece3-d8eafdd71819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+-------+-----+\n",
            "|   category|region|product|sales|\n",
            "+-----------+------+-------+-----+\n",
            "|Electronics| North| Laptop|50000|\n",
            "|Electronics| North| Laptop|55000|\n",
            "|Electronics| South| Laptop|45000|\n",
            "|Electronics| South| Laptop|48000|\n",
            "|Electronics| North| Tablet|30000|\n",
            "|Electronics| South| Tablet|25000|\n",
            "|   Clothing| North|  Shirt|20000|\n",
            "|   Clothing| South|  Shirt|22000|\n",
            "+-----------+------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "product_performance_data = [\n",
        "    (\"Electronics\", \"North\", \"Laptop\", 50000),\n",
        "    (\"Electronics\", \"North\", \"Laptop\", 55000),\n",
        "    (\"Electronics\", \"South\", \"Laptop\", 45000),\n",
        "    (\"Electronics\", \"South\", \"Laptop\", 48000),\n",
        "    (\"Electronics\", \"North\", \"Tablet\", 30000),\n",
        "    (\"Electronics\", \"South\", \"Tablet\", 25000),\n",
        "    (\"Clothing\", \"North\", \"Shirt\", 20000),\n",
        "    (\"Clothing\", \"South\", \"Shirt\", 22000)\n",
        "]\n",
        "\n",
        "product_performance_df = spark.createDataFrame(product_performance_data, [\"category\", \"region\", \"product\", \"sales\"])\n",
        "product_performance_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26a1b8b-a896-42b9-9070-179fb16262c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-----------------+-----------+---------+---------+\n",
            "|   category|product|transaction_count|total_sales|avg_sales|std_sales|\n",
            "+-----------+-------+-----------------+-----------+---------+---------+\n",
            "|Electronics| Laptop|                4|     198000|  49500.0|  4203.17|\n",
            "|Electronics| Tablet|                2|      55000|  27500.0|  3535.53|\n",
            "|   Clothing|  Shirt|                2|      42000|  21000.0|  1414.21|\n",
            "+-----------+-------+-----------------+-----------+---------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Electronics\", \"Laptop\", 4, 198000, 49500.0, 4203.17),\n",
        "    (\"Electronics\", \"Tablet\", 2, 55000, 27500.0, 3535.53),\n",
        "    (\"Clothing\", \"Shirt\", 2, 42000, 21000.0, 1414.21)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"category\", \"product\", \"transaction_count\", \"total_sales\", \"avg_sales\", \"std_sales\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba09008f-9487-4927-ea20-24982f114df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-----------------+-----------+---------+---------+\n",
            "|   category|product|transaction_count|total_sales|avg_sales|std_sales|\n",
            "+-----------+-------+-----------------+-----------+---------+---------+\n",
            "|Electronics| Laptop|                4|     198000|  49500.0|  4203.17|\n",
            "|Electronics| Tablet|                2|      55000|  27500.0|  3535.53|\n",
            "|   Clothing|  Shirt|                2|      42000|  21000.0|  1414.21|\n",
            "+-----------+-------+-----------------+-----------+---------+---------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = \\\n",
        "      product_performance_df\\\n",
        "        .groupBy('category','product')\\\n",
        "        .agg(fn.expr('count(1) as transaction_count'),\n",
        "              fn.expr('sum(sales) as total_sales'),\n",
        "              fn.expr('round(avg(sales),1) as avg_sales'),\n",
        "              fn.expr('round(stddev_samp(sales),2) as std_sales'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-22"
      },
      "source": [
        "**Instructor Notes:** Multi-level aggregation with statistical functions. Tests complex grouping and multiple aggregate functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-23"
      },
      "source": [
        "## Problem 23: Data Enrichment with External Reference\n",
        "\n",
        "**Requirement:** Marketing needs customer data enriched with geographic information.\n",
        "\n",
        "**Scenario:** Join customer data with postal code reference table to add city/state information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14fb2c8-cf78-4953-bec8-536051bab273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers:\n",
            "+-----------+-------------+-----------+\n",
            "|customer_id|customer_name|postal_code|\n",
            "+-----------+-------------+-----------+\n",
            "|       C001|     John Doe|      10001|\n",
            "|       C002|   Jane Smith|      90001|\n",
            "|       C003|  Bob Johnson|      60601|\n",
            "|       C004|  Alice Brown|      02101|\n",
            "+-----------+-------------+-----------+\n",
            "\n",
            "Postal Codes:\n",
            "+-----------+-----------+-----+\n",
            "|postal_code|       city|state|\n",
            "+-----------+-----------+-----+\n",
            "|      10001|   New York|   NY|\n",
            "|      90001|Los Angeles|   CA|\n",
            "|      60601|    Chicago|   IL|\n",
            "|      02101|     Boston|   MA|\n",
            "|      33101|      Miami|   FL|\n",
            "+-----------+-----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "customers_geo_data = [\n",
        "    (\"C001\", \"John Doe\", \"10001\"),\n",
        "    (\"C002\", \"Jane Smith\", \"90001\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"60601\"),\n",
        "    (\"C004\", \"Alice Brown\", \"02101\")\n",
        "]\n",
        "\n",
        "postal_codes_data = [\n",
        "    (\"10001\", \"New York\", \"NY\"),\n",
        "    (\"90001\", \"Los Angeles\", \"CA\"),\n",
        "    (\"60601\", \"Chicago\", \"IL\"),\n",
        "    (\"02101\", \"Boston\", \"MA\"),\n",
        "    (\"33101\", \"Miami\", \"FL\")\n",
        "]\n",
        "\n",
        "customers_geo_df = spark.createDataFrame(customers_geo_data, [\"customer_id\", \"customer_name\", \"postal_code\"])\n",
        "postal_codes_df = spark.createDataFrame(postal_codes_data, [\"postal_code\", \"city\", \"state\"])\n",
        "\n",
        "print(\"Customers:\")\n",
        "customers_geo_df.show()\n",
        "print(\"Postal Codes:\")\n",
        "postal_codes_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c855d13f-f141-40ac-9f83-60bef210b22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------+-----+\n",
            "|customer_id|customer_name|postal_code|       city|state|\n",
            "+-----------+-------------+-----------+-----------+-----+\n",
            "|       C001|     John Doe|      10001|   New York|   NY|\n",
            "|       C002|   Jane Smith|      90001|Los Angeles|   CA|\n",
            "|       C003|  Bob Johnson|      60601|    Chicago|   IL|\n",
            "|       C004|  Alice Brown|      02101|     Boston|   MA|\n",
            "+-----------+-------------+-----------+-----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John Doe\", \"10001\", \"New York\", \"NY\"),\n",
        "    (\"C002\", \"Jane Smith\", \"90001\", \"Los Angeles\", \"CA\"),\n",
        "    (\"C003\", \"Bob Johnson\", \"60601\", \"Chicago\", \"IL\"),\n",
        "    (\"C004\", \"Alice Brown\", \"02101\", \"Boston\", \"MA\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"postal_code\", \"city\", \"state\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546dba41-fab3-407b-f3f1-8b6fa4950f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------+-----+\n",
            "|customer_id|customer_name|postal_code|       city|state|\n",
            "+-----------+-------------+-----------+-----------+-----+\n",
            "|       C004|  Alice Brown|      02101|     Boston|   MA|\n",
            "|       C001|     John Doe|      10001|   New York|   NY|\n",
            "|       C003|  Bob Johnson|      60601|    Chicago|   IL|\n",
            "|       C002|   Jane Smith|      90001|Los Angeles|   CA|\n",
            "+-----------+-------------+-----------+-----------+-----+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "join_on = fn.expr(''' cust.postal_code =  post.postal_code ''')\n",
        "\n",
        "result_df = \\\n",
        "      customers_geo_df.alias('cust')\\\n",
        "      .join(postal_codes_df.alias('post'),\n",
        "            join_on,\n",
        "            'inner')\\\n",
        "      .drop(fn.col('post.postal_code'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-23"
      },
      "source": [
        "**Instructor Notes:** Data enrichment with reference table join. Tests lookup operations and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-24"
      },
      "source": [
        "## Problem 24: Conditional Window Functions\n",
        "\n",
        "**Requirement:** Analytics needs to calculate conditional running totals.\n",
        "\n",
        "**Scenario:** Calculate running total of sales, but reset when category changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2abffc0-e464-4e5b-9a68-2b22b6ffd54b"
      },
      "source": [
        "# Source DataFrame\n",
        "\n",
        "category_sales_data = [\n",
        "    (\"2023-01-01\", \"Electronics\", 1000.0),\n",
        "    (\"2023-01-02\", \"Electronics\", 1500.0),\n",
        "    (\"2023-01-03\", \"Clothing\", 800.0),\n",
        "    (\"2023-01-04\", \"Clothing\", 1200.0),\n",
        "    (\"2023-01-05\", \"Electronics\", 2000.0),\n",
        "    (\"2023-01-06\", \"Electronics\", 1800.0)\n",
        "]\n",
        "\n",
        "category_sales_df = spark.createDataFrame(category_sales_data, [\"date\", \"category\", \"sales\"])\n",
        "category_sales_df = category_sales_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "category_sales_df.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------+\n",
            "|      date|   category| sales|\n",
            "+----------+-----------+------+\n",
            "|2023-01-01|Electronics|1000.0|\n",
            "|2023-01-02|Electronics|1500.0|\n",
            "|2023-01-03|   Clothing| 800.0|\n",
            "|2023-01-04|   Clothing|1200.0|\n",
            "|2023-01-05|Electronics|2000.0|\n",
            "|2023-01-06|Electronics|1800.0|\n",
            "+----------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b5880d-dcdd-4507-a1e6-2956a51e8825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------+----------------------+\n",
            "|      date|   category| sales|category_running_total|\n",
            "+----------+-----------+------+----------------------+\n",
            "|2023-01-01|Electronics|1000.0|                1000.0|\n",
            "|2023-01-02|Electronics|1500.0|                2500.0|\n",
            "|2023-01-03|   Clothing| 800.0|                 800.0|\n",
            "|2023-01-04|   Clothing|1200.0|                2000.0|\n",
            "|2023-01-05|Electronics|2000.0|                4500.0|\n",
            "|2023-01-06|Electronics|1800.0|                6300.0|\n",
            "+----------+-----------+------+----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output - CORRECTED\n",
        "\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", \"Electronics\", 1000.0, 1000.0),\n",
        "    (\"2023-01-02\", \"Electronics\", 1500.0, 2500.0),  # 1000 + 1500\n",
        "    (\"2023-01-03\", \"Clothing\", 800.0, 800.0),       # RESET for new category\n",
        "    (\"2023-01-04\", \"Clothing\", 1200.0, 2000.0),     # 800 + 1200\n",
        "    (\"2023-01-05\", \"Electronics\", 2000.0, 4500.0),  # 2500 + 2000 (continues from previous Electronics)\n",
        "    (\"2023-01-06\", \"Electronics\", 1800.0, 6300.0)   # 4500 + 1800\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"category\", \"sales\", \"category_running_total\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03c5098-4a21-47d6-ee4f-7249ed52ea6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------+----------------------+\n",
            "|      date|   category| sales|category_running_total|\n",
            "+----------+-----------+------+----------------------+\n",
            "|2023-01-03|   Clothing| 800.0|                 800.0|\n",
            "|2023-01-04|   Clothing|1200.0|                2000.0|\n",
            "|2023-01-01|Electronics|1000.0|                1000.0|\n",
            "|2023-01-02|Electronics|1500.0|                2500.0|\n",
            "|2023-01-05|Electronics|2000.0|                4500.0|\n",
            "|2023-01-06|Electronics|1800.0|                6300.0|\n",
            "+----------+-----------+------+----------------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "win = Window.partitionBy('category').orderBy(fn.col('date').asc_nulls_last()).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "result_df = category_sales_df\\\n",
        "      .withColumn('category_running_total', fn.sum(fn.col('sales')).over(win))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-24"
      },
      "source": [
        "**Instructor Notes:** Conditional window functions with partition reset. Tests complex window specifications and ordering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-25"
      },
      "source": [
        "## Problem 25: Multi-Column Deduplication\n",
        "\n",
        "**Requirement:** Data quality needs advanced duplicate detection with fuzzy matching.\n",
        "\n",
        "**Scenario:** Identify potential duplicates based on name similarity and other attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9c70df-0963-42ce-d46f-c86009c081cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+--------------------+------------+\n",
            "|cust_id|          name|               email|       phone|\n",
            "+-------+--------------+--------------------+------------+\n",
            "|      1|      John Doe|      john@email.com|123-456-7890|\n",
            "|      2|       Jon Doe|  john.doe@email.com|123-456-7890|\n",
            "|      3|    Jane Smith|      jane@email.com|987-654-3210|\n",
            "|      4|   Jane Smithe|jane.smith@email.com|987-654-3210|\n",
            "|      5|   Bob Johnson|       bob@email.com|111-222-3333|\n",
            "|      6|Robert Johnson|bob.johnson@email...|111-222-3333|\n",
            "+-------+--------------+--------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "fuzzy_duplicates_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\"),\n",
        "    (2, \"Jon Doe\", \"john.doe@email.com\", \"123-456-7890\"),\n",
        "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\"),\n",
        "    (4, \"Jane Smithe\", \"jane.smith@email.com\", \"987-654-3210\"),\n",
        "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\"),\n",
        "    (6, \"Robert Johnson\", \"bob.johnson@email.com\", \"111-222-3333\")\n",
        "]\n",
        "\n",
        "fuzzy_duplicates_df = spark.createDataFrame(fuzzy_duplicates_data, [\"cust_id\", \"name\", \"email\", \"phone\"])\n",
        "fuzzy_duplicates_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61cea12f-e7e2-45f8-e0d1-0b257b0d2136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+--------------------+------------+---------------+\n",
            "|cust_id|          name|               email|       phone|duplicate_group|\n",
            "+-------+--------------+--------------------+------------+---------------+\n",
            "|      1|      John Doe|      john@email.com|123-456-7890|              1|\n",
            "|      2|       Jon Doe|  john.doe@email.com|123-456-7890|              1|\n",
            "|      3|    Jane Smith|      jane@email.com|987-654-3210|              2|\n",
            "|      4|   Jane Smithe|jane.smith@email.com|987-654-3210|              2|\n",
            "|      5|   Bob Johnson|       bob@email.com|111-222-3333|              3|\n",
            "|      6|Robert Johnson|bob.johnson@email...|111-222-3333|              3|\n",
            "+-------+--------------+--------------------+------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (1, \"John Doe\", \"john@email.com\", \"123-456-7890\", 1),\n",
        "    (2, \"Jon Doe\", \"john.doe@email.com\", \"123-456-7890\", 1),\n",
        "    (3, \"Jane Smith\", \"jane@email.com\", \"987-654-3210\", 2),\n",
        "    (4, \"Jane Smithe\", \"jane.smith@email.com\", \"987-654-3210\", 2),\n",
        "    (5, \"Bob Johnson\", \"bob@email.com\", \"111-222-3333\", 3),\n",
        "    (6, \"Robert Johnson\", \"bob.johnson@email.com\", \"111-222-3333\", 3)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"cust_id\", \"name\", \"email\", \"phone\", \"duplicate_group\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c42206-c342-4c6f-fb84-1128699c2a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+---------------------+------------+---------------+\n",
            "|cust_id|name          |email                |phone       |duplicate_group|\n",
            "+-------+--------------+---------------------+------------+---------------+\n",
            "|1      |John Doe      |john@email.com       |123-456-7890|1              |\n",
            "|2      |Jon Doe       |john.doe@email.com   |123-456-7890|1              |\n",
            "|3      |Jane Smith    |jane@email.com       |987-654-3210|2              |\n",
            "|4      |Jane Smithe   |jane.smith@email.com |987-654-3210|2              |\n",
            "|5      |Bob Johnson   |bob@email.com        |111-222-3333|3              |\n",
            "|6      |Robert Johnson|bob.johnson@email.com|111-222-3333|3              |\n",
            "+-------+--------------+---------------------+------------+---------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "win = Window.orderBy('phone_clean','email_soundex')\n",
        "\n",
        "result_df = \\\n",
        "      fuzzy_duplicates_df\\\n",
        "        .withColumn('name_soundex', fn.soundex(fn.col('name')))\\\n",
        "        .withColumn('email_part_1', fn.expr(''' trim(cast(split(email,'@')[0] as string)) '''))\\\n",
        "        .withColumn('email_soundex', fn.expr(''' soundex(split(email_part_1, '\\\\\\\\.')[0]) '''))\\\n",
        "        .withColumn('phone_clean', fn.expr(''' abs(hash(trim(replace(replace(phone,'+',''),'-','')))) '''))\\\n",
        "        .withColumn('duplicate_group', dense_rank().over(win))\\\n",
        "        .drop('email_part_1','email_soundex','phone_clean','name_soundex')\n",
        "\n",
        "result_df.show(truncate = False)\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-25"
      },
      "source": [
        "**Instructor Notes:** Advanced deduplication with grouping logic. Tests complex duplicate identification strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-26"
      },
      "source": [
        "## Problem 26: Complex Data Type Transformations\n",
        "\n",
        "**Requirement:** Data engineering needs to transform nested JSON structures.\n",
        "\n",
        "**Scenario:** Convert array of structs to map and vice versa for different processing needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b90fc06-8e05-430c-d225-2cea7eb56554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------+\n",
            "|user_id|preferences                                           |\n",
            "+-------+------------------------------------------------------+\n",
            "|U001   |[{theme, dark}, {language, en}, {notifications, on}]  |\n",
            "|U002   |[{theme, light}, {language, es}, {notifications, off}]|\n",
            "|U003   |[{theme, dark}, {language, fr}, {notifications, on}]  |\n",
            "+-------+------------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- preferences: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- key: string (nullable = true)\n",
            " |    |    |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "user_preferences_data = [\n",
        "    (\"U001\", [(\"theme\", \"dark\"), (\"language\", \"en\"), (\"notifications\", \"on\")]),\n",
        "    (\"U002\", [(\"theme\", \"light\"), (\"language\", \"es\"), (\"notifications\", \"off\")]),\n",
        "    (\"U003\", [(\"theme\", \"dark\"), (\"language\", \"fr\"), (\"notifications\", \"on\")])\n",
        "]\n",
        "\n",
        "user_preferences_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"preferences\", ArrayType(StructType([\n",
        "        StructField(\"key\", StringType(), True),\n",
        "        StructField(\"value\", StringType(), True)\n",
        "    ])), True)\n",
        "])\n",
        "\n",
        "user_preferences_df = spark.createDataFrame(user_preferences_data, user_preferences_schema)\n",
        "user_preferences_df.show(truncate=False)\n",
        "user_preferences_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18abbbc-82a9-4e1e-f590-8161415c27d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+-------------+\n",
            "|user_id|theme|language|notifications|\n",
            "+-------+-----+--------+-------------+\n",
            "|   U001| dark|      en|           on|\n",
            "|   U002|light|      es|          off|\n",
            "|   U003| dark|      fr|           on|\n",
            "+-------+-----+--------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"U001\", \"dark\", \"en\", \"on\"),\n",
        "    (\"U002\", \"light\", \"es\", \"off\"),\n",
        "    (\"U003\", \"dark\", \"fr\", \"on\")\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"user_id\", \"theme\", \"language\", \"notifications\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d10820-e500-499d-e43f-587fea36efa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+-------------+\n",
            "|user_id|theme|language|notifications|\n",
            "+-------+-----+--------+-------------+\n",
            "|   U001| dark|      en|           on|\n",
            "|   U002|light|      es|          off|\n",
            "|   U003| dark|      fr|           on|\n",
            "+-------+-----+--------+-------------+\n",
            "\n",
            "+-------+-----+--------+-------------+\n",
            "|user_id|theme|language|notifications|\n",
            "+-------+-----+--------+-------------+\n",
            "|   U001| dark|      en|           on|\n",
            "|   U002|light|      es|          off|\n",
            "|   U003| dark|      fr|           on|\n",
            "+-------+-----+--------+-------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "# -- convensional\n",
        "\n",
        "result_df = \\\n",
        "      user_preferences_df\\\n",
        "        .select('*', fn.expr('''inline_outer(preferences)'''))\\\n",
        "        .groupBy('user_id')\\\n",
        "        .agg(fn.expr(''' max(case when key = 'theme' then `value` else null end) as theme '''),\n",
        "            fn.expr(''' max(case when key = 'language' then `value` else null end) as language '''),\n",
        "            fn.expr(''' max(case when key = 'notifications' then `value` else null end) as notifications '''))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "##-- pivoting method\n",
        "\n",
        "result_df = \\\n",
        "      user_preferences_df\\\n",
        "        .select('*', fn.expr('''inline_outer(preferences)'''))\\\n",
        "        .groupBy('user_id')\\\n",
        "        .pivot('key', ['theme','language','notifications'])\\\n",
        "        .agg(fn.expr('max(value)'))\n",
        "\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-26"
      },
      "source": [
        "**Instructor Notes:** Complex data type transformations. Tests array and struct manipulation for data reshaping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-27"
      },
      "source": [
        "## Problem 27: Advanced Partitioning Strategy\n",
        "\n",
        "**Requirement:** Performance optimization for large-scale time-series data.\n",
        "\n",
        "**Scenario:** Implement partitioning strategy for efficient querying of time-series data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf11428-a7f2-4263-e67a-d46ec5705f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+-----+\n",
            "|          timestamp|sensor_id|value|\n",
            "+-------------------+---------+-----+\n",
            "|2023-01-01 10:00:00| Sensor_A| 25.5|\n",
            "|2023-01-01 10:00:00| Sensor_B| 30.2|\n",
            "|2023-01-01 11:00:00| Sensor_A| 26.1|\n",
            "|2023-01-01 11:00:00| Sensor_B| 31.0|\n",
            "|2023-01-02 10:00:00| Sensor_A| 24.8|\n",
            "|2023-01-02 10:00:00| Sensor_B| 29.5|\n",
            "|2023-01-02 11:00:00| Sensor_A| 25.3|\n",
            "|2023-01-02 11:00:00| Sensor_B| 30.1|\n",
            "+-------------------+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "time_series_large_data = [\n",
        "    (\"2023-01-01 10:00:00\", \"Sensor_A\", 25.5),\n",
        "    (\"2023-01-01 10:00:00\", \"Sensor_B\", 30.2),\n",
        "    (\"2023-01-01 11:00:00\", \"Sensor_A\", 26.1),\n",
        "    (\"2023-01-01 11:00:00\", \"Sensor_B\", 31.0),\n",
        "    (\"2023-01-02 10:00:00\", \"Sensor_A\", 24.8),\n",
        "    (\"2023-01-02 10:00:00\", \"Sensor_B\", 29.5),\n",
        "    (\"2023-01-02 11:00:00\", \"Sensor_A\", 25.3),\n",
        "    (\"2023-01-02 11:00:00\", \"Sensor_B\", 30.1)\n",
        "]\n",
        "\n",
        "time_series_large_df = spark.createDataFrame(time_series_large_data, [\"timestamp\", \"sensor_id\", \"value\"])\n",
        "time_series_large_df = time_series_large_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
        "time_series_large_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067eb94a-bdff-483e-85a5-eced89147ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+---------+\n",
            "|      date|sensor_id|min_value|max_value|\n",
            "+----------+---------+---------+---------+\n",
            "|2023-01-01| Sensor_A|     25.5|     26.1|\n",
            "|2023-01-01| Sensor_B|     30.2|     31.0|\n",
            "|2023-01-02| Sensor_A|     24.8|     25.3|\n",
            "|2023-01-02| Sensor_B|     29.5|     30.1|\n",
            "+----------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"2023-01-01\", \"Sensor_A\", 25.5, 26.1),\n",
        "    (\"2023-01-01\", \"Sensor_B\", 30.2, 31.0),\n",
        "    (\"2023-01-02\", \"Sensor_A\", 24.8, 25.3),\n",
        "    (\"2023-01-02\", \"Sensor_B\", 29.5, 30.1)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"date\", \"sensor_id\", \"min_value\", \"max_value\"])\n",
        "expected_df = expected_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b29e76-85be-489a-e489-82eb806e3f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+---------+\n",
            "|      date|sensor_id|min_value|max_value|\n",
            "+----------+---------+---------+---------+\n",
            "|2023-01-01| Sensor_A|     25.5|     26.1|\n",
            "|2023-01-01| Sensor_B|     30.2|     31.0|\n",
            "|2023-01-02| Sensor_A|     24.8|     25.3|\n",
            "|2023-01-02| Sensor_B|     29.5|     30.1|\n",
            "+----------+---------+---------+---------+\n",
            "\n",
            "+----------+---------+---------+---------+\n",
            "|      date|sensor_id|min_value|max_value|\n",
            "+----------+---------+---------+---------+\n",
            "|2023-01-01| Sensor_A|     25.5|     26.1|\n",
            "|2023-01-02| Sensor_B|     29.5|     30.1|\n",
            "|2023-01-02| Sensor_A|     24.8|     25.3|\n",
            "|2023-01-01| Sensor_B|     30.2|     31.0|\n",
            "+----------+---------+---------+---------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "##-- time series analysis in runtime.\n",
        "\n",
        "num_partitions = 200\n",
        "\n",
        "time_series_large_partitioned_df = \\\n",
        "        time_series_large_df\\\n",
        "          .withColumn('date', to_date('timestamp'))\\\n",
        "          .repartition(num_partitions,'date')\\\n",
        "          .cache()\n",
        "\n",
        "time_series_large_partitioned_df\\\n",
        "        .groupBy('date','sensor_id')\\\n",
        "        .agg(fn.expr('min(value) as min_value'),\n",
        "             fn.expr('max(value) as max_value'))\\\n",
        "        .show()\n",
        "\n",
        "time_series_large_partitioned_df.unpersist()\n",
        "\n",
        "# -- for repeated use cases, use the spark warehouse tables. (for run time and for other analytics connetors)\n",
        "\n",
        "# YOUR SOLUTION HERE\n",
        "\n",
        "time_series_large_df\\\n",
        "        .withColumn('date', to_date('timestamp'))\\\n",
        "        .write\\\n",
        "        .bucketBy(50,'date','sensor_id')\\\n",
        "        .sortBy('date','sensor_id')\\\n",
        "        .format('parquet')\\\n",
        "        .mode('overwrite')\\\n",
        "        .saveAsTable('table1')\n",
        "\n",
        "dataframe = spark.read.table('table1')\n",
        "\n",
        "result_df = \\\n",
        "      dataframe\\\n",
        "          .groupBy('date','sensor_id')\\\n",
        "          .agg(fn.expr('min(value) as min_value'),\n",
        "              fn.expr('max(value) as max_value'))\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-27"
      },
      "source": [
        "**Instructor Notes:** Partitioning strategy for performance. Tests date extraction and efficient aggregation patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-28"
      },
      "source": [
        "## Problem 28: Complex Business Logic Implementation\n",
        "\n",
        "**Requirement:** Finance needs commission calculation with tiered rates.\n",
        "\n",
        "**Scenario:** Calculate sales commissions with different rates based on sales tiers.\n",
        ">\n",
        "* Tier 1: First $10,000 of sales → 5% commission\n",
        "\n",
        "\n",
        "* Tier 2: Next $10,000 ($10,001 - $20,000) → 7% commission  \n",
        "\n",
        "* Tier 3: Sales above $20,000 → 9% commission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f113ac-633f-458b-a40f-2a489650d765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+\n",
            "|sales_id|salesperson|sales_amount|\n",
            "+--------+-----------+------------+\n",
            "|    S001|       John|      5000.0|\n",
            "|    S002|       Jane|     15000.0|\n",
            "|    S003|        Bob|      8000.0|\n",
            "|    S004|      Alice|     25000.0|\n",
            "|    S005|    Charlie|     12000.0|\n",
            "+--------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "sales_commissions_data = [\n",
        "    (\"S001\", \"John\", 5000.0),\n",
        "    (\"S002\", \"Jane\", 15000.0),\n",
        "    (\"S003\", \"Bob\", 8000.0),\n",
        "    (\"S004\", \"Alice\", 25000.0),\n",
        "    (\"S005\", \"Charlie\", 12000.0)\n",
        "]\n",
        "\n",
        "sales_commissions_df = spark.createDataFrame(sales_commissions_data, [\"sales_id\", \"salesperson\", \"sales_amount\"])\n",
        "sales_commissions_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b64a50-83c4-416c-c73e-2f4aa052f178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+----------+\n",
            "|sales_id|salesperson|sales_amount|commission|\n",
            "+--------+-----------+------------+----------+\n",
            "|    S001|       John|      5000.0|     250.0|\n",
            "|    S002|       Jane|     15000.0|     850.0|\n",
            "|    S003|        Bob|      8000.0|     400.0|\n",
            "|    S004|      Alice|     25000.0|    1650.0|\n",
            "|    S005|    Charlie|     12000.0|     640.0|\n",
            "+--------+-----------+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "\n",
        "expected_data = [\n",
        "    (\"S001\", \"John\", 5000.0, 250.0),\n",
        "    (\"S002\", \"Jane\", 15000.0, 850.0),\n",
        "    (\"S003\", \"Bob\", 8000.0, 400.0),\n",
        "    (\"S004\", \"Alice\", 25000.0, 1650.0),\n",
        "    (\"S005\", \"Charlie\", 12000.0, 640.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"sales_id\", \"salesperson\", \"sales_amount\", \"commission\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edaaeb7-afac-48c9-f9c8-37c2346bacd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+----------+\n",
            "|sales_id|salesperson|sales_amount|commission|\n",
            "+--------+-----------+------------+----------+\n",
            "|    S001|       John|      5000.0|     250.0|\n",
            "|    S002|       Jane|     15000.0|     850.0|\n",
            "|    S003|        Bob|      8000.0|     400.0|\n",
            "|    S004|      Alice|     25000.0|    1650.0|\n",
            "|    S005|    Charlie|     12000.0|     640.0|\n",
            "+--------+-----------+------------+----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "case_string = fn.expr('''\n",
        "                      case when sales_amount <= 10000\n",
        "                      then sales_amount*0.05\n",
        "                      when sales_amount <= 20000\n",
        "                      then 10000*0.05 + (sales_amount - 10000)*0.07\n",
        "                      when sales_amount > 20000\n",
        "                      then 10000*0.05 + 10000*0.07 + (sales_amount-20000)*0.09\n",
        "                      end\n",
        "                      ''')\n",
        "\n",
        "result_df = \\\n",
        "    sales_commissions_df\\\n",
        "        .withColumn('commission',case_string)\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-28"
      },
      "source": [
        "**Instructor Notes:** Complex business logic with tiered calculations. Tests conditional logic and mathematical operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-29"
      },
      "source": [
        "## Problem 29: Multi-Step Data Transformation Pipeline\n",
        "\n",
        "**Requirement:** ETL pipeline needs complex multi-step data transformation.\n",
        "\n",
        "**Scenario:** Implement a multi-step transformation: clean, enrich, aggregate, and pivot data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9145251c-6b8c-47b0-d648-82c4ac0ba519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+----------+-------+\n",
            "|salesperson|   category| sale_date| amount|\n",
            "+-----------+-----------+----------+-------+\n",
            "|     john  |Electronics|2023-01-15|1000.50|\n",
            "|       Jane|   Clothing|2023-01-16| 800.75|\n",
            "|        bob|Electronics|2023-01-17|1200.25|\n",
            "|      Alice|   Clothing|2023-01-18| 950.00|\n",
            "+-----------+-----------+----------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrame\n",
        "raw_sales_data = [\n",
        "    (\"  john  \", \"Electronics\", \"2023-01-15\", \"1000.50\"),\n",
        "    (\"Jane\", \"Clothing\", \"2023-01-16\", \"800.75\"),\n",
        "    (\"bob\", \"Electronics\", \"2023-01-17\", \"1200.25\"),\n",
        "    (\"Alice\", \"Clothing\", \"2023-01-18\", \"950.00\")\n",
        "]\n",
        "\n",
        "raw_sales_df = spark.createDataFrame(raw_sales_data, [\"salesperson\", \"category\", \"sale_date\", \"amount\"])\n",
        "raw_sales_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d8c907-73d7-4327-98e4-f8262e755ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|   category|total_sales|\n",
            "+-----------+-----------+\n",
            "|Electronics|    2200.75|\n",
            "|   Clothing|    1750.75|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"Electronics\", 2200.75),\n",
        "    (\"Clothing\", 1750.75)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"category\", \"total_sales\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a611b5-62a4-4381-eded-148a04a34ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|   category|total_sales|\n",
            "+-----------+-----------+\n",
            "|Electronics|    2200.75|\n",
            "|   Clothing|    1750.75|\n",
            "+-----------+-----------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "result_df = raw_sales_df\\\n",
        "    .groupBy('category')\\\n",
        "    .agg(fn.expr('sum(amount) as total_sales'))\\\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-29"
      },
      "source": [
        "**Instructor Notes:** Multi-step transformation pipeline. Tests data cleaning, type conversion, and aggregation in sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-30"
      },
      "source": [
        "## Problem 30: Complex Join with Aggregation\n",
        "\n",
        "**Requirement:** Business intelligence needs customer behavior analysis with purchase patterns.\n",
        "\n",
        "**Scenario:** Join customer data with orders and calculate complex behavioral metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "source-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20409a1d-51d7-4d88-bb7e-c6e18e6e1337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers:\n",
            "+-----------+-------------+-----------+\n",
            "|customer_id|customer_name|signup_date|\n",
            "+-----------+-------------+-----------+\n",
            "|       C001|         John| 2023-01-01|\n",
            "|       C002|         Jane| 2023-01-05|\n",
            "|       C003|          Bob| 2023-01-10|\n",
            "+-----------+-------------+-----------+\n",
            "\n",
            "Orders:\n",
            "+--------+-----------+----------+------+\n",
            "|order_id|customer_id|order_date|amount|\n",
            "+--------+-----------+----------+------+\n",
            "|    O001|       C001|2023-01-15| 100.0|\n",
            "|    O002|       C001|2023-01-20| 150.0|\n",
            "|    O003|       C001|2023-02-01| 200.0|\n",
            "|    O004|       C002|2023-01-25| 300.0|\n",
            "|    O005|       C003|2023-02-05| 250.0|\n",
            "+--------+-----------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Source DataFrames\n",
        "customers_behavior_data = [\n",
        "    (\"C001\", \"John\", \"2023-01-01\"),\n",
        "    (\"C002\", \"Jane\", \"2023-01-05\"),\n",
        "    (\"C003\", \"Bob\", \"2023-01-10\")\n",
        "]\n",
        "\n",
        "orders_behavior_data = [\n",
        "    (\"O001\", \"C001\", \"2023-01-15\", 100.0),\n",
        "    (\"O002\", \"C001\", \"2023-01-20\", 150.0),\n",
        "    (\"O003\", \"C001\", \"2023-02-01\", 200.0),\n",
        "    (\"O004\", \"C002\", \"2023-01-25\", 300.0),\n",
        "    (\"O005\", \"C003\", \"2023-02-05\", 250.0)\n",
        "]\n",
        "\n",
        "customers_behavior_df = spark.createDataFrame(customers_behavior_data, [\"customer_id\", \"customer_name\", \"signup_date\"])\n",
        "orders_behavior_df = spark.createDataFrame(orders_behavior_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])\n",
        "\n",
        "print(\"Customers:\")\n",
        "customers_behavior_df.show()\n",
        "print(\"Orders:\")\n",
        "orders_behavior_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expected-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393e23c6-8e19-4430-def1-4af2d21da0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------+---------------+-------------------+\n",
            "|customer_id|customer_name|order_count|total_spent|avg_order_value|days_to_first_order|\n",
            "+-----------+-------------+-----------+-----------+---------------+-------------------+\n",
            "|       C001|         John|          3|      450.0|          150.0|               14.0|\n",
            "|       C002|         Jane|          1|      300.0|          300.0|               20.0|\n",
            "|       C003|          Bob|          1|      250.0|          250.0|               26.0|\n",
            "+-----------+-------------+-----------+-----------+---------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expected Output\n",
        "expected_data = [\n",
        "    (\"C001\", \"John\", 3, 450.0, 150.0, 14.0),\n",
        "    (\"C002\", \"Jane\", 1, 300.0, 300.0, 20.0),\n",
        "    (\"C003\", \"Bob\", 1, 250.0, 250.0, 26.0)\n",
        "]\n",
        "\n",
        "expected_df = spark.createDataFrame(expected_data, [\"customer_id\", \"customer_name\", \"order_count\", \"total_spent\", \"avg_order_value\", \"days_to_first_order\"])\n",
        "expected_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solution-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bc8b25-8bfd-4e6c-8d06-642360b4f3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------+---------------+-------------------+\n",
            "|customer_id|customer_name|order_count|total_spent|avg_order_value|days_to_first_order|\n",
            "+-----------+-------------+-----------+-----------+---------------+-------------------+\n",
            "|       C002|         Jane|          1|      300.0|          300.0|                 20|\n",
            "|       C001|         John|          3|      450.0|          150.0|                 14|\n",
            "|       C003|          Bob|          1|      250.0|          250.0|                 26|\n",
            "+-----------+-------------+-----------+-----------+---------------+-------------------+\n",
            "\n",
            "✓ DataFrames are equal!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "join_on = fn.expr(''' cust.customer_id = ord.customer_id ''')\n",
        "\n",
        "merge_dataframe = \\\n",
        "    customers_behavior_df.alias('cust')\\\n",
        "      .join(orders_behavior_df.alias('ord'),\n",
        "            join_on,\n",
        "            'inner')\\\n",
        "      .drop('ord.customer_id')\\\n",
        "      .repartition(10,fn.col('cust.customer_id'),fn.col('cust.customer_name'))\\\n",
        "      .cache()\n",
        "\n",
        "result_df = \\\n",
        "      merge_dataframe\\\n",
        "        .groupBy('cust.customer_id','cust.customer_name')\\\n",
        "        .agg(fn.expr(''' count(ord.order_id) as order_count '''),\n",
        "            fn.expr(''' sum(ord.amount) as total_spent '''),\n",
        "            fn.expr(''' avg(ord.amount) as avg_order_value '''),\n",
        "            fn.expr(''' date_diff(min(ord.order_date),min(cust.signup_date)) as days_to_first_order '''),\n",
        "            )\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "merge_dataframe.unpersist()\n",
        "\n",
        "# Test your solution\n",
        "assert_dataframe_equal(result_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructor-note-30"
      },
      "source": [
        "**Instructor Notes:** Complex join with multiple aggregations and date calculations. Tests comprehensive data analysis patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "completion-note"
      },
      "source": [
        "# Set 2 Complete!\n",
        "\n",
        "You've completed all 30 Easy/Medium problems in Set 2. These problems cover:\n",
        "- Advanced joins and deduplication\n",
        "- Complex window functions\n",
        "- Multi-level aggregations\n",
        "- Advanced UDFs and data validation\n",
        "- Nested data operations\n",
        "- Performance optimization strategies\n",
        "- Complex business logic implementation\n",
        "\n",
        "Ready for Set 3 with Medium difficulty problems?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}