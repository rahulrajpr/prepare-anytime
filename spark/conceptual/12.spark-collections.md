# PySpark Interview Preparation Guide

## 4. Spark Collections - Deep Dive

### 4.1 Collections Overview & Fundamentals

#### 4.1.1 What are collections in Apache Spark?

**Definition:** Collections in Spark are complex data types that allow storing multiple values within a single DataFrame column, enabling efficient handling of structured and semi-structured data.

**Detailed Explanation:**
Collections transform Spark from a simple tabular processing engine into a powerful tool for handling real-world nested data. Instead of normalizing everything into separate tables and performing expensive joins, collections let you keep logically related data together in the same row.

**Real-world Analogy:** Think of collections like folders in a filing cabinet:

- **Array**: Like a folder containing multiple related documents in order
- **Map**: Like a folder with labeled tabs for different types of information

#### 4.1.2 What are the two main collection types in Spark (ArrayType and MapType)?

**Main Collection Types:**

- **ArrayType**: Ordered collection of elements (like Python list)
- **MapType**: Key-value pairs (like Python dictionary)

**Interview Tip:** Remember that StructType is NOT a collection - it's a composite type with fixed schema.

#### 4.1.3 What is ArrayType? What kind of data does it store?

**ArrayType Definition:** An ordered, index-based collection that stores elements of the same data type.

**Stores:** Lists of values like `["apple", "banana", "orange"]` or `[1, 2, 3, 4, 5]`

**Practical Implementation:**

```python
from pyspark.sql.types import ArrayType, StringType

# Schema definition for array column
schema = StructType([
    StructField("user_id", StringType()),
    StructField("tags", ArrayType(StringType()))  # Array of strings
])

# Creating array from multiple columns
df.withColumn("full_name", array("first_name", "middle_name", "last_name"))
```

**Default Behavior:** Arrays maintain insertion order and allow duplicate elements.

#### 4.1.4 What is MapType? What kind of data does it store?

**MapType Definition:** A collection of key-value pairs where keys are unique and of the same type, and values are of the same type.

**Stores:** Key-value pairs like `{"name": "John", "age": 30, "city": "NYC"}`

**Practical Implementation:**

**python**

```
from pyspark.sql.types import MapType, StringType, IntegerType

# Schema definition for map column
schema = StructType([
    StructField("user_id", StringType()),
    StructField("preferences", MapType(StringType(), StringType()))  # Map string→string
])

# Creating map from columns
df.withColumn("user_profile", create_map(
    lit("name"), col("user_name"),
    lit("age"), col("user_age"),
    lit("city"), col("user_city")
))
```

**Key Characteristic:** Each row can have different keys in its Map column.

#### 4.1.5 Are Structs considered collections in Spark? Why or why not?

**Answer:** NO, Structs are NOT considered collections.

**Detailed Explanation:**
Collections (Arrays and Maps) are **variable-sized containers** that can hold multiple elements of the same type. Structs are **fixed-schema composite types** with named fields that can be of different types.

**Comparison:**

* **Collections** : Homogeneous elements, variable size per row
* **Structs** : Heterogeneous fields, fixed schema across all rows

**Interview Tip:** This is a common trick question - always clarify that Structs are composite types, not collections.

#### 4.1.6 What collection functions work on Arrays?

**Common Array Functions:**

* **Transformation** : `explode()`, `posexplode()`, `array_distinct()`, `array_intersect()`
* **Querying** : `array_contains()`, `size()`, `element_at()`
* **Sorting** : `array_sort()`, `sort_array()`, `reverse()`
* **Aggregation** : `collect_list()`, `collect_set()`

#### 4.1.7 What collection functions work on Maps?

**Common Map Functions:**

* **Key/Value Access** : `map_keys()`, `map_values()`, `element_at()`
* **Transformation** : `map_concat()`, `map_filter()`, `transform_keys()`, `transform_values()`
* **Querying** : `size()`

#### 4.1.8 Do collection functions work on Structs?

**Answer:** Generally NO - Structs have their own set of functions

**Struct-specific Functions:**

* Field access: `col("struct_field.sub_field")`
* Struct creation: `struct()`, `named_struct()`
* Field manipulation: `withField()`, `dropFields()`

#### 4.1.9 What is the purpose of using collections in DataFrames?

**Key Purposes with Practical Benefits:**

1. **Data Modeling** :

* Represent real-world hierarchical data naturally
* Example: User with multiple phone numbers, orders with multiple items

1. **Performance Optimization** :
   **python**

```
# Without collections - requires expensive join
users.join(phones, "user_id")  # Shuffle and network I/O

# With collections - data co-located
users.withColumn("phone_numbers", collect_list("phone"))  # No shuffle
```

1. **Semantic Grouping** :

* Keep logically related data together
* Example: Product features, user preferences, event history

1. **Schema Flexibility** :

* Handle JSON, XML, and other semi-structured data formats
* Accommodate evolving data schemas

#### 4.1.10 How do collections enable efficient management of semi-structured data?

**Efficiency Benefits with Examples:**

1. **Reduced Shuffling** :
   **python**

```
# Bad: Normalized approach - shuffle for every analysis
orders.join(order_items, "order_id").groupBy("product_id").count()

# Good: Collection approach - no shuffle needed
orders.withColumn("items", collect_list(struct("product_id", "quantity")))
      .select(explode("items").alias("item"))
      .groupBy("item.product_id").count()
```

* **Fewer Joins** :
* Keep related data in same partition
* Avoid network transfer during joins
* **Better Compression** :
* Similar data grouped together compresses better
* Array of similar values → better compression ratio
* **Predicate Pushdown** :
  **python**

```
# Can filter without exploding entire array
df.filter(array_contains(col("tags"), "urgent"))
```

## 4.2 Map vs Struct - Critical Comparison

### 4.2.1 What is the key difference between StructType and MapType?

**Critical Difference:** Structs have FIXED schema with predefined field names, while Maps have DYNAMIC schema with flexible keys.

**Detailed Technical Comparison:**

| Aspect                      | StructType                            | MapType                      |
| --------------------------- | ------------------------------------- | ---------------------------- |
| **Schema Definition** | Compile-time, static                  | Runtime, dynamic             |
| **Field/Key Names**   | Fixed, known in advance               | Variable, can differ per row |
| **Type Safety**       | Strong - each field has specific type | Weak - all values same type  |
| **Performance**       | Faster access, better optimization    | Slower, more flexible        |
| **Memory Usage**      | More predictable                      | Variable per row             |

### 4.2.2 Are field names in Structs fixed or dynamic?

**Struct Fields:** FIXED - defined at schema creation time and consistent across all rows

**Example:**

**python**

```
# All rows must have these exact field names
StructType([
    StructField("first_name", StringType()),
    StructField("last_name", StringType()), 
    StructField("age", IntegerType())
])
```

### 4.2.3 Are keys in Maps fixed or dynamic?

**Map Keys:** DYNAMIC - can vary per row and even be completely different across rows

**Example:**

**python**

```
# Row1: {"theme": "dark", "language": "en"}
# Row2: {"notifications": true, "timezone": "EST"}  # Different keys!
# Row3: {}  # Even empty map is valid
```

### 4.2.4 When are field names in Structs defined?

**Definition Time:** At DataFrame schema definition or StructType creation - BEFORE data processing

**Practical Implication:** You cannot add new struct fields without schema migration.

### 4.2.5 Can different rows in a Map column have different keys?

**Answer:** YES - each row can have completely different keys in a Map column

**Real-world Use Case:** User preferences where different users set different types of preferences.

### 4.2.6 How do you access a Struct field?

**Access Methods:**

**python**

```
# Method 1: Dot notation (most common)
df.select(col("user_profile.first_name"))

# Method 2: getField() method
df.select(col("user_profile").getField("first_name"))

# Method 3: SQL expression
df.selectExpr("user_profile.first_name")
```

**Performance:** Dot notation is generally fastest and most optimized.

### 4.2.7 How do you access a Map value?

**Access Methods:**

**python**

```
# Method 1: Bracket notation
df.select(col("user_preferences")["theme"])

# Method 2: getItem() method  
df.select(col("user_preferences").getItem("theme"))

# Method 3: element_at() function
df.select(element_at(col("user_preferences"), "theme"))
```

**Interview Tip:**`element_at()` is null-safe and preferred in production code.

### 4.2.8 When should you use Structs over Maps?

**Use Structs When:**

1. **Schema is Stable and Known** :
   **python**

```
# User profile with fixed attributes
StructType([
    StructField("name", StringType()),
    StructField("email", StringType()),
    StructField("age", IntegerType())
])
```

1. **Performance is Critical** :

* Struct field access is faster than map key lookup
* Better Catalyst optimizer support

1. **Type Safety Required** :

* Each field has specific data type
* Compile-time schema validation

1. **Data Quality Enforcement** :

* All rows must have same fields
* Nullability constraints per field

### 4.2.9 When should you use Maps over Structs?

**Use Maps When:**

1. **Schema is Flexible/Variable** :
   **python**

```
# User settings that vary by user
# User1: {"theme": "dark", "language": "en"}
# User2: {"notifications": true, "currency": "USD"}
```

1. **Handling Semi-structured Data** :

* JSON with varying keys
* Configuration data
* Feature flags

1. **Schema Evolution** :

* New attributes can be added without schema changes
* Backward compatibility

1. **Sparse Data** :

* Many possible attributes, but each row uses few

### 4.2.10 Do Struct fields have a fixed order?

**Answer:** YES - field order is fixed and meaningful in Structs

**Practical Impact:** When selecting `col("struct.*")`, fields appear in schema definition order.

### 4.2.11 Is key order guaranteed in Maps?

**Answer:** NO - key order is NOT guaranteed in Maps

**Technical Reason:** Maps are typically implemented as hash tables where order is not preserved.

### 4.2.12 Provide an example use case for Structs.

**Struct Example:** Employee records with fixed attributes

**python**

```
employee_schema = StructType([
    StructField("employee_id", StringType(), False),  # Not nullable
    StructField("personal_info", StructType([
        StructField("first_name", StringType(), False),
        StructField("last_name", StringType(), False),
        StructField("date_of_birth", DateType(), True)  # Nullable
    ])),
    StructField("employment_info", StructType([
        StructField("department", StringType(), False),
        StructField("salary", DecimalType(10,2), True),
        StructField("start_date", DateType(), False)
    ]))
])

# All employees have exactly these fields in this order
```

### 4.2.13 Provide an example use case for Maps.

**Map Example:** Product attributes that vary by product type

**python**

```
# Electronics: {"warranty": "2 years", "voltage": "110V", "weight": "5kg"}
# Clothing: {"size": "M", "color": "blue", "material": "cotton"}
# Books: {"author": "John Doe", "pages": "300", "publisher": "XYZ Press"}

# Each product type has different relevant attributes
product_df.withColumn("attributes", create_map(
    lit("category"), col("product_category"),
    lit("price"), col("product_price")
))
```

### 4.2.14 What happens when you know all attributes upfront - Struct or Map?

**Answer:** Use STRUCT - better performance, type safety, and schema enforcement

**Decision Framework:**

* Known, fixed attributes → Struct
* Evolving, variable attributes → Map
* Mixed scenario → Use Struct for core attributes, Map for extended attributes

### 4.2.15 What happens when keys are variable or semi-structured - Struct or Map?

**Answer:** Use MAP - handles flexible schemas and evolving data structures

**Real-world Example:** Feature stores where different ML models need different feature sets.

### 4.2.16 Can you have optional fields in Structs?

**Answer:** YES - using `nullable=True` in StructField definition

**Implementation:**

**python**

```
StructField("middle_name", StringType(), True)  # Nullable = optional
```

### 4.2.17 Can you have optional keys in Maps?

**Answer:** Maps don't have "optional keys" concept - keys either exist or don't exist per row

**Difference:** In Structs, field exists but value can be null. In Maps, key may not exist at all.

### 4.2.18 Compare schema rigidity: Struct vs Map.

**Schema Rigidity Spectrum:**

**text**

```
Most Rigid ←-------------------→ Most Flexible
   Struct                         Map
   
Characteristics:
- Struct: Compile-time schema validation, better performance
- Map: Runtime schema flexibility, handles semi-structured data
```

**Interview Tip:** Choose based on your data characteristics, not personal preference.

## 4.3 Array vs Map - Comparison

### 4.3.1 What is the key difference between ArrayType and MapType?

**Key Difference:** Arrays use integer indices for positional access, Maps use string keys for associative access

**Practical Implications:**

* **Arrays** : When you care about order and position
* **Maps** : When you care about meaningful labels and lookup

### 4.3.2 Does ArrayType maintain order?

**Answer:** YES - Arrays maintain insertion order and position matters

**Example:**`["first", "second", "third"]` - position 0 always contains "first"

### 4.3.3 Is order guaranteed in MapType?

**Answer:** NO - Map key order is not guaranteed and should not be relied upon

**Example:**`{"z": 1, "a": 2, "m": 3}` may be stored in any order

### 4.3.4 What is ArrayType best used for?

**Array Best For:**

1. **Ordered Sequences** :
   **python**

```
# Timeline events, process steps, navigation history
["page_view", "add_to_cart", "checkout", "purchase"]
```

 **Homogeneous Collections** :

**python**

```
# Tags, categories, features
["spark", "big-data", "analytics", "python"]
```

 **Position-based Data** :

**python**

```
# Coordinates, time series data
[[1, 2], [3, 4], [5, 6]]  # Array of arrays
```

### 4.3.5 What is MapType best used for?

**Map Best For:**

1. **Key-Value Configurations** :
   **python**

```
# User settings, application config
{"theme": "dark", "language": "en", "notifications": true}
```

 **Flexible Attributes** :

**python**

```
# Product specs that vary by category
{"color": "red", "size": "XL", "material": "cotton"}
```

 **Lookup Tables within Rows** :

**python**

```
# Feature stores, metadata
{"feature1": 0.75, "feature2": 0.33, "feature3": 0.91}
```

### 4.3.6 Can array elements be of different types?

**Answer:** NO - all elements in Array must be same type

**Enforcement:** Spark validates type consistency at runtime

**python**

```
# This will fail - mixed types not allowed
array(lit("string"), lit(123))  # Error: type mismatch
```

### 4.3.7 Can map values be of different types?

**Answer:** NO - all values in Map must be same type (but different from key type)

**Type Consistency:**

* All keys: Same type
* All values: Same type
* Key type ≠ Value type (usually)

### 4.3.8 How do you access array elements by position?

**Array Access Methods:**

**python**

```
# 0-based indexing (standard programming convention)
df.select(
    col("items")[0].alias("first_item"),      # Position 0
    col("items").getItem(2).alias("third_item")  # Position 2
)

# With error handling
df.select(
    element_at(col("items"), 1).alias("first_item"),      # 1-based, null-safe
    try_element_at(col("items"), 100).alias("safe_access") # Returns null for invalid
)
```

**Interview Tip:**`element_at()` uses 1-based indexing, bracket notation uses 0-based.

### 4.3.9 How do you access map values by key?

**Map Access Methods:**

**python**

```
df.select(
    col("settings")["theme"].alias("theme"),           # Bracket notation
    col("settings").getItem("language").alias("lang"), # getItem method
    element_at(col("settings"), "timezone").alias("tz") # element_at function
)

# Safe access for potentially missing keys
df.select(
    when(col("settings")["theme"].isNull(), "default")
     .otherwise(col("settings")["theme"]).alias("safe_theme")
)
```

**Best Practice:** Use `element_at()` or null-checking for production code.

## 4.4 Advanced Collection Operations & Nested Data

### 4.4.1 How do you work with nested data structures in Spark?

**Working with Nested Data Patterns:**

1. **Field Access** :
   **python**

```
# Struct field access
df.select(col("user.address.city"), col("user.profile.age"))
```

 **Array Operations** :

**python**

```
# Transform arrays
df.withColumn("sorted_tags", array_sort(col("tags")))
  .withColumn("tag_count", size(col("tags")))
```

 **Map Operations** :

**python**

```
# Work with maps
df.withColumn("setting_keys", map_keys(col("settings")))
  .withColumn("has_theme", col("settings")["theme"].isNotNull())
```

 **Combined Operations** :

**python**

```
# Complex nested query
df.filter(
    array_contains(col("user.preferences.languages"), "en") &
    (col("user.profile.age") > 18)
)
```

### 4.4.2 What are the performance implications of deeply nested schemas?

**Performance Implications:**

1. **Positive Effects** :

* **Data Locality** : Related data in same partition
* **Reduced Shuffling** : Fewer joins needed
* **Better Compression** : Similar data grouped together

1. **Negative Effects** :

* **Memory Overhead** : More complex objects use more memory
* **Serialization Cost** : Nested structures serialize/deserialize slower
* **Query Planning** : Complex schemas slow down Catalyst optimizer

**Performance Optimization Tips:**

**python**

```
# Instead of deeply nested:
df.select(col("very.deeply.nested.field"))

# Consider flattening during read:
df.select(col("nested.*"))  # Expands all fields at once
```

### 4.4.3 How do you flatten nested structures?

**Flattening Strategies:**

1. **Explode Arrays** :
   **python**

```
# Convert array elements to separate rows
df.select(explode(col("items")).alias("item"))
  .select("item.*")  # Expand struct fields
```

 **Flatten Structs** :

**python**

```
# Expand all struct fields to top-level columns
df.select("user.*")  # Creates user_field1, user_field2, etc.
```

 **Select Specific Nested Fields** :

**python**

```
# Bring nested fields to top level with aliases
df.select(
    col("user.profile.name").alias("user_name"),
    col("user.address.city").alias("user_city")
)
```

 **Combined Approach** :

**python**

```
# Complex flattening
df.select(explode(col("orders")).alias("order"))
  .select(
      "order.order_id",
      "order.customer.*",      # Flatten customer struct
      explode("order.items").alias("item")
  )
  .select("*", "item.*")       # Flatten item struct
  .drop("item")
```

### 4.4.4 When should you denormalize data vs keep it normalized in Spark?

**Decision Framework:**

**Denormalize (Use Collections) When:**

* Frequent analytical queries across related data
* Read-heavy workloads
* Data doesn't change frequently
* Want to avoid expensive joins

**Keep Normalized (Separate Tables) When:**

* Frequent updates to individual elements
* Many-to-many relationships
* Storage efficiency is critical
* Different access patterns for different parts

**Example Trade-off:**

**python**

```
# Denormalized - good for analytics
orders_with_items = orders.withColumn(
    "items", collect_list(struct("product_id", "quantity", "price"))
)

# Normalized - good for transactional updates
orders_table = orders.select("order_id", "customer_id", "order_date")
order_items_table = order_items.select("order_id", "product_id", "quantity", "price")
```

### 4.4.5 How do you handle schema evolution with complex types?

**Schema Evolution Strategies:**

1. **Add New Fields as Nullable** :
   **python**

```
# Original schema
StructType([StructField("name", StringType()), StructField("age", IntegerType())])

# Evolved schema - new field is nullable
StructType([
    StructField("name", StringType()),
    StructField("age", IntegerType()), 
    StructField("email", StringType(), True)  # New nullable field
])
```

 **Use Maps for Flexible Parts** :

**python**

```
# Core fields in struct, extensions in map
StructType([
    StructField("core_data", StructType([...])),
    StructField("extended_attrs", MapType(StringType(), StringType()))
])
```

 **Schema Merging with Parquet** :

**python**

```
spark.read.option("mergeSchema", "true").parquet("data/")
```

 **Version Your Schemas** :

**python**

```
# Add schema_version field
df.withColumn("schema_version", lit("2.0"))
```

### 4.4.6 Can you have arrays of structs?

**Answer:** YES - very common and powerful pattern

**Practical Example:**

**python**

```
# Array of user structs
schema = ArrayType(StructType([
    StructField("user_id", StringType()),
    StructField("user_name", StringType()),
    StructField("actions", ArrayType(StringType()))  # Nested array!
]))

# Querying array of structs
df.select(explode(col("users")).alias("user"))
  .filter(col("user.user_name") == "john_doe")
  .select(explode(col("user.actions")).alias("action"))
```

### 4.4.7 Can you have maps of arrays?

**Answer:** YES

**Example:**

**python**

```
# Map where values are arrays
MapType(StringType(), ArrayType(StringType()))

# Practical use: User preferences by category
{"favorite_colors": ["blue", "green"], "hobbies": ["reading", "hiking"]}
```

### 4.4.8 Can you have arrays of maps?

**Answer:** YES

**Example:**

**python**

```
# Array of maps
ArrayType(MapType(StringType(), StringType()))

# Practical use: List of configuration sets
[{"role": "admin", "access": "full"}, {"role": "user", "access": "readonly"}]
```

### 4.4.9 How do you query nested arrays of structs?

**Query Patterns for Complex Nesting:**

1. **Filter Array of Structs** :
   **python**

```
# Find users who have specific preference
df.filter(array_contains(col("users.preferences"), "dark_mode"))
```

 **Explode and Query** :

**python**

```
# Explode then filter
df.select(explode(col("orders")).alias("order"))
  .filter(col("order.amount") > 1000)
  .select("order.customer.name")
```

 **Exists with Lambda** :

**python**

```
# Check if any struct in array meets condition
df.filter(exists(col("users"), lambda x: x["age"] > 18))
```

 **Aggregate within Nested Structures** :

**python**

```
# Calculate average of nested array values
df.withColumn("avg_score", aggregate(
    col("scores"), 
    lit(0.0), 
    lambda acc, x: acc + x["value"],
    lambda acc: acc / size(col("scores"))
))
```

### 4.4.10 What is the performance impact of deeply nested collections?

**Performance Impact Analysis:**

1. **Explosion Risk** :
   **python**

```
# Dangerous - can create massive data expansion
df.select(explode(col("users")).alias("user"))
  .select(explode(col("user.orders")).alias("order"))
  .select(explode(col("order.items")).alias("item"))
# 10 users × 100 orders × 10 items = 10,000x data expansion!
```

* **Memory Usage** :
* Each nested level adds memory overhead
* Complex objects have higher GC pressure
* **Serialization Costs** :
* Nested structures serialize slower
* Network transfer of nested data is less efficient
* **Optimization Strategies** :
  **python**

```
# Filter before exploding
df.filter(size(col("users")) > 0)
  .filter(size(col("users.orders")) > 0)
  .select(explode(col("users")).alias("user"))

# Use lateral view in SQL for better optimization
df.createOrReplaceTempView("data")
spark.sql("""
  SELECT user, order, item
  FROM data
  LATERAL VIEW explode(users) u AS user
  LATERAL VIEW explode(user.orders) o AS order  
  LATERAL VIEW explode(order.items) i AS item
""")
```

**Interview Tip:** Always consider the data explosion factor when working with nested collections and test with sample data first.
