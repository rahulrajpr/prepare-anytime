# PySpark Study Guide - Structs, Types, and Operations

## 3.10.1 Struct Deep Dive & Comparisons

### What is a Struct in Apache Spark?
A struct is a complex data type that groups multiple fields together into a single column, similar to a row or object with named attributes.

### What is the purpose of using StructType in DataFrames?
StructType defines the schema of a DataFrame, specifying column names, data types, and nullable constraints for structured data.

### How do structs enable representation of nested or hierarchical data?
Structs allow embedding complex objects within columns, representing parent-child relationships without flattening data into multiple columns.

### What is the difference between struct() and named_struct()?
- `struct()`: Creates struct from column references, uses original column names
- `named_struct()`: Creates struct with explicitly defined field names (SQL only)

### How do you define field names in named_struct()?
Alternate between field name (string) and value: `named_struct('field1', value1, 'field2', value2)`

### What field names does struct() generate by default?
Uses the column names passed to it; if expressions are used, generates names like `col1`, `col2`, etc.

### How do you access fields in a struct created with named_struct()?
Use dot notation: `df.select(col("struct_col.field_name"))` or `df.struct_col.field_name`

### What happens to field names when using struct() with column names vs literals?
- **Column names**: Preserves original column names
- **Literals**: Generates automatic names like `col1`, `col2`

### When should you use named_struct() over struct()?
When you need explicit control over field names in SQL expressions or when column names don't match desired struct field names.

### When should you use struct() over named_struct()?
When working in PySpark DataFrame API and existing column names are appropriate for the struct fields.

### What does the schema look like for named_struct('city','value','state','value')?
```
struct<city:string, state:string>
```

### What does the schema look like for struct('value1', 'value2')?
```
struct<value1:datatype, value2:datatype>
```

### Can you nest structs within structs?
Yes, structs can contain other structs, creating multi-level nested structures.

### How do you access deeply nested struct fields?
Chain dot notation: `df.select("parent.child.grandchild")` or `col("parent.child.grandchild")`

---

## 3.11 Type Conversion & Casting

### How do you cast columns using cast() function?
```python
df.select(col("column").cast("int"))
df.select(col("column").cast(IntegerType()))
```

### What is the difference between cast() and astype()?
No difference - `astype()` is an alias for `cast()`. Both perform the same type conversion.

### What happens when casting fails (e.g., string "abc" to integer)?
Returns `NULL` instead of raising an error.

### How do you handle casting errors gracefully?
Use `try_cast()` which returns NULL on failure, or check for NULLs after casting with `isNull()`.

### What does try_cast() do in Spark SQL?
Same as `cast()` - returns NULL on conversion failure (Spark 3.0+). In PySpark, just use `cast()`.

---

## 3.11.1 Numeric Type Casting Functions

### What is tinyint() function and what data type does it cast to?
Casts to TinyInt (1-byte integer), range: -128 to 127

### What is smallint() function and when would you use it?
Casts to SmallInt (2-byte integer), range: -32,768 to 32,767. Use for small integer ranges to save space.

### What is the difference between int() and bigint() casting?
- `int()`: 4-byte integer (-2.1B to 2.1B)
- `bigint()`: 8-byte integer (-9.2 quintillion to 9.2 quintillion)

### When should you use tinyint vs smallint vs int vs bigint?
- **TinyInt**: Age, small counts (< 128)
- **SmallInt**: Quantities, years
- **Int**: General integers, IDs
- **BigInt**: Large IDs, timestamps, big numbers

### What are the value ranges for tinyint, smallint, int, and bigint?
- **TinyInt**: -128 to 127
- **SmallInt**: -32,768 to 32,767
- **Int**: -2,147,483,648 to 2,147,483,647
- **BigInt**: -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807

---

## 3.11.2 Other Specific Casting Functions

### What does binary() function do?
Casts values to binary (byte array) type.

### What is boolean() casting function used for?
Converts values to boolean (true/false). Non-zero numbers → true, zero → false.

### How do you use date() function for type casting?
```python
col("date_string").cast("date")
```

### What does decimal() function do?
Casts to decimal with specified precision and scale: `cast("decimal(10,2)")`

### What is the difference between double() and float() casting?
- **Float**: 4 bytes, ~7 decimal digits precision
- **Double**: 8 bytes, ~15 decimal digits precision

### What does string() function do for type conversion?
Converts any data type to string representation.

### How do you use timestamp() function?
```python
col("timestamp_string").cast("timestamp")
```

---

## 3.11.3 Type Conversion (to_ Functions)

### What is to_char() function? What does it convert from and to?
Converts numbers/dates/timestamps to string with format patterns.
```python
to_char(col("date"), "yyyy-MM-dd")
```

### What is to_varchar() function used for?
Alias for `to_char()` - converts to string with formatting.

### What does to_number() function do? When do you use it?
Converts string to numeric type with format specification.
```python
to_number(col("price"), "999.99")
```

### What is the difference between to_date() and date() casting?
- `to_date()`: Accepts format parameter for parsing
- `cast("date")`: Uses default format, less flexible

### What is the difference between to_timestamp() and timestamp() casting?
- `to_timestamp()`: Accepts format parameter
- `cast("timestamp")`: Uses default format

### What does to_json() function do? What data types can it convert?
Converts structs, arrays, and maps to JSON strings.

### What is to_binary() function used for?
Converts string to binary format.

### When would you use to_ functions vs direct casting with cast()?
- **to_ functions**: When you need format control or specific parsing rules
- **cast()**: For simple type conversions without format specifications

---

## 3.11.4 FLOAT vs DOUBLE vs DECIMAL

### What is the precision difference between FLOAT, DOUBLE, and DECIMAL?
- **FLOAT**: ~7 decimal digits
- **DOUBLE**: ~15 decimal digits  
- **DECIMAL**: Up to 38 digits (configurable)

### How much storage does each numeric type use?
- **FLOAT**: 4 bytes
- **DOUBLE**: 8 bytes
- **DECIMAL**: Variable (depends on precision)

### What types of arithmetic do FLOAT and DOUBLE use?
**Approximate arithmetic** (binary floating-point), while DECIMAL uses **exact arithmetic** (decimal).

### Do FLOAT and DOUBLE have rounding errors? What about DECIMAL?
Yes, FLOAT/DOUBLE have rounding errors due to binary representation. DECIMAL has **no rounding errors** for decimal operations.

### Which numeric type is fastest for computations?
FLOAT and DOUBLE are fastest due to hardware support.

### When should you use FLOAT or DOUBLE for data processing?
For scientific calculations, measurements, statistics where small rounding errors are acceptable.

### When should you ALWAYS use DECIMAL instead of FLOAT/DOUBLE?
For **financial calculations, monetary values, prices, tax calculations** - anywhere precision is legally/financially critical.

### Why is DECIMAL the only choice for financial and monetary data?
Because FLOAT/DOUBLE accumulate rounding errors that can cause incorrect financial calculations and regulatory compliance issues.

### What is the maximum precision supported by DECIMAL in Spark?
**38 digits** (precision = 38)

### What are the performance trade-offs between DECIMAL and FLOAT/DOUBLE?
DECIMAL is slower but exact; FLOAT/DOUBLE are faster but approximate.

### Examples where using FLOAT/DOUBLE would cause problems in financial calculations
```python
# Problem: 0.1 + 0.2 in FLOAT/DOUBLE
0.1 + 0.2 = 0.30000000000000004  # Wrong!

# Problem: Summing prices
sum([10.1, 20.2, 30.3]) = 60.599999999  # Cents off!

# Problem: Tax calculation
999.99 * 1.08 = 1079.9891999... instead of 1079.99
```

---

## 3.12 Null Handling & Data Cleaning

### What is the difference between dropna() and fillna()?
- `dropna()`: Removes rows with NULL values
- `fillna()`: Replaces NULL values with specified values

### How do you drop rows with nulls in specific columns using dropna(subset=[])?
```python
df.dropna(subset=["col1", "col2"])
```

### What are the different threshold options in dropna()?
- `thresh=n`: Keep rows with at least `n` non-null values
- `how='any'`: Drop if any NULL exists (default)
- `how='all'`: Drop only if all values are NULL

### How do you fill nulls with different values for different columns?
```python
df.fillna({"col1": 0, "col2": "Unknown", "col3": 99.9})
```

### What does na.replace() do?
Replaces **existing non-null values** with other values (value substitution).

### What is the critical difference between na.replace() and na.fill()?
- `na.replace()`: Replaces **existing values** (e.g., 0 → 999)
- `na.fill()`: Replaces **NULL values** only

### Is na.replace() used for handling missing data or replacing existing values?
**Replacing existing values** - not for NULL handling.

### Can you use na.replace() to replace NULL values? Why or why not?
**No**, because NULL is not a value - it represents absence of data. Use `fillna()` instead.

### How do you use isNull() and isNotNull() for filtering?
```python
df.filter(col("column").isNull())
df.filter(col("column").isNotNull())
```

### What is nanvl() used for (NaN value handling)?
Returns second argument if first is NaN, otherwise returns first argument.
```python
nanvl(col("value"), lit(0))
```

### How do you distinguish between null and NaN in Spark?
- **NULL**: `isNull()`, represents missing/unknown data
- **NaN**: `isnan()`, represents "Not a Number" (invalid floating-point)

### What is the difference between NULL and NaN in terms of meaning and data types?
- **NULL**: Any data type, means "missing/unknown"
- **NaN**: Float/Double only, means "undefined arithmetic result" (0/0, ∞-∞)

### How do NULL and NaN behave differently in comparisons?
- **NULL**: `NULL = NULL` → NULL (unknown)
- **NaN**: `NaN = NaN` → false (IEEE standard)

### What does dropDuplicates() do? How do you specify subset of columns?
```python
df.dropDuplicates()  # All columns
df.dropDuplicates(["col1", "col2"])  # Specific columns
```

### Does dropDuplicates() preserve the order of rows?
No, order is not guaranteed after deduplication.

---

## 3.12.1 COALESCE, NVL, and NVL2 Functions

### What does COALESCE() function do?
Returns the **first non-NULL value** from a list of arguments.

### How many arguments can COALESCE() accept?
Unlimited - takes 2 or more arguments.

### What does NVL() function do? How is it different from COALESCE()?
Returns second argument if first is NULL. NVL takes **exactly 2 arguments**, COALESCE takes 2+.

### How many arguments does NVL() accept?
Exactly **2 arguments**.

### What does NVL2() function do?
Returns second argument if first is NOT NULL, third argument if first IS NULL.

### What are the three arguments in NVL2() and what do they represent?
1. Expression to check
2. Value if NOT NULL
3. Value if NULL

### Compare COALESCE() vs NVL() vs NVL2() - when to use each?
- **COALESCE()**: Multiple fallback values
- **NVL()**: Simple NULL replacement (2 values only)
- **NVL2()**: Different logic for NULL vs NOT NULL

### Is NVL() a SQL standard function or Oracle compatibility function?
**Oracle compatibility function** - COALESCE is SQL standard.

### Can you use COALESCE() with more than 2 arguments? Provide an example.
```python
coalesce(col("phone1"), col("phone2"), col("phone3"), lit("No Phone"))
```

### What does NVL2(NULL, 'Y', 'N') return?
`'N'` (because first argument is NULL)

### What does NVL(NULL, 'X') return?
`'X'` (because first argument is NULL)

### How would you replicate NVL2() behavior using CASE WHEN?
```python
when(col("expr").isNotNull(), lit("value_if_not_null"))
  .otherwise(lit("value_if_null"))
```

---

## 3.13 Column Expressions & SQL Functions

### What is the difference between using column names as strings vs Column objects?
- **Strings**: Simple column references (`"column_name"`)
- **Column objects**: Support complex expressions (`col("name").alias("new_name")`)

### When must you use col() or F.col() instead of string column names?
When using transformations, functions, or operations on columns (e.g., `col("price") * 1.1`).

### What does expr() function allow you to do?
Write **SQL expressions as strings** in DataFrame API.
```python
df.select(expr("price * quantity AS total"))
```

### How do you reference columns from different DataFrames after a join?
Use DataFrame reference: `df1["column"]` or `df1.column` to disambiguate.

### What is the alias() method used for?
Rename columns in transformations.
```python
col("old_name").alias("new_name")
```

### What does name() method return for a Column object?
Returns the column name as a string (if simple column reference).

---

## 3.14 Conditional Logic & Case Statements

### How do you create complex conditional logic using when().when().otherwise()?
```python
when(col("age") < 18, "Minor")
  .when(col("age") < 65, "Adult")
  .otherwise("Senior")
```

### What happens if you don't provide an otherwise() clause?
Returns **NULL** for rows not matching any `when()` condition.

### How do you implement SQL CASE WHEN logic in PySpark?
Use `when().when().otherwise()` chain or `expr()` with SQL syntax.

### Can you nest when() conditions? Provide an example.
```python
when(col("type") == "A",
  when(col("status") == "active", "A-Active")
    .otherwise("A-Inactive"))
.otherwise("Other")
```

---

## 3.15 JSON Functions

### What is the difference between JSON as a file vs JSON as a column value?
- **File**: Read with `spark.read.json()`, auto-infers schema
- **Column**: JSON string within a column, requires parsing with `from_json()`

### Why does Spark convert JSON to Structs (not Maps) by default when parsing?
Structs have **fixed schema** (typed, optimized), while Maps are dynamic (slower, type-unsafe).

### What are the performance implications of Struct vs Map for JSON data?
- **Struct**: Faster, optimized, Catalyst optimization
- **Map**: Slower, no optimization, generic key-value access

### How do you parse JSON strings using from_json()?
```python
from pyspark.sql.functions import from_json
df.select(from_json(col("json_col"), schema))
```

### What schema do you need to provide for from_json()?
Either a DDL string or StructType schema matching the JSON structure.

### How do you convert structs to JSON using to_json()?
```python
from pyspark.sql.functions import to_json
df.select(to_json(col("struct_col")))
```

### What is the difference between from_json() and to_json()?
- `from_json()`: JSON string → Struct
- `to_json()`: Struct/Map/Array → JSON string

### What does get_json_object() do?
Extracts a **single field** from JSON string using JSON path.
```python
get_json_object(col("json"), "$.field.subfield")
```

### How do you use json_tuple() to extract multiple fields?
```python
df.select(json_tuple(col("json"), "field1", "field2"))
```

### What is the difference between from_json() and json_tuple()?
- `from_json()`: Parses entire JSON with schema
- `json_tuple()`: Extracts specific fields without full parsing

### What is the difference between from_json() and get_json_object() in terms of efficiency?
- `get_json_object()`: Extracts one field, lightweight
- `from_json()`: Parses full JSON structure, heavier but more comprehensive

### When should you use from_json() vs get_json_object()?
- `from_json()`: Need multiple fields or full structure
- `get_json_object()`: Need 1-2 specific fields only

### What does schema_of_json() function do?
Infers and returns the schema of a JSON string sample.
```python
schema_of_json(lit('{"name":"John","age":30}'))
```

### What does json_array_length() return?
Number of elements in a JSON array string.

### What does json_object_keys() return?
Array of keys from a JSON object string.

---

## 3.16 Advanced Column Operations

### What does lit() function do? When do you use it?
Creates a literal/constant column value.
```python
df.select(lit(100).alias("constant"))
```

### How do you create a column with constant values across all rows?
```python
df.withColumn("constant", lit("value"))
```

### What is input_file_name() function used for?
Returns the name of the file being read (useful for tracking data source).

### How do you use spark_partition_id() to see data distribution?
```python
df.withColumn("partition", spark_partition_id())
```

### What does hash() function compute?
Computes a hash value (integer) for given columns.

### What is md5() and sha1() used for?
Cryptographic hash functions returning hex strings.
```python
md5(col("data"))  # 32-char hex
sha1(col("data"))  # 40-char hex
```

### How do you use crc32() for checksums?
```python
crc32(col("data"))  # Returns CRC32 checksum
```

### What does base64() and unbase64() do?
- `base64()`: Encode binary to Base64 string
- `unbase64()`: Decode Base64 string to binary

### How do you generate random values using rand() and randn()?
```python
rand()   # Uniform distribution [0, 1)
randn()  # Normal distribution (mean=0, std=1)
```

---

## 3.17 Set Operations on DataFrames

### What is the difference between union() and unionAll()?
**No difference in DataFrame API** - both are aliases and **keep duplicates**.

### What is the critical inconsistency between DataFrame API and Spark SQL?
| Operation | DataFrame API | Spark SQL |
|-----------|--------------|-----------|
| `union()` | Keeps duplicates | Removes duplicates |
| `unionAll()` | Keeps duplicates | Keeps duplicates |

### In DataFrame API, do union() and unionAll() keep or remove duplicates?
Both **keep duplicates** (they're aliases).

### In Spark SQL, does UNION keep or remove duplicates?
**Removes duplicates** (like SQL standard).

### In Spark SQL, does UNION ALL keep or remove duplicates?
**Keeps duplicates**.

### Why is this inconsistency important to remember?
Can cause bugs when switching between DataFrame API and SQL - same code behaves differently.

### What does unionByName() do? How is it different from union()?
Matches columns **by name** instead of position.
```python
df1.unionByName(df2)
```

### What does unionByName() do when schemas differ between DataFrames?
Throws error unless `allowMissingColumns=True` is set.

### What is the allowMissingColumns parameter in unionByName()?
Allows union when schemas don't match - fills missing columns with NULL.

### What does unionByName(allowMissingColumns=True) enable?
Union of DataFrames with different column sets (missing columns filled with NULL).

### When would you use union() vs unionByName()?
- `union()`: Same schema, position-based (faster)
- `unionByName()`: Different column order or partially different schemas

### What is "strict mode" vs "flexible mode" vs "forgiving mode" for unions?
- **Strict**: Schema must match exactly
- **Flexible**: Match by name (`unionByName()`)
- **Forgiving**: Allow missing columns (`allowMissingColumns=True`)

### How do you use intersect() to find common rows?
```python
df1.intersect(df2)  # Returns unique common rows
```

### What is the difference between intersect() and intersectAll()?
- `intersect()`: Returns **unique** common rows (removes duplicates)
- `intersectAll()`: Returns **all** common rows (keeps duplicates based on minimum count)

### Does intersect() show unique or all common records?
**Unique** common records only.

### Does intersectAll() show unique or all common records?
**All** common records (respecting duplicate counts).

### How does intersectAll() handle duplicate counts?
Returns minimum count from both DataFrames for each distinct row.

### What does intersect() answer vs intersectAll()?
- `intersect()`: "What rows exist in both?" (set intersection)
- `intersectAll()`: "What rows appear in both, counting occurrences?" (bag intersection)

### When would you use intersect() vs intersectAll()?
- `intersect()`: Find unique matching records
- `intersectAll()`: Match records with exact duplicate counts

### What does subtract() (or exceptAll()) do?
Returns rows in first DataFrame but **not** in second.
- `subtract()` / `except()`: Removes duplicates (set difference)
- `exceptAll()`: Keeps duplicates (bag difference)

### Do set operations require the same schema in both DataFrames?
Yes, for `union()`, `intersect()`, `subtract()`. Use `unionByName()` for different schemas.

### How do set operations handle duplicates?
- **union/unionAll**: Keeps all
- **intersect**: Removes in result
- **intersectAll**: Keeps minimum count
- **subtract/except**: Removes in result
- **exceptAll**: Subtracts counts

---

## 3.18 Comparison & Logical Operators

### What is the difference between = and <=> (null-safe equality)?
- `=`: NULL compared to anything returns NULL
- `<=>`: NULL <=> NULL returns **true**

### How does = handle NULL comparisons?
Returns **NULL** (not true, not false) - `NULL = NULL` → NULL

### How does <=> handle NULL comparisons?
Treats NULLs as equal values - `NULL <=> NULL` → **true**

### When should you use <=> instead of =?
When you need to **match NULL values** as equal (e.g., joins, comparisons including NULLs).

### What is the DataFrame API equivalent of <=>?
```python
col("a").eqNullSafe(col("b"))
```

### Explain the AND/OR truth table with NULL values.
```
TRUE AND NULL = NULL
FALSE AND NULL = FALSE (short-circuit)
NULL AND NULL = NULL

TRUE OR NULL = TRUE (short-circuit)
FALSE OR NULL = NULL
NULL OR NULL = NULL
```

### What does TRUE AND NULL return?
**NULL** (unknown)

### What does FALSE OR NULL return?
**NULL** (unknown)

### What is short-circuit evaluation in AND/OR operations?
- **AND**: If first is FALSE, don't evaluate second (result is FALSE)
- **OR**: If first is TRUE, don't evaluate second (result is TRUE)

---

## 3.19 Window Functions

### What is rowsBetween in window functions? Provide examples.
Defines window frame based on **physical row positions** relative to current row.
```python
from pyspark.sql.window import Window

# 2 rows before to current row
Window.orderBy("date").rowsBetween(-2, 0)

# Current row to 3 rows after
Window.orderBy("date").rowsBetween(0, 3)

# All preceding rows to current
Window.orderBy("date").rowsBetween(Window.unboundedPreceding, 0)

# Current to all following rows
Window.orderBy("date").rowsBetween(0, Window.unboundedFollowing)
```

### What is rangeBetween in window functions? How does it differ from rowsBetween?
Defines window frame based on **logical value range** in ORDER BY column.
```python
# All rows within 7 days before current row's date
Window.orderBy("date").rangeBetween(-7, 0)

# rowsBetween: Counts 3 physical rows
# rangeBetween: Includes all rows within value range
```

**Key Difference**:
- `rowsBetween`: Physical position (3 rows = exactly 3 rows)
- `rangeBetween`: Logical value (3 units = all rows within ±3 of ORDER BY value)

### Does data shuffling occur during window function operations? Why or why not?
**Yes**, shuffling occurs because data must be **partitioned** (by `partitionBy`) and **sorted** (by `orderBy`) to compute window functions correctly.

### What is the difference between CUME_DIST() and PERCENT_RANK()?
Both are ranking functions but calculate differently:
- **CUME_DIST()**: Cumulative distribution (percentile)
- **PERCENT_RANK()**: Relative rank as percentage

### What does CUME_DIST() calculate?
Percentage of rows with values **≤ current row's value**.
```python
cume_dist().over(Window.orderBy("score"))
# Score: 60, 70, 70, 80, 90
# CUME_DIST: 0.2, 0.6, 0.6, 0.8, 1.0
```

**Formula**: (number of rows ≤ current value) / (total rows)

### What does PERCENT_RANK() calculate?
Relative rank of current row as a **percentage** of total rows.
```python
percent_rank().over(Window.orderBy("score"))
# Score: 60, 70, 70, 80, 90
# PERCENT_RANK: 0.0, 0.25, 0.25, 0.75, 1.0
```

**Formula**: (rank - 1) / (total rows - 1)

### When would you use CUME_DIST() vs PERCENT_RANK()?
- **CUME_DIST()**: "What percentile is this value?" (e.g., top 25% of scores)
- **PERCENT_RANK()**: "What is the relative position?" (e.g., ranking with ties)

### What is asc_nulls_first vs asc_nulls_last in ordering?
Controls NULL placement in ascending sort:
- `asc_nulls_first()`: NULLs appear **first** (before all values)
- `asc_nulls_last()`: NULLs appear **last** (after all values)

```python
df.orderBy(col("column").asc_nulls_first())
df.orderBy(col("column").asc_nulls_last())
```

### Where do NULLs appear with asc_nulls_first?
**At the beginning** of sorted results (smallest position).

### Where do NULLs appear with asc_nulls_last?
**At the end** of sorted results (largest position).

---

## Quick Reference Examples

### Struct Operations
```python
from pyspark.sql.functions import struct, col

# Create struct
df.select(struct("city", "state").alias("location"))

# Access struct fields
df.select(col("location.city"), col("location.state"))

# Nested struct access
df.select("address.location.city")
```

### Type Casting
```python
# Basic casting
df.select(col("price").cast("double"))
df.select(col("date_str").cast("date"))

# With format
from pyspark.sql.functions import to_date, to_timestamp
df.select(to_date(col("date_str"), "yyyy-MM-dd"))
df.select(to_timestamp(col("ts_str"), "yyyy-MM-dd HH:mm:ss"))
```

### NULL Handling
```python
# Drop nulls
df.dropna()
df.dropna(subset=["col1", "col2"])
df.dropna(thresh=2)

# Fill nulls
df.fillna(0)
df.fillna({"col1": 0, "col2": "Unknown"})

# COALESCE
from pyspark.sql.functions import coalesce, lit
df.select(coalesce(col("phone1"), col("phone2"), lit("N/A")))
```

### Conditional Logic
```python
from pyspark.sql.functions import when

df.select(
    when(col("age") < 18, "Minor")
    .when(col("age") < 65, "Adult")
    .otherwise("Senior").alias("age_group")
)
```

### JSON Operations
```python
from pyspark.sql.functions import from_json, to_json, get_json_object
from pyspark.sql.types import StructType, StructField, StringType

# Parse JSON
schema = StructType([
    StructField("name", StringType()),
    StructField("age", StringType())
])
df.select(from_json(col("json_str"), schema).alias("data"))

# Convert to JSON
df.select(to_json(col("struct_col")).alias("json"))

# Extract single field
df.select(get_json_object(col("json"), "$.name"))
```

### Set Operations
```python
# Union (keeps duplicates in DataFrame API)
df1.union(df2)
df1.unionByName(df2, allowMissingColumns=True)

# Intersect
df1.intersect(df2)        # Unique common rows
df1.intersectAll(df2)     # All common rows with counts

# Subtract
df1.subtract(df2)         # df1 - df2 (unique)
df1.exceptAll(df2)        # df1 - df2 (with counts)
```

### Window Functions
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, sum

windowSpec = Window.partitionBy("dept").orderBy("salary")

df.select(
    col("*"),
    row_number().over(windowSpec).alias("row_num"),
    rank().over(windowSpec).alias("rank"),
    lag("salary", 1).over(windowSpec).alias("prev_salary"),
    sum("salary").over(windowSpec.rowsBetween(-2, 0)).alias("rolling_sum")
)
```

---

## Study Tips
1. **Practice casting**: Especially DECIMAL vs FLOAT/DOUBLE for financial data
2. **Master NULL handling**: Understand NULL vs NaN, COALESCE vs NVL
3. **Window functions**: Practice rowsBetween vs rangeBetween
4. **Set operations**: Remember DataFrame API vs SQL UNION behavior
5. **JSON parsing**: Know when to use from_json() vs get_json_object()
6. **Structs**: Understand nested access and when to flatten
