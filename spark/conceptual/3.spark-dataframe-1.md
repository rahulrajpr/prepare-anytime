# ðŸ“Š DataFrame & Dataset API

## 3.1 Basic DataFrame Operations

### 3.1.1 What is the toDF() method and what is its purpose?
* **Definition:** A transformation method that converts collections, RDDs, or Datasets into **DataFrames**.
* **Purpose:** Enables schema application and optimizes execution through the **Catalyst optimizer**.

### 3.1.2 What does the collect() method do? Explain how it brings data from executor nodes to the driver.
* **Definition:** An **action** that retrieves **all data** from distributed partitions to the **driver node**.
* **Process:** Triggers computation across all executors, transfers partition data over the network, and aggregates results in the **driver memory** as a local collection.

### 3.1.3 What are the risks of using collect() on large datasets?
1.  **Driver Memory Overflow:** Risk of `OutOfMemoryError` when the dataset exceeds driver JVM capacity.
2.  **Network Saturation:** High bandwidth consumption during data transfer.
3.  **Application Failure:** Single point of failure that can crash the entire application.

### 3.1.4 What does dataframe.schema.simpleString() return?
* **Returns:** A **tree-structured string representation** of the DataFrame's schema.
* **Format:** Column names with their data types and nullability constraints in a hierarchical format.

### 3.1.5 What does dataframe.rdd.getNumPartitions() return and what is its significance?
* **Returns:** The **number of partitions** in the underlying RDD.
* **Significance:** Directly determines the level of **parallelism**, impacts memory distribution, and helps identify data skew for performance tuning.

---

## 3.1.1 Action Methods - Return Type Comparison

| Method | Returns | Return Type | Key Difference / Danger |
| :--- | :--- | :--- | :--- |
| **first()** | First row of the DataFrame. | Single **Row** object. | Most efficient for single row access. |
| **head()** | First row (default); First *n* rows (head(*n*)). | Single **Row** (default); **Array[Row]** (with *n*). | `head(1)` is functionally similar to `first()`. |
| **take(n)** | First *n* rows from the DataFrame. | **Array[Row]**. | Used for **sampling small data subsets**. |
| **collect()** | All rows from the DataFrame. | **Array[Row]**. | **DANGEROUS:** Transfers the entire dataset to driver memory. |

### Detailed Comparisons
* **first() vs head():** `first()` always returns a single `Row`; `head()` returns a single `Row` by default but `Array[Row]` when a parameter is specified.
* **`first()` vs `head(1)[0]`:** Functionally equivalent, though `first()` is the more efficient API call for single row access.
* **`take(3)` vs `head(3)`:** **No Difference.** Both methods return `Array[Row]` containing the specified number of rows.
* **`take()` vs `collect()`:** Use **`take()`** for sampling, debugging, or previewing small subsets. Use **`collect()`** only when the dataset is guaranteed to be small.

---

## 3.1.2 Sorting Methods - Performance Comparison

| Method | Scope of Sorting | Triggers Shuffle | Network I/O Cost | Use Case |
| :--- | :--- | :--- | :--- | :--- |
| **sort() / orderBy()** | **Global** across all partitions. | **Yes** (required for global order). | **High** (data shuffling between nodes). | Global ordering is mandatory. |
| **sortWithinPartitions()** | **Local** within each partition. | **No** (operates locally). | **Zero** (no cross-partition data movement). | Pre-sorting for windowing, or when local order suffices. |

### Detailed Differences
* **sort() vs orderBy():** They are **aliases**; `sort()` internally calls `orderBy()` and has identical implementation, both performing global sorting.
* **sort() vs sortWithinPartitions():**
    * `sort()` ensures the entire DataFrame is ordered.
    * `sortWithinPartitions()` only ensures rows *within* a single partition are ordered, but not necessarily relative to rows in other partitions.
* **Performance:** `sortWithinPartitions()` is significantly more **efficient** as it avoids the high cost associated with **shuffle operations** and global coordination.

---

## 3.2 Schema Management

### 3.2.1 What are the three approaches to define schemas in Spark DataFrame Reader API?
1.  **Infer Schema:** Automatic detection from the data source (easiest, but slowest).
2.  **Explicit Schema:** Programmatic definition by the developer (best performance).
3.  **Implicit Schema:** Schema is dictated by the file format itself (e.g., Parquet, Avro).

### 3.2.2 What are the performance implications of using inferSchema vs explicit schema specification?
* **inferSchema:** Incurs **performance overhead** because Spark must scan the entire dataset to infer types.
* **Explicit Schema:** Offers **better performance** due to direct type specification, bypassing the initial data scanning phase.

### 3.2.3 What are the two ways to supply an explicit schema for DataFrame Reader?
1.  **StructType/StructField:** Programmatic schema definition for type safety and complex nested schemas (e.g., in production code).
2.  **DDL String:** SQL-like string notation for simple schemas and rapid prototyping (e.g., `"col_a INT, col_b STRING"`).

---

## 3.3 Spark Data Types

### 3.3.1 What are the primitive data types in Spark?
* `StringType`, `IntegerType`, `LongType`, `DoubleType`, `FloatType`, `BooleanType`, `ByteType`, `ShortType`.

### 3.3.2 What complex data types does Spark support?
* **ArrayType:** For ordered collections (lists).
* **MapType:** For key-value pairs (dictionaries/hash maps).
* **StructType:** For nested structures (structs/records).

### 3.3.3 How do you define ArrayType, MapType, and StructType in a schema?
* **ArrayType:** `ArrayType(elementType, containsNull)` (specifies element type and nullability).
* **MapType:** `MapType(keyType, valueType, valueContainsNull)` (specifies key type, value type, and value nullability).
* **StructType:** Defined as a collection of **StructField** elements, enabling hierarchical data structures.

### 3.3.4 What is the difference between nullable=true and nullable=False in schema definition?
* **`nullable=true`:** The column accepts `null` values (**default** behavior).
* **`nullable=false`:** The column rejects `null` values, enforcing a strict data constraint.

### 3.3.5 What are DateType and TimestampType? How do they differ?
* **DateType:** Represents a **calendar date** only (year-month-day), without any time component.
* **TimestampType:** Represents a **date and time** with microsecond precision, crucially including timezone information.

### 3.3.6 What is DecimalType and when should you use it instead of DoubleType?
* **DecimalType:** A **fixed-point decimal** type with precise scale and precision.
* **Use Case:** Use it for **financial calculations**, currency operations, or whenever **exact decimal representation** is required, as floating-point (`DoubleType`) is susceptible to precision loss.

### 3.3.7 What is BinaryType and what are its use cases?
* **BinaryType:** Raw **byte array** storage.
* **Use Cases:** Storing image files, serialized custom objects, cryptographic data, or custom binary formats.
