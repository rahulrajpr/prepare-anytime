# PySpark Interview Preparation Guide

## 3.23 Advanced Transformations

### 3.23.1 What is the difference between map() and flatMap() methods?

**Critical Differences with Detailed Explanation:**

| Aspect                     | map()                  | flatMap()                                           |
| -------------------------- | ---------------------- | --------------------------------------------------- |
| **Input → Output**  | 1 element → 1 element | 1 element → 0, 1, or multiple elements             |
| **Output Structure** | Same number of rows    | Different number of rows (can increase or decrease) |
| **Return Type**      | Returns same structure | Returns flattened structure                         |
| **Use Case**         | Transform elements     | Explode elements into multiple rows                 |

**Detailed Explanation:**

- **map()** is like a 1:1 transformation function. Each input element produces exactly one output element. The number of rows in your RDD/DataFrame remains the same.
- **flatMap()** is like a 1:many transformation. Each input element can produce zero, one, or multiple output elements. This is commonly used for operations that "explode" data.

**Practical Examples:**

```python
# map() example: Convert names to uppercase
# Input: ["alice", "bob"] → Output: ["ALICE", "BOB"] (same 2 elements)

# flatMap() example: Split sentences into words  
# Input: ["hello world", "spark tutorial"] → Output: ["hello", "world", "spark", "tutorial"] (4 elements)
```

### 3.23.2 When would you use map() vs flatMap()?

**Detailed Decision Guide:**

**Use map() when:**

* You need to transform each element independently
* The output has the same number of elements as input
* Examples: data type conversion, string manipulation, mathematical operations

**Use flatMap() when:**

* You need to split elements into multiple parts
* You want to filter out some elements (return empty list)
* Examples: tokenizing text, exploding arrays, parsing nested structures

### 3.23.3 What is the cogroup() operation and how does it differ from join operations?

**Detailed Comparison:**

| Operation           | Output Structure                      | Performance                  | Use Case                       |
| ------------------- | ------------------------------------- | ---------------------------- | ------------------------------ |
| **cogroup()** | Returns (key, (iterable1, iterable2)) | More flexible                | Custom aggregation logic       |
| **join()**    | Returns combined rows                 | Optimized for standard joins | Standard relational operations |

**cogroup() Deep Dive:**

* cogroup() groups elements from multiple RDDs by key and returns pairs where the key is associated with iterables from each RDD
* This gives you more control over how you combine the data compared to standard joins
* Useful when you need custom aggregation logic that doesn't fit standard join patterns

**Example:**

**python**

```
# cogroup() allows you to process related data from multiple sources differently
# You might want to apply different logic to data from different sources
```

### 3.23.4 How do you use mapPartitions() and when is it more efficient than map()?

**Detailed Efficiency Analysis:**

**mapPartitions() vs map() Performance:**

* **map()** : Processes one element at a time - high function call overhead
* **mapPartitions()** : Processes entire partition at once - lower overhead

**When mapPartitions() is More Efficient:**

1. **Database Operations** : Open connection once per partition instead of per row
2. **Expensive Initialization** : Set up resources once per partition
3. **Batch Processing** : More efficient for operations that work better on batches

**Example Use Case:**

**python**

```
def process_partition(iterator):
    # Expensive setup - done once per partition
    db_connection = create_connection()
    results = []
    for row in iterator:
        # Process each row with shared connection
        results.append(process_row(db_connection, row))
    db_connection.close()
    return results

rdd.mapPartitions(process_partition)
```

### 3.23.7 How do you use pivot() to reshape data from long to wide format?

**Detailed Pivot Explanation:**

**What Pivot Does:**

* Transforms data from long format (multiple rows per entity) to wide format (single row per entity with multiple columns)
* Creates new columns based on unique values in a specified column

**Before Pivot (Long Format):**

**text**

```
| id | category | value |
|----|----------|-------|
| 1  | A        | 10    |
| 1  | B        | 20    |
| 2  | A        | 15    |
```

**After Pivot (Wide Format):**

**text**

```
| id | A   | B   |
|----|-----|-----|
| 1  | 10  | 20  |
| 2  | 15  | null|
```

**Complete Syntax:**

**python**

```
df.groupBy("id").pivot("category").agg(sum("value"))
```

## 3.24 Function Comparisons & When to Use What

### 3.24.9 repartition() vs coalesce() - when to use which?

**Detailed Performance and Use Case Analysis:**

| Aspect                      | repartition()                             | coalesce()                                       |
| --------------------------- | ----------------------------------------- | ------------------------------------------------ |
| **Shuffle Behavior**  | ✅ ALWAYS full shuffle                    | ❌ MINIMIZES shuffle (merges partitions locally) |
| **Partition Count**   | Can increase OR decrease                  | Can ONLY decrease partitions                     |
| **Data Distribution** | Completely redistributes data evenly      | Merges existing partitions (may be unbalanced)   |
| **Performance Cost**  | High (network I/O + shuffle)              | Low (minimal data movement)                      |
| **Use Case**          | Need balanced data, increasing partitions | Reducing partitions efficiently after filters    |

**When to Choose repartition():**

* You need to INCREASE the number of partitions
* Data is severely skewed and causing performance issues
* You're preparing data for operations that require good data distribution (joins, aggregations)
* Production environment where reliability is critical

**When to Choose coalesce():**

* You're REDUCING partitions after significant data filtering
* Development environment where speed is more important than perfect balance
* Writing output files and want to control file count without shuffle overhead
* Network bandwidth is limited

### 3.24.14 approx_count_distinct() vs countDistinct() - accuracy vs performance trade-off

**Detailed Trade-off Analysis:**

**countDistinct():**

* **Accuracy** : 100% exact count
* **Performance** : Slow - requires full data processing and deduplication
* **Memory Usage** : High - needs to track all distinct values
* **Use Case** : When exact counts are legally or business-critical

**approx_count_distinct():**

* **Accuracy** : ~95-99.9% accurate (configurable)
* **Performance** : 10-100x faster than countDistinct()
* **Memory Usage** : Low - uses probabilistic data structures
* **Use Case** : Analytics, dashboards, exploratory analysis where approximate counts are acceptable

**Technical Details:**

* approx_count_distinct() uses HyperLogLog algorithm
* Default relative standard deviation: 5% (0.05)
* Can be tuned for better accuracy: `approx_count_distinct(col, 0.01)` for 1% error

## 3.26 Repartitioning Methods - Complete Analysis

### 3.26.17 What is the difference between repartition() and repartitionByRange()?

**Detailed Technical Comparison:**

**repartition() - Hash Partitioning:**

* **Method** : Uses hash function on partitioning columns
* **Data Distribution** : Even distribution based on hash values
* **Ordering** : No ordering guarantee within partitions
* **Performance** : Fast - simple hash computation
* **Use Case** : General-purpose partitioning, joins, aggregations

**repartitionByRange() - Range Partitioning:**

* **Method** : Samples data to determine range boundaries
* **Data Distribution** : Sorted ranges - similar values in same partitions
* **Ordering** : Values sorted within each partition
* **Performance** : Slower - requires data sampling
* **Use Case** : Range queries, sorting operations, time-series data

**Practical Example:**

**python**

```
# Hash partitioning - good for general distribution
df.repartition(4, "user_id")

# Range partitioning - good for time-based queries
df.repartitionByRange(4, "timestamp")
```

## 3.27 Repartition vs Coalesce - Critical Decision Guide

### 3.27.19 What are critical decision points for using repartition()?

**Detailed Decision Framework:**

**Use repartition() in these CRITICAL scenarios:**

1. **Data Skew Prevention**
   * When certain keys dominate your dataset (e.g., 80% of data has same key)
   * Symptoms: Some tasks take much longer than others in Spark UI
   * Risk: OOM errors, failed tasks, poor cluster utilization
2. **Memory Management**
   * When single partition grows too large for executor memory
   * Before operations that expand data size (explode, joins)
   * When you see "java.lang.OutOfMemoryError" in executor logs
3. **Performance Optimization**
   * Before expensive wide transformations (joins, groupBy, window functions)
   * When data distribution affects shuffle performance
   * To ensure optimal parallelism across cluster
4. **Production Reliability**
   * In production pipelines where consistent performance is required
   * When dealing with variable data distributions
   * To make performance more predictable and reproducible

**Real-world Example:**

**python**

```
# ❌ Risky - coalesce might create skewed partitions
filtered_df = large_df.filter(col("status") == "active")
result = filtered_df.coalesce(10)  # Could create OOM if data is skewed

# ✅ Safe - repartition ensures balanced data  
filtered_df = large_df.filter(col("status") == "active")
result = filtered_df.repartition(10)  # Balanced distribution, no OOM risk
```

### 3.27.20 What are critical decision points for using coalesce()?

**Detailed Decision Framework:**

**Use coalesce() in these SAFE scenarios:**

1. **After Significant Data Reduction**
   * When filtering removes most of the data (e.g., 90% reduction)
   * After operations that naturally reduce data size
   * When remaining data fits comfortably in fewer partitions
2. **Development and Testing**
   * During development when speed is more important than perfection
   * For quick iterations and testing
   * When data size is small and skew is unlikely
3. **Output File Management**
   * When writing to storage and you want to control file count
   * To avoid creating too many small files
   * When file size matters more than perfect data distribution
4. **Network-Constrained Environments**
   * When network bandwidth is limited
   * To minimize data transfer costs
   * In environments with expensive network I/O

**Safe Usage Pattern:**

**python**

```
# ✅ Safe coalesce usage
# After filtering out 95% of data
heavily_filtered = large_df.filter(col("year") == 2024)

# Data is now small and unlikely to be skewed
optimized_df = heavily_filtered.coalesce(4)  # Safe and efficient

# Writing to controlled number of files
optimized_df.write.coalesce(1).parquet("output.parquet")
```

**Warning Signs to Avoid coalesce():**

* Data size is still large after filtering
* You see uneven task durations in previous operations
* Dealing with known skewed keys (user_id = 0, null values, etc.)
* In production critical path
