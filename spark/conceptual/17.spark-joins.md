# PySpark Joins - Complete Interview Preparation Guide

**Document Purpose:** Comprehensive guide for PySpark join operations interview preparation with merit-based content depth.

---

## Table of Contents

1. [Join Types & Fundamentals](#1-join-types--fundamentals)
2. [Join Strategies & Execution](#2-join-strategies--execution)
3. [Broadcast Join Deep Dive](#3-broadcast-join-deep-dive)
4. [Join Optimization Techniques](#4-join-optimization-techniques)
5. [Shuffle Operations & Performance](#5-shuffle-operations--performance)
6. [Advanced Topics](#6-advanced-topics)

---

## 1. Join Types & Fundamentals

### 1.1 What are the different join types in PySpark?

**Answer:** PySpark supports five main join types that determine which records appear in the result based on matching conditions.

| Join Type | Returns | Typical Use Case |
|-----------|---------|------------------|
| **Inner** | Only matching records from both tables | Find common/existing relationships |
| **Left (Left Outer)** | All left table + matching right records | Preserve primary dataset with optional details |
| **Right (Right Outer)** | All right table + matching left records | Preserve secondary dataset |
| **Full (Full Outer)** | All records from both tables | Complete data reconciliation |
| **Cross** | Cartesian product of all combinations | Matrix operations (use sparingly) |

**Code Example:**

```python
# Sample data
employees = spark.createDataFrame([
    (1, "John"), (2, "Jane"), (3, "Bob")
], ["id", "name"])

departments = spark.createDataFrame([
    (1, "Engineering"), (2, "Sales")
], ["emp_id", "dept"])

# Inner Join - only matching records
inner_result = employees.join(departments, 
    employees.id == departments.emp_id, "inner")
# Returns: 2 rows (John-Engineering, Jane-Sales)

# Left Join - all employees
left_result = employees.join(departments, 
    employees.id == departments.emp_id, "left")
# Returns: 3 rows (Bob has null department)

# Full Join - all data
full_result = employees.join(departments, 
    employees.id == departments.emp_id, "full")
# Returns: All employees and departments, nulls where no match
```

**Interview Tip:** Always clarify business requirements - INNER may lose data, LEFT/FULL preserve information.

---

### 1.2 How do join types affect result size and NULL handling?

**Answer:** Each join type has specific implications for result cardinality and missing data handling.

| Join Type | Result Size | NULL Behavior | Data Loss Risk |
|-----------|-------------|---------------|----------------|
| Inner | Smallest (intersection) | No NULLs from join | ‚ö†Ô∏è High - unmatched records dropped |
| Left | Left table size + matches | NULLs in right columns | Medium - right table data may be lost |
| Right | Right table size + matches | NULLs in left columns | Medium - left table data may be lost |
| Full | Largest (union) | NULLs on both sides | ‚úÖ None - all data preserved |

**Business Impact:**
- **Inner Join:** Use when relationship must exist (e.g., valid orders with customers)
- **Left Join:** Use to preserve primary dataset (e.g., all employees, with department if assigned)
- **Full Join:** Use for complete auditing or reconciliation scenarios

---

### 1.3 What is the difference between equi-join and non-equi-join?

**Answer:** The join condition determines which join strategies Spark can use.

**Equi-Join** (Equality-based):
- Uses `=` operator in join condition
- Enables hash-based join strategies (faster)
- Most common in analytics

```python
# Equi-join - supports hash joins
df1.join(df2, df1.id == df2.id)
```

**Non-Equi-Join** (Other comparisons):
- Uses `<`, `>`, `<=`, `>=`, `BETWEEN`, `!=`
- Forces nested loop joins (slower)
- Common in range-based matching

```python
# Non-equi-join - uses nested loop
df1.join(df2, (df1.salary > df2.min_salary) & (df1.salary < df2.max_salary))
```

**Performance Impact:** Equi-joins are typically 10-100x faster due to hash table optimization.

---

## 2. Join Strategies & Execution

### 2.1 What are the main join strategies in Spark?

**Answer:** Spark uses different physical execution strategies based on data characteristics and configuration.

| Strategy | When Used | Best For | Performance Characteristic |
|----------|-----------|----------|---------------------------|
| **Broadcast Hash Join** | Small √ó Large table | Dimension-fact joins | Fastest - no large table shuffle |
| **Sort-Merge Join** | Large √ó Large table | Both tables large | Scalable - distributed sorting |
| **Shuffle Hash Join** | Medium √ó Medium | Specific cases | Fast but memory-intensive |
| **Broadcast Nested Loop** | Non-equi joins | Range/inequality conditions | Slowest - nested iteration |
| **Cartesian Join** | No join condition | Cross products | Extremely expensive |

**Key Decision Factor:** Table size relative to `spark.sql.autoBroadcastJoinThreshold` (default: 10MB)

---

### 2.2 How does Sort-Merge Join work?

**Answer:** The default strategy for large-large table joins, involving three phases.

**Execution Flow:**

```
Phase 1: SHUFFLE BOTH TABLES
‚îú‚îÄ Partition by join key (hash partitioning)
‚îú‚îÄ Write partitioned data to disk
‚îî‚îÄ Network transfer to appropriate executors

Phase 2: SORT PARTITIONS
‚îú‚îÄ Each partition sorted by join key
‚îî‚îÄ Enables efficient sequential merge

Phase 3: MERGE PARTITIONS
‚îú‚îÄ Co-located partitions merged
‚îî‚îÄ Output joined records
```

**When Spark Chooses This:**
- Both tables exceed broadcast threshold
- No manual hints provided
- Default fallback when other strategies unavailable

**Performance Characteristics:**
- ‚úÖ Memory-efficient (streams through sorted data)
- ‚ö†Ô∏è Shuffle overhead (both tables shuffled)
- ‚úÖ Scales to very large datasets

---

### 2.3 When does Spark choose Broadcast Join vs Sort-Merge Join?

**Answer:** Decision based on table statistics and configuration thresholds.

**Decision Tree:**

```
Is one table < spark.sql.autoBroadcastJoinThreshold (10MB)?
‚îú‚îÄ YES ‚Üí Can statistics be computed?
‚îÇ   ‚îú‚îÄ YES ‚Üí Broadcast Hash Join ‚úÖ
‚îÇ   ‚îî‚îÄ NO ‚Üí Sort-Merge Join (safe default)
‚îî‚îÄ NO ‚Üí Both tables large
    ‚îî‚îÄ Sort-Merge Join ‚úÖ
```

**Manual Override:**

```python
from pyspark.sql.functions import broadcast

# Force broadcast even if above threshold
large_df.join(broadcast(small_df), "key")
```

**Interview Tip:** Spark uses Catalyst optimizer to analyze table statistics - run `ANALYZE TABLE` for better decisions.

---

### 2.4 What is Shuffle Hash Join and when is it used?

**Answer:** An alternative to sort-merge that trades memory for speed by building hash tables after shuffle.

**Characteristics:**
- Both tables shuffled by join key
- One side builds hash table per partition
- No sorting required (saves time)
- Requires `spark.sql.join.preferSortMergeJoin = false`

**When Better than Sort-Merge:**
- Medium-sized tables (100MB - 1GB)
- Sufficient executor memory available
- Hash table fits comfortably in memory

**Trade-off:** Faster execution but higher memory consumption than sort-merge.

---

### 2.5 What is Cartesian Join and why avoid it?

**Answer:** Produces every possible combination of rows from both tables.

**Result Size:** `Table1_rows √ó Table2_rows`

```python
# Explicit Cartesian join
df1.crossJoin(df2)  # 1000 rows √ó 2000 rows = 2,000,000 rows

# Accidental Cartesian (missing join condition)
df1.join(df2)  # ‚ö†Ô∏è Dangerous!
```

**Why Avoid:**
- Exponential result growth
- Massive shuffle and memory usage
- Usually indicates logic error
- Spark warns: "CARTESIAN JOIN DETECTED"

**Valid Use Cases:** Small lookup tables, matrix operations, intentional all-combinations scenarios.

---

### 2.6 What is Broadcast Nested Loop Join?

**Answer:** Used for non-equi joins when one table is small enough to broadcast.

**Execution:**
1. Small table broadcast to all executors
2. For each row in large table, iterate through all broadcasted rows
3. Evaluate complex join condition

**When Used:**
- Non-equi join conditions (`<`, `>`, `BETWEEN`)
- Cross joins with broadcast hint
- One table small, complex conditions

```python
# Example: Range-based join
df1.join(broadcast(df2), 
    (df1.value >= df2.min_val) & (df1.value <= df2.max_val))
```

**Performance:** Much slower than hash joins but better than non-broadcast nested loop.

---

## 3. Broadcast Join Deep Dive

### 3.1 How does Broadcast Join work internally?

**Answer:** Three-phase execution that eliminates shuffle of the large table.

**Phase 1: Broadcast Phase**
- Driver collects small table data
- Serializes and compresses data
- Distributes to all executors via Torrent protocol (peer-to-peer)

**Phase 2: Hash Build Phase**
- Each executor receives broadcast data
- Builds in-memory hash table: `Map[JoinKey ‚Üí List[Rows]]`
- Handles duplicate keys with value lists

**Phase 3: Probe Phase**
- Large table processed partition-by-partition locally
- Each row probes hash table for matches
- Outputs joined results immediately

**Key Benefit:** Large table never shuffled across network.

```
Visualization:
Driver: [Small Table (5MB)] ‚îÄbroadcast‚îÄ> All Executors
                                          ‚Üì
Executor 1: [Build HashTable] ‚Üê [Scan Large Table Partition 1] ‚Üí Results
Executor 2: [Build HashTable] ‚Üê [Scan Large Table Partition 2] ‚Üí Results
Executor N: [Build HashTable] ‚Üê [Scan Large Table Partition N] ‚Üí Results
```

---

### 3.2 What is spark.sql.autoBroadcastJoinThreshold?

**Answer:** Configuration controlling automatic broadcast join selection based on table size.

**Default:** 10MB (10 √ó 1024 √ó 1024 = 10,485,760 bytes)

```python
# View current setting
spark.conf.get("spark.sql.autoBroadcastJoinThreshold")

# Increase to 50MB
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 52428800)

# Disable auto-broadcast
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
```

**Important:** This compares **estimated table size**, not in-memory size. A 5MB Parquet file may become 50MB in memory!

---

### 3.3 How to manually force broadcast joins?

**Answer:** Use `broadcast()` function or SQL hints to override Spark's automatic decision.

**Method 1: DataFrame API**

```python
from pyspark.sql.functions import broadcast

# Force broadcast of right table
result = large_df.join(broadcast(small_df), "key")

# Can broadcast in any position
result = broadcast(dim_table).join(fact_table, "key")
```

**Method 2: SQL Hints**

```python
spark.sql("""
    SELECT /*+ BROADCAST(d) */ 
           e.*, d.dept_name
    FROM employees e
    JOIN departments d ON e.dept_id = d.id
""")
```

**When to Use Manual Hints:**
- You know table is small but Spark doesn't have statistics
- Table slightly above threshold but broadcast is still faster
- Testing performance comparisons

**Warning:** Forcing broadcast of large tables causes OOM errors!

---

### 3.4 Why does a 5MB Parquet file become 50MB in memory?

**Answer:** Memory representation includes object overhead, decompression, and encoding changes.

**Expansion Factors:**

| Format | Size | Reason |
|--------|------|--------|
| **Parquet (disk)** | 5MB | Compressed (Snappy/GZIP), dictionary encoding, columnar storage |
| **In-Memory (JVM)** | 50MB | Uncompressed, object headers, pointers, UTF-16 strings, row-based |

**Specific Overhead Sources:**

1. **Compression Removal:** Parquet uses Snappy (2-4x compression)
2. **Object Headers:** Java object metadata (12-16 bytes per object)
3. **String Encoding:** UTF-8 (disk) ‚Üí UTF-16 (Java memory) = 2x size
4. **Collection Overhead:** Arrays, Lists add pointer storage
5. **Padding & Alignment:** JVM memory alignment requirements

**Typical Expansion:** 5-10x for Parquet, 3-5x for CSV

**Interview Tip:** Always test with `df.cache().count()` and check Spark UI Storage tab for actual memory usage.

---

### 3.5 How to calculate DataFrame in-memory size?

**Answer:** Multiple methods with varying accuracy levels.

**Method 1: Cache and Check UI (Most Accurate)**

```python
df.cache()
df.count()  # Trigger caching
# Check Spark UI ‚Üí Storage tab ‚Üí See "Size in Memory"
```

**Method 2: Explain Plan (Estimates)**

```python
df.explain("cost")
# Shows estimated size statistics if available
```

**Method 3: Analyze Statistics**

```sql
ANALYZE TABLE my_table COMPUTE STATISTICS;
ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1, col2;

DESCRIBE EXTENDED my_table;  -- View collected stats
```

**Method 4: Approximate Calculation (Rough)**

```python
# Very rough estimate - not production-ready
import sys

def rough_size_estimate(df):
    sample = df.limit(100).collect()
    avg_row_size = sum(sys.getsizeof(row) for row in sample) / len(sample)
    total_rows = df.count()
    return avg_row_size * total_rows

estimated_bytes = rough_size_estimate(df)
```

**Best Practice:** Use Method 1 for accurate sizing before broadcast decisions.

---

### 3.6 What happens if broadcast data exceeds executor memory?

**Answer:** Task failures, OOM errors, and potential job failure unless adaptive execution intervenes.

**Failure Progression:**

```
Broadcast Too Large
    ‚Üì
Executor OutOfMemoryError
    ‚Üì
Task Failure (retry 3x by default)
    ‚Üì
Stage Failure
    ‚Üì
Job Failure (unless Adaptive Execution enabled)
```

**Symptoms in Spark UI:**
- Tasks failing with `java.lang.OutOfMemoryError: Java heap space`
- Excessive garbage collection time
- Uneven memory usage across executors
- Long task durations with eventual failure

**Mitigation Strategies:**

```python
# 1. Increase executor memory
spark.conf.set("spark.executor.memory", "8g")

# 2. Lower broadcast threshold
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 5242880)  # 5MB

# 3. Enable Adaptive Execution (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.autoBroadcastJoinThreshold", 10485760)

# 4. Avoid broadcast hint for large tables
# Don't force: broadcast(large_table)
```

---

### 3.7 Can you broadcast multiple tables in a multi-join query?

**Answer:** Yes, but each broadcast consumes executor memory independently.

```python
# Multiple broadcasts - memory usage adds up
result = large_fact_table \
    .join(broadcast(dim_product), "product_id") \
    .join(broadcast(dim_customer), "customer_id") \
    .join(broadcast(dim_date), "date_id")

# Memory Required Per Executor:
# dim_product (20MB) + dim_customer (15MB) + dim_date (5MB) = 40MB
```

**Considerations:**

| Factor | Impact |
|--------|--------|
| **Memory Multiplication** | Each broadcast stored on every executor |
| **Network Overhead** | Multiple broadcasts = multiple transfers |
| **Broadcast Timeout** | Multiple broadcasts increase chance of timeout |
| **Executor Memory** | Must accommodate all concurrent broadcasts |

**Best Practice:** Limit to 2-3 small dimension table broadcasts per query.

---

### 3.8 How does Broadcast Join perform with skewed data?

**Answer:** Performance depends on which side has skew.

**Broadcast Side Skewed (Small Table):**
- ‚úÖ No significant impact
- Hash table handles skew well
- All executors have same data

**Large Table Skewed:**
- ‚ö†Ô∏è Some tasks process much more data
- Hot spot executors still benefit from local hash lookup
- Much better than sort-merge join with skew
- No shuffle means skew less problematic than distributed joins

**Example Scenario:**

```python
# Customer table (broadcast): 10K customers
# Orders table (large): 100M orders, 1 customer has 50M orders (50% skew)

# Broadcast join handles this well:
# - Customer table broadcast to all executors (fast)
# - Hot executor processes 50M orders but with local hash lookup
# - No shuffle of orders table

# Sort-merge join would be worse:
# - Orders shuffled by customer_id
# - One partition gets 50M orders (memory pressure)
# - One executor becomes bottleneck
```

**Interview Tip:** Broadcast joins naturally handle skew better than shuffle-based joins.

---

### 3.9 How to monitor broadcast join performance in Spark UI?

**Answer:** Multiple tabs provide broadcast-specific metrics.

**SQL Tab:**
- Query plan shows "BroadcastHashJoin" node
- Broadcast exchange stage visible
- Time taken for broadcast vs probe

**Stages Tab:**
- Look for "Broadcast Exchange" stage
- Check duration and data size
- Monitor task distribution

**Storage Tab:**
- Broadcast variables listed with sizes
- Memory consumption per broadcast
- Cached vs broadcasted data

**Key Metrics to Watch:**

| Metric | Good | Bad |
|--------|------|-----|
| **Broadcast Time** | < 1 sec | > 10 sec |
| **Broadcast Size** | < 10MB | > 100MB |
| **Task Duration** | Even distribution | High skew |
| **Shuffle Read** | 0 bytes | High values (broadcast failed) |

```python
# Enable detailed explain
df.explain("formatted")  # See full execution plan
```

---

### 3.10 What is broadcast timeout and how to configure it?

**Answer:** Maximum time Spark waits for broadcast completion before failing.

**Default:** 300 seconds (5 minutes)

```python
# View current timeout
spark.conf.get("spark.sql.broadcastTimeout")

# Increase for slow networks or large broadcasts
spark.conf.set("spark.sql.broadcastTimeout", "600")  # 10 minutes
```

**When to Increase:**
- Slow network between driver and executors
- Large broadcast tables (near threshold)
- Many concurrent broadcasts
- Distributed environments with high latency

**Warning:** Increasing timeout doesn't solve fundamental issues - if broadcast consistently times out, consider:
- Reducing table size
- Using sort-merge join instead
- Adding more network bandwidth
- Optimizing data serialization

---

## 4. Join Optimization Techniques

### 4.1 What are the key strategies for join optimization?

**Answer:** Systematic approach covering strategy selection, partitioning, and data preparation.

**Optimization Hierarchy:**

**1. Strategy Selection (Highest Impact)**
- Choose broadcast for small-large joins
- Use bucketing for repeated large-large joins
- Avoid Cartesian joins

**2. Data Preparation**
- Filter before joining (reduce data volume)
- Select only needed columns (reduce width)
- Handle nulls explicitly (avoid surprises)

**3. Partitioning & Distribution**
- Repartition by join key for large tables
- Use bucketing for frequent join patterns
- Balance partition counts

**4. Caching & Reuse**
- Cache intermediate results for multi-joins
- Persist filtered datasets

**5. Skew Handling**
- Identify and salt skewed keys
- Use broadcast-replicate for extreme skew

```python
# Optimized join pattern
optimized_result = (
    large_table
    .filter(col("date") >= "2024-01-01")           # 1. Filter early
    .select("id", "key_col", "value_col")          # 2. Column pruning
    .repartition(200, "key_col")                   # 3. Pre-partition
    .join(broadcast(small_dim), "key_col")         # 4. Broadcast join
)
```

---

### 4.2 How does bucketing eliminate shuffle in joins?

**Answer:** Pre-organizes data by join key at write time, enabling shuffle-free joins.

**Bucketing Mechanism:**

```
Write Time:
Data ‚Üí Hash(join_key) % num_buckets ‚Üí Bucket File N ‚Üí Disk
(Co-locates same keys in same files across tables)

Read & Join Time:
Bucket 0 from Table A ‚îÄ‚îê
                        ‚îú‚Üí Local Join ‚Üí Results
Bucket 0 from Table B ‚îÄ‚îò
(No shuffle needed - keys already co-located)
```

**Implementation:**

```python
# Step 1: Write bucketed tables
df1.write \
    .bucketBy(50, "join_key") \
    .sortBy("join_key") \
    .mode("overwrite") \
    .saveAsTable("bucketed_table1")

df2.write \
    .bucketBy(50, "join_key") \
    .sortBy("join_key") \
    .mode("overwrite") \
    .saveAsTable("bucketed_table2")

# Step 2: Join without shuffle
result = spark.sql("""
    SELECT * FROM bucketed_table1 t1
    JOIN bucketed_table2 t2 ON t1.join_key = t2.join_key
""")

# Verify in explain plan - should show "BucketedSortMergeJoin"
result.explain()
```

**Benefits:**
- ‚úÖ Zero shuffle (massive performance gain)
- ‚úÖ Reduced network I/O
- ‚úÖ Better for repeated joins on same key

**Requirements:**
- Same number of buckets for both tables
- Same bucketing column(s)
- Tables must be read from catalog (not DataFrames)

---

### 4.3 How to verify bucketing is being utilized?

**Answer:** Check execution plan and table metadata.

**Method 1: Explain Plan**

```python
df.explain()
# Look for "BucketedSortMergeJoin" (not "SortMergeJoin")
```

**Method 2: Table Properties**

```sql
DESCRIBE EXTENDED bucketed_table;
-- Look for: Num Buckets: 50
--           Bucket Columns: [join_key]
```

**Method 3: Spark UI SQL Tab**
- Query plan visualization
- Should show no "Exchange" (shuffle) stage before join

**Common Issues:**
- Reading as DataFrame instead of from catalog breaks bucketing
- Mismatched bucket counts between tables
- Different bucketing columns

---

### 4.4 When should you pre-repartition DataFrames before joining?

**Answer:** When both tables are large and will undergo sort-merge join.

**Repartitioning Benefits:**

```python
# Without repartitioning - Spark may create suboptimal partitions
df1.join(df2, "key")  # Default partitioning may not align

# With repartitioning - explicit control over distribution
df1_repartitioned = df1.repartition(200, "key")
df2_repartitioned = df2.repartition(200, "key")
result = df1_repartitioned.join(df2_repartitioned, "key")
```

**When to Repartition:**
- Both tables large (> 1GB)
- Join key has good cardinality
- Current partition count suboptimal
- Seeing skew in join tasks

**When NOT to Repartition:**
- One table small (use broadcast instead)
- Tables already well-partitioned
- Would add unnecessary shuffle

**Optimal Partition Count:**
- Start with `spark.sql.shuffle.partitions` (default: 200)
- Aim for 100-200MB per partition
- Formula: `total_data_size / 150MB`

---

### 4.5 What is salting and how does it address data skew?

**Answer:** Technique to distribute skewed keys across multiple partitions by adding random suffixes.

**Salting Process:**

```python
from pyspark.sql.functions import rand, concat, lit, col

# Step 1: Add salt to large table (skewed side)
salt_range = 10  # Use 0-9 as salt values
large_salted = large_df.withColumn(
    "salted_key", 
    concat(col("join_key"), lit("_"), (rand() * salt_range).cast("int"))
)

# Step 2: Replicate small table for all salt values
small_salted = small_df \
    .crossJoin(spark.range(salt_range).toDF("salt")) \
    .withColumn("salted_key", concat(col("join_key"), lit("_"), col("salt")))

# Step 3: Join on salted keys
result = large_salted.join(small_salted, "salted_key") \
    .drop("salted_key", "salt")
```

**How It Works:**

```
Before Salting:
Key "A": 1M records ‚Üí Single partition (bottleneck)

After Salting (salt_range=10):
Key "A_0": 100K records ‚Üí Partition 1
Key "A_1": 100K records ‚Üí Partition 2
...
Key "A_9": 100K records ‚Üí Partition 10
(Distributes load across 10 partitions)
```

**When to Use:**
- Severe key skew (80/20 or worse distribution)
- Small table can be replicated (memory allows)
- Broadcast not feasible

**Trade-off:** Small table replicated N times (memory cost).

---

### 4.6 How to identify skewed join keys?

**Answer:** Analyze key distribution and monitor Spark UI task durations.

**Method 1: Key Distribution Analysis**

```python
# Identify top skewed keys
key_dist = df.groupBy("join_key") \
    .count() \
    .orderBy(col("count").desc())

key_dist.show(20)

# Calculate skew ratio
total_records = df.count()
top_key_count = key_dist.first()["count"]
skew_ratio = (top_key_count / total_records) * 100
print(f"Top key represents {skew_ratio}% of data")
```

**Method 2: Percentile Analysis**

```python
from pyspark.sql.functions import expr

percentiles = df.groupBy("join_key").count() \
    .selectExpr(
        "percentile_approx(count, 0.5) as median",
        "percentile_approx(count, 0.95) as p95",
        "percentile_approx(count, 0.99) as p99",
        "max(count) as max_count"
    )
percentiles.show()

# If max_count >> p99, you have severe skew
```

**Method 3: Spark UI Inspection**
- **Stages Tab:** Task duration histogram
- Look for few tasks taking 10x longer than median
- Check "Max" vs "Median" task duration

**Skew Indicators:**
- One task takes > 10x median duration
- Top key > 5% of total records
- p99/median ratio > 100

---

### 4.7 How does filter pushdown improve join performance?

**Answer:** Reduces data volume before join operation, minimizing shuffle and memory usage.

**Filter Pushdown Principles:**

```python
# ‚ùå BAD: Filter after join (processes all data)
result = large_df.join(small_df, "key") \
    .filter(col("date") >= "2024-01-01")

# ‚úÖ GOOD: Filter before join (reduces join input)
filtered_large = large_df.filter(col("date") >= "2024-01-01")
result = filtered_large.join(small_df, "key")
```

**Performance Impact:**

```
Without Filter Pushdown:
100M rows ‚Üí Shuffle ‚Üí Join ‚Üí Filter ‚Üí 10M rows (output)
(Shuffle 100M rows, 90% wasted work)

With Filter Pushdown:
100M rows ‚Üí Filter ‚Üí 10M rows ‚Üí Shuffle ‚Üí Join ‚Üí 10M rows (output)
(Shuffle only 10M rows, 90% reduction)
```

**Automatic Pushdown:**
Spark's Catalyst optimizer automatically pushes filters down when:
- Filter is on columns from one table only
- Predicate is deterministic
- Data source supports filter pushdown (Parquet, ORC, JDBC)

**Manual Optimization:**

```python
# Explicit filtering for complex predicates
df1_filtered = df1.filter(
    (col("date") >= "2024-01-01") & 
    (col("status") == "active")
)
df2_filtered = df2.filter(col("amount") > 1000)

result = df1_filtered.join(df2_filtered, "key")
```

---

### 4.8 Why select only required columns before joining?

**Answer:** Column pruning reduces memory footprint, shuffle data size, and network I/O.

**Column Pruning Impact:**

```python
# ‚ùå BAD: Join all columns (unnecessary data transfer)
df1.join(df2, "key")  # df1 has 50 columns, df2 has 30 columns

# ‚úÖ GOOD: Select only needed columns
df1.select("key", "col1", "col2") \
    .join(df2.select("key", "col3"), "key")
```

**Performance Benefits:**

| Scenario | Without Pruning | With Pruning | Improvement |
|----------|-----------------|--------------|-------------|
| **Columns** | 50 + 30 = 80 | 3 + 2 = 5 | 16x fewer columns |
| **Row Size** | ~5KB | ~300 bytes | 17x smaller rows |
| **Shuffle** | 500GB | 30GB | 17x less network |
| **Memory** | 50GB | 3GB | 17x less RAM |

**Automatic Pruning:**
Catalyst optimizer performs column pruning when possible, but explicit selection:
- Makes intent clear
- Ensures pruning before custom UDFs
- Works better with complex queries

---

### 4.9 Should you cache before or after filtering?

**Answer:** Always filter first, then cache - to avoid wasting memory on unused data.

```python
# ‚ùå BAD: Cache entire table, then filter
cached_all = large_df.cache()
filtered = cached_all.filter("date >= '2024-01-01'")  # Wastes memory on old data

# ‚úÖ GOOD: Filter first, then cache
filtered = large_df.filter("date >= '2024-01-01'")
cached_filtered = filtered.cache()  # Only caches relevant data
```

**Memory Impact Example:**

```
Dataset: 10GB total, 1GB after filtering

Bad Pattern:
- Cache 10GB ‚Üí Use 1GB ‚Üí Waste 9GB memory

Good Pattern:
- Filter to 1GB ‚Üí Cache 1GB ‚Üí Efficient memory use
```

**Caching Strategy for Joins:**

```python
# Multi-join scenario
base_data = spark.table("fact_table") \
    .filter(col("year") == 2024) \
    .select("id", "key1", "key2", "amount") \
    .cache()  # Cache after filter & projection

# Reuse cached data
result1 = base_data.join(dim1, "key1")
result2 = base_data.join(dim2, "key2")
# Base data read from cache, not recomputed
```

---

### 4.10 How to optimize joins when both tables are large?

**Answer:** Multi-pronged approach combining several techniques.

**Strategy Combination:**

```python
# 1. Statistics collection
spark.sql("ANALYZE TABLE large_table1 COMPUTE STATISTICS")
spark.sql("ANALYZE TABLE large_table2 COMPUTE STATISTICS")

# 2. Optimal partition count
spark.conf.set("spark.sql.shuffle.partitions", "300")

# 3. Pre-filter and project
table1_prep = spark.table("large_table1") \
    .filter(col("date") >= "2024-01-01") \
    .select("join_key", "col1", "col2")

table2_prep = spark.table("large_table2") \
    .filter(col("status") == "active") \
    .select("join_key", "col3")

# 4. Repartition if needed
table1_repart = table1_prep.repartition(300, "join_key")
table2_repart = table2_prep.repartition(300, "join_key")

# 5. Join and cache if reused
result = table1_repart.join(table2_repart, "join_key")

# 6. If result used multiple times
result.cache()
```

**Advanced: Bucketing for Repeated Joins**

```python
# For ETL pipelines with repeated join patterns
spark.sql("""
    CREATE TABLE bucketed_table1
    USING parquet
    CLUSTERED BY (join_key) INTO 100 BUCKETS
    AS SELECT * FROM source_table1
""")

# Subsequent joins are shuffle-free
```

**Monitoring:**
- Check Spark UI for shuffle size
- Monitor task skew
- Watch for spill to disk

---

## 5. Shuffle Operations & Performance

### 5.1 What is shuffle in the context of joins?

**Answer:** Data redistribution across executors to co-locate records with matching join keys.

**Shuffle Phases:**

```
MAP PHASE (Exchange):
Executor 1: [Read Data] ‚Üí Hash(key) ‚Üí [Write to Shuffle Files]
Executor 2: [Read Data] ‚Üí Hash(key) ‚Üí [Write to Shuffle Files]
           ‚Üì                            ‚Üì
       [Shuffle Write]          [Local Disk Storage]

REDUCE PHASE:
[Network Transfer] ‚Üí Executor 1: [Read Shuffle Data] ‚Üí [Sort & Merge]
                  ‚Üí Executor 2: [Read Shuffle Data] ‚Üí [Sort & Merge]
```

**Shuffle Metrics:**
- **Shuffle Write:** Data written during map phase (bytes, records)
- **Shuffle Read:** Data read during reduce phase
- **Shuffle Spill:** Data spilled to disk when memory insufficient

---

### 5.2 How to minimize shuffle in join operations?

**Answer:** Use shuffle-free strategies when possible, otherwise optimize shuffle configuration.

**Shuffle Elimination:**

1. **Broadcast Joins** - no shuffle of large table
2. **Bucketed Tables** - pre-shuffled at write time
3. **Co-partitioned Data** - already distributed correctly

**Shuffle Optimization:**

```python
# 1. Appropriate partition count
spark.conf.set("spark.sql.shuffle.partitions", "200")

# 2. Increase shuffle memory
spark.conf.set("spark.sql.shuffle.spill.compress", "true")
spark.conf.set("spark.sql.shuffle.compress", "true")

# 3. Reduce data volume before shuffle
df.filter(...).select(...).join(...)
```

**Monitoring Shuffle:**

```python
# Enable metrics
spark.conf.set("spark.sql.statistics.histogram.enabled", "true")

# Check shuffle in explain
df.explain()  # Look for "Exchange" operators
```

---

### 5.3 What is shuffle spill and how does it affect performance?

**Answer:** Occurs when shuffle data exceeds available memory, forcing disk writes.

**Spill Process:**

```
Memory Fills ‚Üí Spill to Disk ‚Üí Later Read from Disk
(Fast RAM)     (Slow I/O)       (Slow I/O)
```

**Performance Impact:**

| Metric | Without Spill | With Spill | Degradation |
|--------|---------------|------------|-------------|
| **Speed** | RAM speed (GB/s) | Disk speed (MB/s) | 10-100x slower |
| **Task Duration** | Seconds | Minutes | 10-60x longer |
| **Resource** | Memory only | Memory + Disk I/O | I/O contention |

**Detection in Spark UI:**

```
Stages Tab ‚Üí Stage Details:
- Spill (Memory): 500 MB ‚Üê Amount spilled from memory
- Spill (Disk): 500 MB ‚Üê Amount written to disk
```

**Prevention:**

```python
# 1. Increase executor memory
spark.conf.set("spark.executor.memory", "8g")

# 2. Increase memory fraction for execution
spark.conf.set("spark.memory.fraction", "0.8")  # Default: 0.6

# 3. Reduce partition data size
spark.conf.set("spark.sql.shuffle.partitions", "400")  # More, smaller partitions

# 4. Enable compression
spark.conf.set("spark.sql.shuffle.compress", "true")
```

---

### 5.4 How does spark.sql.shuffle.partitions affect join performance?

**Answer:** Controls parallelism and partition size during shuffle operations.

**Impact Analysis:**

```python
# Default
spark.conf.get("spark.sql.shuffle.partitions")  # 200
```

**Too Few Partitions:**
- ‚ùå Large partition size ‚Üí Memory pressure ‚Üí Spill
- ‚ùå Underutilized cluster (fewer tasks than cores)
- ‚ùå Skew more likely

**Too Many Partitions:**
- ‚ùå Small partition size ‚Üí Task overhead
- ‚ùå Scheduler overhead
- ‚ùå Many small files

**Optimal Calculation:**

```
optimal_partitions = total_shuffle_data / target_partition_size

Example:
- Total data: 100GB
- Target size: 128MB per partition
- Optimal: 100GB / 128MB = 800 partitions

spark.conf.set("spark.sql.shuffle.partitions", "800")
```

**Dynamic Adjustment (Spark 3.0+):**

```python
# Enable Adaptive Query Execution
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
# Spark automatically adjusts partition count at runtime
```

---

### 5.5 How to identify shuffle-heavy joins in Spark UI?

**Answer:** Examine multiple UI tabs for shuffle indicators.

**SQL Tab Investigation:**

```
Query Plan Visualization:
‚îú‚îÄ Look for "Exchange" nodes (shuffle operations)
‚îú‚îÄ Check operation details for "shuffle partitions"
‚îî‚îÄ Compare shuffle vs non-shuffle time
```

**Stages Tab Analysis:**

```
Stage Details:
‚îú‚îÄ Input: 100GB
‚îú‚îÄ Shuffle Write: 100GB ‚Üê Large shuffle
‚îú‚îÄ Shuffle Read: 100GB  ‚Üê Both tables shuffled
‚îú‚îÄ Duration: 30 min     ‚Üê Long shuffle time
‚îî‚îÄ Spill: 50GB         ‚Üê Memory pressure
```

**Warning Signs:**

| Indicator | Value | Interpretation |
|-----------|-------|----------------|
| **Shuffle Write** | > 50% of input | Heavy shuffle |
| **Shuffle Time** | > 50% of stage time | Shuffle bottleneck |
| **Spill (Disk)** | > 0 | Memory insufficient |
| **Task Skew** | Max/Median > 10 | Data skew issue |

**Comparison:**

```
Broadcast Join:
- Shuffle Write: 0 bytes ‚úÖ
- Shuffle Read: 0 bytes ‚úÖ

Sort-Merge Join:
- Shuffle Write: 100GB ‚ö†Ô∏è
- Shuffle Read: 100GB ‚ö†Ô∏è
```

---

## 6. Advanced Topics

### 6.1 What are semi joins and anti joins?

**Answer:** Special join types that return rows from left table based on right table existence.

**Semi Join (LEFT SEMI):**
Returns left rows that **have matches** in right table (no right columns returned).

```python
# Find employees who have placed orders
employees.join(orders, employees.id == orders.emp_id, "left_semi")
# Returns: Only employee columns, for employees with orders

# Equivalent to (but more efficient than):
employees.join(orders, "emp_id").select(employees["*"]).distinct()
```

**Anti Join (LEFT ANTI):**
Returns left rows that **have NO matches** in right table.

```python
# Find employees who have NOT placed orders
employees.join(orders, employees.id == orders.emp_id, "left_anti")
# Returns: Only employee columns, for employees without orders

# Equivalent to (but more efficient than):
employees.join(orders, "emp_id", "left") \
    .filter(orders.id.isNull()) \
    .select(employees["*"])
```

**Performance Benefits:**
- No duplication handling needed
- No right table columns transferred
- Optimized execution plans

**Use Cases:**
- **Semi:** Filtering based on existence
- **Anti:** Finding missing/unmatched records

---

### 6.2 How does Adaptive Query Execution improve joins?

**Answer:** AQE dynamically optimizes joins at runtime based on actual data statistics.

**AQE Features for Joins:**

```python
# Enable AQE (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.autoBroadcastJoinThreshold", "10485760")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
```

**Runtime Optimizations:**

1. **Dynamic Join Strategy:**
   - Starts as sort-merge join
   - Switches to broadcast if actual size permits
   - Based on shuffle file statistics

2. **Dynamic Coalescing:**
   - Reduces partition count if too many small partitions
   - Improves efficiency without manual tuning

3. **Dynamic Skew Handling:**
   - Detects skewed partitions at runtime
   - Splits large partitions automatically
   - Enables `spark.sql.adaptive.skewJoin.enabled`

**Before AQE:**
```
Plan: Sort-Merge Join (based on estimates)
Execute: Sort-Merge Join (even if broadcast better)
```

**With AQE:**
```
Plan: Sort-Merge Join (initial plan)
Runtime: Collect statistics ‚Üí Switch to Broadcast Join ‚úÖ
```

---

### 6.3 How does data type affect join performance?

**Answer:** Join key data types significantly impact hashing, comparison, and memory usage.

**Performance Ranking (Fastest to Slowest):**

| Data Type | Hash Speed | Comparison | Memory | Use for Joins |
|-----------|------------|------------|--------|---------------|
| **Integer/Long** | ‚ö° Fastest | Direct | Compact | ‚úÖ Best choice |
| **Timestamp** | ‚ö° Fast | Numeric | Compact | ‚úÖ Good |
| **String (short)** | üî∂ Moderate | Character-by-character | Variable | ‚ö†Ô∏è OK if short |
| **String (long)** | üêå Slow | Length + content | Large | ‚ùå Avoid |
| **Decimal** | üî∂ Moderate | Precision handling | Larger | ‚ö†Ô∏è Necessary for precision |
| **Struct/Array** | üêå Very Slow | Complex | Large | ‚ùå Avoid |

**Optimization Example:**

```python
# ‚ùå SLOW: String join key
df1.join(df2, df1.customer_name == df2.customer_name)

# ‚úÖ FAST: Integer join key
# Add surrogate key at source
df1.join(df2, df1.customer_id == df2.customer_id)
```

**String Join Tips:**
- Use hash codes for long strings: `hash(col("long_string"))`
- Normalize case if case-insensitive: `lower(col("name"))`
- Trim whitespace: `trim(col("name"))`

---

### 6.4 How do NULL values in join keys affect joins?

**Answer:** NULLs never match in equi-joins, potentially causing data loss or skew.

**NULL Behavior by Join Type:**

```python
df1 = spark.createDataFrame([(1, "A"), (None, "B")], ["id", "val1"])
df2 = spark.createDataFrame([(1, "X"), (None, "Y")], ["id", "val2"])

# Inner join - NULLs don't match
df1.join(df2, "id", "inner")
# Result: Only (1, "A", "X") - NULL rows dropped

# Left join - preserves left NULLs
df1.join(df2, "id", "left")
# Result: (1, "A", "X"), (None, "B", None)

# Full join - preserves all NULLs separately
df1.join(df2, "id", "full")
# Result: (1, "A", "X"), (None, "B", None), (None, None, "Y")
```

**Common Issues:**

1. **Unintentional Data Loss:**
```python
# Business logic expects all records, but NULLs get dropped
orders.join(customers, orders.customer_id == customers.id, "inner")
# Orders with NULL customer_id silently excluded!
```

2. **NULL Skew:**
```python
# Many NULLs in join key ‚Üí skewed partition
# If using left join, NULL partition becomes huge
```

**Handling Strategies:**

```python
# Strategy 1: Filter NULLs if business allows
df1.filter(col("join_key").isNotNull()) \
    .join(df2.filter(col("join_key").isNotNull()), "join_key")

# Strategy 2: Replace NULLs with placeholder
df1.fillna({"join_key": -9999}) \
    .join(df2.fillna({"join_key": -9999}), "join_key")

# Strategy 3: Use null-safe equality (joins NULLs with NULLs)
df1.join(df2, df1.join_key.eqNullSafe(df2.join_key))
```

---

### 6.5 What is the relationship between partitioning and bucketing?

**Answer:** Complementary data organization techniques serving different purposes.

**Comparison:**

| Aspect | Partitioning | Bucketing |
|--------|-------------|-----------|
| **Purpose** | Filter optimization | Join optimization |
| **Organization** | Directory-based | File-based within partitions |
| **Granularity** | Coarse (date, region) | Fine (ID, key) |
| **Predicate Pushdown** | ‚úÖ Enabled | ‚ùå Not applicable |
| **Join Optimization** | ‚ùå Limited | ‚úÖ Shuffle-free joins |

**Combined Usage:**

```python
# Partition by date (filter optimization)
# Bucket by customer_id (join optimization)
df.write \
    .partitionBy("date") \
    .bucketBy(100, "customer_id") \
    .sortBy("customer_id") \
    .saveAsTable("optimized_table")

# Query benefits from both:
spark.sql("""
    SELECT * FROM optimized_table
    WHERE date = '2024-01-01'  -- Partition pruning
""").join(other_table, "customer_id")  -- Bucketed join
```

**Physical Layout:**

```
/warehouse/optimized_table/
‚îú‚îÄ‚îÄ date=2024-01-01/
‚îÇ   ‚îú‚îÄ‚îÄ part-00000-bucket-00  ‚Üê customer_id hash 0
‚îÇ   ‚îú‚îÄ‚îÄ part-00001-bucket-01  ‚Üê customer_id hash 1
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ date=2024-01-02/
‚îÇ   ‚îú‚îÄ‚îÄ part-00000-bucket-00
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

**Best Practice:** Partition by low-cardinality column (date, region), bucket by high-cardinality join key (ID).

---

### 6.6 How to collect and use table statistics for join optimization?

**Answer:** Statistics enable Spark's Catalyst optimizer to make better join strategy decisions.

**Statistics Collection:**

```sql
-- Basic table statistics (row count, size)
ANALYZE TABLE my_table COMPUTE STATISTICS;

-- Column-level statistics (cardinality, min/max, nulls)
ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1, col2, join_key;

-- View statistics
DESCRIBE EXTENDED my_table;
DESCRIBE EXTENDED my_table col_name;  -- Column-specific
```

**What Statistics Provide:**

```
Table Statistics:
‚îú‚îÄ Row Count: 1,000,000
‚îú‚îÄ Total Size: 500MB (on disk)
‚îî‚îÄ Last Analyzed: 2024-11-11

Column Statistics (join_key):
‚îú‚îÄ Distinct Count: 10,000
‚îú‚îÄ Null Count: 50
‚îú‚îÄ Min Value: 1
‚îî‚îÄ Max Value: 999999
```

**Impact on Join Strategy:**

```python
# Without statistics
df1.join(df2, "key")
# Spark guesses: Uses sort-merge join (safe default)

# After ANALYZE TABLE
spark.sql("ANALYZE TABLE df1 COMPUTE STATISTICS")
spark.sql("ANALYZE TABLE df2 COMPUTE STATISTICS")

df1.join(df2, "key")
# Spark knows: df2 is 8MB ‚Üí Uses broadcast join ‚úÖ
```

**Best Practices:**
- Run `ANALYZE TABLE` after major data changes
- Analyze join key columns specifically
- Re-analyze periodically in ETL pipelines
- Check if statistics are stale: `DESCRIBE EXTENDED`

---

### 6.7 How does join reordering optimization work?

**Answer:** Catalyst optimizer reorders multi-join queries to minimize intermediate result sizes.

**Concept:**

```
Query:
A JOIN B JOIN C

Possible Orders:
1. (A JOIN B) JOIN C
2. (A JOIN C) JOIN B
3. (B JOIN C) JOIN A

Optimizer chooses order that minimizes intermediate data volume.
```

**Optimization Example:**

```python
# Original query
result = large_fact \
    .join(dimension_a, "key_a") \
    .join(dimension_b, "key_b") \
    .join(dimension_c, "key_c")

# Catalyst may reorder to:
# 1. Join most selective dimension first (reduces data early)
# 2. Broadcast small dimensions
# 3. Minimize shuffle operations
```

**Factors Influencing Reordering:**
- Table sizes (from statistics)
- Filter selectivity
- Join key cardinality
- Available broadcast candidates

**Configuration:**

```python
# Enable/disable join reordering
spark.conf.set("spark.sql.cbo.enabled", "true")  # Cost-Based Optimizer
spark.conf.set("spark.sql.cbo.joinReorder.enabled", "true")
```

**Manual Control:**

```python
# Force specific join order (disable reordering for this query)
spark.conf.set("spark.sql.cbo.joinReorder.enabled", "false")

# Or use explicit ordering with parentheses in SQL
spark.sql("SELECT * FROM (a JOIN b ON ...) JOIN c ON ...")
```

---

## 7. Scenario-Based Interview Questions

### 7.1 Scenario: E-commerce Order-Customer Join with Missing Customers

**Question:** You have 10M orders and 5M customers. Some orders have customer_ids that don't exist in the customer table (deleted accounts). How do you identify and handle these orphaned orders?

**Answer:** Use LEFT ANTI JOIN to find orphaned records, then decide on business logic.

```python
# Step 1: Identify orphaned orders
orphaned_orders = orders.join(
    customers, 
    orders.customer_id == customers.id, 
    "left_anti"
)

print(f"Found {orphaned_orders.count()} orphaned orders")

# Step 2: Business logic - three options

# Option A: Flag orphaned orders with special customer
orphaned_flagged = orphaned_orders.withColumn("customer_id", lit(-1))
valid_orders = orders.join(customers, "customer_id", "inner")
all_orders = valid_orders.union(orphaned_flagged)

# Option B: Create separate reporting table
orphaned_orders.write.mode("overwrite").saveAsTable("orphaned_orders_audit")

# Option C: Join with LEFT join and filter later
orders_with_customers = orders.join(customers, "customer_id", "left")
# Separate processing based on customer.id IS NULL
```

**Interview Points:**
- Shows understanding of data quality issues
- Demonstrates knowledge of semi/anti joins
- Considers business implications

---

### 7.2 Scenario: Real-Time Stream Joining Historical Dimension

**Question:** You have a streaming DataFrame of transactions (10K/sec) that needs to join with a slowly-changing customer dimension table (5M rows, updated daily). How do you optimize this?

**Answer:** Broadcast the dimension table and handle updates efficiently.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast, current_timestamp

# Load and broadcast dimension table
customer_dim = spark.table("customer_dimension").cache()
broadcast_customers = broadcast(customer_dim)

# Stream join
transaction_stream = spark.readStream \
    .format("kafka") \
    .option("subscribe", "transactions") \
    .load()

# Join streaming data with broadcasted dimension
enriched_stream = transaction_stream.join(
    broadcast_customers,
    transaction_stream.customer_id == broadcast_customers.id,
    "left"  # Left join to preserve all transactions
)

# Write to sink
enriched_stream.writeStream \
    .format("parquet") \
    .option("checkpointLocation", "/tmp/checkpoint") \
    .start()

# Challenge: Dimension table updates
# Solution: Refresh broadcast periodically
def refresh_dimension():
    global broadcast_customers
    customer_dim = spark.table("customer_dimension").cache()
    broadcast_customers = broadcast(customer_dim)

# Schedule refresh (e.g., every hour)
```

**Optimization Strategies:**
- ‚úÖ Broadcast dimension (small, frequently accessed)
- ‚úÖ Cache dimension to avoid repeated reads
- ‚úÖ LEFT join preserves all stream records
- ‚ö†Ô∏è Handle dimension updates with scheduled refresh
- ‚ö†Ô∏è Monitor broadcast size as dimension grows

**Edge Case:** What if dimension table grows beyond broadcast threshold?
- Switch to Delta Lake with version travel
- Use stream-to-stream join with compacted dimension stream
- Implement incremental dimension loading

---

### 7.3 Scenario: Three-Way Join Performance Problem

**Question:** You need to join three tables: Facts (100GB), DimProduct (500MB), DimCustomer (200MB). Current query takes 2 hours. How do you optimize?

**Answer:** Strategic join ordering with mixed strategies.

```python
# ‚ùå POOR: Sequential joins without strategy
result = facts \
    .join(dim_product, "product_id") \
    .join(dim_customer, "customer_id")
# Problem: First join might create huge intermediate result

# ‚úÖ OPTIMAL: Broadcast both dimensions, filter early
result = (
    facts
    .filter(col("date") >= "2024-01-01")  # Reduce facts first
    .select("product_id", "customer_id", "amount", "date")  # Column pruning
    .join(broadcast(dim_product), "product_id")  # Broadcast join 1
    .join(broadcast(dim_customer), "customer_id")  # Broadcast join 2
)

# ‚úÖ ALTERNATIVE: If dimensions too large to broadcast
# Pre-filter dimensions, then use sort-merge
filtered_products = dim_product.filter(col("category") == "Electronics")
filtered_customers = dim_customer.filter(col("region") == "US")

result = facts \
    .repartition(400, "product_id") \
    .join(filtered_products.repartition(400, "product_id"), "product_id") \
    .repartition(400, "customer_id") \
    .join(filtered_customers.repartition(400, "customer_id"), "customer_id")
```

**Performance Analysis:**

| Approach | Shuffle | Time | Best For |
|----------|---------|------|----------|
| Sequential joins | 3 shuffles | 2 hours | ‚ùå Never |
| Broadcast both dims | 0 shuffles | 10 min | ‚úÖ Dims < threshold |
| Mixed strategy | 1 shuffle | 30 min | ‚ö†Ô∏è One dim large |
| Bucketed tables | 0 shuffles | 5 min | ‚úÖ Repeated queries |

---

### 7.4 Scenario: Self-Join for Hierarchical Data

**Question:** You have an employee table with manager_id referencing employee_id. Find all employees with their manager names. Handle cases where CEO has NULL manager.

**Answer:** Self-join with proper NULL handling.

```python
# Employee data
employees = spark.createDataFrame([
    (1, "Alice", None),      # CEO
    (2, "Bob", 1),           # Reports to Alice
    (3, "Charlie", 1),       # Reports to Alice
    (4, "David", 2),         # Reports to Bob
    (5, "Eve", 2)            # Reports to Bob
], ["id", "name", "manager_id"])

# Self-join to get manager names
from pyspark.sql.functions import coalesce, lit

emp_with_managers = employees.alias("emp").join(
    employees.alias("mgr"),
    col("emp.manager_id") == col("mgr.id"),
    "left"  # LEFT join preserves CEO with NULL manager
).select(
    col("emp.id"),
    col("emp.name").alias("employee"),
    coalesce(col("mgr.name"), lit("No Manager")).alias("manager")
)

# Result:
# 1, Alice, No Manager
# 2, Bob, Alice
# 3, Charlie, Alice
# 4, David, Bob
# 5, Eve, Bob
```

**Advanced: Multi-level Hierarchy**

```python
# Find all employees and their reporting chain (2 levels up)
emp_with_chain = employees.alias("emp") \
    .join(
        employees.alias("mgr1"),
        col("emp.manager_id") == col("mgr1.id"),
        "left"
    ) \
    .join(
        employees.alias("mgr2"),
        col("mgr1.manager_id") == col("mgr2.id"),
        "left"
    ) \
    .select(
        col("emp.name").alias("employee"),
        col("mgr1.name").alias("direct_manager"),
        col("mgr2.name").alias("manager_of_manager")
    )
```

**Edge Cases:**
- Circular references (A manages B, B manages A)
- Orphaned employees (manager_id doesn't exist)
- Deep hierarchies (need recursive approach)

---

### 7.5 Scenario: Deduplication Before Join

**Question:** Your source table has duplicates. Joining without deduplication creates exponential results. How do you handle this?

**Answer:** Deduplicate before join with proper strategy.

```python
# Problem demonstration
orders = spark.createDataFrame([
    (1, 101, 100.0),
    (1, 101, 100.0),  # Duplicate
    (2, 102, 200.0)
], ["order_id", "customer_id", "amount"])

customers = spark.createDataFrame([
    (101, "Alice"),
    (101, "Alice"),  # Duplicate
    (102, "Bob")
], ["id", "name"])

# ‚ùå BAD: Join without dedup
bad_join = orders.join(customers, orders.customer_id == customers.id)
# Result: 4 rows for order_id=1 (2 orders √ó 2 customers)

# ‚úÖ GOOD: Dedup before join
from pyspark.sql.functions import row_number
from pyspark.sql.window import Window

# Method 1: Use distinct (if all columns should be unique)
orders_dedup = orders.distinct()
customers_dedup = customers.distinct()

# Method 2: Use dropDuplicates on key columns
orders_dedup = orders.dropDuplicates(["order_id"])
customers_dedup = customers.dropDuplicates(["id"])

# Method 3: Use window functions (keep latest based on criteria)
window_spec = Window.partitionBy("order_id").orderBy(col("timestamp").desc())
orders_dedup = orders.withColumn("rn", row_number().over(window_spec)) \
    .filter(col("rn") == 1) \
    .drop("rn")

# Now join
good_join = orders_dedup.join(customers_dedup, 
    orders_dedup.customer_id == customers_dedup.id)
```

**Performance Consideration:**

```python
# Check duplicate ratio before deciding strategy
duplicate_ratio = 1 - (orders.distinct().count() / orders.count())
if duplicate_ratio > 0.1:  # More than 10% duplicates
    print("High duplication - dedup before join recommended")
```

---

### 7.6 Scenario: Time-Range Join (Event Correlation)

**Question:** Join website clicks with ad impressions where click occurred within 24 hours after impression. This is a non-equi join - how do you optimize it?

**Answer:** Use time-window optimization and potentially bucketing by time.

```python
from pyspark.sql.functions import col, unix_timestamp

# Data
impressions = spark.table("ad_impressions")  # 1B rows
clicks = spark.table("ad_clicks")  # 100M rows

# ‚ùå SLOW: Direct non-equi join (Cartesian + filter)
slow_result = impressions.join(
    clicks,
    (impressions.user_id == clicks.user_id) &
    (clicks.click_time >= impressions.impression_time) &
    (clicks.click_time <= impressions.impression_time + expr("INTERVAL 24 HOURS"))
)

# ‚úÖ OPTIMIZED: Pre-filter and partition by time buckets
from pyspark.sql.functions import date_trunc

# Bucket by date for efficient range scan
impressions_bucketed = impressions \
    .withColumn("date_bucket", date_trunc("day", col("impression_time"))) \
    .repartition(200, "user_id", "date_bucket")

clicks_bucketed = clicks \
    .withColumn("date_bucket", date_trunc("day", col("click_time"))) \
    .repartition(200, "user_id", "date_bucket")

# Join on user_id + date bucket first (equi-join)
result = impressions_bucketed.join(
    clicks_bucketed,
    (impressions_bucketed.user_id == clicks_bucketed.user_id) &
    (impressions_bucketed.date_bucket == clicks_bucketed.date_bucket)
).filter(
    # Then apply time range filter on smaller result set
    (col("click_time") >= col("impression_time")) &
    (col("click_time") <= col("impression_time") + expr("INTERVAL 24 HOURS"))
)

# ‚úÖ ALTERNATIVE: Use interval join (Spark 3.0+)
result = impressions.join(
    clicks,
    expr("""
        user_id = clicks.user_id AND
        click_time BETWEEN impression_time AND impression_time + INTERVAL 24 HOURS
    """)
)
```

**Performance Improvement:**
- Without optimization: 2+ hours
- With time bucketing: 20 minutes
- 6x speedup by converting partial join to equi-join

---

### 7.7 Scenario: Handling Skewed Keys in Production

**Question:** In production, you notice 80% of join time spent on 5% of tasks. Analysis shows customer_id "GUEST" appears in 70% of orders. How do you handle this?

**Answer:** Targeted salting for skewed keys only.

```python
from pyspark.sql.functions import when, rand, concat, lit

# Step 1: Identify skewed keys
skewed_keys = orders.groupBy("customer_id").count() \
    .filter(col("count") > 1000000) \
    .select("customer_id").collect()

skewed_key_values = [row.customer_id for row in skewed_keys]
print(f"Skewed keys: {skewed_key_values}")  # ['GUEST']

# Step 2: Apply salting only to skewed keys
salt_range = 20  # Split GUEST into 20 partitions

# Salt large table (orders)
orders_salted = orders.withColumn(
    "join_key",
    when(
        col("customer_id").isin(skewed_key_values),
        concat(col("customer_id"), lit("_"), (rand() * salt_range).cast("int"))
    ).otherwise(col("customer_id"))
)

# Salt small table (customers) - replicate skewed keys only
customers_base = customers.filter(~col("id").isin(skewed_key_values)) \
    .withColumn("join_key", col("id"))

customers_skewed = customers.filter(col("id").isin(skewed_key_values)) \
    .crossJoin(spark.range(salt_range).toDF("salt")) \
    .withColumn("join_key", concat(col("id"), lit("_"), col("salt"))) \
    .drop("salt")

customers_salted = customers_base.union(customers_skewed)

# Step 3: Join on salted key
result = orders_salted.join(customers_salted, "join_key") \
    .drop("join_key")
```

**Before vs After:**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Max task time | 45 min | 8 min | 82% faster |
| Median task time | 2 min | 2 min | Same |
| Task skew ratio | 22.5x | 4x | 82% less skew |
| Total time | 50 min | 12 min | 76% faster |

---

## 8. Edge Cases & Error Handling

### 8.1 Edge Case: Join on Floating Point Numbers

**Problem:** Floating point precision issues cause join mismatches.

```python
# Data with floating point keys
df1 = spark.createDataFrame([(0.1 + 0.2, "A")], ["key", "val1"])
df2 = spark.createDataFrame([(0.3, "B")], ["key", "val2"])

# ‚ùå This join returns 0 rows! (0.1 + 0.2 ‚â† 0.3 in floating point)
result = df1.join(df2, "key")

# ‚úÖ Solution 1: Round to fixed precision
from pyspark.sql.functions import round

df1_rounded = df1.withColumn("key", round(col("key"), 6))
df2_rounded = df2.withColumn("key", round(col("key"), 6))
result = df1_rounded.join(df2_rounded, "key")

# ‚úÖ Solution 2: Use range-based join
result = df1.join(
    df2,
    (df1.key >= df2.key - 0.0001) & (df1.key <= df2.key + 0.0001)
)

# ‚úÖ Solution 3: Convert to integers (if possible)
df1_int = df1.withColumn("key", (col("key") * 1000000).cast("long"))
df2_int = df2.withColumn("key", (col("key") * 1000000).cast("long"))
result = df1_int.join(df2_int, "key")
```

**Interview Tip:** Always ask about data types in join keys - floating point is a red flag.

---

### 8.2 Edge Case: Join with Empty DataFrames

**Problem:** What happens when joining with empty DataFrames?

```python
# Create empty DataFrame with schema
empty_df = spark.createDataFrame([], schema="id INT, name STRING")
orders_df = spark.createDataFrame([(1, 100)], ["id", "amount"])

# Test different join types
inner_result = orders_df.join(empty_df, "id", "inner")
# Result: Empty (0 rows) ‚úÖ

left_result = orders_df.join(empty_df, "id", "left")
# Result: Original orders with NULL for empty_df columns ‚úÖ

right_result = orders_df.join(empty_df, "id", "right")
# Result: Empty (0 rows) ‚úÖ

full_result = orders_df.join(empty_df, "id", "full")
# Result: Original orders with NULL for empty_df columns ‚úÖ
```

**Production Scenario:**

```python
# Defensive programming for potential empty DataFrames
def safe_join(df1, df2, join_key, join_type="inner"):
    """Join with empty DataFrame handling"""
    
    # Check if either DataFrame is empty
    if df1.rdd.isEmpty():
        if join_type in ["right", "full"]:
            return df2
        else:
            return df1  # Return empty with correct schema
    
    if df2.rdd.isEmpty():
        if join_type in ["left", "full"]:
            return df1
        else:
            return df2  # Return empty with correct schema
    
    # Both non-empty - proceed with join
    return df1.join(df2, join_key, join_type)
```

---

### 8.3 Edge Case: Join with ALL NULL Keys

**Problem:** Entire join key column is NULL in one or both tables.

```python
# All NULLs in join key
df1 = spark.createDataFrame([(None, "A"), (None, "B")], ["key", "val1"])
df2 = spark.createDataFrame([(None, "X"), (None, "Y")], ["key", "val2"])

# Inner join: Returns empty (NULLs don't match)
inner_result = df1.join(df2, "key", "inner")  # 0 rows

# Left join: Preserves left, no matches from right
left_result = df1.join(df2, "key", "left")  # 2 rows, right columns NULL

# Full join: Preserves all rows separately
full_result = df1.join(df2, "key", "full")  # 4 rows (2 from each side)
```

**Detection & Handling:**

```python
# Detect all-NULL join keys before expensive join
def check_null_keys(df, join_key):
    null_count = df.filter(col(join_key).isNull()).count()
    total_count = df.count()
    null_ratio = null_count / total_count if total_count > 0 else 0
    
    if null_ratio > 0.9:
        print(f"WARNING: {null_ratio*100:.1f}% of join keys are NULL!")
        return False
    return True

# Use before join
if check_null_keys(df1, "key") and check_null_keys(df2, "key"):
    result = df1.join(df2, "key")
else:
    # Handle high NULL scenario
    result = df1.join(df2, df1.key.eqNullSafe(df2.key))  # NULL-safe join
```

---

### 8.4 Edge Case: Complex Key Join (Multiple Columns with NULLs)

**Problem:** Join on multiple columns where some rows have NULLs in different columns.

```python
# Multi-column join with partial NULLs
df1 = spark.createDataFrame([
    (1, "A", 100),
    (1, None, 200),  # NULL in col2
    (None, "A", 300),  # NULL in col1
], ["col1", "col2", "value1"])

df2 = spark.createDataFrame([
    (1, "A", "X"),
    (1, None, "Y"),
    (None, "A", "Z"),
], ["col1", "col2", "value2"])

# Standard join - only (1, "A") matches
result = df1.join(df2, ["col1", "col2"], "inner")  # 1 row

# NULL-safe join - matches NULLs too
result_null_safe = df1.join(
    df2,
    (df1.col1.eqNullSafe(df2.col1)) & (df1.col2.eqNullSafe(df2.col2))
)  # 3 rows
```

**Business Scenario:**

```python
# E-commerce: Match orders by customer AND session
# Some orders have NULL session (direct purchases)

# Strategy: Separate NULL and non-NULL handling
orders_with_session = orders.filter(col("session_id").isNotNull())
orders_without_session = orders.filter(col("session_id").isNull())

# Join with session for session-tracked orders
matched_with_session = orders_with_session.join(
    sessions, 
    ["customer_id", "session_id"]
)

# Join only on customer_id for direct orders
matched_without_session = orders_without_session.join(
    customers,
    "customer_id"
)

# Union results
final_result = matched_with_session.union(matched_without_session)
```

---

### 8.5 Edge Case: Broadcast OOM During Join Execution

**Problem:** Job starts successfully but fails mid-execution with OOM on broadcast.

```python
# Scenario: DataFrame size estimation is wrong
small_df = spark.table("dimension_table")  # Estimated: 5MB

# Spark decides to broadcast
result = large_fact.join(small_df, "key")

# Mid-execution: actual size is 500MB ‚Üí Executors OOM
```

**Solutions:**

```python
# Solution 1: Disable broadcast for specific query
from pyspark.sql.functions import broadcast

spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)  # Disable
result = large_fact.join(small_df, "key")  # Uses sort-merge
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10485760)  # Re-enable

# Solution 2: Collect accurate statistics
spark.sql("ANALYZE TABLE dimension_table COMPUTE STATISTICS")
spark.sql("REFRESH TABLE dimension_table")

# Solution 3: Adaptive execution fallback (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
# AQE will switch strategy if broadcast fails

# Solution 4: Manual override - force shuffle join
from pyspark.sql import functions as F

result = large_fact.hint("NO_BROADCAST_HASH").join(
    small_df.hint("NO_BROADCAST_HASH"), 
    "key"
)
```

---

### 8.6 Edge Case: Duplicate Column Names After Join

**Problem:** Both tables have same column names (other than join key).

```python
# Both tables have "status" column
df1 = spark.createDataFrame([
    (1, "active", 100)
], ["id", "status", "amount"])

df2 = spark.createDataFrame([
    (1, "verified", "Alice")
], ["id", "status", "name"])

# ‚ùå This creates ambiguous references
result = df1.join(df2, "id")
# Columns: id, status, amount, status, name (duplicate "status")

# Attempting to select causes error
result.select("status")  # AnalysisException: Ambiguous reference

# ‚úÖ Solution 1: Alias DataFrames
result = df1.alias("orders").join(df2.alias("customers"), "id") \
    .select(
        "orders.id",
        col("orders.status").alias("order_status"),
        col("customers.status").alias("customer_status"),
        "amount",
        "name"
    )

# ‚úÖ Solution 2: Rename before join
df1_renamed = df1.withColumnRenamed("status", "order_status")
df2_renamed = df2.withColumnRenamed("status", "customer_status")
result = df1_renamed.join(df2_renamed, "id")

# ‚úÖ Solution 3: Drop duplicate columns from one side
result = df1.join(df2.drop("status"), "id")
```

---

### 8.7 Edge Case: Join Key Data Type Mismatch

**Problem:** Join keys have incompatible types - join returns 0 rows silently.

```python
# df1 has integer key, df2 has string key
df1 = spark.createDataFrame([(1, "A")], ["id", "val1"])
df2 = spark.createDataFrame([("1", "B")], ["id", "val2"])

# ‚ùå This join returns 0 rows (1 ‚â† "1")
result = df1.join(df2, "id")

# Detection
def check_join_key_types(df1, df2, key):
    type1 = dict(df1.dtypes)[key]
    type2 = dict(df2.dtypes)[key]
    if type1 != type2:
        print(f"WARNING: Type mismatch on key '{key}': {type1} vs {type2}")
        return False
    return True

# ‚úÖ Solution: Cast to common type
df2_fixed = df2.withColumn("id", col("id").cast("int"))
result = df1.join(df2_fixed, "id")

# ‚úÖ Alternative: Cast both to string (safer for mixed types)
result = df1.join(df2, df1.id.cast("string") == df2.id.cast("string"))
```

**Real Production Bug:**

```python
# CSV files read with inferSchema=False (all strings)
csv_df = spark.read.csv("data.csv", header=True)  # id is StringType

# Parquet files have proper types
parquet_df = spark.read.parquet("data.parquet")  # id is IntegerType

# Join silently fails - zero matches!
result = csv_df.join(parquet_df, "id")

# Fix: Always specify schema or cast explicitly
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True)
])

csv_df = spark.read.csv("data.csv", header=True, schema=schema)
```

---

### 8.8 Edge Case: Extremely High Cardinality Keys

**Problem:** Join key has billions of unique values (UUID, timestamps).

```python
# Scenario: Joining on UUID (near-100% unique keys)
events = spark.table("events")  # 10B rows, event_uuid
metadata = spark.table("metadata")  # 10B rows, event_uuid

# Challenge: Poor hash distribution, many small partitions
result = events.join(metadata, "event_uuid")

# Spark creates 200 partitions (default), each with millions of unique keys
# Result: High memory pressure, poor parallelism
```

**Solutions:**

```python
# Solution 1: Increase shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", "2000")  # 10x more partitions

# Solution 2: Use Adaptive Query Execution
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# Solution 3: Pre-aggregate if possible
# Instead of joining all events, maybe group first?
events_agg = events.groupBy("event_uuid").agg(
    F.count("*").alias("event_count"),
    F.max("timestamp").alias("latest_time")
)
result = events_agg.join(metadata, "event_uuid")  # Much smaller join

# Solution 4: Consider alternative join key
# Can you join on something with lower cardinality?
# e.g., user_id instead of event_uuid, then filter events
```

---

### 8.9 Edge Case: Cross-Join Warning in SQL

**Problem:** Missing join condition triggers Cartesian product warning.

```python
# ‚ùå Accidental Cartesian join
spark.sql("""
    SELECT * FROM table1, table2
    WHERE table1.date = '2024-01-01'
""")
# ERROR: Detected cartesian product - missing join condition

# ‚úÖ Fix 1: Add join condition
spark.sql("""
    SELECT * FROM table1, table2
    WHERE table1.id = table2.id
    AND table1.date = '2024-01-01'
""")

# ‚úÖ Fix 2: Enable cross join explicitly if intended
spark.conf.set("spark.sql.crossJoin.enabled", "true")
spark.sql("SELECT * FROM small_table1, small_table2")  # Intentional cross join
```

**Production Safety:**

```python
# Keep cross join disabled by default
spark.conf.set("spark.sql.crossJoin.enabled", "false")

# Only enable for specific queries that need it
with spark.conf.set("spark.sql.crossJoin.enabled", "true"):
    result = df1.crossJoin(df2)  # Intentional
```

---

### 8.10 Edge Case: Joining DataFrames with Different Partitioning Schemes

**Problem:** Pre-partitioned DataFrames with different partition schemes.

```python
# df1 partitioned by date (365 partitions)
df1 = spark.table("partitioned_by_date")  # Partitions: date=2024-01-01, date=2024-01-02...

# df2 partitioned by region (50 partitions)
df2 = spark.table("partitioned_by_region")  # Partitions: region=US, region=EU...

# Joining requires full shuffle of both tables
result = df1.join(df2, "product_id")  # Expensive - both tables shuffled

# Solution: Choose join key that aligns with partitioning if possible
# Or accept the shuffle cost and optimize elsewhere (broadcast, filtering)
```

---

## 9. Missing Topics & Additional Aspects

### 9.1 Explain Join Hints in Detail

**Answer:** Join hints override Spark's Catalyst optimizer decisions.

**Available Hints:**

```python
# BROADCAST / BROADCASTJOIN / MAPJOIN
df1.hint("BROADCAST").join(df2, "key")

# MERGE / SHUFFLE_MERGE / MERGEJOIN
df1.hint("MERGE").join(df2, "key")  # Force sort-merge

# SHUFFLE_HASH
df1.hint("SHUFFLE_HASH").join(df2, "key")  # Force shuffle hash

# SHUFFLE_REPLICATE_NL
df1.hint("SHUFFLE_REPLICATE_NL").join(df2)  # Force nested loop
```

**SQL Syntax:**

```sql
-- Multiple hints
SELECT /*+ BROADCAST(t1), MERGE(t2, t3) */
FROM t1 JOIN t2 ON t1.id = t2.id
     JOIN t3 ON t2.id = t3.id
```

**When to Use:**
- ‚úÖ You have better domain knowledge than optimizer
- ‚úÖ Testing performance of different strategies
- ‚ö†Ô∏è Use sparingly - optimizer is usually right
- ‚ùå Don't hardcode hints in production without monitoring

---

### 9.2 Partitioning Strategies Specific to Joins

**Partition By Range** (for range-based joins):

```python
# Optimize for time-range queries
df.write \
    .partitionBy("year", "month") \
    .bucketBy(100, "user_id") \
    .saveAsTable("time_partitioned")

# Efficient time-range + user join
result = spark.sql("""
    SELECT * FROM time_partitioned
    WHERE year = 2024 AND month = 1  -- Partition pruning
""").join(other_table, "user_id")  -- Bucketed join
```

---

### 9.3 Handling Schema Evolution in Joins

**Problem:** Joining tables with evolved schemas (added/removed columns).

```python
# Old version: (id, name)
# New version: (id, name, email, created_date)

# Challenge: Join across different versions
old_data = spark.read.parquet("data/v1/")
new_data = spark.read.parquet("data/v2/")

# ‚úÖ Solution: Select common columns before join
from pyspark.sql.functions import lit

common_cols = set(old_data.columns) & set(new_data.columns)

old_normalized = old_data.select(*common_cols)
new_normalized = new_data.select(*common_cols)

result = old_normalized.union(new_normalized) \
    .join(dimension, "key")

# ‚úÖ Alternative: Add missing columns with NULLs
new_cols = set(new_data.columns) - set(old_data.columns)
for col_name in new_cols:
    old_data = old_data.withColumn(col_name, lit(None))
```

---

### 9.4 Join Performance with Compressed Data

**Question:** How does data compression affect join performance?

**Answer:** Compression trades CPU for I/O and storage.

```python
# Different compression formats
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")  # Fast, moderate compression
# vs
spark.conf.set("spark.sql.parquet.compression.codec", "gzip")  # Slow, high compression

# Join performance impact:
# Snappy: Faster decompression ‚Üí faster joins
# GZIP: Slower decompression but less data read from disk
```

**Recommendation:**
- Use **Snappy** for frequently joined tables (better CPU performance)
- Use **GZIP** for archival/cold data (better storage efficiency)

---

### 9.5 Monitoring Join Memory Usage

**Critical Metrics:**

```python
# Key configurations to monitor
spark.conf.get("spark.executor.memory")  # Total executor memory
spark.conf.get("spark.memory.fraction")  # Fraction for execution & storage (default: 0.6)
spark.conf.get("spark.memory.storageFraction")  # Within fraction, for storage (default: 0.5)

# Execution memory for joins = executor.memory √ó memory.fraction √ó (1 - storageFraction)
# Example: 8GB √ó 0.6 √ó 0.5 = 2.4GB per executor for joins/aggregations
```

**Spark UI Indicators:**

- **Executors Tab:** Memory used / Max memory
- **Stages Tab:** Spill (Memory), Spill (Disk)
- **SQL Tab:** Peak execution memory

---

## Summary & Key Takeaways

### Join Strategy Selection Flowchart

```
START: Need to Join Two Tables
    ‚Üì
Is one table < 10MB (broadcast threshold)?
    ‚îú‚îÄ YES ‚Üí Use BROADCAST JOIN ‚ö°
    ‚îÇ         (Fastest, no large table shuffle)
    ‚îÇ
    ‚îî‚îÄ NO ‚Üí Both tables large
            ‚Üì
        Are tables frequently joined on same key?
            ‚îú‚îÄ YES ‚Üí Use BUCKETING üéØ
            ‚îÇ         (One-time cost, shuffle-free joins)
            ‚îÇ
            ‚îî‚îÄ NO ‚Üí Use SORT-MERGE JOIN üìä
                    (Default, scalable, memory-efficient)
                    ‚Üì
                Is there severe data skew?
                    ‚îú‚îÄ YES ‚Üí Apply SALTING üßÇ
                    ‚îî‚îÄ NO ‚Üí Use as-is with optimization
```

### Critical Configuration Parameters

```python
# Broadcast join threshold
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10485760)  # 10MB

# Shuffle partitions (adjust based on data size)
spark.conf.set("spark.sql.shuffle.partitions", 200)

# Adaptive Query Execution (Spark 3.0+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# Memory configuration
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.memory.fraction", "0.8")
```

### Performance Checklist

**Before Joining:**
- [ ] Collect table statistics: `ANALYZE TABLE`
- [ ] Filter data early
- [ ] Select only required columns
- [ ] Check for NULL values in join keys
- [ ] Identify potential data skew

**During Join:**
- [ ] Choose appropriate join type (inner/left/full)
- [ ] Consider broadcast for small tables
- [ ] Use bucketing for repeated joins
- [ ] Monitor Spark UI for shuffle metrics

**After Join:**
- [ ] Cache if result reused
- [ ] Verify no excessive spill to disk
- [ ] Check for task skew
- [ ] Validate result correctness

### Common Anti-Patterns to Avoid

‚ùå **Don't:**
- Force broadcast on large tables
- Join all columns when only few needed
- Cache entire table before filtering
- Ignore data skew warnings
- Use Cartesian joins accidentally
- Join without statistics on large tables

‚úÖ **Do:**
- Filter and project before joining
- Collect statistics for large tables
- Monitor Spark UI for bottlenecks
- Use appropriate join hints when needed
- Handle NULL values explicitly
- Test with realistic data volumes

---

**End of Document**
