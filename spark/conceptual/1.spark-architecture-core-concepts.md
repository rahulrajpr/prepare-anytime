# Spark Architecture & Core Concepts - Interview Q&A

## 1.1 Cluster Architecture

### 1.1.1 What is the Spark cluster architecture? Explain the roles of driver, worker nodes, executors, and cores.
**Spark follows a master-slave architecture:**
- **Driver Node:** The master node that runs the main program, converts user code into tasks, and coordinates execution
- **Worker Nodes:** Slave machines that run the actual computation tasks
- **Executors:** JVM processes on worker nodes that execute tasks and store data
- **Cores:** Computational units that determine how many tasks an executor can run in parallel

### 1.1.2 What is the difference between an executor, a worker node, and a thread in Spark?
- **Worker Node:** Physical/virtual machine (hardware)
- **Executor:** JVM process running on worker node (software container)
- **Thread:** Smallest execution unit within executor; each core runs one task thread

### 1.1.3 How do these components interact during job execution?
1. Driver creates execution plan and coordinates with cluster manager
2. Executors launch on worker nodes
3. Driver sends tasks to executors
4. Executors run tasks and report back to driver
5. Driver tracks progress and handles failures

### 1.1.4 What is the role of the cluster manager in Spark architecture?
**Cluster Manager allocates and manages cluster resources:**
- Launches executors on worker nodes
- Manages resource allocation (CPU, memory)
- Supported managers: Standalone, YARN, Mesos, Kubernetes

## 1.2 Execution Model

### 1.2.1 What is a DAG (Directed Acyclic Graph) in Spark and how does Spark use it for task scheduling?
**DAG is Spark's execution plan:**
- Directed: Data flows one way
- Acyclic: No loops
- Used to break execution into stages and optimize task scheduling

### 1.2.2 What is the difference between the DAG Scheduler and the Task Scheduler?
- **DAG Scheduler:** Converts logical plan to physical execution plan (stages)
- **Task Scheduler:** Launches tasks on executors via cluster manager

### 1.2.3 What is lazy evaluation in Spark? What are its advantages?
**Lazy evaluation = Delayed execution until action is called**

**Advantages:**
- Optimization opportunities (Catalyst optimizer)
- Avoids intermediate computation
- Better resource utilization

### 1.2.4 Explain the execution hierarchy: spark-submit → applications → jobs → stages → tasks.
**Execution Hierarchy:**
1. **spark-submit:** Launches application
2. **Application:** User program with driver + executors
3. **Job:** Triggered by one action
4. **Stage:** Set of tasks between shuffles
5. **Task:** Smallest work unit on data partition

### 1.2.5 What happens on the driver node when an action is called on a DataFrame?
1. Catalyst optimization of logical plan
2. Physical plan generation
3. DAG creation with stages
4. Task scheduling and distribution
5. Progress monitoring

### 1.2.6 What happens when a task completes on an executor node? How does the driver track progress?
- Executor sends status update to driver
- Driver's Task Scheduler tracks completion
- Progress visible in Spark UI
- Results aggregated for final action

### 1.2.7 Where does the data go after an action like collect() is executed?
**All data sent to driver node** - can cause OOM with large datasets

## 1.3 Operations & Transformations

### 1.3.1 What is the difference between transformations and actions in Spark? Provide examples.

| Aspect | Transformations | Actions |
|--------|-----------------|---------|
| **Execution** | Lazy | Eager |
| **Result** | New DataFrame/RDD | Result to driver/write output |
| **Examples** | `map()`, `filter()`, `select()` | `count()`, `collect()`, `save()` |

### 1.3.2 What are narrow dependency transformations? Provide examples.
**Narrow = No shuffle required**
- Input partition → One output partition
- **Examples:** `map()`, `filter()`, `union()`

### 1.3.3 What are wide dependency transformations? Provide examples.
**Wide = Shuffle required**
- Input partition → Multiple output partitions
- **Examples:** `groupBy()`, `join()`, `distinct()`, `repartition()`

### 1.3.4 Why are narrow transformations more efficient than wide transformations?
1. **No data shuffling** over network
2. **Pipelining** possible
3. **Better fault tolerance** - local recomputation
4. **Lower I/O overhead**
