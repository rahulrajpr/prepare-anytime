## 1. Spark Architecture & Core Concepts

### 1.1 Cluster Architecture

#### 1.1.1 What is the Spark cluster architecture? Explain the roles of driver, worker nodes, executors, and cores.

**Definition:** Spark cluster architecture is a distributed computing framework based on a master-slave pattern that enables parallel processing of large-scale data across a cluster of machines.

**Components:**

- **Driver Node:** Master process that coordinates job execution, maintains application state, and schedules tasks
- **Worker Nodes:** Slave machines that host executor processes and execute tasks
- **Executors:** JVM processes on worker nodes that run tasks and store data in memory/disk
- **Cores:** CPU units within executors that enable parallel task execution

**Key Points:**

- One driver per Spark application, multiple workers
- Each executor can run multiple tasks concurrently based on available cores
- Driver communicates with cluster manager to allocate resources

#### 1.1.2 What is the difference between an executor, a worker node, and a thread in Spark?

**Definition:**

- **Worker Node:** Physical/virtual machine in the cluster that hosts executor processes
- **Executor:** JVM process running on a worker node that executes tasks and caches data
- **Thread:** Lightweight execution unit within an executor that runs individual tasks

**Comparison Table:**

| Component             | Scope            | Function                  | Lifecycle              |
| --------------------- | ---------------- | ------------------------- | ---------------------- |
| **Worker Node** | Physical machine | Hosts executors           | Entire cluster session |
| **Executor**    | JVM process      | Runs tasks, stores data   | Spark application      |
| **Thread**      | Execution unit   | Executes individual tasks | Single task            |

**Key Points:**

- One worker node can host multiple executors
- One executor can run multiple threads (tasks) concurrently
- Threads share executor's memory and resources

#### 1.1.3 How do these components interact during job execution?

**Execution Flow:**

1. Driver program creates SparkContext
2. SparkContext connects to cluster manager
3. Cluster manager allocates executors on worker nodes
4. Driver sends task code to executors
5. Executors execute tasks and report back to driver
6. Driver tracks progress and handles failures

**Key Points:**

- Driver maintains the DAG and schedules tasks
- Executors pull tasks from driver and execute them
- Data shuffling happens directly between executors
- Driver collects results from executors after action completion

#### 1.1.4 What is the role of the cluster manager in Spark architecture?

**Definition:** The cluster manager is an external service that manages and allocates resources across the Spark cluster.

**Primary Roles:**

- Resource allocation and management
- Executor lifecycle management
- Node monitoring and failure detection

**Supported Cluster Managers:**

- **Standalone:** Spark's built-in simple cluster manager
- **YARN:** Hadoop's resource manager
- **Kubernetes:** Container orchestration platform
- **Mesos:** General-purpose cluster manager

**Interview Tip:** Know which cluster manager you've used and its specific advantages for your use cases.

### 1.2 Execution Model

#### 1.2.1 What is a DAG (Directed Acyclic Graph) in Spark and how does Spark use it for task scheduling?

**Definition:** DAG is Spark's internal representation of the logical execution plan showing dependencies between operations.

**How Spark Uses DAG:**

- Builds DAG of RDD transformations (lazy evaluation)
- Optimizes execution plan by pipelining narrow transformations
- Identifies shuffle boundaries for stage creation
- Enables fault tolerance by recomputing lost partitions

**Key Points:**

- Vertices represent RDDs, edges represent transformations
- DAG Scheduler splits DAG into stages of tasks
- Enables optimization opportunities before physical execution

#### 1.2.2 What is the difference between the DAG Scheduler and the Task Scheduler?

**Comparison Table:**

| Aspect           | DAG Scheduler            | Task Scheduler                |
| ---------------- | ------------------------ | ----------------------------- |
| **Level**  | Logical planning         | Physical execution            |
| **Input**  | RDD lineage              | Stages from DAG               |
| **Output** | Stages                   | Task sets for executors       |
| **Focus**  | Optimization, pipelining | Resource allocation, locality |

**Key Points:**

- DAG Scheduler creates execution plan, Task Scheduler executes it
- DAG Scheduler handles stage boundaries at shuffle operations
- Task Scheduler manages task distribution and fault tolerance

#### 1.2.3 What is lazy evaluation in Spark? What are its advantages?

**Definition:** Lazy evaluation is Spark's execution strategy where transformations are not executed immediately but are recorded as part of DAG, and execution only happens when an action is called.

**Advantages:**

- **Optimization:** Spark can analyze entire DAG and optimize execution plan
- **Pipelining:** Multiple narrow transformations can be combined into single stage
- **Efficiency:** Avoids unnecessary intermediate computations
- **Fault Tolerance:** Enables lineage-based recomputation of lost data

**Interview Tip:** Emphasize that lazy evaluation is key to Spark's performance - it's what enables Catalyst optimizer to work effectively.

#### 1.2.4 Explain the execution hierarchy: spark-submit → applications → jobs → stages → tasks.

**Execution Hierarchy:**

1. **spark-submit:** Launches a Spark application on the cluster
2. **Application:** One instance of SparkContext running user program
3. **Job:** Sequence of stages triggered by one action (e.g., `count()`, `save()`)
4. **Stage:** Set of tasks that can be executed together (separated by shuffle boundaries)
5. **Task:** Smallest unit of work sent to one executor

**Key Points:**

- One application → Multiple jobs → Multiple stages → Many tasks
- Stages are determined by shuffle dependencies
- Tasks are parallel units working on different data partitions

#### 1.2.5 What happens on the driver node when an action is called on a DataFrame?

**Driver Process:**

1. **DAG Construction:** Builds complete lineage of transformations
2. **Logical Optimization:** Applies Catalyst optimizer rules
3. **Physical Planning:** Generates executable physical plan
4. **Stage Creation:** Splits DAG into stages at shuffle boundaries
5. **Task Scheduling:** Creates task sets and submits to Task Scheduler
6. **Progress Tracking:** Monitors task execution and handles failures

**Interview Tip:** Mention Catalyst optimizer's role in query optimization for bonus points.

#### 1.2.6 What happens when a task completes on an executor node? How does the driver track progress?

**Task Completion Flow:**

1. Executor sends `StatusUpdate` message to driver
2. Driver updates task progress in TaskScheduler
3. For shuffle operations, shuffle data is written to disk
4. Driver tracks completed tasks and available results
5. If all tasks in stage complete, driver launches next stage

**Progress Tracking:**

- TaskScheduler maintains task status (PENDING, RUNNING, FINISHED, FAILED)
- Spark UI gets updates for real-time monitoring
- Driver handles task failures by retrying or marking stage as failed

#### 1.2.7 Where does the data go after an action like collect() is executed?

**Data Flow for collect():**

1. Each executor serializes its partition data
2. Data sent over network to driver node
3. Driver deserializes and aggregates all data
4. Final result stored in driver's memory as local collection

**Key Considerations:**

- **Risk:** Driver out-of-memory if dataset is too large
- **Use Case:** Only for small datasets that fit in driver memory
- **Alternative:** Use `take(N)`, `foreach()`, or distributed operations instead

**Interview Tip:** Always warn about the dangers of `collect()` on large datasets - this is a common interview question.

### 1.3 Operations & Transformations

#### 1.3.1 What is the difference between transformations and actions in Spark? Provide examples.

**Definition:**

- **Transformations:** Lazy operations that create new RDDs/DataFrames (e.g., `map`, `filter`, `groupBy`)
- **Actions:** Eager operations that trigger computation and return results (e.g., `count`, `collect`, `save`)

**Comparison Table:**

| Aspect              | Transformations                        | Actions                              |
| ------------------- | -------------------------------------- | ------------------------------------ |
| **Execution** | Lazy                                   | Eager                                |
| **Output**    | New DataFrame                          | Result value                         |
| **Examples**  | `select()`, `filter()`, `join()` | `count()`, `show()`, `write()` |

**Key Points:**

- Transformations build execution plan, actions trigger execution
- Multiple transformations can be pipelined before action
- Each action triggers separate execution of entire DAG

#### 1.3.2 What are narrow dependency transformations? Provide examples.

**Definition:** Narrow transformations where each input partition contributes to only one output partition, allowing pipelined execution without shuffling.

**Characteristics:**

- No data movement between nodes required
- Can be executed within same stage
- More efficient than wide transformations

**Examples:**

- `filter()`, `map()`, `flatMap()`
- `union()` (if same partitioning)
- `mapPartitions()`

#### 1.3.3 What are wide dependency transformations? Provide examples.

**Definition:** Wide transformations where input partitions contribute to multiple output partitions, requiring data shuffling across nodes.

**Characteristics:**

- Requires data shuffle between executors
- Creates stage boundaries in DAG
- More expensive due to network I/O

**Examples:**

- `groupBy()`, `aggregate()`, `reduceByKey()`
- `join()` (unless both inputs co-partitioned)
- `distinct()`, `repartition()`

#### 1.3.4 Why are narrow transformations more efficient than wide transformations?

**Efficiency Factors:**

- **No Shuffle:** Data processed locally without network transfer
- **Pipelining:** Multiple narrow transformations combined into single stage
- **Memory:** Less memory overhead from shuffle buffers
- **Network:** Reduced network I/O and serialization costs

**Performance Impact:**

- Narrow: Linear scalability with partitions
- Wide: Network bottleneck potential, disk I/O for shuffle spills

**Interview Tip:** Always prefer narrow transformations when possible, and use wide transformations judiciously with proper partitioning strategies.
