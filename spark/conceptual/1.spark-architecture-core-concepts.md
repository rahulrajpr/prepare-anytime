# Spark Architecture & Core Concepts - Interview Q&A

## 1.1 Cluster Architecture

### 1.1.1 What is the Spark cluster architecture? Explain the roles of driver, worker nodes, executors, and cores.

**Definition:** Spark cluster architecture is a distributed computing framework based on a master-slave pattern that enables parallel processing of large-scale data across a cluster of machines.

**Components:**
- **Driver Node:** The master process that runs the main() function, maintains application state, and coordinates cluster operations
- **Worker Nodes:** Slave machines that host executor processes and perform data processing tasks
- **Executors:** JVM processes on worker nodes responsible for task execution and in-memory data storage
- **Cores:** Computational units representing parallel task execution capacity; each core can process one task thread

### 1.1.2 What is the difference between an executor, a worker node, and a thread in Spark?

**Definitions:**
- **Worker Node:** Physical or virtual machine providing computational resources
- **Executor:** Process-level abstraction representing a container for task execution
- **Thread:** Thread-level abstraction within executor enabling concurrent task processing

**Hierarchical Relationship:**
Worker Node (hardware) → Executor (process) → Thread (execution context)

### 1.1.3 How do these components interact during job execution?

**Execution Workflow:**
1. Driver program initiates SparkContext and constructs DAG
2. Cluster manager allocates executor resources on worker nodes
3. Driver serializes tasks and dispatches to executors
4. Executors deserialize tasks and execute computations
5. Results are aggregated and returned to driver program

### 1.1.4 What is the role of the cluster manager in Spark architecture?

**Definition:** Cluster manager is an external service responsible for resource allocation and management across the distributed computing environment.

**Primary Responsibilities:**
- Resource negotiation and allocation
- Executor lifecycle management
- Fault tolerance and resource monitoring

## 1.2 Execution Model

### 1.2.1 What is a DAG (Directed Acyclic Graph) in Spark and how does Spark use it for task scheduling?

**Definition:** DAG is a finite directed graph with no directed cycles representing the sequence of computations in a Spark application.

**Spark Implementation:**
- Vertices represent RDD transformations
- Edges represent dependencies between operations
- Enables optimization through pipelining and stage boundary identification

### 1.2.2 What is the difference between the DAG Scheduler and the Task Scheduler?

**DAG Scheduler:**
- Converts logical execution plan to physical execution plan
- Identifies stage boundaries based on shuffle dependencies
- Optimizes execution through pipelining

**Task Scheduler:**
- Manages task distribution to executors
- Handles fault tolerance through task retries
- Implements scheduling policies (FIFO/FAIR)

### 1.2.3 What is lazy evaluation in Spark? What are its advantages?

**Definition:** Lazy evaluation is a computation strategy where expressions are not evaluated until their results are explicitly required.

**Advantages:**
- **Optimization Opportunity:** Catalyst optimizer can analyze entire query plan
- **Resource Efficiency:** Avoids unnecessary intermediate computations
- **Fault Tolerance:** Enables lineage-based recomputation

### 1.2.4 Explain the execution hierarchy: spark-submit → applications → jobs → stages → tasks.

**Hierarchical Model:**
1. **spark-submit:** Application deployment mechanism
2. **Application:** Self-contained computation program
3. **Job:** Execution unit triggered by Spark action
4. **Stage:** Task set bounded by shuffle operations
5. **Task:** Atomic computation unit on data partition

### 1.2.5 What happens on the driver node when an action is called on a DataFrame?

**Driver Execution Sequence:**
1. Logical plan optimization via Catalyst optimizer
2. Physical plan generation with cost-based optimization
3. DAG construction with stage demarcation
4. Task serialization and distribution
5. Result aggregation and completion handling

### 1.2.6 What happens when a task completes on an executor node? How does the driver track progress?

**Task Completion Protocol:**
- Executor sends TaskResult message to driver
- TaskScheduler updates task status and metrics
- Spark UI reflects real-time progress visualization
- Speculative execution for straggler mitigation

### 1.2.7 Where does the data go after an action like collect() is executed?

**Data Flow:** All partition data is transferred over network to driver's JVM memory space

**Risk Consideration:** Potential driver OutOfMemoryError due to single-node data accumulation

## 1.3 Operations & Transformations

### 1.3.1 What is the difference between transformations and actions in Spark? Provide examples.

**Transformation Definition:** Operations that create new datasets from existing ones through lazy evaluation.

**Action Definition:** Operations that return values to driver program or write to external storage, triggering computation.

| Category | Execution | Output | Examples |
|----------|-----------|--------|----------|
| Transformations | Lazy | New RDD/DataFrame | map, filter, groupBy |
| Actions | Eager | Result to driver | count, collect, save |

### 1.3.2 What are narrow dependency transformations? Provide examples.

**Definition:** Transformations where each input partition contributes to exactly one output partition, enabling pipelined execution.

**Characteristics:**
- No data shuffling required
- Deterministic partition mapping
- Examples: map, filter, union, mapValues

### 1.3.3 What are wide dependency transformations? Provide examples.

**Definition:** Transformations requiring data redistribution across partitions, creating shuffle dependencies.

**Characteristics:**
- Input partitions contribute to multiple output partitions
- Network shuffle operations required
- Examples: groupByKey, reduceByKey, join, distinct

### 1.3.4 Why are narrow transformations more efficient than wide transformations?

**Performance Advantages:**
1. **Network Efficiency:** No data shuffling across nodes
2. **Pipelining:** Multiple operations can execute sequentially
3. **Fault Tolerance:** Local recomputation from narrow dependencies
4. **Resource Optimization:** Reduced I/O and serialization overhead
