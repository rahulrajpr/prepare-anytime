# Section 14: Monitoring & Troubleshooting

## 14.1 Spark UI

**1780. How do you explore and navigate the Spark UI for performance analysis?**

The Spark UI is your performance investigation headquarters. Access it at `http://<driver-node>:4040` while your application runs.

**Key tabs to navigate:**

* **Jobs** - See which jobs ran, their duration, stages breakdown
* **Stages** - Dive into individual stages, see task distribution, identify stragglers
* **Storage** - Check cached DataFrames, memory usage, spill to disk
* **Environment** - Review all Spark configurations applied
* **Executors** - Monitor resource usage per executor, GC time, shuffle metrics
* **SQL** - See DataFrame/SQL query plans, execution times, data scanned

**Pro tip:** Start with the Jobs tab to find slow jobs, then drill into Stages to see which specific stage is problematic, then examine individual tasks to spot skew or bottlenecks.

**1781. Is Spark UI only available during active Spark sessions?**

By default,  **yes** —the UI disappears when your application ends. But you can preserve it using the  **Spark History Server** .

Enable history server:

```python
spark.conf.set("spark.eventLog.enabled", "true")
spark.conf.set("spark.eventLog.dir", "hdfs:///spark-logs")
```

Then start the history server:

```bash
$SPARK_HOME/sbin/start-history-server.sh
```

Now you can browse completed applications at `http://<history-server>:18080`. Essential for post-mortem analysis—investigate what went wrong after failures without having to reproduce the issue.

**1782. How can you preserve execution history and logs using log4j?**

Log4j configuration lets you control what gets logged and where. The key file is `log4j.properties` in your Spark conf directory.

**Common configurations:**

```properties
# Set root logger level (INFO is default, DEBUG for troubleshooting)
log4j.rootCategory=INFO, console

# Direct logs to file for persistence
log4j.appender.file=org.apache.log4j.RollingFileAppender
log4j.appender.file.File=/var/log/spark/spark.log
log4j.appender.file.MaxFileSize=10MB
log4j.appender.file.MaxBackupIndex=5

# Quiet down noisy libraries
log4j.logger.org.apache.spark.repl.Main=WARN
log4j.logger.org.apache.hadoop=WARN
```

This preserves logs to files that survive beyond application lifetime. Crucial for debugging issues that only appear in production, especially intermittent failures.

---

## 14.2 Metrics & Accumulators

**1783. What are accumulators in Spark?**

Accumulators are **write-only variables** that allow executors to safely add values that only the driver can read. Think of them as distributed counters—perfect for tracking metrics across your cluster without breaking Spark's functional programming model.

Key property:  **add-only from workers, read-only from driver** . This one-way communication avoids the complexity and performance hit of two-way communication.

**1784. How are accumulators used for distributed counting and metrics collection?**

Classic use case: counting records that meet certain conditions without collecting all data to driver.

```python
# Create accumulator
error_count = spark.sparkContext.longAccumulator("Error Records")

def process_with_tracking(row):
    if row['status'] == 'error':
        error_count.add(1)  # Each executor increments
    return row

df.foreach(process_with_tracking)

# Only driver can read the total
print(f"Total errors: {error_count.value}")
```

**Common uses:**

* Count malformed records during ETL
* Track how many records hit each branch in conditional logic
* Monitor cache hits vs misses
* Debug data quality issues (nulls, outliers, duplicates)

**Critical gotcha:** Accumulators in transformations can double-count if Spark retries failed tasks. Use them in actions for accurate counts.

**1785. What metrics indicate performance bottlenecks (skew, spilling, GC time)?**

Watch these red flags in Spark UI:

**Data Skew Indicators:**

* **Task Duration Variance** - Some tasks take 10x longer than median
* **Data Read Variance** - One task reads 1GB while others read 100MB
* **Stragglers** - 95% of tasks finish fast, then you wait for a few slow ones
* Location: Stages tab → Task Metrics → look for outliers in duration/input size

**Memory Pressure Indicators:**

* **Spill to Disk** - "Shuffle Spill (Memory)" > 0 in Stage metrics
* **Storage Memory Usage** - Consistently near 100% in Executors tab
* **GC Time > 10%** - "GC Time" column in Executors tab shows high percentage
* **OOM Errors** - Check executor logs for "java.lang.OutOfMemoryError"

**Shuffle Bottlenecks:**

* **Shuffle Read/Write Size** - Terabytes shuffled = slow job guaranteed
* **Shuffle Read Blocked Time** - Tasks waiting for shuffle data to arrive
* Location: Stages tab → "Shuffle Read" and "Shuffle Write" columns

**Rule of thumb:** If GC time > 10% of execution time, you have memory issues. If max task time > 3x median, you have skew. If shuffle size is TBs, you need to reduce data movement.

---

## 14.3 Common Errors & Debugging

**1786. What is the "out of memory" error and common causes in Spark?**

The dreaded OOM error means an executor ran out of heap memory. There are different flavors:

**Driver OOM:**

* **Cause:** Collecting too much data to driver (`collect()`, `toPandas()` on huge DataFrame)
* **Symptom:** "java.lang.OutOfMemoryError: Java heap space" on driver
* **Fix:** Don't collect large datasets. Use `take(n)` or aggregate before collecting. Increase driver memory with `spark.driver.memory`.

**Executor OOM:**

* **Cause:** Processing partitions too large for executor memory
* **Symptom:** Task failures with OOM in executor logs
* **Fix:** Increase partitions (`repartition()` to split data), increase executor memory (`spark.executor.memory`), or reduce memory-intensive operations (fewer cached DataFrames).

**Broadcast OOM:**

* **Cause:** Broadcasting table too large for executor memory
* **Symptom:** OOM during broadcast join
* **Fix:** Disable broadcast for that join (`spark.sql.autoBroadcastJoinThreshold = -1`), or reduce broadcast table size with filtering/aggregation.

**Common root causes:**

1. **Data skew** - One partition has 100x more data
2. **Too few partitions** - Each partition is huge
3. **Memory leak** - Caching too many DataFrames
4. **Collection to driver** - Trying to collect 1TB to driver with 4GB heap

**1787. Why do you get "Task not serializable" errors? How do you fix them?**

This error means Spark tried to send your code to executors but couldn't serialize (convert to bytes) some object in your closure.

**Common culprits:**

**1. Non-serializable objects in closures:**

```python
# BAD - db_connection isn't serializable
db_connection = create_connection()
df.map(lambda x: db_connection.query(x))  # ERROR!

# GOOD - create connection inside the function
def query_with_connection(x):
    db_connection = create_connection()  # Created on executor
    return db_connection.query(x)

df.map(query_with_connection)
```

**2. Referencing self/class in non-serializable way:**

```python
# BAD - captures entire class instance
class MyProcessor:
    def process(self, df):
        return df.map(lambda x: self.helper(x))  # ERROR - self not serializable

# GOOD - make method static or extract value
class MyProcessor:
    def process(self, df):
        helper_func = self.helper  # Extract method reference
        return df.map(lambda x: helper_func(x))
```

**3. SparkSession/SparkContext in closure:**

```python
# BAD - trying to send spark to executors
df.foreach(lambda x: spark.createDataFrame([x]))  # ERROR!

# GOOD - spark operations stay on driver
results = df.collect()
spark.createDataFrame(results)
```

**Quick fix:** If you see this error, look at your lambda/function—anything it references must be serializable. Extract only what you need before the closure.

**1788. What causes "Stage X has Y failed attempts" and how do you debug it?**

This means a stage failed repeatedly—Spark tried multiple times but kept hitting problems.

**Common causes:**

**1. Executor failures (OOM, node crashes):**

* Check executor logs in Spark UI → Executors tab
* Look for OOM errors or "lost executor" messages
* Fix: Increase executor memory, reduce partition size

**2. Data skew causing timeouts:**

* One task processes 100x more data, times out
* Spark retries → same task fails again
* Fix: Repartition on skewed key with salting

**3. Network issues (shuffle fetch failures):**

* Tasks can't fetch shuffle data from dead executors
* Errors like "shuffle fetch failed" in logs
* Fix: Increase `spark.shuffle.io.retryWait`, reduce shuffle size

**4. Bad data causing exceptions:**

* Some records cause code to crash (division by zero, null pointer)
* Same bad record causes failure every retry
* Fix: Add defensive null checks, use `try/except` in UDFs

**Debugging strategy:**

1. Go to Stages tab → find failed stage
2. Click stage → scroll to failed tasks
3. Look at error messages in task logs
4. Check if same task ID fails repeatedly (data issue) or different tasks fail (resource issue)

**1789. What is data skew and what are the symptoms in Spark UI?**

Data skew means your data is unevenly distributed across partitions—some partitions have way more data than others.

**Visual symptoms in Spark UI:**

**Stages tab:**

* **Task Duration** - Min: 5 seconds, Median: 10 seconds, Max: 5 minutes (huge variance)
* **Input Size** - Most tasks read 100MB, one task reads 10GB
* **Timeline view** - Most tasks finish quickly, then you wait forever for 1-2 stragglers

**SQL tab (if using DataFrames):**

* Look at "Exchange" (shuffle) nodes
* Hover over them to see partition size distribution
* One or few partitions significantly larger

**Executors tab:**

* One executor has 10x more "Shuffle Write" than others
* That executor's tasks take 10x longer

**Real-world example:**
Processing user events where 1% of users generate 80% of events. When you group by user_id, those "power users" all land in few partitions → massive skew.

**How to spot:** If your job has 1000 tasks, 999 finish in 1 minute, and you wait 30 more minutes for the last task—that's skew.

**1790. How do you identify and fix shuffle spill to disk issues?**

**Identification:**

**In Spark UI Stages tab, look for:**

* "Shuffle Spill (Memory)" > 0 - Data didn't fit in memory during shuffle
* "Shuffle Spill (Disk)" > 0 - Data was written to disk temporarily
* High "Shuffle Read Blocked Time" - Tasks waiting for data to be read from disk

**Bad signs:**

* Spill size approaching input size = most data spilled
* Many GBs spilled per task = serious memory pressure

**Root causes:**

1. **Too much data per partition** during shuffle operations (join, groupBy, repartition)
2. **Too little memory** allocated per executor
3. **Too many cached DataFrames** competing for memory

**Fixes:**

**Increase partitions to reduce per-partition data:**

```python
# Before: 200 partitions (default), each has 5GB
df.repartition(1000)  # Now each has 1GB

# Or let Spark handle it
spark.conf.set("spark.sql.shuffle.partitions", "1000")
```

**Increase executor memory:**

```python
spark.conf.set("spark.executor.memory", "8g")  # More heap space
spark.conf.set("spark.memory.fraction", "0.8")  # More for execution/storage
```

**Reduce shuffle size:**

* Filter before shuffle: `df.filter(...).groupBy(...)` not `df.groupBy(...).filter(...)`
* Aggregate earlier: pre-aggregate before join
* Broadcast small tables: avoid shuffle entirely for small dimension tables

**Uncache unused DataFrames:**

```python
df.unpersist()  # Free up memory
```

**1791. What are best practices for naming columns to avoid conflicts?**

**Use descriptive, collision-free names:**

**Avoid these common pitfalls:**

**1. Same column name in both join tables:**

```python
# BAD - both have 'id' and 'name'
left.join(right, "id").select("id", "name")  # Which name?

# GOOD - prefix with table context
left.select(
    col("id").alias("user_id"),
    col("name").alias("user_name")
).join(right.select(
    col("id").alias("order_id"),
    col("name").alias("product_name")
), left.user_id == right.order_id)
```

**2. Special characters causing parsing issues:**

```python
# Problematic names
"order.date"      # Dot looks like nested field
"user-name"       # Dash can cause issues
"count(*)"        # Parentheses/asterisk are special

# Safe names
"order_date"
"user_name"
"total_count"
```

**3. Reserved keywords:**
Avoid SQL reserved words as column names: `select`, `from`, `where`, `join`, `group`, `order`, etc.

**Best practices:**

* Use **snake_case** consistently: `order_date`, `customer_id`
* **Prefix** columns after joins: `src_id`, `tgt_id` or `left_id`, `right_id`
* **Be specific** : Not `id`, but `customer_id`, `order_id`, `product_id`
* **Avoid abbreviations** that could collide: `cust` vs `customer`, `ord` vs `order`

**1792. How do you handle special characters in column names?**

If you're stuck with column names containing spaces, dots, or special characters (often from source systems), use backticks:

```python
# Column name with spaces/special chars
df.select("`order date`", "`customer.name`", "`amount($)`")

# In SQL
spark.sql("SELECT `order date`, `customer.name` FROM table")

# Better: rename them immediately
df = df.withColumnRenamed("order date", "order_date") \
       .withColumnRenamed("customer.name", "customer_name") \
       .withColumnRenamed("amount($)", "amount_usd")
```

**When to use backticks:**

* Reading from external sources with bad naming (CSV headers, database tables)
* One-off operations before cleanup

**When to rename:**

* Any data you'll work with repeatedly
* Before any joins or complex transformations
* At ingestion time as part of standardization

**Pro tip:** Make column renaming part of your data ingestion pipeline—clean names from the start saves headaches later.

**1793. What is the impact of data types on performance (e.g., StringType vs IntegerType)?**

Data types dramatically affect performance—wrong types waste memory and slow processing.

**Storage size comparison:**

* **IntegerType** : 4 bytes per value
* **LongType** : 8 bytes per value
* **StringType** : Variable (typically 10-50+ bytes for IDs stored as strings)
* **Example:** 1 billion user IDs: 4GB as integers vs ~20GB as strings

**Processing speed:**

* **Numeric comparisons** : Extremely fast (single CPU instruction)
* **String comparisons** : Much slower (byte-by-byte comparison)
* **Example:** Filtering 100M rows on integer column: seconds. Same on string column: minutes.

**Common performance killers:**

**1. Storing IDs as strings:**

```python
# BAD - user_id as string "123456"
df_slow = df.withColumn("user_id", col("id").cast("string"))

# GOOD - user_id as long
df_fast = df.withColumn("user_id", col("id").cast("long"))
```

Joins and groupBys on integer IDs are **5-10x faster** than on string IDs.

**2. Using strings for boolean flags:**

```python
# BAD - "true"/"false" strings
df.withColumn("is_active", lit("true"))  # 4-8 bytes per value

# GOOD - boolean type
df.withColumn("is_active", lit(True))  # 1 byte per value
```

**3. Not using appropriate numeric types:**

```python
# BAD - everything as double
df.withColumn("age", col("age").cast("double"))  # 8 bytes

# GOOD - use smallest type that fits
df.withColumn("age", col("age").cast("byte"))  # 1 byte (sufficient for ages)
```

**When to use strings:** Only for actual text data (names, descriptions, addresses) or categorical data with hundreds+ of distinct values where mapping to integers isn't worth it.

**Rule of thumb:** If it's a number, store it as a number. If it's a flag, use boolean. Reserve strings for actual text.

**1794. Why should you avoid using `count()` multiple times on the same DataFrame?**

Every `count()` triggers a full scan of the data—it's an **action** that reads the entire DataFrame.

**The problem:**

```python
# BAD - three full scans!
total = df.count()
nulls = df.filter(col("age").isNull()).count()
valid = df.filter(col("age").isNotNull()).count()
```

Each `count()` re-reads all data from source, re-applies all transformations. If your DataFrame represents 1TB of data, you just scanned 3TB.

**The fix - single pass with aggregation:**

```python
# GOOD - one scan!
stats = df.agg(
    count("*").alias("total"),
    count(when(col("age").isNull(), 1)).alias("nulls"),
    count(when(col("age").isNotNull(), 1)).alias("valid")
).collect()[0]

total = stats["total"]
nulls = stats["nulls"]
valid = stats["valid"]
```

**Or use cache if you must count multiple times:**

```python
df.cache()  # Materialize once
total = df.count()
nulls = df.filter(col("age").isNull()).count()
valid = df.filter(col("age").isNotNull()).count()
df.unpersist()  # Clean up
```

But caching has memory cost—single-pass aggregation is better.

**1795. What happens when you mix transformation logic with actions improperly?**

Mixing transformations and actions carelessly causes redundant computation and confusing code flow.

**Problem 1: Actions inside transformations**

```python
# BAD - count() action called once per partition!
def process(partition):
    records = list(partition)
    total = df.count()  # This triggers full job... every partition!
    return records

df.mapPartitions(process)  # Disaster
```

**Problem 2: Multiple actions on non-cached DataFrame**

```python
# BAD - re-computes expensive transformation twice
expensive_df = df.join(large_table, "id").groupBy("category").agg(...)
count = expensive_df.count()        # Computes join + groupBy
results = expensive_df.collect()    # Computes join + groupBy AGAIN
```

**Problem 3: Unnecessary early materialization**

```python
# BAD - premature collection
intermediate = df.filter(...).collect()  # Brings to driver
final_df = spark.createDataFrame(intermediate).groupBy(...)  # Back to executors

# GOOD - keep transformations lazy
final_df = df.filter(...).groupBy(...)  # All distributed
```

**Best practices:**

* **Keep transformations pure** - no actions inside map/filter/etc
* **Cache before multiple actions** - if you'll trigger multiple actions on same DataFrame
* **Combine actions when possible** - use single `agg()` instead of multiple `count()`/`sum()`/etc
* **Minimize driver-executor round trips** - avoid collect → createDataFrame patterns

---

# Section 18.2: Data Sampling & Debugging

**1827. How do you use `sample()` for testing on subset of data?**

`sample()` is your friend for development and testing—work with 1% of data while building logic, then run on 100% in production.

```python
# Take 10% random sample
sample_df = df.sample(fraction=0.1, seed=42)

# Work with sample during development
sample_df.filter(...).groupBy(...).agg(...)  # Fast iteration

# Switch to full data for production
# df.filter(...).groupBy(...).agg(...)
```

**Key use cases:**

* **Development** - Test transformations on small subset before running on TBs
* **Debugging** - Reproduce issues with manageable data size
* **Data profiling** - Quick statistics on sample instead of full scan
* **Model training** - Sample for faster training iterations

**Critical:** Use a **seed** for reproducibility—same seed gives same sample every time. Essential for debugging and testing.

**1828. What does `sample(withReplacement, fraction, seed)` mean?**

Three parameters control sampling behavior:

**1. withReplacement (boolean):**

* `False` (default): Each record appears at most once—simple random sample
* `True`: Records can appear multiple times—for bootstrap sampling

```python
# Simple sample - each row appears 0 or 1 times
df.sample(withReplacement=False, fraction=0.1)

# Bootstrap sample - some rows may appear 2-3 times, others not at all
df.sample(withReplacement=True, fraction=0.1)
```

**2. fraction (float 0.0-1.0):**

* Expected proportion of data to sample
* `0.1` = ~10% of rows, `0.5` = ~50% of rows
* Not exact count—statistical sampling, might get 9.8% or 10.3%

**3. seed (integer):**

* Random seed for reproducibility
* Same seed = same sample every run
* Different seed = different random sample

```python
df.sample(False, 0.1, seed=42)  # Always same 10%
df.sample(False, 0.1, seed=99)  # Different 10%
```

**Typical usage:**

```python
# Development: consistent 1% sample
dev_sample = df.sample(False, 0.01, seed=42)

# Testing: different samples to verify robustness
test1 = df.sample(False, 0.1, seed=123)
test2 = df.sample(False, 0.1, seed=456)
```

**1829. What is stratified sampling using `sampleBy()`?**

`sampleBy()` samples different fractions from different groups—ensures representation of rare categories.

**Why it matters:**
Simple random sampling might miss rare categories. If you have 99% "regular" customers and 1% "VIP", a 10% sample might have zero VIPs.

**Stratified sampling ensures balance:**

```python
# Sample different fractions per customer type
fractions = {
    "regular": 0.05,  # 5% of regular customers
    "premium": 0.20,  # 20% of premium customers
    "vip": 1.0        # 100% of VIPs (they're rare)
}

stratified = df.sampleBy("customer_type", fractions, seed=42)
```

**Use cases:**

* **Imbalanced data** - Ensure minority classes represented
* **A/B testing** - Different sample rates for control vs treatment
* **Cost optimization** - Heavy sampling of cheap queries, light sampling of expensive ones
* **Quality checks** - 100% of suspicious records, 1% of normal records

**Example: Fraud detection**

```python
# Fraud is 0.1% of transactions
# Sample heavily from fraud, lightly from normal
fractions = {"fraud": 1.0, "normal": 0.01}
df.sampleBy("label", fractions, seed=42)
# Result: balanced dataset for model training
```

**1830. How do you use `limit()` for quick data inspection?**

`limit(n)` returns first n rows—fastest way to peek at data structure without processing everything.

```python
# Quick peek at first 10 rows
df.limit(10).show()

# Check schema and few sample values
df.limit(5).printSchema()
df.limit(5).show(truncate=False)
```

**When to use limit vs sample:**

* **limit()** : Fast, deterministic, first N rows—good for schema inspection
* **sample()** : Random selection, representative of full data—good for profiling

**Critical gotcha:** `limit()` gives you whatever rows Spark processes first—often from first partition only. Not representative of full dataset.

```python
# BAD for data profiling
df.limit(1000)  # Might all be from January if partitioned by month

# GOOD for data profiling
df.sample(False, 0.001, seed=42)  # Random sample from full dataset
```

**Use limit for:**

* Schema inspection: `df.limit(1).printSchema()`
* Quick existence check: `df.limit(1).count() > 0`
* Fast failure during development: fail fast if data structure wrong
* Testing transformations: `df.filter(...).limit(100).show()` to verify logic

**1831. What does `show(n, truncate)` do? What are the parameters?**

`show()` displays DataFrame contents in tabular format—your primary data inspection tool.

**Parameters:**

**1. n (default=20):** Number of rows to display

```python
df.show()         # First 20 rows
df.show(5)        # First 5 rows
df.show(100)      # First 100 rows
```

**2. truncate (default=True):**

* `True` or number: Truncate strings to 20 characters (or specified width)
* `False`: Show full strings without truncation

```python
# Truncated (default)
df.show()
# +----+--------------------+
# | id |                name|
# +----+--------------------+
# |  1 |    This is a ver...|  # Truncated!
# +----+--------------------+

# Full content
df.show(truncate=False)
# +----+------------------------------------------+
# | id |                                      name|
# +----+------------------------------------------+
# |  1 |This is a very long string that shows ...|
# +----+------------------------------------------+

# Custom truncation width
df.show(10, truncate=50)  # Show 10 rows, truncate strings at 50 chars
```

**3. vertical (optional, default=False):** Show records vertically (one per line)

```python
df.show(n=1, vertical=True)
# -RECORD 0------------------
#  id             | 1
#  name           | John
#  email          | john@example.com
#  created_date   | 2024-01-15
```

**Practical usage:**

```python
# Quick data check
df.show(5, truncate=False)

# Inspect long text fields
df.select("description").show(10, truncate=100)

# Detailed record inspection
df.filter(col("id") == 12345).show(vertical=True)
```

**1832. How do you use `printSchema()` for debugging?**

`printSchema()` displays DataFrame structure—column names, data types, nullability. Essential first step when debugging data issues.

```python
df.printSchema()

# Output:
# root
#  |-- id: long (nullable = true)
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)
#  |-- email: string (nullable = true)
#  |-- created_date: date (nullable = true)
#  |-- metadata: struct (nullable = true)
#  |    |-- source: string (nullable = true)
#  |    |-- version: integer (nullable = true)
```

**What it reveals:**

**1. Data types:** Catch type mismatches early

```python
# Debugging joins failing silently
left_df.printSchema()   # id: string
right_df.printSchema()  # id: long
# Aha! Need to cast for join to work
```

**2. Nullability:** Understand which columns can be null

```python
# Why am I getting nulls in output?
df.printSchema()  # age: integer (nullable = true)
# Expected - age can be null
```

**3. Nested structures:** See complex types (structs, arrays)

```python
# How do I access nested fields?
df.printSchema()
# |-- address: struct
# |    |-- street: string
# |    |-- city: string
# Access: df.select("address.city")
```

**Debugging workflow:**

```python
# 1. Check schema immediately after reading
raw_df = spark.read.csv("data.csv", header=True)
raw_df.printSchema()  # All strings! Need to specify schema.

# 2. Verify schema after transformations
transformed = raw_df.withColumn("age", col("age").cast("int"))
transformed.printSchema()  # age: integer now

# 3. Compare schemas before joins
left.printSchema()
right.printSchema()
# Ensure join keys have matching types
```

**1833. What does `explain()` show? How do you read the physical plan?**

`explain()` shows Spark's execution plan—how it will actually execute your query. Essential for performance debugging.

```python
df.explain()
```

**Output shows two plans:**

**1. Parsed Logical Plan:** Your code, as Spark understands it
**2. Analyzed Logical Plan:** After resolving column names, types
**3. Optimized Logical Plan:** After Catalyst optimizer improvements
**4. Physical Plan:** Actual execution strategy

**How to read the physical plan (bottom-up):**

```python
result = df.filter(col("age") > 25).select("name", "age")
result.explain()

# == Physical Plan ==
# *(1) Project [name#1, age#2]              ← 3. Finally, project columns
# +- *(1) Filter (age#2 > 25)               ← 2. Then filter
#    +- FileScan parquet [name#1, age#2]    ← 1. Start: scan file
```

**Read from bottom to top** (data flows upward):

1. **FileScan** - Read data from source
2. **Filter** - Apply where clause
3. **Project** - Select specific columns

**Key things to look for:**

**1. Exchange (shuffle) operations:**

```
Exchange hashpartitioning(id#1, 200)  ← SHUFFLE! Expensive!
```

Too many shuffles = slow job. Look for ways to reduce.

**2. Broadcast joins:**

```
BroadcastHashJoin [id#1], [id#2]  ← Good! Small table broadcasted
```

vs regular shuffle join:

```
SortMergeJoin [id#1], [id#2]  ← Requires shuffle
```

**3. Partition pruning:**

```
PartitionFilters: [year#10 = 2024]  ← Good! Skipping partitions
PushedFilters: [age > 25]            ← Good! Filter pushed to scan
```

**4. Whole stage code generation (*):**

```
*(1) Project ...  ← Asterisk means optimized code generation
```

**1834. What does `explain(extended=True)` reveal?**

`explain(True)` or `explain(extended=True)` shows **all optimization stages** in detail—the full journey from your code to execution plan.

```python
df.explain(extended=True)
```

**Shows five plans:**

**1. Parsed Logical Plan** - Raw translation of your DataFrame code

```
'Project ['name, 'age]
+- 'Filter ('age > 25)
   +- 'UnresolvedRelation [users]
```

Your code, literally translated. Still has unresolved references (the quotes).

**2. Analyzed Logical Plan** - After Spark resolves all references

```
Project [name#1, age#2]
+- Filter (age#2 > 25)
   +- Relation[name#1, age#2] parquet
```

All columns resolved to actual field IDs. Types validated.

**3. Optimized Logical Plan** - After Catalyst optimizer's transformations

```
Project [name#1, age#2]
+- Filter (age#2 > 25)
   +- FileScan parquet [name#1, age#2] 
      PushedFilters: [age > 25]
```

Optimizations applied:

* Filter pushed down to scan (read less data)
* Column pruning (only read needed columns)
* Predicate pushdown to storage layer

**4. Physical Plan** - Actual execution strategy with concrete operators
Shows same as regular `explain()` but after seeing all optimization steps.

**When to use extended explain:**

**Debugging query optimization issues:**

```python
# Why isn't my filter being pushed down?
df.filter(...).explain(extended=True)
# Check Optimized Plan - is PushedFilters present?
```

**Understanding performance problems:**

```python
# Why is this query slow?
slow_query.explain(extended=True)
# Compare Parsed vs Optimized - what optimizations were applied?
# Check Physical Plan - unexpected shuffles? Missing broadcasts?
```

**Verifying partition pruning:**

```python
# Are my partition filters working?
partitioned_df.filter(col("year") == 2024).explain(extended=True)
# Look for PartitionFilters in Optimized Plan
```

**Learning Spark's optimizer:**
See exactly what transformations Catalyst applies. Educational for understanding what Spark does automatically vs what you need to help with.

**Pro tip:** Usually regular `explain()` is enough. Use `extended=True` when you need to debug **why** Spark made certain optimization decisions or **why** an expected optimization didn't happen.

---

## Summary

**Key Debugging Tools:**

* **Spark UI** - Visual performance analysis (Jobs → Stages → Tasks)
* **Accumulators** - Distributed metrics without collecting data
* **explain()** - Understand execution plans and optimization
* **printSchema()** - Verify data types and structure
* **show()** - Inspect actual data values

**Common Issues & Quick Fixes:**

* **OOM errors** → Increase partitions or memory, avoid collecting large data
* **Task not serializable** → Keep closures simple, don't reference non-serializable objects
* **Data skew** → Repartition with salting, broadcast small tables
* **Shuffle spill** → Increase partitions, increase memory, reduce shuffle size

**Performance Best Practices:**

* Use appropriate data types (integers not strings for IDs)
* Cache before multiple actions
* Single-pass aggregations instead of multiple counts
* Sample for development, full data for production
* Read physical plans to understand and optimize execution
