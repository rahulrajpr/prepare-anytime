# 12. RDDs (Resilient Distributed Datasets)

## 12.1 RDD Fundamentals

### 12.1.1 What is an RDD (Resilient Distributed Dataset)?

**Conceptual Clarity:** RDD (Resilient Distributed Dataset) is Spark's fundamental data abstraction - an immutable, partitioned collection of elements that can be operated on in parallel across a cluster.

**Key Characteristics:**

* **Resilient:** Fault-tolerant through lineage tracking
* **Distributed:** Data partitioned across cluster nodes
* **Dataset:** Collection of Java, Scala, or Python objects
* **Immutable:** Cannot be modified, only transformed into new RDDs
* **Lazy Evaluation:** Computations only happen when actions are called

**Deep Dive & Merits:**

* RDDs are the building blocks of all Spark data structures
* Provide low-level control over data processing
* Support both in-memory and disk-based processing
* Enable fine-grained optimizations for specific use cases

**Practical Implementation:**

```python
# Creating RDDs from different sources
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("RDD Fundamentals")
sc = SparkContext(conf=conf)

# From collection
rdd = sc.parallelize([1, 2, 3, 4, 5], numSlices=4)

# From text file
text_rdd = sc.textFile("hdfs://path/to/file.txt")

# From DataFrame
df_rdd = spark.range(100).rdd
```

**Interview Tips:** RDDs are Spark's foundational data structure, though DataFrames are now preferred for most use cases.

---

### 12.1.2 What makes RDDs resilient?

**Resilience Mechanism:** RDDs achieve resilience through lineage (dependency graph) rather than data replication.

**Resilience Components:**

* **Lineage Tracking:** RDDs remember the sequence of transformations used to build them
* **Dependency Graph:** Maintains parent-child relationships between RDDs
* **Recomputation:** Lost partitions can be recomputed from original data using lineage
* **Persistence:** Optional caching to avoid recomputation

**Code Example:**

```python
# RDD Lineage Example
rdd1 = sc.parallelize([1, 2, 3, 4, 5])
rdd2 = rdd1.map(lambda x: x * 2)
rdd3 = rdd2.filter(lambda x: x > 5)
rdd4 = rdd3.flatMap(lambda x: [x, x+1])
result = rdd4.reduce(lambda a, b: a + b)  # Result: 51

# Lineage: parallelize → map → filter → flatMap → reduce
# Lost partitions can be recomputed from source using this chain
```

**Interview Tips:** RDD resilience is about "recomputation" not "replication".

---

### 12.1.3 How are RDDs fault-tolerant?

**Fault Tolerance Mechanisms:**

| Mechanism                   | How It Works                                                               | When Used                                   |
| --------------------------- | -------------------------------------------------------------------------- | ------------------------------------------- |
| Lineage-based Recomputation | Recomputes lost partitions from original data using transformation lineage | Default behavior for all RDDs               |
| Checkpointing               | Saves RDD to reliable storage (HDFS) to break long lineage chains          | For iterative algorithms with long lineages |
| Persistence/Caching         | Stores RDD partitions in memory or disk across operations                  | When RDD will be reused multiple times      |

**Fault Recovery Process:**

1. **Failure Detection:** Executor failure detected by driver
2. **Lineage Analysis:** Identify lost partitions and their lineage
3. **Recomputation:** Schedule tasks to recompute lost partitions from nearest checkpoint or original source
4. **Continuation:** Resume execution with recomputed data

**Code Example:**

```python
# Complex RDD lineage
base_rdd = sc.parallelize(range(1000), 10)

processed_rdd = (base_rdd
    .map(lambda x: x * 2)
    .filter(lambda x: x % 3 == 0)
    .flatMap(lambda x: [x, x * 10])
    .groupByKey()
    .mapValues(list)
)

# Spark automatically recomputes lost partitions using lineage

# For long lineages, checkpoint to reliable storage
sc.setCheckpointDir("hdfs://checkpoint-dir/")
processed_rdd.checkpoint()

# Or persist for reuse
from pyspark import StorageLevel
processed_rdd.persist(StorageLevel.MEMORY_AND_DISK)
```

**Interview Tips:** RDD fault tolerance enables large-scale computations reliably without data replication overhead.

---

### 12.1.4 What is the difference between narrow dependency and wide dependency in RDDs?

**Conceptual Clarity:** Dependencies define how child RDD partitions depend on parent RDD partitions, which determines the need for data shuffling.

| Aspect            | Narrow Dependency                  | Wide Dependency                               |
| ----------------- | ---------------------------------- | --------------------------------------------- |
| Partition Mapping | 1:1 partition mapping              | Shuffle required, many-to-many mapping        |
| Data Movement     | No data transfer between nodes     | Data exchange between nodes required          |
| Performance       | Efficient, no shuffle              | Expensive, involves shuffle                   |
| Fault Recovery    | Fast recomputation                 | Expensive recomputation                       |
| Examples          | `map()`,`filter()`,`union()` | `groupByKey()`,`reduceByKey()`,`join()` |
| Parallelism       | Full parallelism possible          | Limited by shuffle                            |

**Code Examples:**

```python
# Narrow Dependency (no shuffle)
rdd = sc.parallelize([1, 2, 3, 4, 5])
mapped = rdd.map(lambda x: x * 2)
filtered = rdd.filter(lambda x: x > 3)

# Wide Dependency (requires shuffle)
data = [("a", 1), ("b", 2), ("a", 3), ("b", 4)]
grouped = sc.parallelize(data).groupByKey()
reduced = sc.parallelize(data).reduceByKey(lambda a, b: a + b)
```

**Interview Tips:** Wide dependencies are performance bottlenecks and should be minimized.

---

## 12.2 RDD vs DataFrame

### 12.2.1 Compare RDDs and DataFrames

**Comprehensive Comparison:**

| Aspect           | RDD                                             | DataFrame                               |
| ---------------- | ----------------------------------------------- | --------------------------------------- |
| Optimization     | No built-in optimization, developer responsible | Catalyst optimizer + Tungsten execution |
| Type Safety      | Compile-time type safety (Scala/Java)           | Runtime schema enforcement              |
| Performance      | Slower (Java objects, GC overhead)              | Faster (columnar, code generation)      |
| Ease of Use      | Lower-level API, more code required             | Higher-level, SQL-like operations       |
| Memory Usage     | Higher (Java object overhead)                   | Lower (columnar format, off-heap)       |
| Schema           | Unstructured, schema not enforced               | Structured, schema required             |
| Language Support | Scala, Java, Python, R                          | Scala, Java, Python, R, SQL             |
| Use Cases        | Custom algorithms, low-level operations         | Structured data, ETL, analytics         |

#### 12.2.1a Optimization capabilities

**RDD:** Manual optimization, no query planning, no predicate pushdown

**DataFrame:** Catalyst optimizer with logical/physical planning, predicate pushdown, column pruning, constant folding, Tungsten execution

```python
# RDD - no optimization
rdd = sc.textFile("data.csv").map(lambda l: l.split(",")).filter(lambda f: f[2] == "NY")

# DataFrame - automatic optimization with predicate pushdown
df = spark.read.csv("data.csv", header=True)
result = df.filter(df.city == "NY").select("name", "salary")
```

**Interview Tips:** DataFrames optimize automatically; RDDs require manual optimization.

#### 12.2.1b Type safety

**RDD (Python):** Runtime type checking only
**DataFrame:** Runtime schema enforcement
**Dataset (Scala/Java):** Compile-time type safety + DataFrame optimizations

```python
# Python RDD - runtime errors only
rdd = sc.parallelize([1, 2, 3])
# rdd.map(lambda x: x.split(","))  # Fails at runtime

# DataFrame - schema validation at runtime
df = spark.createDataFrame([(1, "John")], ["id", "name"])
# df.select(df.id + "string")  # Runtime type error
```

**Interview Tips:** Scala Datasets provide both type safety and optimization; Python offers optimization with runtime checking.

#### 12.2.1c Performance

**Performance Comparison:**

| Operation   | RDD    | DataFrame     | Speedup                                 |
| ----------- | ------ | ------------- | --------------------------------------- |
| Filtering   | Slower | 2-10x faster  | Predicate pushdown, columnar processing |
| Aggregation | Slower | 5-100x faster | Tungsten, code generation, off-heap     |
| Joins       | Slower | 2-50x faster  | Optimized join strategies, statistics   |
| Memory      | Higher | Lower         | Columnar format, less GC pressure       |

**Interview Tips:** DataFrames are typically 2-10x faster than RDDs for structured data operations.

#### 12.2.1d Ease of use

**DataFrame API is more concise and SQL-like:**

```python
# DataFrame approach
logs_df = spark.read.text("server_logs.txt")

from pyspark.sql.functions import split, col

result = (logs_df
    .withColumn("ip", split(col("value"), " ").getItem(0))
    .groupBy("ip")
    .count()
    .filter(col("count") > 100)
    .orderBy(col("count").desc())
)

# Or with SQL
logs_df.createOrReplaceTempView("logs")
result = spark.sql("""
    SELECT split(value, ' ')[0] as ip, COUNT(*) as count
    FROM logs
    GROUP BY ip
    HAVING count > 100
    ORDER BY count DESC
""")
```

**Interview Tips:** DataFrames are preferred for ease of use and automatic optimizations.

---

### 12.2.2 When would you prefer using RDDs over DataFrames/Datasets?

**Use RDDs when:**

✅ Implementing custom algorithms not expressible in SQL

✅ Working with non-tabular data (graphs, text corpora)

✅ Needing fine-grained control over execution

✅ Integrating with legacy RDD-based systems

✅ Performing low-level operations not supported by DataFrames

**Example Use Cases:**

```python
# 1. Custom iterative algorithms
def custom_ml_algorithm(rdd):
    for iteration in range(100):
        rdd = rdd.mapPartitions(complex_transformation)
        if iteration % 10 == 0:
            rdd.checkpoint()  # Manage lineage
    return rdd

# 2. Graph processing (non-tabular)
def page_rank(vertices, edges, iterations=10):
    ranks = vertices.map(lambda v: (v, 1.0))
    for i in range(iterations):
        contributions = edges.join(ranks).flatMap(compute_contributions)
        ranks = contributions.reduceByKey(lambda x, y: x + y)
    return ranks

# 3. Custom partitioning and memory control
rdd = sc.parallelize(data).partitionBy(100, custom_partitioner)
rdd.persist(StorageLevel.MEMORY_ONLY_SER)
```

**Interview Tips:** Start with DataFrames for structured data; use RDDs only when you need their specific capabilities.

---

## 12.3 API Hierarchy

### 12.3.1 How do Spark SQL, Dataset API, DataFrame API, Catalyst Optimizer, and RDD relate to each other?

**Spark API Hierarchy:**

```
Spark SQL (SQL Queries)
     ↓
Dataset API (Scala/Java - Type Safe)
     ↓
DataFrame API (Python/R/Scala/Java - Untyped)
     ↓
Catalyst Optimizer (Query Optimization)
     ↓
Tungsten Execution (Binary Format + Code Gen)
     ↓
RDD (Foundation)
     ↓
Spark Core
```

**Key Points:**

* **Spark SQL:** SQL interface on top of DataFrames
* **Dataset API:** Type-safe (Scala/Java only)
* **DataFrame API:** Untyped, works across all languages
* **Catalyst Optimizer:** Works with DataFrame/Dataset, not RDDs
* **RDD:** Foundation for all higher-level APIs

**Same Operation, Different APIs:**

```python
# SQL
spark.sql("SELECT name, COUNT(*) FROM users GROUP BY name HAVING COUNT(*) > 1")

# DataFrame
df.groupBy("name").count().filter("count > 1")

# RDD
df.rdd.map(lambda row: (row["name"], 1)).reduceByKey(lambda a, b: a + b).filter(lambda x: x[1] > 1)
```

All compile to RDD operations executed on Spark Core.

---

### 12.3.2 What is Spark Core and how does it relate to higher-level APIs?

**Spark Core Components:**

| Component         | Function                     | Relation to Higher APIs             |
| ----------------- | ---------------------------- | ----------------------------------- |
| RDD API           | Fundamental data structure   | Foundation for DataFrames/Datasets  |
| Scheduler         | Task scheduling              | Used by all higher-level APIs       |
| Memory Management | Memory allocation & spilling | Enhanced by Tungsten for DataFrames |
| Fault Tolerance   | Lineage-based recovery       | Inherited by all APIs               |
| Storage System    | Persistence layers           | Used for caching in all APIs        |

**Key Points:**

* Spark Core is the foundation for all APIs
* Handles cluster management, memory, and fault tolerance
* Higher-level APIs add optimizations but rely on Core for execution

```python
# All APIs use the same Spark Core
conf = SparkConf().setAppName("MyApp")
sc = SparkContext(conf=conf)  # Spark Core entry point
spark = SparkSession.builder.getOrCreate()  # Built on Spark Core

# RDD, DataFrame, and SQL all use same underlying infrastructure
rdd = sc.parallelize([1, 2, 3])
df = spark.range(3)
sql_result = spark.sql("SELECT * FROM range(3)")
```

**Interview Tips:** Understanding this hierarchy helps debug performance issues and choose the right API.

---

## Final RDD Interview Tips

* **Know when to use RDDs vs DataFrames** - structured data → DataFrames, custom algorithms → RDDs
* **Understand the performance trade-offs** - RDDs give control, DataFrames give optimization
* **Be familiar with the API hierarchy** - it shows Spark's evolution and design philosophy
* **Practice explaining narrow vs wide dependencies** - crucial for performance tuning
* **Remember that RDDs are still relevant** for specific use cases despite DataFrames being the default
