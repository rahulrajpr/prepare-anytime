## 5. User-Defined Functions (UDFs)

### 5.1 UDF Registration & Usage

#### 5.1.1 How do you register a UDF for use in DataFrame functions?

**UDF Registration for DataFrame API:**

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Define Python function
def reverse_string(s):
    return s[::-1] if s else None

# Register UDF with return type
reverse_udf = udf(reverse_string, StringType())

# Use in DataFrame operations
df.withColumn("reversed_name", reverse_udf(col("name")))
```

#### 5.1.2 How do you register a UDF for use in SQL expressions?

**UDF Registration for SQL:**

**python**

```
from pyspark.sql.types import IntegerType

# Register UDF for SQL usage
spark.udf.register("sql_reverse", reverse_string, StringType())

# Use in SQL expressions
df.createOrReplaceTempView("people")
spark.sql("SELECT name, sql_reverse(name) as reversed_name FROM people")
```

#### 5.1.3 When is a UDF available in the Spark catalog?

**Availability Timeline:**

* **DataFrame UDF** : Available immediately after registration in current session
* **SQL UDF** : Available in Spark catalog after registration, accessible across entire SparkSession

#### 5.1.4 How do you list all registered functions using spark.catalog.listFunctions()?

**Function Discovery:**

**python**

```
# List all available functions
functions = spark.catalog.listFunctions()

# Filter for custom UDFs
udfs = [f for f in functions if not f.isBuiltIn]
for udf in udfs:
    print(f"UDF: {udf.name}, Database: {udf.database}")

# Check if specific UDF exists
if spark.catalog.functionExists("sql_reverse"):
    print("UDF is registered")
```

#### 5.1.5 What are the performance implications of UDFs compared to built-in functions?

**Critical Performance Differences:**

| Aspect                  | Built-in Functions                          | UDFs                              |
| ----------------------- | ------------------------------------------- | --------------------------------- |
| **Optimization**  | Catalyst optimizer can rewrite and optimize | Black box - no optimization       |
| **Execution**     | Vectorized, native code                     | Row-by-row, Python interpreter    |
| **Serialization** | No data movement                            | Python-JVM serialization overhead |
| **Performance**   | 10-100x faster                              | Slower due to overhead            |
| **Memory**        | Efficient columnar processing               | Object creation per row           |

**Performance Impact Example:**

**python**

```
# Built-in - fast and optimized
df.withColumn("upper_name", upper(col("name")))

# UDF - 10x slower, no optimization  
def custom_upper(s):
    return s.upper() if s else None

upper_udf = udf(custom_upper, StringType())
df.withColumn("upper_name", upper_udf(col("name")))
```

**Interview Tip:** Always prefer built-in functions over UDFs for performance-critical operations.

## 6. Data Sources & I/O Operations

### 6.1 Reading Data - Basics

#### 6.1.1 What is the difference between spark.read.table() and spark.read.parquet()?

**Key Differences:**

| Method                                  | Data Source            | Requirements                  | Use Case                  |
| --------------------------------------- | ---------------------- | ----------------------------- | ------------------------- |
| `spark.read.table("table_name")`      | Hive Metastore table   | Table must exist in metastore | Reading registered tables |
| `spark.read.parquet("path/to/files")` | Parquet files directly | Files in supported storage    | Direct file access        |

**Example Usage:**

**python**

```
# Reading from Hive table
df_table = spark.read.table("sales_db.customer_orders")

# Reading Parquet files directly  
df_parquet = spark.read.parquet("/data/customer_orders/")

# They can point to the same underlying data!
```

#### 6.1.2 What does the read.option('samplingRatio', 'true') do during schema inference?

**Sampling Ratio Purpose:** Controls what fraction of data is sampled for schema inference

**Default Behavior:**`samplingRatio=1.0` (sample all data) for reliable inference

**Performance Trade-off:**

**python**

```
# Fast but potentially inaccurate
df = spark.read.option("samplingRatio", 0.1).csv("data.csv")  # Sample 10%

# Slow but accurate  
df = spark.read.option("samplingRatio", 1.0).csv("data.csv")  # Sample 100%
```

#### 6.1.3 What is the option('dateFormat', 'fmt') used for? What are common date format patterns?

**DateFormat Usage:** Specifies custom date parsing patterns

**Common Patterns:**

**python**

```
# Different date formats in data
df = spark.read.option("dateFormat", "yyyy-MM-dd").csv("dates.csv")
df = spark.read.option("dateFormat", "MM/dd/yyyy").csv("us_dates.csv")
df = spark.read.option("dateFormat", "dd-MM-yyyy").csv("eu_dates.csv")

# With timestamp
df = spark.read.option("timestampFormat", "yyyy-MM-dd HH:mm:ss").csv("timestamps.csv")
```

**Popular Format Patterns:**

* `"yyyy-MM-dd"` - ISO standard (2024-01-15)
* `"MM/dd/yyyy"` - US format (01/15/2024)
* `"dd-MM-yyyy"` - European format (15-01-2024)
* `"yyyyMMdd"` - Compact format (20240115)

#### 6.1.4 How do you handle corrupted or malformed rows when reading CSV files?

**Corruption Handling Strategies:**

**python**

```
# Option 1: Permissive mode (default) - store corrupt records in column
df = spark.read.option("mode", "PERMISSIVE") \
              .option("columnNameOfCorruptRecord", "_corrupt_record") \
              .csv("data.csv")

# Option 2: Drop malformed - skip bad rows
df = spark.read.option("mode", "DROPMALFORMED").csv("data.csv")

# Option 3: Fail fast - throw exception on corruption
df = spark.read.option("mode", "FAILFAST").csv("data.csv")
```

#### 6.1.5 How do you achieve parallelism when reading from non-partitioned data files?

**Parallelism Strategies:**

1. **Multiple Files** : Read directory with many small files
2. **File Splitting** : For splittable formats (text, CSV with known row boundaries)
3. **Repartition After Read** :
   **python**

```
df = spark.read.csv("large_file.csv").repartition(100)
```

 **maxPartitionBytes** : Control partition size

**python**

```
df = spark.read.option("maxPartitionBytes", "134217728")  # 128MB
```

#### 6.1.6 What are the different Spark data sources and sinks available?

**Built-in Data Sources:**

* **File Formats** : CSV, JSON, Parquet, ORC, Avro, Text
* **Databases** : JDBC (PostgreSQL, MySQL, etc.)
* **NoSQL** : Cassandra, MongoDB (via connectors)
* **Streaming** : Kafka, Kinesis
* **Cloud Storage** : S3, GCS, Azure Blob Storage

#### 6.1.7 What is the findspark library and when do you use it?

**findspark Purpose:** Helps locate Spark installation in Python environment

**Usage Pattern:**

**python**

```
import findspark
findspark.init()  # Auto-detects SPARK_HOME or searches common locations

# Now you can import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
```

**When to Use:** Local development, Jupyter notebooks, when Spark isn't in default PATH

### 6.1.0 Pandas vs Spark File Reading - Core Differences

#### 6.1.0.1 What is the fundamental difference between Pandas and Spark file reading?

**Core Architectural Difference:**

* **Pandas** : Single-machine processing - one process reads entire file
* **Spark** : Distributed processing - multiple executors read file parts in parallel

#### 6.1.0.2 What processing model does Pandas use - single-machine or distributed?

**Pandas:** SINGLE-MACHINE processing

**Implication:** Limited by single machine's memory and CPU

#### 6.1.0.3 What processing model does Spark use - single-machine or distributed?

**Spark:** DISTRIBUTED processing

**Implication:** Scales across cluster, limited by cluster resources

#### 6.1.0.4 Can Pandas read files directly from HTTP/HTTPS URLs?

**Answer:** YES - Pandas can read directly from HTTP/HTTPS URLs

**Example:**

**python**

```
import pandas as pd

# Pandas can download and read in one step
df_pandas = pd.read_csv("https://example.com/data.csv")
```

#### 6.1.0.5 Can Spark read files directly from HTTP/HTTPS URLs?

**Answer:** NO - Spark cannot read directly from HTTP/HTTPS URLs

**Reason:** HTTP is not a distributed file system that all executors can access

#### 6.1.0.6 Why doesn't Spark support reading from HTTP/HTTPS URLs directly?

**Technical Reason:** Spark's distributed architecture requires that ALL executors can access the same data source simultaneously and consistently.

#### 6.1.0.7 What would happen if each Spark executor downloaded from HTTP independently?

**Chaos Scenario:**

* Different executors might get different data (if URL serves dynamic content)
* Network issues could cause partial downloads
* No coordination between executors on data partitioning
* Impossible to guarantee data consistency

#### 6.1.0.8 What problem does independent HTTP downloading cause for data consistency?

**Consistency Issues:**

1. **Race Conditions** : Executors downloading at different times get different data
2. **Partial Failures** : Some executors succeed, others fail
3. **Data Skew** : Uneven distribution of downloaded data
4. **No Atomicity** : Cannot guarantee all-or-nothing data access

#### 6.1.0.9 Does HTTP URL reading violate distributed computing principles? Why?

**Answer:** YES - violates distributed computing principles

**Principles Violated:**

* **Consistency** : All nodes should see same data
* **Atomicity** : Operations should succeed or fail completely
* **Coordinated Access** : Distributed systems need coordination mechanisms

#### 6.1.0.10 What file systems does Spark support for distributed reading?

**Supported Distributed File Systems:**

* **HDFS** (Hadoop Distributed File System)
* **S3** (Amazon Simple Storage Service)
* **GCS** (Google Cloud Storage)
* **Azure Blob Storage**
* **Local file system** (if all executors can access same path)

#### 6.1.0.11 Can Spark read from local file systems? What is the requirement?

**Answer:** YES, with requirement

**Requirement:** The local file path must be accessible from ALL executors

**Cluster Scenario:** In distributed cluster, local paths are different on each machine

#### 6.1.0.12 Can Spark read from HDFS (Hadoop Distributed File System)?

**Answer:** YES - native support

**Example:**`spark.read.parquet("hdfs://namenode:port/path/to/data")`

#### 6.1.0.13 Can Spark read from AWS S3?

**Answer:** YES - with proper configuration

**Example:**`spark.read.parquet("s3a://bucket-name/path/to/data")`

#### 6.1.0.14 Can Spark read from Google Cloud Storage?

**Answer:** YES - with GCS connector

**Example:**`spark.read.parquet("gs://bucket-name/path/to/data")`

#### 6.1.0.15 Can Spark read from Azure Blob Storage?

**Answer:** YES - with Azure connector

**Example:**`spark.read.parquet("wasbs://container@storageaccount.blob.core.windows.net/path")`

#### 6.1.0.16 What are the three workarounds for reading HTTP URLs in Spark?

**Workaround Strategies:**

1. **Download then Read** : Download file to distributed storage first
2. **Pandas Bridge** : Use Pandas to download, then convert to Spark
3. **Manual Download** : Custom download logic with distribution

#### 6.1.0.17 How do you use the "download then read" workaround for HTTP URLs?

**Download Then Read Pattern:**

**python**

```
import requests
import tempfile

# 1. Download using requests (single machine)
url = "https://example.com/large_data.csv"
response = requests.get(url)

# 2. Save to distributed storage (HDFS, S3, etc.)
with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
    tmp_file.write(response.content)
    temp_path = tmp_file.name

# 3. Copy to distributed storage (pseudo-code)
distributed_path = copy_to_hdfs(temp_path)

# 4. Read with Spark
df = spark.read.csv(distributed_path)
```

#### 6.1.0.18 How do you use the "Pandas bridge" workaround for HTTP URLs?

**Pandas Bridge Pattern:**

**python**

```
import pandas as pd

# 1. Download and read with Pandas (single machine)
pandas_df = pd.read_csv("https://example.com/data.csv")

# 2. Convert to Spark DataFrame (brings all data to driver)
spark_df = spark.createDataFrame(pandas_df)

# 3. Distribute data if needed
distributed_df = spark_df.repartition(10)
```

**Limitation:** All data must fit in driver memory

#### 6.1.0.19 How do you use the "manual download" workaround for HTTP URLs?

**Manual Download with Distribution:**

**python**

```
# Custom function to download and distribute
def download_and_distribute(url, spark_context):
    # Download on driver
    content = requests.get(url).content.decode('utf-8')
    lines = content.split('\n')
  
    # Distribute lines to executors
    rdd = spark_context.parallelize(lines)
  
    # Parse as CSV
    header = lines[0]
    data_rdd = rdd.filter(lambda x: x != header)
  
    return data_rdd

# Usage
url = "https://example.com/data.csv"
rdd = download_and_distribute(url, spark.sparkContext)
df = spark.read.csv(rdd)
```

### 6.1.0.2 Pandas vs Spark File Reading - Practical Implementation

#### 6.1.0.2.1 What is the core requirement for Spark file reading: any accessible path or distributed storage?

**Core Requirement:** DISTRIBUTED STORAGE

**Explanation:** All executors must be able to read different parts of the same dataset simultaneously and consistently.

#### 6.1.0.2.2 Why is coordinated access critical for parallel processing in Spark?

**Coordinated Access Importance:**

* **Data Partitioning** : Executors need to read non-overlapping data segments
* **Fault Tolerance** : Failed tasks can be retried with same data
* **Consistency** : All transformations see the same underlying data
* **Performance** : Parallel reading without conflicts or duplication

#### 6.1.0.2.3 What happens if Spark allowed HTTP URLs and each executor downloaded independently?

**Disaster Scenario:**

* Executor 1 downloads file version A
* Executor 2 downloads file version B (if file changed)
* Executor 3 gets network error and fails
* Result: Inconsistent, partial, corrupted data

#### 6.1.0.2.4 Would independent HTTP downloads guarantee data consistency across executors?

**Answer:** NO - impossible to guarantee consistency

**Reasons:**

* HTTP servers might serve different content
* Network issues cause partial downloads
* No atomic snapshot of the data

#### 6.1.0.2.5 What distributed computing principle would HTTP URL reading violate?

**Violated Principle:****Shared-Nothing Architecture**

In shared-nothing architecture:

* Each node operates independently
* But they operate on the same consistent dataset
* HTTP breaks this by potentially giving different data to different nodes

#### 6.1.0.2.6 What is the key difference between "accessible path" (Pandas) vs "distributed storage" (Spark)?

**Key Difference:**

| Pandas "Accessible Path" | Spark "Distributed Storage"  |
| ------------------------ | ---------------------------- |
| Single machine access    | All cluster nodes access     |
| Sequential reading       | Parallel reading             |
| No coordination needed   | Requires coordination        |
| Any URL or local path    | Specific distributed systems |

#### 6.1.0.2.7 How do you implement the "download then read" workaround for HTTP URLs in Spark?

**Complete Implementation:**

**python**

```
def http_to_spark_dataframe(url, spark_session, target_storage_path):
    """
    Download from HTTP URL and make available to Spark
    """
    import requests
    import os
  
    # 1. Download on driver
    print(f"Downloading from {url}")
    response = requests.get(url)
    response.raise_for_status()
  
    # 2. Save to temporary local file
    temp_file = "/tmp/temp_download.csv"
    with open(temp_file, 'wb') as f:
        f.write(response.content)
  
    # 3. Copy to distributed storage (HDFS example)
    distributed_path = f"{target_storage_path}/downloaded_data.csv"
    os.system(f"hdfs dfs -put {temp_file}{distributed_path}")
  
    # 4. Clean up local file
    os.remove(temp_file)
  
    # 5. Read with Spark from distributed storage
    df = spark_session.read.csv(distributed_path, header=True)
    return df

# Usage
df = http_to_spark_dataframe(
    "https://example.com/data.csv", 
    spark, 
    "hdfs://cluster/path/to/storage"
)
```

#### 6.1.0.2.8 How do you implement the "Pandas bridge" workaround for HTTP URLs?

**Pandas Bridge Implementation:**

**python**

```
def pandas_bridge_http(url, spark_session, sampling_ratio=1.0):
    """
    Use Pandas to download HTTP data, then convert to Spark
    Best for small to medium datasets that fit in memory
    """
    import pandas as pd
  
    # 1. Read with Pandas (handles HTTP automatically)
    if sampling_ratio < 1.0:
        # Sample if dataset is too large
        pandas_df = pd.read_csv(url, nrows=int(1000000 * sampling_ratio))
    else:
        pandas_df = pd.read_csv(url)
  
    # 2. Convert to Spark DataFrame
    spark_df = spark_session.createDataFrame(pandas_df)
  
    return spark_df

# Usage for small dataset
small_df = pandas_bridge_http("https://example.com/small_data.csv", spark)

# Usage with sampling for exploration
sample_df = pandas_bridge_http("https://example.com/large_data.csv", spark, sampling_ratio=0.1)
```

#### 6.1.0.2.9 What is the "manual download + distribute" workaround? When is it useful?

**Manual Distribution Pattern:**

**python**

```
def manual_http_distribution(url, spark_context, num_partitions=10):
    """
    Manually download and distribute data across cluster
    Useful when you need custom parsing logic
    """
    import requests
    import csv
    from io import StringIO
  
    # 1. Download on driver
    response = requests.get(url)
    content = response.text
  
    # 2. Parse and distribute
    reader = csv.reader(StringIO(content))
    header = next(reader)  # Skip header
  
    # Convert to list of rows
    rows = list(reader)
  
    # 3. Distribute across cluster
    rdd = spark_context.parallelize(rows, numPartitions=num_partitions)
  
    # 4. Convert to structured data
    def parse_row(row):
        # Custom parsing logic
        return {
            'col1': row[0],
            'col2': int(row[1]),
            'col3': float(row[2])
        }
  
    parsed_rdd = rdd.map(parse_row)
    return parsed_rdd

# Usage
rdd = manual_http_distribution("https://example.com/data.csv", spark.sparkContext)
df = rdd.toDF()
```

#### 6.1.0.2.10 How do you verify all Spark executors can access the file path?

**Access Verification:**

**python**

```
def verify_path_accessibility(spark_session, file_path):
    """
    Verify that all executors can access a file path
    """
    # Try to read metadata (doesn't load data)
    try:
        # For file-based sources
        df = spark_session.read.format("parquet").load(file_path)
        print(f"Successfully accessed: {file_path}")
        return True
    except Exception as e:
        print(f"Access failed: {e}")
        return False

# Test different storage systems
paths_to_test = [
    "hdfs://cluster/path/data.parquet",
    "s3a://bucket/path/data.parquet", 
    "file:///local/path/data.parquet"  # Only works if local to all executors
]

for path in paths_to_test:
    verify_path_accessibility(spark, path)
```

#### 6.1.0.2.11 What are the performance implications of each HTTP URL workaround?

**Performance Analysis:**

| Workaround                    | Performance                     | Memory Impact        | Use Case                    |
| ----------------------------- | ------------------------------- | -------------------- | --------------------------- |
| **Download then Read**  | Medium (network + copy)         | Low                  | Large files, production     |
| **Pandas Bridge**       | Fast for small data             | High (driver memory) | Small datasets, exploration |
| **Manual Distribution** | Slow (single-threaded download) | Medium               | Custom parsing needed       |

#### 6.1.0.2.12 When should you use "download then read" vs "Pandas bridge" approach?

**Decision Framework:**

**Use "Download then Read" When:**

* Large datasets (>1GB)
* Production environment
* Need Spark optimizations (predicate pushdown, etc.)
* Data will be used multiple times

**Use "Pandas Bridge" When:**

* Small datasets (< driver memory)
* Quick data exploration
* Prototyping and development
* One-time analysis

### 6.1.1 CSV Reading Options & Gotchas

#### 6.1.1.1 What does option('header', 'true') do when reading CSV files?

**Header Option:** Treats first row as column names

**Usage:**

**python**

```
# With header
df = spark.read.option("header", "true").csv("data_with_header.csv")

# Without header (Spark generates col1, col2, ...)
df = spark.read.option("header", "false").csv("data_no_header.csv")
```

**Gotcha:** If header is true but file has no header, first data row becomes column names!

#### 6.1.1.2 What is option('inferSchema', 'true') and what are its performance implications?

**InferSchema:** Automatically detects column data types from data

**Performance Impact:**

**python**

```
# Fast but may infer wrong types
df = spark.read.option("inferSchema", "true").csv("data.csv")

# Slow but accurate - reads data twice!
# 1st pass: sample data for type inference
# 2nd pass: read with inferred schema
```

**Better Approach:** Specify schema explicitly for production:

**python**

```
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

custom_schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("city", StringType(), True)
])

df = spark.read.schema(custom_schema).csv("data.csv")  # No inference needed
```

#### 6.1.1.3 How do you specify custom delimiters using option('sep', ',')?

**Custom Delimiters:**

**python**

```
# Tab-separated
df = spark.read.option("sep", "\t").csv("data.tsv")

# Pipe-separated  
df = spark.read.option("sep", "|").csv("data.psv")

# Custom multi-character delimiter
df = spark.read.option("sep", "||").csv("data.custom")
```

#### 6.1.1.4 What does option('quote', '"') control?

**Quote Character:** Defines character used to quote fields containing special characters

**Usage:**

**python**

```
# Handle fields with commas: "Smith, John",30,"NY, USA"
df = spark.read.option("quote", "\"").csv("data_with_quotes.csv")

# Different quote character
df = spark.read.option("quote", "'").csv("data_with_single_quotes.csv")
```

#### 6.1.1.5 How do you handle multi-line records using option('multiLine', 'true')?

**Multi-line Records:** For CSV fields containing newlines

**Example:**

**python**

```
# Without multiLine - fails on records like:
# 1,"This is a
# multi-line field",30

# With multiLine - handles properly
df = spark.read.option("multiLine", "true") \
              .option("quote", "\"") \
              .csv("multiline_data.csv")
```

**Performance Note:** multiLine=true is slower and may not work well with distributed reading

#### 6.1.1.6 What does option('escape', '\') do?

**Escape Character:** Defines character to escape special characters within quoted fields

**Usage:**

**python**

```
# Handle: "Value with \"quotes\" inside",30
df = spark.read.option("escape", "\\").csv("data_with_escapes.csv")
```

#### 6.1.1.7 What is option('nullValue', 'NULL') used for?

**Null Value:** Specifies string representation of null values

**Usage:**

**python**

```
# Treat "NULL", "N/A", "empty" as null
df = spark.read.option("nullValue", "NULL") \
              .option("nullValue", "N/A") \
              .option("nullValue", "") \
              .csv("data_with_nulls.csv")
```

#### 6.1.1.8 How does option('mode', 'PERMISSIVE') differ from 'DROPMALFORMED' and 'FAILFAST'?

**Parsing Modes:**

| Mode                          | Behavior                                 | Use Case                           |
| ----------------------------- | ---------------------------------------- | ---------------------------------- |
| **PERMISSIVE**(default) | Sets malformed fields to null, continues | Data exploration, dirty data       |
| **DROPMALFORMED**       | Drops entire malformed rows              | Clean datasets, skip bad records   |
| **FAILFAST**            | Throws exception on first malformed row  | Data validation, strict processing |

**Example:**

**python**

```
# Continue with nulls for bad records
df1 = spark.read.option("mode", "PERMISSIVE").csv("dirty_data.csv")

# Skip bad records entirely  
df2 = spark.read.option("mode", "DROPMALFORMED").csv("dirty_data.csv")

# Fail immediately on bad data
df3 = spark.read.option("mode", "FAILFAST").csv("clean_data.csv")
```

#### 6.1.1.9 What is option('columnNameOfCorruptRecord', '_corrupt_record') used for?

**Corrupt Record Column:** Stores malformed records in a separate column when using PERMISSIVE mode

**Usage:**

**python**

```
df = spark.read.option("mode", "PERMISSIVE") \
              .option("columnNameOfCorruptRecord", "_corrupt_record") \
              .csv("data.csv")

# Later analyze corrupt records
corrupt_records = df.filter(col("_corrupt_record").isNotNull())
```

#### 6.1.1.10 How do you handle files with different encodings using option('encoding', 'UTF-8')?

**Encoding Support:**

**python**

```
# UTF-8 (default)
df = spark.read.option("encoding", "UTF-8").csv("data.csv")

# Latin-1
df = spark.read.option("encoding", "ISO-8859-1").csv("legacy_data.csv")

# UTF-16
df = spark.read.option("encoding", "UTF-16").csv("unicode_data.csv")
```

#### 6.1.1.11 What does option('ignoreLeadingWhiteSpace', 'true') and option('ignoreTrailingWhiteSpace', 'true') do?

**White Space Handling:**

**python**

```
# Trim spaces around fields
df = spark.read.option("ignoreLeadingWhiteSpace", "true") \
              .option("ignoreTrailingWhiteSpace", "true") \
              .csv("data_with_spaces.csv")

# Handles: "  value  " → "value"
```

#### 6.1.1.12 Why might you get different results with inferSchema=true on partial data?

**Inference Risks:**

* Early rows might be all numbers → inferred as integer
* Later rows have decimals → runtime errors
* Mixed types in column → some data lost or incorrect casting

**Example:**

**python**

```
# Data:
# id,value
# 1,100
# 2,200
# 3,300.5  ← Decimal appears later!

# inferSchema sees first rows: value → IntegerType
# But 300.5 gets truncated to 300 or causes error!
```

### 6.1.2 JSON Reading Options

#### 6.1.2.1 What is option('multiLine', 'true') important for when reading JSON?

**Multi-line JSON:** Handles JSON objects spanning multiple lines

**Usage:**

**python**

```
# Single-line JSON (each line is complete JSON)
df = spark.read.option("multiLine", "false").json("single_line.json")

# Multi-line JSON (pretty-printed, objects span lines)
df = spark.read.option("multiLine", "true").json("multi_line.json")
```

#### 6.1.2.2 How does JSON schema inference work differently from CSV?

**JSON Inference Differences:**

* Infers nested structures (structs, arrays)
* Handles missing fields gracefully
* Can infer complex types from sample data

**Example:**

**python**

```
# JSON with nested structure
df = spark.read.option("inferSchema", "true").json("nested_data.json")
# Infers: root
#  |-- user: struct
#  |    |-- name: string
#  |    |-- age: integer
#  |-- tags: array
#  |    |-- element: string
```

#### 6.1.2.3 What does option('primitivesAsString', 'true') do?

**Primitives as String:** Treats all primitive JSON values as strings

**Usage:**

**python**

```
# All numbers, booleans become strings
df = spark.read.option("primitivesAsString", "true").json("data.json")
# {"age": 30} → age becomes string "30"
```

#### 6.1.2.4 How do you handle JSON files with inconsistent schemas?

**Schema Handling Strategies:**

**python**

```
# Option 1: Permissive mode with columnForCorruptRecords
df = spark.read.option("mode", "PERMISSIVE") \
              .option("columnNameOfCorruptRecord", "_corrupt_record") \
              .json("inconsistent_data.json")

# Option 2: Specify schema to enforce structure
from pyspark.sql.types import StructType, StructField, StringType

enforced_schema = StructType([
    StructField("required_field", StringType(), False)
])
df = spark.read.schema(enforced_schema).json("data.json")
```

### 6.1.3 Parquet Reading Options

#### 6.1.3.1 Does Parquet require schema inference? Why or why not?

**Answer:** NO - Parquet files contain embedded schema

**Reason:** Parquet is a self-describing format - schema is stored with data

**Benefit:** Faster reading, no inference needed, consistent schema

#### 6.1.3.2 What is option('mergeSchema', 'true') used for in Parquet?

**Merge Schema:** Combines schemas from multiple Parquet files

**Usage:**

**python**

```
# Read directory with Parquet files that have different schemas
df = spark.read.option("mergeSchema", "true").parquet("partitioned_data/")

# Handles schema evolution across partitions
```

#### 6.1.3.3 How does Parquet handle predicate pushdown?

**Predicate Pushdown:** Filters are applied at file reading time, skipping irrelevant data

**Example:**

**python**

```
# Spark only reads rows where age > 30 from Parquet files
df = spark.read.parquet("user_data/")
result = df.filter(col("age") > 30)  # Pushdown happens here
```

#### 6.1.3.4 What are the advantages of columnar storage in Parquet for read performance?

**Columnar Storage Benefits:**

* **Column Pruning** : Read only needed columns
* **Better Compression** : Similar data values compress better
* **Predicate Pushdown** : Skip irrelevant data during read
* **Vectorized Processing** : Process columns in batches

### 6.1.4 ORC & Avro Reading

#### 6.1.4.1 How does ORC compare to Parquet for read performance?

**ORC vs Parquet:**

| Aspect                      | ORC             | Parquet               |
| --------------------------- | --------------- | --------------------- |
| **Origin**            | Hive community  | Apache community      |
| **Compression**       | Better for text | Better for mixed data |
| **Hive Integration**  | Excellent       | Good                  |
| **Spark Integration** | Good            | Excellent             |
| **ACID Support**      | Yes (Hive)      | Limited               |

#### 6.1.4.2 What is Avro's advantage for schema evolution?

**Avro Schema Evolution:**

* Forward and backward compatibility
* Schema stored with data
* Handles added/removed fields gracefully

**Usage:**

**python**

```
df = spark.read.format("avro").load("data.avro")
```

#### 6.1.4.3 When would you choose ORC over Parquet?

**Choose ORC When:**

* Working primarily with Hive
* Need ACID transactions
* Heavy text data with good compression
* Existing Hive/ORC infrastructure

**Choose Parquet When:**

* Spark-first environment
* Mixed data types
* Better ecosystem support
* Cloud-native deployments
