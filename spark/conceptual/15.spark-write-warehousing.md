# PySpark Interview Preparation Guide

## 6.3.4 Detailed Comparison

### 6.3.4.1 What separation level does partitionBy() operate at - directory, file, or row?

**partitionBy() Level:** DIRECTORY level separation

**Detailed Explanation:**
partitionBy() creates separate physical directories for each distinct value of the partitioning column. This is a coarse-grained separation that organizes data at the storage level.

### 6.3.4.2 What separation level does bucketBy() operate at - directory, file, or row?

**bucketBy() Level:** FILE level separation

**Detailed Explanation:**
bucketBy() organizes data within the same directory but distributes rows across multiple files based on hash values. This is a fine-grained separation that organizes data at the file level.

### 6.3.4.3 What separation level does sortBy() operate at - directory, file, or row?

**sortBy() Level:** ROW level organization

**Detailed Explanation:**
sortBy() organizes data within each file by sorting rows. This doesn't create separation but optimizes the internal arrangement of data within files.

### 6.3.4.4 Is partitionBy() visible in the file system?

**partitionBy() Visibility:** YES - creates visible directory structure

**File System Example:**

output/
├── country=US/
│   └── part-00000.parquet
├── country=CA/
│   └── part-00000.parquet
└── country=MX/
└── part-00000.parquet

**text**

```
### 6.3.4.5 Is bucketBy() visible in the file system?
**bucketBy() Visibility:** NO - files look normal in directories

**File System Example:**
```

output/
├── part-00000.parquet  # Contains bucket 0 data
├── part-00001.parquet  # Contains bucket 1 data
├── part-00002.parquet  # Contains bucket 2 data
└── _SUCCESS

**text**

```
### 6.3.4.6 Is sortBy() visible in the file system?
**sortBy() Visibility:** NO - internal file optimization only

### 6.3.4.7 How do you access data with partitionBy() - direct folder navigation?
**partitionBy() Access:** Direct folder navigation and predicate pushdown

**Query Optimization:**
```python
# Spark only reads relevant partitions
df = spark.read.parquet("partitioned_data/")
result = df.filter(col("country") == "US")  
# Only reads from country=US/ directory
```

### 6.3.4.8 How do you access data with bucketBy() - hash calculation?

**bucketBy() Access:** Hash-based data localization

**Join Optimization:**

**python**

```
# When joining on bucketed columns, Spark can avoid shuffle
bucketed_sales.join(bucketed_customers, "customer_id")
# Data with same customer_id hashes are co-located
```

### 6.3.4.9 How do you access data with sortBy() - sequential scanning?

**sortBy() Access:** Optimized sequential scanning and better compression

**Performance Benefits:**

* Range queries can skip large portions of data
* Similar data values grouped together for better compression
* Faster aggregations on sorted columns

### 6.3.4.10 What is the optimal use case for partitionBy() - low or high cardinality?

**partitionBy() Cardinality:** LOW cardinality (10-1000 distinct values)

**Optimal Scenarios:**

* Date partitions (year, month, day)
* Geographic regions (country, state)
* Business units or departments

### 6.3.4.11 What is the optimal use case for bucketBy() - low or high cardinality?

**bucketBy() Cardinality:** HIGH cardinality (thousands to millions of distinct values)

**Optimal Scenarios:**

* User IDs, product IDs, transaction IDs
* Any column used frequently in joins
* Columns with even value distribution

### 6.3.4.12 What is the optimal use case for sortBy() - specific access pattern?

**sortBy() Use Case:** Specific access patterns and range queries

**Optimal Scenarios:**

* Time-series data with timestamp ranges
* Sequential ID lookups
* Range-based aggregations

### 6.3.4.13 What performance benefit does partitionBy() provide?

**partitionBy() Benefits:**

* **Partition Pruning** : Skip reading irrelevant data
* **Parallel Processing** : Process partitions independently
* **Data Management** : Easy to manage data lifecycle per partition

### 6.3.4.14 What performance benefit does bucketBy() provide?

**bucketBy() Benefits:**

* **Shuffle-free Joins** : Avoid data movement during joins
* **Even Distribution** : Prevent data skew in operations
* **Predicate Pushdown** : Filter on bucket columns efficiently

### 6.3.4.15 What performance benefit does sortBy() provide?

**sortBy() Benefits:**

* **Better Compression** : Similar values compress better
* **Range Query Optimization** : Skip data outside query range
* **Faster Scans** : Sequential reads are faster than random access

### 6.3.4.16 How does partitionBy() impact file organization - multiple folders?

**partitionBy() Organization:** Creates directory hierarchy with multiple folders

### 6.3.4.17 How does bucketBy() impact file organization - fixed files in one folder?

**bucketBy() Organization:** Creates fixed number of files in same directory

### 6.3.4.18 How does sortBy() impact file organization - same files, sorted internally?

**sortBy() Organization:** Same file structure, but data sorted within files

## 6.3.5 When to Use Each Method

### 6.3.5.1 When should you use partitionBy() - clear categories like country/year?

**partitionBy() Use Cases:**

**python**

```
# Clear categorical divisions
df.write.partitionBy("year", "month", "country").parquet("data/")

# Data with natural hierarchies
df.write.partitionBy("department", "team").parquet("hr_data/")
```

### 6.3.5.2 When should you use partitionBy() - frequent filtering by categories?

**Filtering Use Case:**

**python**

```
# When queries frequently filter by specific columns
# Common patterns: date ranges, geographic regions, status codes
df.write.partitionBy("status", "region").parquet("business_data/")
```

### 6.3.5.3 When should you use partitionBy() - data lifecycle management?

**Lifecycle Management:**

**python**

```
# Easy to drop old partitions
# df.write.partitionBy("year", "month").parquet("logs/")
# Later: hdfs dfs -rm -r logs/year=2022/month=01/
```

### 6.3.5.4 What formats work with partitionBy() - files or tables or both?

**partitionBy() Support:** BOTH files and tables

**Supported Everywhere:**

* File formats: Parquet, ORC, JSON, CSV
* Table formats: Hive, Delta, Iceberg
* All write methods: `write.parquet()`, `write.saveAsTable()`

### 6.3.5.5 When should you use bucketBy() - high-cardinality columns?

**bucketBy() High Cardinality:**

**python**

```
# User IDs, product IDs, any unique identifiers
df.write.bucketBy(50, "user_id").sortBy("timestamp").saveAsTable("user_activity")
```

### 6.3.5.6 When should you use bucketBy() - frequently joined tables?

**Join Optimization:**

**python**

```
# Tables frequently joined on customer_id
sales.write.bucketBy(100, "customer_id").saveAsTable("sales")
customers.write.bucketBy(100, "customer_id").saveAsTable("customers")
# Join without shuffle!
```

### 6.3.5.7 When should you use bucketBy() - need even data distribution?

**Data Distribution:**

**python**

```
# Prevent skew in large tables
df.write.bucketBy(200, "tenant_id").saveAsTable("multi_tenant_data")
```

### 6.3.5.8 What formats work with bucketBy() - files or tables?

**bucketBy() Support:** TABLES only via `saveAsTable()`

**Limitation:** Cannot use with direct file writes like `write.parquet()`

### 6.3.5.9 When should you use sortBy() - range queries (BETWEEN, >, <)?

**Range Query Optimization:**

**python**

```
# Time-series data with range queries
df.write.sortBy("event_timestamp").parquet("time_series_data/")
```

### 6.3.5.10 When should you use sortBy() - natural ordering (timestamps)?

**Natural Ordering:**

**python**

```
# Data naturally ordered by time or sequence
df.write.sortBy("created_at", "sequence_id").saveAsTable("ordered_events")
```

### 6.3.5.11 When should you use sortBy() - better compression?

**Compression Benefits:**

**python**

```
# Similar values grouped together compress better
df.write.sortBy("product_category", "price_range").parquet("product_data/")
```

### 6.3.5.12 What formats work with sortBy() - files or tables?

**sortBy() Support:** TABLES only via `saveAsTable()`

**Limitation:** Cannot use with direct file writes

## 6.3.6 Critical Limitations

### 6.3.6.1 Can file formats (CSV, JSON, Parquet, ORC) use bucketBy()?

**bucketBy() with Files:** NO - only works with `saveAsTable()`

**Error Example:**

**python**

```
# This will FAIL
df.write.bucketBy(10, "user_id").parquet("output/")
# Error: bucketBy can only be used when saving to a table
```

### 6.3.6.2 Can file formats optimize joins at write time?

**Join Optimization:** NO - file formats cannot pre-optimize joins

**Reason:** File formats lack metadata about data distribution relationships

### 6.3.6.3 Can file formats use sortBy() during write?

**sortBy() with Files:** NO - only works with `saveAsTable()`

### 6.3.6.4 Must you pre-sort DataFrames before writing to file formats?

**Pre-sorting Required:** YES for file formats

**Workaround:**

**python**

```
# Sort before writing to files
df_sorted = df.orderBy("timestamp", "user_id")
df_sorted.write.parquet("sorted_data/")
```

### 6.3.6.5 Can JDBC writes use partitionBy()?

**JDBC partitionBy():** NO - database handles partitioning

**Reason:** Database manages its own storage and partitioning strategies

### 6.3.6.6 Why doesn't JDBC support partitionBy() - who handles partitioning?

**Partitioning Responsibility:** DATABASE handles partitioning

**Database-specific:**

* PostgreSQL: Table partitioning
* MySQL: Partitioned tables
* Oracle: Partitioning schemes
* Spark cannot control database storage layout

### 6.3.6.7 Can JDBC writes use bucketBy()?

**JDBC bucketBy():** NO - database handles indexing

### 6.3.6.8 Why doesn't JDBC support bucketBy() - who handles indexing?

**Indexing Responsibility:** DATABASE handles indexing

**Database Optimization:**

* Databases use B-tree indexes, hash indexes, etc.
* Spark bucketing doesn't translate to database concepts
* Database optimizer handles join optimization

### 6.3.6.9 Can JDBC writes use sortBy()?

**JDBC sortBy():** NO - database handles query optimization

### 6.3.6.10 Why doesn't JDBC support sortBy() - who handles query optimization?

**Optimization Responsibility:** DATABASE query optimizer

**Database Control:**

* Databases have sophisticated query optimizers
* They decide execution plans based on statistics
* Spark cannot influence database internal optimizations

### 6.3.6.11 Does saveAsTable support the full feature set?

**saveAsTable Features:** YES - supports all three methods

**Full Capability:**

**python**

```
df.write \
  .partitionBy("year", "month") \
  .bucketBy(100, "user_id") \
  .sortBy("timestamp") \
  .saveAsTable("optimized_table")
```

### 6.3.6.12 Is saveAsTable the only method for bucketing and sorting during write?

**Exclusive Method:** YES - only `saveAsTable()` supports bucketing and sorting

### 6.3.6.13 Does saveAsTable require metastore integration?

**Metastore Requirement:** YES - requires Hive metastore or Spark catalog

**Dependency:**

* Stores table metadata in metastore
* Tracks partitioning, bucketing, sorting information
* Enables query optimization across sessions

## 6.3.7 Best Practices

### 6.3.7.1 For maximum performance with saveAsTable, what three-layer optimization should you use?

**Three-Layer Optimization:**

**python**

```
df.write \
  .partitionBy("date") \          # Layer 1: Partitioning (coarse)
  .bucketBy(100, "user_id") \     # Layer 2: Bucketing (medium)  
  .sortBy("timestamp") \          # Layer 3: Sorting (fine)
  .saveAsTable("optimized_table")
```

### 6.3.7.2 What should you use partitionBy() for in the three-layer optimization?

**partitionBy() Role:** Data elimination through partition pruning

**Ideal Columns:**

* Low cardinality (10-1000 values)
* Frequently filtered in queries
* Natural data hierarchies (date, location)

### 6.3.7.3 What should you use bucketBy() for in the three-layer optimization?

**bucketBy() Role:** Join optimization and data co-location

**Ideal Columns:**

* High cardinality join keys
* Even value distribution
* Frequently used in equality joins

### 6.3.7.4 What should you use sortBy() for in the three-layer optimization?

**sortBy() Role:** Query performance and compression

**Ideal Columns:**

* Range query predicates
* Natural ordering sequences
* Columns with many repeated values

### 6.3.7.5 What is the ideal cardinality range for partitionBy() - 10-1000 distinct values?

**partitionBy() Cardinality:** 10-1000 distinct values

**Reasoning:**

* Too low: Not enough parallelism benefit
* Too high: Too many small files, metadata overhead

### 6.3.7.6 What cardinality does bucketBy() handle well - millions of distinct values?

**bucketBy() Cardinality:** Millions of distinct values

**Reasoning:**

* Hash distribution handles high cardinality well
* Fixed number of buckets regardless of distinct values
* Ideal for unique identifiers

### 6.3.7.7 Are there cardinality limits for sortBy()?

**sortBy() Cardinality:** No specific limits

**Consideration:**

* Sorting cost increases with data size
* Benefits depend on query patterns, not cardinality

### 6.3.7.8 What is the file management risk with partitionBy() - too many small files?

**partitionBy() Risk:** Small file problem

**Scenario:**

* 1000 partitions × 10 files per partition = 10,000 files
* Metadata overhead slows down operations
* Storage inefficiency

### 6.3.7.9 What is the file management characteristic of bucketBy() - fixed file count?

**bucketBy() Characteristic:** Fixed file count

**Benefit:**

* Number of files = number of buckets
* Predictable file count regardless of data volume
* No small file problem from partitioning

### 6.3.7.10 Does sortBy() impact file count?

**sortBy() File Impact:** NO - doesn't change file count

**Behavior:**

* Only affects internal row ordering within files
* Same number of files as without sorting

## 6.3.8 Summary - Support Matrix

### 6.3.8.1 Is partitionBy() universal (files + tables)?

**partitionBy() Support:** UNIVERSAL - works with files and tables

### 6.3.8.2 Is bucketBy() exclusive to saveAsTable?

**bucketBy() Support:** EXCLUSIVE to `saveAsTable()`

### 6.3.8.3 Is sortBy() exclusive to saveAsTable?

**sortBy() Support:** EXCLUSIVE to `saveAsTable()`

### 6.3.8.4 Can you use bucketBy() or sortBy() with file formats (CSV, JSON, Parquet)?

**File Format Support:** NO - only through `saveAsTable()`

### 6.3.8.5 Can you use any organization methods with JDBC writer?

**JDBC Support:** NO organization methods supported

### 6.3.8.6 Can you use partitionBy() with JDBC (who handles database partitioning)?

**JDBC partitionBy():** NO - database manages partitioning

## 6.3.9 Key Insights

### 6.3.9.1 What does partitionBy() organize - your storage?

**partitionBy() Organizes:** STORAGE layout and data files

### 6.3.9.2 What does bucketBy() organize - your data relationships?

**bucketBy() Organizes:** DATA relationships and join patterns

### 6.3.9.3 What does sortBy() organize - your data access patterns?

**sortBy() Organizes:** DATA access patterns and query performance

### 6.3.9.4 What is the key principle for choosing between these methods?

**Selection Principle:** Choose based on your query patterns and data characteristics

**Decision Framework:**

* **Partitioning** : For data elimination and lifecycle management
* **Bucketing** : For join optimization and data co-location
* **Sorting** : For range queries and compression benefits

## 6.4 Bucketing

### 6.4.1 What is bucketing in Spark? How do you use bucketBy() when writing data?

**Bucketing Definition:** A data organization technique that distributes data across fixed number of files based on hash values of specified columns.

**bucketBy() Usage:**

**python**

```
df.write \
  .bucketBy(50, "user_id") \        # 50 buckets on user_id column
  .sortBy("timestamp") \            # Optional: sort within buckets
  .saveAsTable("bucketed_table")    # Must use saveAsTable
```

### 6.4.2 How does bucketing work: bucket numbers, columns, and hash functions?

**Bucketing Mechanism:**

1. **Hash Function** : Spark applies hash function to bucket columns
2. **Bucket Assignment** : `hash(column_value) % numBuckets` determines bucket
3. **File Distribution** : Each bucket writes to separate file
4. **Co-location** : Same column values always go to same bucket

### 6.4.3 What is the purpose of using sortBy() in combination with bucketBy()?

**Combined Purpose:** Optimize both joins and range queries

**Benefits:**

* **bucketBy** : Enables shuffle-free joins
* **sortBy** : Enables range query optimization within buckets
* **Together** : Maximum performance for complex queries

### 6.4.4 How does bucketing with sorting optimize sort-merge joins by eliminating shuffle?

**Join Optimization:**

**python**

```
# Both tables bucketed and sorted same way
table1.write.bucketBy(100, "id").sortBy("id").saveAsTable("table1")
table2.write.bucketBy(100, "id").sortBy("id").saveAsTable("table2")

# Join process:
# 1. Same id values are in same buckets (bucketBy)
# 2. Data is pre-sorted within buckets (sortBy)  
# 3. No shuffle needed - merge sorted buckets directly
```

### 6.4.5 Can you use bucketBy() with partitionBy() together?

**Combined Usage:** YES - they work together

**Example:**

**python**

```
df.write \
  .partitionBy("year", "month") \   # First level: partitioning
  .bucketBy(50, "user_id") \        # Second level: bucketing within partitions
  .sortBy("timestamp") \            # Third level: sorting within buckets
  .saveAsTable("fully_optimized_table")
```

### 6.4.6 What are the limitations of bucketing?

**Bucketing Limitations:**

1. **saveAsTable Only** : Cannot use with direct file writes
2. **Metastore Dependency** : Requires Hive metastore
3. **Fixed Buckets** : Cannot change bucket count without rewriting data
4. **Hash Skew** : Poor distribution if hash function creates skew

### 6.4.7 How do you read bucketed tables to take advantage of bucketing?

**Reading Bucketed Tables:**

**python**

```
# Simply read the table - Spark automatically detects bucketing
df = spark.read.table("bucketed_table")

# For joins to use bucketing, both tables must be bucketed same way
bucketed_df1 = spark.read.table("bucketed_table1")
bucketed_df2 = spark.read.table("bucketed_table2")
result = bucketed_df1.join(bucketed_df2, "join_column")
```

### 6.4.8 What happens if you change the number of buckets after writing data?

**Bucket Count Change:** Requires full data rewrite

**Process:**

**python**

```
# Original table with 50 buckets
df.write.bucketBy(50, "user_id").saveAsTable("old_table")

# To change to 100 buckets:
spark.sql("CREATE TABLE new_table LIKE old_table")
spark.sql("INSERT INTO new_table SELECT * FROM old_table")
# Or use CTAS
spark.sql("CREATE TABLE new_table AS SELECT * FROM old_table")
```

## 6.4.1 Bucketing - Syntax & Implementation

### 6.4.1.1 What is the exact syntax for using all three methods together in saveAsTable?

**Complete Syntax:**

**python**

```
df.write \
  .partitionBy("year", "month", "country") \    # Partitioning
  .bucketBy(100, "user_id", "product_id") \     # Bucketing (multiple columns)
  .sortBy("timestamp", "sequence_id") \         # Sorting (multiple columns)
  .option("path", "/warehouse/tables/") \       # Optional: custom path
  .saveAsTable("database.optimized_table")
```

### 6.4.1.2 Can you use bucketBy without sortBy? What happens?

**bucketBy without sortBy:** YES - works fine

**Result:**

* Still get bucketing benefits for joins
* Lose range query optimization within buckets
* Data unordered within each bucket file

### 6.4.1.3 Can you use sortBy without bucketBy? What happens?

**sortBy without bucketBy:** NO - will throw error

**Error:**

**python**

```
# This FAILS
df.write.sortBy("timestamp").saveAsTable("table")
# Error: sortBy must be used with bucketBy
```

### 6.4.1.4 What happens if you try to use bucketBy with df.write.parquet()?

**File Write Attempt:** Throws error

**Error Example:**

**python**

```
df.write.bucketBy(10, "user_id").parquet("output/")
# AnalysisException: 'bucketBy' can only be used when saving to a table
```

### 6.4.1.5 What error message do you get when using bucketBy with file formats?

**Error Message:**

**text**

```
AnalysisException: 'bucketBy' can only be used when saving to a table
```

### 6.4.1.6 How do you create a bucketed table from an existing non-bucketed table?

**Conversion Process:**

**python**

```
# Method 1: CTAS (Create Table As Select)
spark.sql("""
  CREATE TABLE bucketed_table
  USING parquet
  CLUSTERED BY (user_id) INTO 50 BUCKETS
  AS SELECT * FROM non_bucketed_table
""")

# Method 2: INSERT INTO
spark.sql("""
  CREATE TABLE bucketed_table
  USING parquet
  CLUSTERED BY (user_id) INTO 50 BUCKETS
""")
spark.sql("INSERT INTO bucketed_table SELECT * FROM non_bucketed_table")
```

## 6.4.2 Bucketing - Verification & Validation

### 6.4.2.1 How do you verify bucketing is preserved after reading a bucketed table?

**Verification Methods:**

**python**

```
# Method 1: Check query plan
df = spark.read.table("bucketed_table")
df.explain()  # Look for "Bucketed" in plan

# Method 2: Check table properties
spark.sql("DESCRIBE EXTENDED bucketed_table").show(truncate=False)

# Method 3: Check data distribution
df.groupBy(spark_partition_id()).count().show()
```

### 6.4.2.2 What Spark configuration controls bucket pruning optimization?

**Bucket Pruning Config:**

**python**

```
# Enable bucket pruning
spark.conf.set("spark.sql.sources.bucketing.enabled", "true")
spark.conf.set("spark.sql.sources.bucketing.autoBucketedScan", "true")
```

### 6.4.2.3 How do you check if a table is bucketed using Spark catalog?

**Catalog Inspection:**

**python**

```
# Check table metadata
tables = spark.catalog.listTables("database_name")
for table in tables:
    if table.name == "target_table":
        print(f"Table: {table.name}")
        # Bucketing info available in table properties

# SQL approach
spark.sql("DESCRIBE FORMATTED target_table").filter(
    col("col_name").contains("Bucket")
).show()
```

### 6.4.2.4 How do you verify sortBy ordering is preserved in the data files?

**Sort Verification:**

**python**

```
# Read data and check ordering
df = spark.read.table("sorted_bucketed_table")
# Check if data is sorted within buckets
window_spec = Window.partitionBy("user_id").orderBy("timestamp")
df.withColumn("prev_timestamp", lag("timestamp").over(window_spec)) \
  .filter(col("timestamp") < col("prev_timestamp")) \
  .count()  # Should be 0 if properly sorted
```

### 6.4.2.5 What does DESCRIBE EXTENDED show for bucketed tables?

**DESCRIBE EXTENDED Output:**

**text**

```
+--------------------------+----------------------------------------------------+
|col_name                  |data_type                                           |
+--------------------------+----------------------------------------------------+
|# col_name                |data_type                                           |
|user_id                   |bigint                                              |
|timestamp                 |timestamp                                           |
|                          |                                                    |
|# Detailed Table Information|                                                    |
|Database                  |default                                             |
|Table                     |bucketed_table                                      |
|Owner                     |user                                                |
|Created Time              |Thu Jan 01 00:00:00 UTC 2024                        |
|Last Access               |UNKNOWN                                             |
|Created By                |Spark 3.4.0                                         |
|Type                      |MANAGED                                             |
|Provider                  |parquet                                             |
|Table Properties          |[bucketing_version=2, numBuckets=50]                |
|Bucket Columns            |[`user_id`]                                         |
|Sort Columns              |[`timestamp`]                                       |
|Num Buckets               |50                                                  |
+--------------------------+----------------------------------------------------+
```

### 6.4.2.6 How do you inspect physical files to verify partitionBy structure?

**File System Inspection:**

**bash**

```
# HDFS
hdfs dfs -ls -R /warehouse/database.db/partitioned_table/

# Local file system
ls -la /warehouse/database.db/partitioned_table/
tree /warehouse/database.db/partitioned_table/
```

**Expected Structure:**

**text**

```
partitioned_table/
├── year=2024/
│   ├── month=01/
│   │   └── part-00000.parquet
│   └── month=02/
│       └── part-00000.parquet
└── year=2023/
    └── month=12/
        └── part-00000.parquet
```

## 6.4.3 Bucketing - Sizing & Configuration

### 6.4.3.1 How many buckets should you create for a table with 1TB data?

**Bucket Count Guideline:** 100-500 buckets for 1TB data

**Reasoning:**

* Target file size: 1-5GB per bucket
* 1TB / 2GB per bucket = 500 buckets
* Consider cluster resources and parallelism

### 6.4.3.2 What is the formula for calculating optimal bucket count?

**Bucket Count Formula:**

**python**

```
total_data_size_gb = 1000  # 1TB
target_file_size_gb = 2    # Aim for 2GB files

optimal_buckets = max(1, total_data_size_gb // target_file_size_gb)
# Result: 500 buckets

# Adjust for cluster size
executor_cores = 100  # Total cores in cluster
adjusted_buckets = max(executor_cores, optimal_buckets)
```

### 6.4.3.3 What happens if you create too many buckets (e.g., 10,000)?

**Too Many Buckets Problems:**

1. **Small File Problem** : Many tiny files
2. **Metadata Overhead** : Namenode/metastore pressure
3. **Task Overhead** : Many small tasks
4. **Inefficient** : Storage and processing inefficiency

### 6.4.3.4 What happens if you create too few buckets (e.g., 2)?

**Too Few Buckets Problems:**

1. **Data Skew** : Large files, uneven distribution
2. **Limited Parallelism** : Few tasks for processing
3. **Join Inefficiency** : Less effective for join optimization

### 6.4.3.5 How does bucket count affect join performance?

**Join Performance Impact:**

* **Optimal** : Both tables have same bucket count on join key
* **Mismatched** : May still work but less efficient
* **Too Few** : Data skew and reduced parallelism
* **Too Many** : Task overhead outweighs benefits

### 6.4.3.6 Should bucket count be a power of 2? Why or why not?

**Power of 2:** RECOMMENDED but not required

**Benefits:**

* Better hash distribution
* Aligns with hardware architecture
* Historical best practice from hash tables

## 6.4.4 Bucketing - Performance Impact

### 6.4.4.1 How does sortBy improve compression ratio in Parquet/ORC?

**Compression Improvement:**

* Similar values grouped together in columns
* Parquet/ORC can use run-length encoding
* Dictionary encoding more effective
* Typical improvement: 20-50% better compression

### 6.4.4.2 What is the performance cost of sortBy during write operations?

**sortBy Write Cost:** SIGNIFICANT - most expensive operation

**Cost Factors:**

* Full data shuffle required
* Memory intensive for large datasets
* Can double or triple write time

### 6.4.4.3 Does partitionBy increase or decrease write time? Why?

**partitionBy Write Impact:** INCREASES write time slightly

**Reasons:**

* Additional metadata operations
* Multiple directory creations
* Small overhead for partition management

### 6.4.4.4 How does bucketBy affect write parallelism?

**bucketBy Parallelism:** FULL parallelism maintained

**Behavior:**

* Each bucket can be written in parallel
* No reduction in write parallelism
* Similar performance to non-bucketed writes

### 6.4.4.5 What is the memory overhead of sortBy during writes?

**sortBy Memory Overhead:** HIGH - requires substantial memory

**Memory Usage:**

* Needs to buffer and sort data per partition
* Can cause executor OOM with large partitions
* Monitor via Spark UI: "Shuffle Write" metrics

## 6.4.5 Bucketing - Schema Evolution & Modification

### 6.4.5.1 Can you change bucket count after table creation? What's the process?

**Bucket Count Change:** YES, but requires full rewrite

**Process:**

**python**

```
# Method 1: CTAS with new bucket count
spark.sql("""
  CREATE TABLE new_bucketed_table
  USING parquet
  CLUSTERED BY (user_id) INTO 100 BUCKETS  -- Changed from 50 to 100
  AS SELECT * FROM old_bucketed_table
""")

# Method 2: Export and reimport
df = spark.read.table("old_bucketed_table")
df.write.bucketBy(100, "user_id").saveAsTable("new_bucketed_table")
```

### 6.4.5.2 Can you add new partition columns to an existing partitioned table?

**Partition Column Addition:** NO - requires table recreation

**Limitation:** Spark doesn't support dynamic partition column addition

### 6.4.5.3 What happens to bucketing when you use ALTER TABLE?

**ALTER TABLE Impact:** Bucketing metadata preserved

**Safe Operations:**

* Adding/dropping columns
* Changing column comments
* Modifying table properties

**Unsafe Operations:**

* Changing bucket columns requires recreation
* Changing bucket count requires recreation

### 6.4.5.4 Can you change sortBy columns after table creation?

**sortBy Column Change:** NO - requires table recreation

### 6.4.5.5 How do you migrate from non-bucketed to bucketed table?

**Migration Process:**

**python**

```
# Step 1: Read existing table
df = spark.read.table("non_bucketed_table")

# Step 2: Write as bucketed table
df.write \
  .bucketBy(100, "user_id") \
  .sortBy("timestamp") \
  .saveAsTable("bucketed_table")

# Step 3: Verify and switch applications
# Step 4: Drop old table (optional)
spark.sql("DROP TABLE non_bucketed_table")
```

## 6.4.6 Bucketing - Reading Optimizations

### 6.4.6.1 How do you read a bucketed table to leverage bucket-based joins?

**Reading for Joins:** Simply read normally

**Automatic Optimization:**

**python**

```
table1 = spark.read.table("bucketed_table1")  # Bucketed on user_id
table2 = spark.read.table("bucketed_table2")  # Bucketed on user_id

# Spark automatically uses bucketing for join
result = table1.join(table2, "user_id")
result.explain()  # Shows "SortMergeJoin" without exchange
```

### 6.4.6.2 What configuration enables bucketed table optimization during reads?

**Bucket Optimization Config:**

**python**

```
# Essential configurations
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.sources.bucketing.enabled", "true")
spark.conf.set("spark.sql.sources.bucketing.autoBucketedScan", "true")
```

### 6.4.6.3 How does Spark know to use bucketing during joins?

**Join Detection:**

1. **Metadata Check** : Spark checks if both tables are bucketed
2. **Bucket Compatibility** : Same bucket columns and count
3. **Join Condition** : Join matches bucket columns
4. **Plan Optimization** : Chooses bucket-based join strategy

### 6.4.6.4 What happens if you read a bucketed table without preserving bucketing?

**Bucket Preservation:** Automatic - Spark preserves bucketing

**Unless:**

* You apply transformations that break bucketing
* You filter on non-bucket columns before join
* You repartition the DataFrame

### 6.4.6.5 Can you filter on bucket columns to reduce data scan?

**Bucket Column Filtering:** YES - bucket pruning

**Example:**

**python**

```
# If table is bucketed on user_id
df = spark.read.table("bucketed_table")
filtered = df.filter(col("user_id").isin([1, 2, 3]))
# Spark only reads buckets containing these user_ids
```
