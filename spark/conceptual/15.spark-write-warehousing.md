# Section 13: Spark SQL Tables & Data Management

## 13.1 Table Types & Fundamentals

**1772. What are the two main types of tables in Spark SQL?**

Spark SQL has **Managed Tables** and  **External Tables** . Managed tables mean Spark controls everything—both the schema metadata and the actual data files. External tables are just registrations—Spark knows about the table structure, but the data lives elsewhere and someone else manages it.

**1773. What is a managed table and who controls its lifecycle?**

A managed table is Spark's complete responsibility. Spark decides where files live (warehouse directory), how they're organized, and deletes everything when you DROP the table. Full ownership, full control.

**1774. What is an external table and who controls its lifecycle?**

External tables are pointers—Spark registers the schema and knows where to find data (HDFS, S3, etc.), but doesn't control the actual files. The data owner manages lifecycle; Spark just knows how to read it.

**1775. What happens to the underlying data when you DROP a managed table?**

Everything is deleted—both metadata and all physical files are permanently removed from the warehouse directory. There's no undo.

**1776. What happens to the underlying data when you DROP an external table?**

Only the table registration disappears. The data files stay exactly where they are. You can recreate the table later pointing to the same location.

### 13.1.2 Creating Tables

**1777. How do you create a managed table using `saveAsTable()`?**

```python
df.write.mode("overwrite").saveAsTable("database.managed_table")
```

Spark handles everything—picks the storage location, organizes files, manages metadata.

**1778. How do you create an external table with a custom LOCATION?**

```python
df.write \
  .option("path", "/user/data/external_location/") \
  .saveAsTable("database.external_table")
```

The `path` option makes it external—data stays at your specified location, not in Spark's warehouse.

**1779. What is the default location for managed tables?**

Check `spark.sql.warehouse.dir`. Default is usually `spark-warehouse/` locally or `/user/hive/warehouse/` in cluster mode. This is Spark's home base for storing managed table data.

**1780. Can you convert a managed table to external or vice versa?**

Yes, but it only changes the metadata flag—no data moves. Use ALTER TABLE to change the EXTERNAL property, but understand this is just relabeling, not reorganizing.

## 13.2 Warehouse Configuration

**1781. What does `spark.sql.warehouse.dir` specify?**

The root directory where all managed table data lives. Every managed table gets a subdirectory here, organized by database and table name.

**1782. What is `sparkSession.enableHiveSupport()` and when do you need it?**

It switches from Spark's temporary in-memory catalog to Hive metastore—a persistent, shared metadata database. Need it for production where tables must survive across sessions and be shared across applications.

**1783. Why would you enable Hive support for managed tables?**

Four reasons: **Persistence** (tables survive session restarts), **Sharing** (multiple apps access same tables), **Production-ready** (industry standard), and **Features** (access to Hive's mature ecosystem).

### 13.3 Catalog Operations

**1784. What is the Spark Catalog API?**

Your programmatic interface to metadata—methods like `listTables()`, `getTable()` to explore databases, tables, and columns without writing SQL. It's `spark.catalog` providing a Python wrapper around the metastore.

**1785. How do you switch databases using `spark.catalog.setCurrentDatabase()`?**

```python
spark.catalog.setCurrentDatabase("production")
```

Sets the default context—subsequent unqualified table references use this database.

**1786. How do you list available tables using `spark.catalog.listTables()`?**

Returns inventory of all tables with their names and types (managed/external, temporary/permanent).

### 13.4 Tables vs Files

**1787. What are the advantages of using Spark SQL tables vs raw Parquet files for external tools (ODBC/JDBC, Tableau, Power BI)?**

 **Discoverability** —BI tools browse table catalogs automatically vs needing exact file paths.  **Access Control** —apply permissions at table level.  **Standard SQL** —any SQL tool can query.  **Schema Management** —centralized, version-controlled.

**1788. What are the advantages of using Spark SQL tables vs raw Parquet files for internal Spark API usage?**

 **Convenience** —reference by name not path.  **Optimization** —query planner has statistics for smart decisions.  **Automatic features** —partition pruning, predicate pushdown, bucketed joins.  **Metadata** —schema evolution, statistics, bucketing info.

## 13.5 Table Optimization Techniques

### 13.5.1 Partitioning - Core Concepts

**1789. What is table partitioning and how does it work?**

Physical directory organization based on column values—like creating folders organized by year, then month. Each unique partition value gets its own directory with its own data files.

**1790. What separation level does `partitionBy()` operate at - directory, file, or row?**

 **DIRECTORY level** —most visible, coarse-grained. You literally see separate folders for each partition value in your file system.

**1791. How do you create a partitioned table using `saveAsTable()`?**

```python
df.write.partitionBy("year", "month").saveAsTable("partitioned_table")
```

Creates nested hierarchy: `table/year=2024/month=01/`, `table/year=2024/month=02/`, etc.

**1792. Is `partitionBy()` visible in the file system? What does the structure look like?**

Absolutely visible:

```
table/
├── country=US/
│   └── year=2024/
│       └── part-00000.parquet
├── country=CA/
    └── year=2024/
        └── part-00000.parquet
```

**1793. How does partition pruning optimize query performance?**

When you query `WHERE year = 2024`, Spark only reads the `year=2024/` directory—completely skips other year directories. Eliminates entire directories from scan. With 10 years of data, might read only 10% of files.

**1794. What is the optimal cardinality range for partition columns (10-1000 distinct values)?**

Sweet spot is  **10-1000 distinct values** , typically  **100-500 partitions** . Too few = limited parallelism. Too many = metadata nightmare and small files.

**1795. What happens if you partition on a column with too many distinct values?**

Disaster—partition by `user_id` with millions of users = millions of directories. NameNode memory exhaustion, impossibly slow file listing, tiny files everywhere. Performance optimization becomes performance catastrophe.

**1796. What is the "small file problem" with over-partitioning?**

Scenario: 1000 partitions × 10 files = 10,000 tiny files @ 1MB each.

Problems: **NameNode strain** (~150 bytes RAM per file), **Listing overhead** (slow directory scans), **I/O inefficiency** (many open/close operations vs few large reads), **Task overhead** (one task per file by default).

**1797. Can you use multiple columns for partitioning?**

Yes—creates hierarchy. `partitionBy("year", "month", "day")` produces nested directories. Order matters: put most-filtered column first for better pruning.

**1798. How do you add new partitions to an existing partitioned table?**

Append mode auto-creates new partition directories:

```python
new_data.write.mode("append").partitionBy("year", "month").saveAsTable("logs")
```

**1799. How do you drop old partitions for data lifecycle management?**

```sql
ALTER TABLE logs DROP PARTITION (year=2020, month=1)
```

Perfect for retention policies—keep last 6 months, drop older monthly.

**1800. What file formats support `partitionBy()` - files, tables, or both?**

 **BOTH** —universal across all formats (Parquet, ORC, JSON, CSV) and write methods. Partitioning is storage-level, format-agnostic.

### 13.5.2 Bucketing Tables

**1801. What is bucketing in Spark SQL tables?**

Hash-based row distribution across fixed number of files. Formula: `bucket_id = hash(column) % numBuckets`. Same hash = same bucket, always. This predictable distribution enables shuffle-free joins.

**1802. What separation level does `bucketBy()` operate at - directory, file, or row?**

 **FILE level** —within a directory, distributes rows across fixed number of files based on hash. More granular than partitioning.

**1803. How do you create a bucketed table using `bucketBy()` and `saveAsTable()`?**

```python
df.write \
  .bucketBy(100, "customer_id") \
  .sortBy("timestamp") \
  .saveAsTable("bucketed_table")
```

Critical:  **Only works with `saveAsTable()`** —requires metastore for metadata.

**1804. How does the hash function determine bucket assignment?**

Example with 10 buckets:

* "alice" → hash=98765432 → 98765432 % 10 = 2 → bucket 2
* "bob" → hash=123456789 → 123456789 % 10 = 9 → bucket 9
* "alice" (again) → **always** bucket 2

Consistency enables co-location across tables.

**1805. Is `bucketBy()` visible in the file system or is it metadata-only?**

 **Metadata-only** —files look normal: `part-00000.parquet`, `part-00001.parquet`. Bucketing info lives only in metastore. This is why it requires `saveAsTable()`.

**1806. What is the optimal cardinality for bucket columns - high or low?**

 **High cardinality** —millions of distinct values ideal. User IDs, transaction IDs, product IDs all excellent. Hash function distributes high-cardinality data evenly.

**1807. How does bucketing eliminate shuffle in joins?**

If both tables bucketed on join key with same bucket count, rows that need joining are  **already co-located** :

* Table A bucket 5: all rows where hash(user_id) % 100 = 5
* Table B bucket 5: all rows where hash(user_id) % 100 = 5
* Join: Match bucket 5 to bucket 5 locally—zero network transfer

Spark joins bucket-by-bucket in parallel with zero shuffle.

**1808. Can you use `bucketBy()` with file formats like Parquet or CSV directly?**

 **NO** —only `saveAsTable()`. File formats can't embed bucketing metadata in headers.

**1809. What error do you get when trying to use `bucketBy()` with `write.parquet()`?**

`AnalysisException: 'bucketBy' can only be used when saving to a table`

**1810. Why is `bucketBy()` exclusive to `saveAsTable()`?**

Needs: **Metastore** to remember configuration, **Query planner** to check metadata for optimization, **Persistence** across sessions. Only `saveAsTable()` provides this.

**1811. Can you use `bucketBy()` without `sortBy()`?**

Yes—get join benefits without sorting overhead. Sorting is optional bonus for range queries.

**1812. How many buckets should you create for a 1TB table?**

Target  **1-5GB per bucket** : 1TB ÷ 2GB =  **500 buckets** . Also consider cluster size—minimum = executor cores for full parallelism. Sweet spot:  **100-500 buckets** .

**1813. What is the formula for calculating optimal bucket count?**

```
optimal_buckets = max(
    total_data_size / target_file_size,
    executor_cores
)
```

Round to power of 2 (64, 128, 256, 512) for better hash properties.

**1814. What happens if you create too many buckets (e.g., 10,000)?**

Small file problem redux: 10,000 tiny files = metadata overhead, task overhead, inefficient I/O. Defeats the purpose.

**1815. What happens if you create too few buckets (e.g., 2)?**

Two massive files = limited parallelism, potential memory issues, loses distribution benefits. Can't effectively use multi-core processing.

**1816. Should bucket count be a power of 2? Why or why not?**

 **Recommended not required** . Powers of 2 give slightly better hash distribution. But 100 works fine—just not as mathematically "clean."

**1817. Can you change the bucket count after table creation?**

No in-place modification—must rewrite entire table. Every row re-hashed and redistributed to new buckets.

### 13.5.3 Sorting Within Tables

**1818. What is `sortBy()` and how does it work with bucketing?**

Orders rows within each bucket file.  **Must be used with `bucketBy()`** —adds range query optimization on top of join optimization.

**1819. What separation level does `sortBy()` operate at - directory, file, or row?**

 **ROW level** —finest-grained. Arranges rows within files without changing file count or directory structure.

**1820. Can you use `sortBy()` without `bucketBy()`?**

No—throws error. Sorting requires bucketing as foundation.

**1821. How does `sortBy()` improve compression ratio in Parquet/ORC?**

Similar values grouped together enable better algorithms: **Run-Length Encoding** (sorted sequences compress better), **Dictionary Encoding** (more efficient), **Delta Encoding** (sorted timestamps have small deltas). Typical: 20-50% better compression.

**1822. How does `sortBy()` optimize range queries (BETWEEN, >, <)?**

For `WHERE timestamp BETWEEN dates` on sorted column: Parquet statistics enable row group skipping, pre-sorted data allows early termination, sequential access = better CPU cache. Result: 5x-50x faster.

**1823. What is the performance cost of `sortBy()` during write operations?**

 **Significant** —most expensive write operation. Full data shuffle, memory-intensive, can double/triple write time, risk of OOM. Trade-off: higher write cost for better read performance.

**1824. Is `sortBy()` visible in the file system?**

No—metadata-only. Not visible in file system.

**1825. Does `sortBy()` impact file count?**

No—only affects row order within files. Same file count as without sorting.

**1826. Can you use `sortBy()` with file formats directly or only with tables?**

**Tables only** via `saveAsTable()`. Cannot use with direct file writes.

### 13.5.4 Combining Optimization Techniques

**1827. Can you use `partitionBy()`, `bucketBy()`, and `sortBy()` together?**

Yes—the three-layer optimization:

```python
df.write \
  .partitionBy("year", "month") \       # Layer 1: Coarse
  .bucketBy(100, "user_id") \          # Layer 2: Medium
  .sortBy("timestamp") \               # Layer 3: Fine
  .saveAsTable("optimized_table")
```

**1828. What is the three-layer optimization strategy for maximum performance?**

1. **Partitioning** (coarse) - Data elimination via partition pruning
2. **Bucketing** (medium) - Join optimization via co-location
3. **Sorting** (fine) - Range queries + compression

**1829. What should you use `partitionBy()` for in the three-layer optimization?**

Data elimination through partition pruning. Use low cardinality (10-1000 values), frequently filtered columns (dates, regions, status).

**1830. What should you use `bucketBy()` for in the three-layer optimization?**

Join optimization and data co-location. Use high cardinality join keys (user_id, product_id), frequently joined columns.

**1831. What should you use `sortBy()` for in the three-layer optimization?**

Query performance and compression. Use range query columns (timestamps, sequences), repeated values.

**1832. How do you write a table with all three optimizations combined?**

```python
df.write \
  .partitionBy("year", "month", "country") \
  .bucketBy(200, "user_id") \
  .sortBy("event_timestamp", "session_id") \
  .saveAsTable("production.optimized_events")
```

**1833. Does `partitionBy()` increase or decrease write time? Why?**

**Increases slightly** (10-20%)—additional metadata operations, multiple directory creations, partition management overhead.

**1834. How does `bucketBy()` affect write parallelism?**

Maintains full parallelism—each bucket written in parallel. No reduction in concurrent writes.

**1835. What is the memory overhead of `sortBy()` during writes?**

 **HIGH** —must buffer and sort data per partition. Memory-intensive, can cause OOM with large partitions. Monitor "Shuffle Write" metrics in Spark UI.

## 13.6 Reading & Query Optimization

### 13.6.1 Reading Optimized Tables

**1836. How do you read a bucketed table to leverage bucket-based joins?**

Read normally—Spark auto-detects from metastore:

```python
t1 = spark.table("bucketed_table1")  # Bucketed on user_id
t2 = spark.table("bucketed_table2")  # Bucketed on user_id
result = t1.join(t2, "user_id")  # Automatically uses bucketing
```

**1837. What configuration enables bucketed table optimization during reads?**

```python
spark.conf.set("spark.sql.sources.bucketing.enabled", "true")
spark.conf.set("spark.sql.sources.bucketing.autoBucketedScan.enabled", "true")
```

**1838. How does Spark know to use bucketing during joins automatically?**

Checks metadata → verifies both tables bucketed → confirms same columns and count → confirms join uses bucket columns → chooses shuffle-free strategy.

**1839. What happens if you read a bucketed table without preserving bucketing?**

Automatic preservation unless you: apply `repartition()`, use `coalesce()`, or do transformations that break bucketing. Filtering usually preserves.

**1840. Can you filter on bucket columns to reduce data scan (bucket pruning)?**

Yes—Spark only reads relevant buckets:

```python
df.filter(col("user_id").isin([100, 200, 300]))
# Only reads buckets containing these IDs
```

**1841. How do you verify that bucketing is preserved after reading a table?**

Check query plan: `df.explain()` - Look for "Bucketed: true"

Check metadata: `DESCRIBE EXTENDED table_name`

**1842. What Spark configuration controls bucket pruning optimization?**

`spark.sql.sources.bucketing.enabled = true` enables both bucket-based joins and bucket pruning.

### 13.6.2 Join Optimization with Bucketing

**1843. How does bucketing eliminate shuffle in sort-merge joins?**

Both tables bucketed same way → Same IDs in same buckets → Join bucket 0 to bucket 0, bucket 1 to bucket 1, etc. → All joins local, zero network transfer.

**1844. What conditions must be met for a shuffle-free bucketed join?**

1. Both tables bucketed on join column(s)
2. Same bucket count
3. Same bucket columns
4. Join on bucket column with equality
5. Configuration enabled

**1845. Do both tables need the same bucket count for optimal joins?**

 **Yes** —different bucket counts force fallback to shuffle join despite both being bucketed.

**1846. What happens if two bucketed tables have different bucket counts?**

Shuffle required—Spark can't use bucket optimization. Performance becomes regular shuffle join.

**1847. How do you verify a join is using bucketing by examining the query plan?**

```python
result.explain()
```

Look for: "SortMergeJoin" **without** "Exchange". "Exchange" = shuffle happening.

**1848. What does "SortMergeJoin" without "Exchange" indicate in the plan?**

Shuffle-free bucketed join achieved. No data movement, optimal performance.

## 13.7 Table Inspection & Verification

**1849. How do you check if a table is bucketed using `DESCRIBE EXTENDED`?**

```sql
DESCRIBE EXTENDED database.table_name;
```

Shows: Num Buckets, Bucket Columns, Sort Columns, all bucketing metadata.

**1850. What information does `DESCRIBE EXTENDED` show for bucketed tables?**

Complete metadata: bucket count, bucket columns, sort columns, table properties including `bucketing_version`, partition info.

**1851. How do you verify `sortBy()` ordering is preserved in data files?**

Use window functions—compare each row to previous with `lag()`. Should find zero violations where current < previous.

**1852. How do you inspect physical files to verify `partitionBy()` structure?**

File system commands: `hdfs dfs -ls -R /path/` or `tree /path/`. You'll see explicit directory hierarchy with partition values.

**1853. How do you use Spark catalog to check table bucketing metadata?**

`spark.catalog.listColumns("table")` shows which columns are partitions/buckets. SQL: `DESCRIBE FORMATTED table` filters for bucket-related rows.

**1854. What does `spark.catalog.getTable()` return for table metadata?**

Basic info: name, database, type (managed/external), whether temporary. Doesn't include detailed bucketing—use DESCRIBE EXTENDED for that.

**1855. How do you check table properties using SQL commands?**

```sql
SHOW TBLPROPERTIES table_name;
SHOW CREATE TABLE table_name;  -- Complete DDL
DESCRIBE FORMATTED table_name;
```

## 13.8 Schema Evolution & Modification

**1856. Can you add new columns to an existing table?**

```sql
ALTER TABLE table_name ADD COLUMNS (new_col STRING, another_col INT);
```

Or append data with new schema using `mergeSchema` option.

**1857. Can you change partition columns after table creation?**

No—requires table recreation and full rewrite. Fundamental structural property.

**1858. Can you change bucket columns after table creation?**

No—requires table recreation. Data must be physically rehashed and redistributed.

**1859. Can you change `sortBy()` columns after table creation?**

No—requires table recreation and full rewrite.

**1860. What happens to bucketing metadata when you use `ALTER TABLE`?**

Preserved for safe operations: adding/dropping columns, comments, table properties. Unsafe: changing bucket/sort columns requires recreation.

**1861. How do you migrate from a non-bucketed to bucketed table?**

Read existing → Write with bucketing → Verify → Switch apps → Drop old.

**1862. How do you migrate from a non-partitioned to partitioned table?**

Read existing → Ensure partition columns exist → Write with partitioning → Verify structure → Switch apps.

**1863. What is the process for changing bucket count on an existing table?**

Full rewrite required: Read current table → Write new table with different bucket count → Verify → Rename tables → Drop old after validation.

## 13.9 Format & Writer Limitations

### 13.9.1 File Format Limitations

**1864. Can file formats (CSV, JSON, Parquet, ORC) use `bucketBy()` directly?**

 **NO** —only `saveAsTable()` works. File formats can't embed bucketing metadata.

**1865. Can file formats use `sortBy()` during write?**

 **NO** —only `saveAsTable()`.

**1866. Must you pre-sort DataFrames before writing to file formats?**

Yes for sorted output:

```python
df.orderBy("timestamp").write.parquet("output/")
```

But Spark won't remember it's sorted—no benefit for future reads.

**1867. How do you achieve sorted output when writing to Parquet files?**

Pre-sort DataFrame then write. Limitation: sorting not preserved in metadata.

**1868. Can file formats optimize joins at write time?**

No—lack metadata about data distribution. Can't pre-optimize joins.

**1869. Why can't file formats support bucketing metadata?**

No external metadata repository. File headers don't include bucketing info. Can't determine bucketing from file inspection.

### 13.9.2 JDBC Writer Limitations

**1870. Can JDBC writes use `partitionBy()`?**

 **NO** —database controls its own partitioning.

**1871. Why doesn't JDBC support `partitionBy()` - who handles partitioning?**

 **Database handles partitioning** —each database has its own schemes (PostgreSQL table partitioning, MySQL partitioned tables, Oracle partition schemes). Spark can't control database storage layout.

**1872. Can JDBC writes use `bucketBy()`?**

 **NO** —database handles indexing.

**1873. Why doesn't JDBC support `bucketBy()` - who handles indexing?**

 **Database handles indexing** —uses B-tree, hash, or other indexes. Bucketing doesn't map to database concepts. Database optimizer handles joins.

**1874. Can JDBC writes use `sortBy()`?**

 **NO** —database handles query optimization.

**1875. Why doesn't JDBC support `sortBy()` - who handles query optimization?**

**Database query optimizer** decides execution plans based on statistics. Spark can't influence database internal optimizations.

**1876. Does the database handle its own partitioning and indexing strategies?**

Yes—each database system has its own strategies. Spark writes rows; database organizes according to its schemes.

### 13.9.3 Writer Support Matrix

**1877. Is `partitionBy()` universal (works with files and tables)?**

 **YES** —works with both file formats and tables.

**1878. Is `bucketBy()` exclusive to `saveAsTable()`?**

 **YES** —only works with `saveAsTable()`.

**1879. Is `sortBy()` exclusive to `saveAsTable()`?**

 **YES** —requires `bucketBy()` + `saveAsTable()`.

**1880. Can you use `bucketBy()` or `sortBy()` with file formats?**

 **NO** —both require `saveAsTable()`.

**1881. Can you use any organization methods with the JDBC writer?**

 **NO** —database controls its own storage organization.

**1882. Does `saveAsTable()` support the full feature set of all three methods?**

 **YES** —supports all: partitionBy, bucketBy, sortBy.

**1883. Is `saveAsTable()` the only method for bucketing and sorting during write?**

 **YES** —exclusive method.

**1884. Does `saveAsTable()` require metastore integration?**

 **YES** —requires Hive metastore or compatible catalog.

## 13.10 Best Practices & Guidelines

### 13.10.1 Partitioning Best Practices

**1885. When should you use `partitionBy()` - clear categories like country/year?**

Use for: Clear categorical divisions (year/month/country), natural hierarchies (department/team), frequently filtered columns.

**1886. When should you use `partitionBy()` - frequent filtering by categories?**

When queries frequently filter by specific columns—date ranges, geographic regions, status codes.

**1887. When should you use `partitionBy()` - data lifecycle management needs?**

Perfect for retention policies—easy to drop old partitions: `ALTER TABLE DROP PARTITION (year=2020)`.

**1888. What is the ideal cardinality range for `partitionBy()` to avoid small files?**

 **10-1000 distinct values** , sweet spot  **100-500 partitions** .

**1889. What is the file management risk with `partitionBy()` - too many small files?**

1000 partitions × 10 files = 10,000 files. Causes: NameNode memory pressure, slow listings, inefficient I/O, task overhead.

**1890. How do you avoid the small file problem with partitioning?**

Coalesce before writing, use `maxRecordsPerFile` option, periodic compaction, choose appropriate partition columns.

### 13.10.2 Bucketing Best Practices

**1891. When should you use `bucketBy()` - high-cardinality join columns?**

Use for: High-cardinality columns (user IDs, product IDs, transaction IDs) used frequently in joins.

**1892. When should you use `bucketBy()` - frequently joined tables?**

When tables frequently joined on specific columns—bucket both tables same way for shuffle-free joins.

**1893. When should you use `bucketBy()` - need even data distribution?**

Use to prevent skew in large tables—hash distribution ensures evenness across buckets.

**1894. What cardinality does `bucketBy()` handle well - millions of distinct values?**

 **High cardinality** —millions of distinct values. Hash distribution works great with many unique values.

**1895. What is the file management characteristic of `bucketBy()` - fixed file count?**

**Fixed file count** = number of buckets. Predictable regardless of data volume. No small file problem.

**1896. How does fixed bucket count prevent the small file problem?**

Files grow with data, not file count. Can't accidentally create millions of files. Controlled, predictable.

### 13.10.3 Sorting Best Practices

**1897. When should you use `sortBy()` - range queries (BETWEEN, >, <)?**

Ideal for: Time-series data with range queries, any column used in BETWEEN/>/< filters.

**1898. When should you use `sortBy()` - natural ordering like timestamps?**

When data naturally ordered by time or sequence—event logs, transaction history, message queues.

**1899. When should you use `sortBy()` - better compression needs?**

When similar values should group for compression—categories, price ranges, demographics. Typical 20-50% improvement.

**1900. Are there cardinality limits for `sortBy()`?**

No specific limits—benefits depend on query patterns, not distinct value counts.

**1901. Does `sortBy()` impact file count?**

No—only affects row order within files. Same file count as without sorting.

## 13.11 Performance Considerations

**1902. What does `partitionBy()` organize - your storage layout?**

Organizes  **STORAGE** —physical directory structure for data elimination via partition pruning.

**1903. What does `bucketBy()` organize - your data relationships?**

Organizes  **DATA RELATIONSHIPS** —co-locates related data for join optimization.

**1904. What does `sortBy()` organize - your data access patterns?**

Organizes  **ACCESS PATTERNS** —orders data for efficient sequential reads and range queries.

**1905. What is the key principle for choosing between these optimization methods?**

Choose based on  **query patterns** : Frequent filtering → partition. Frequent joins → bucket. Range queries → sort.

**1906. How does bucket count affect join performance?**

Optimal: same bucket count on both tables = shuffle-free. Different counts = fallback to shuffle. Too few = limited parallelism. Too many = task overhead.

**1907. How does `sortBy()` improve compression ratio in Parquet/ORC?**

Similar values grouped → better run-length encoding, dictionary encoding, delta encoding. Typical 20-50% better compression.

**1908. What performance benefit does `partitionBy()` provide - partition pruning?**

Skip entire directories—for 100 partitions, might read only 1%. Massive reduction in data scanned.

**1909. What performance benefit does `bucketBy()` provide - shuffle-free joins?**

10x-100x faster joins—no network transfer, local joins only, reduced memory pressure.

**1910. What performance benefit does `sortBy()` provide - range query optimization?**

5x-50x faster range queries—row group skipping, early termination, better cache utilization.

## 13.12 Table Management Operations

**1911. How do you create a table from an existing DataFrame?**

`df.write.saveAsTable("table")` or SQL `CREATE TABLE AS SELECT`.

**1912. How do you insert data into an existing table?**

`df.write.mode("append").saveAsTable("table")` or SQL `INSERT INTO`.

**1913. How do you overwrite data in an existing table?**

`df.write.mode("overwrite").saveAsTable("table")` or SQL `INSERT OVERWRITE`.

**1914. How do you append data to an existing table?**

`df.write.mode("append").saveAsTable("table")`.

**1915. What is CTAS (Create Table As Select) and how do you use it?**

Single atomic operation creating and populating table. Schema inferred from query, optimized layout applied during creation.

**1916. How do you drop a table using SQL?**

`DROP TABLE [IF EXISTS] table_name;` - Removes metadata + data for managed tables.

**1917. How do you truncate a table using SQL?**

`TRUNCATE TABLE table_name;` - Removes data, keeps structure/metadata.

**1918. How do you rename a table using `ALTER TABLE`?**

`ALTER TABLE old_name RENAME TO new_name;` - Metadata operation only, no data movement.

**1919. How do you add comments to tables and columns?**

```sql
COMMENT ON TABLE table_name IS 'Description';
-- Or during creation with COMMENT keyword
```

**1920. How do you set table properties using SQL?**

```sql
ALTER TABLE table_name SET TBLPROPERTIES ('key'='value');
SHOW TBLPROPERTIES table_name;
```

---

## Summary - Support Matrix

| Method        | File Formats | saveAsTable | JDBC  |
| ------------- | ------------ | ----------- | ----- |
| partitionBy() | ✅ YES       | ✅ YES      | ❌ NO |
| bucketBy()    | ❌ NO        | ✅ YES      | ❌ NO |
| sortBy()      | ❌ NO        | ✅ YES      | ❌ NO |

## Key Takeaways

**Optimization Strategy:**

1. **partitionBy()** for data elimination (low cardinality filters)
2. **bucketBy()** for join optimization (high cardinality keys)
3. **sortBy()** for range queries and compression
4. Combine all three for maximum performance

**Critical Limitations:**

* Bucketing/sorting require `saveAsTable()` (metastore dependency)
* File formats: only partitioning works
* JDBC: none of these work (database controls storage)

**Cardinality Guidelines:**

* Partitioning: 10-1000 values (100-500 optimal)
* Bucketing: Millions of values (high cardinality perfect)
* Sorting: No limits (depends on query patterns)
