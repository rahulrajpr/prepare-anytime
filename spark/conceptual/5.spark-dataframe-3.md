# Spark Aggregation Functions - Study Material

## 3.7 Aggregate Functions

### Basic Aggregation Functions

**1. `sum()` vs `sumDistinct()` vs `approx_count_distinct()`**
- `sum()`: Total sum of all values
- `sumDistinct()`: Sum of only unique/distinct values
- `approx_count_distinct()`: Approximate count of distinct values (different purpose)

**2. When to use `approx_count_distinct()` vs `countDistinct()`**
- Use `approx_count_distinct()` for large datasets where exact count isn't critical
- Faster and more memory-efficient
- Provides approximate results with controllable error margin

**3. `approx_count_distinct()` vs `count_min_sketch()`**
- `approx_count_distinct()`: HyperLogLog algorithm, fixed accuracy parameter
- `count_min_sketch()`: Count-Min Sketch algorithm, tracks frequency of items

**4. What each measures**
- `approx_count_distinct()`: Measures cardinality (number of distinct values)
- `count_min_sketch()`: Measures frequency distribution of values

**5. Direct results vs serialized data**
- `approx_count_distinct()`: Returns direct numeric result
- `count_min_sketch()`: Returns serialized binary data (sketch object)

**6. `avg()` with null values**
- Ignores null values; calculates average of non-null values only

**7. Using `min()` and `max()`**
- `min("column")`: Returns minimum value
- `max("column")`: Returns maximum value
- Work on numeric, string, and date types

**8. `stddev()` and `variance()`**
- `stddev()`: Standard deviation (measure of spread/dispersion)
- `variance()`: Variance (square of standard deviation)
- Used for statistical analysis

**9. `corr()` function**
- Calculates Pearson correlation coefficient between two columns
- Returns value between -1 and 1
- Measures linear relationship strength

**10. `percentile_approx()` usage**
- `percentile_approx("column", 0.5)`: Median (50th percentile)
- `percentile_approx("column", [0.25, 0.75])`: Multiple percentiles
- Approximate calculation for large datasets

### Grouping Functions

**11. `grouping()` and `grouping_id()` purpose**
- Distinguish between actual NULL values and NULL values from aggregation
- Used with `ROLLUP` and `CUBE` operations

**12. `grouping(col)` when column is aggregated**
- Returns `1` when column is aggregated (NULL represents aggregation)

**13. `grouping(col)` when column is in grouping level**
- Returns `0` when column is part of the current grouping level

**14. `grouping_id()` return value and calculation**
- Returns integer representing bitmask of all grouping columns
- Calculated by treating grouping() results as binary digits

**15. Bitmask in `grouping_id()`**
- Binary representation where each bit represents a column's grouping status
- Example: columns A,B,C → grouping_id converts (1,0,1) to decimal 5

**16. When to use `grouping(col)` vs `grouping_id()`**
- `grouping(col)`: Check single column's aggregation status
- `grouping_id()`: Identify specific aggregation level with multiple columns

### ROLLUP Basics

**17. `GROUP BY` vs `GROUP BY WITH ROLLUP`**
- `GROUP BY`: Single level aggregation
- `ROLLUP`: Hierarchical aggregations with subtotals and grand total

**18. `ROLLUP` hierarchical aggregations**
- Creates right-to-left hierarchy
- Example: `ROLLUP(A, B, C)` → (A,B,C), (A,B), (A), ()

**19. `ROLLUP` NULL handling**
- NULLs in result indicate aggregation levels (not missing data)
- Use `grouping()` to distinguish aggregation NULLs from data NULLs

---

## 3.7.1 Aggregation After GroupBy - Critical Distinction

**1. Can you use `select()` after `groupBy()`?**
- **NO** - `select()` is not available on GroupedData objects

**2. Object type from `groupBy()`**
- Returns `GroupedData` object (not DataFrame)

**3. Methods on GroupedData**
- `agg()`, `count()`, `sum()`, `avg()`, `min()`, `max()`, `mean()`, `pivot()`

**4. DataFrame vs GroupedData methods**
- DataFrame: `select()`, `filter()`, `withColumn()`, etc.
- GroupedData: Only aggregation methods listed above

**5. Can you use `agg()` on regular DataFrame?**
- **YES** - `agg()` works on both DataFrame and GroupedData

**6. Are `agg()` and `select()` interchangeable on DataFrame?**
- Mostly YES for aggregations, but `agg()` is more explicit
- `select()` more versatile (supports non-aggregation operations)

**7. Only way to aggregate after `groupBy()`**
- Must use GroupedData methods: `agg()`, `count()`, `sum()`, etc.

**8. Why `df.groupBy("col").select(sum("value"))` fails**
- GroupedData doesn't have `select()` method
- Type mismatch error

**9. Correct syntax after groupBy**
- `df.groupBy("col").agg(sum("value"))`
- `df.groupBy("col").sum("value")`

---

## 3.7.2 ROLLUP and CUBE - Advanced Grouping

### ROLLUP

**1. Purpose of `rollup()` vs `groupBy()`**
- `rollup()`: Creates hierarchical subtotals
- `groupBy()`: Single-level grouping only

**2. Aggregation levels in `rollup()`**
- n+1 levels (where n = number of columns)
- Includes all levels from most detailed to grand total

**3. Formula for result rows in `rollup()`**
- Not fixed formula; depends on data cardinality
- Creates one row per unique combination at each level

### CUBE

**4. Purpose of `cube()` vs `rollup()`**
- `cube()`: All possible combinations (multi-dimensional analysis)
- `rollup()`: Hierarchical left-to-right aggregations only

**5. Combinations in `cube()`**
- All possible subsets of grouping columns

**6. Formula for combinations in `cube()`**
- 2^n combinations (where n = number of columns)

### Performance & Usage

**7. Speed comparison**
- `groupBy()` fastest (single pass)
- `rollup()` moderate (hierarchical aggregations)
- `cube()` slowest (exponential combinations)

**8. When to use `rollup()`**
- Hierarchical data (org charts, time periods, geography)
- Need subtotals at different levels
- Natural ordering/hierarchy exists

**9. When to use `cube()`**
- Multi-dimensional analysis
- All combination cross-tabulations needed
- Business intelligence dashboards

**10. NULL values indicating levels**
- NULL in column = that column is aggregated at this level
- Use `grouping()` to confirm it's aggregation NULL, not data NULL

**11. Hierarchical relationships**
- `rollup()`: Strict left-to-right hierarchy (A→B→C)
- `cube()`: No hierarchy, all independent combinations

**12. Typical `rollup()` use cases**
- Financial reports (Region→Country→City)
- Time hierarchies (Year→Quarter→Month)
- Organizational hierarchies (Department→Team→Employee)

**13. Typical `cube()` use cases**
- OLAP analysis
- Cross-dimensional reports (Product × Region × Time)
- What-if scenario analysis

**14. `grouping_id()` with ROLLUP/CUBE**
- Identifies which aggregation level each row represents
- Each unique grouping_id = different aggregation combination
- Lower values = more detailed, higher = more aggregated

---

## 3.7.3 SQL Database Support for ROLLUP and CUBE

**1. Major SQL databases supporting `ROLLUP`**
- Oracle, SQL Server, PostgreSQL, DB2, Teradata

**2. Major SQL databases supporting `CUBE`**
- Oracle, SQL Server, PostgreSQL (9.5+), DB2

**3. MySQL ROLLUP/CUBE support**
- Partial support: `WITH ROLLUP` modifier only
- No `CUBE` support

**4. MySQL ROLLUP syntax**
- `GROUP BY col1, col2 WITH ROLLUP`
- (Modifier syntax, not function)

**5. SQLite ROLLUP/CUBE support**
- **NO** - Neither supported

**6. Standard SQL syntax**
- `GROUP BY ROLLUP(col1, col2, col3)`
- `GROUP BY CUBE(col1, col2, col3)`

**7. SQL Server, PostgreSQL, Oracle support**
- **YES** - All three support both `ROLLUP` and `CUBE`

**8. Spark SQL ROLLUP/CUBE support**
- **YES** - Both SQL syntax and DataFrame API
- `df.rollup()`, `df.cube()`, or SQL: `GROUP BY ROLLUP/CUBE`

**9. Trino and Databricks support**
- **YES** - Both fully support `ROLLUP` and `CUBE`

**10. Additional Spark SQL grouping features**
- `GROUPING SETS`: Custom grouping combinations
- More flexible than ROLLUP/CUBE
- Syntax: `GROUP BY GROUPING SETS ((col1, col2), (col1), ())`
