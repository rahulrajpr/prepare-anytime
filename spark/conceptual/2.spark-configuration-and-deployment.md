## 2. Spark Configuration & Deployment

### 2.1 SparkSession & Context

#### 2.1.1 What is the difference between SparkSession and SparkContext?

**Definition:**

- **SparkContext:** Original entry point for Spark functionality (RDD API, since Spark 1.x)
- **SparkSession:** Unified entry point (since Spark 2.x) that includes SparkContext, SQLContext, HiveContext

**Comparison Table:**

| Aspect                  | SparkContext        | SparkSession                    |
| ----------------------- | ------------------- | ------------------------------- |
| **API Coverage**  | RDD operations only | RDD + DataFrame + Dataset + SQL |
| **Spark Version** | 1.x and 2.x         | 2.x and 3.x                     |
| **Creation**      | `SparkContext()`  | `SparkSession.builder()`      |

**Key Points:**

- SparkSession internally contains SparkContext
- Use SparkSession for all new Spark applications
- Backward compatible - can access SparkContext via `spark.sparkContext`

#### 2.1.2 How do DataFrames relate to SparkSession and RDDs to SparkContext?

**Relationships:**

- **DataFrames ↔ SparkSession:** Created and manipulated through SparkSession APIs
- **RDDs ↔ SparkContext:** Created and manipulated through SparkContext APIs

**Code Example:**

```python
# SparkSession for DataFrames
spark = SparkSession.builder.appName("example").getOrCreate()
df = spark.read.csv("data.csv")

# SparkContext for RDDs
sc = spark.sparkContext
rdd = sc.parallelize([1, 2, 3, 4, 5])
```

**Interview Tip:** Know that DataFrames are built on RDDs but with optimized execution plans via Catalyst optimizer.

#### 2.1.3 Where is the SparkSession configuration defined?

**Configuration Sources:**

1. **Programmatically:**`SparkSession.builder.config()`
2. **spark-submit:**`--conf` parameters
3. **Configuration Files:**`spark-defaults.conf`
4. **Environment Variables:**`SPARK_HOME/conf/spark-env.sh`

**Common Configuration Methods:**

**python**

```
# Programmatic configuration
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()
```

**Precedence Order:** Programmatic > spark-submit > Configuration files > Defaults

#### 2.1.4 What are the most important Spark configuration parameters?

**Critical Configurations:**

| Category                     | Key Parameters                                    | Purpose                  |
| ---------------------------- | ------------------------------------------------- | ------------------------ |
| **Memory**             | `spark.executor.memory`,`spark.driver.memory` | JVM heap size            |
| **Cores**              | `spark.executor.cores`,`spark.task.cpus`      | Parallelism control      |
| **Shuffle**            | `spark.sql.shuffle.partitions`                  | Reduce side parallelism  |
| **Dynamic Allocation** | `spark.dynamicAllocation.enabled`               | Auto-scale executors     |
| **Serialization**      | `spark.serializer`                              | Performance optimization |

**Interview Tip:** Be ready to explain how you'd tune these for specific workloads (ETL vs ML).

### 2.2 Cluster Managers

#### 2.2.1 What are the different cluster managers available for Spark?

**Comparison Table:**

| Cluster Manager      | Best For                 | Key Feature                      |
| -------------------- | ------------------------ | -------------------------------- |
| **YARN**       | Hadoop ecosystems        | Mature, resource sharing         |
| **Kubernetes** | Cloud-native, containers | Isolation, scalability           |
| **Standalone** | Simple deployments       | Easy setup, minimal dependencies |
| **Mesos**      | Mixed workloads          | Fine-grained resource sharing    |

**Key Points:**

* YARN: Industry standard for on-prem Hadoop
* Kubernetes: Growing popularity for cloud deployments
* Standalone: Good for testing and small clusters

#### 2.2.2 Which cluster manager is preferred for production environments and why?

**Production Recommendations:**

* **Enterprise/Hadoop:** YARN (mature, integrated security)
* **Cloud-Native:** Kubernetes (containerization, auto-scaling)
* **AWS EMR:** YARN (optimized for AWS ecosystem)
* **Azure HDInsight:** YARN or Kubernetes

**Why YARN Dominates:**

* Battle-tested in large-scale deployments
* Fine-grained resource management
* Strong security integration (Kerberos)
* Ecosystem tooling maturity

#### 2.2.3 Which cluster managers are used in AWS Glue and Databricks platforms?

**Managed Service Architectures:**

* **AWS Glue:** Proprietary serverless engine (not traditional Spark cluster managers)
* **Databricks:** Optimized Apache Spark with proprietary control plane
* **Google Dataproc:** YARN-based managed Hadoop/Spark service

**Key Differentiators:**

* Managed services abstract cluster management complexity
* Auto-scaling, monitoring, and maintenance handled by platform
* Focus shifts from cluster tuning to job optimization

#### 2.2.4 How is YARN used in Databricks, Google Dataproc, and on-premise setups like Cloudera?

**YARN Implementations:**

* **Databricks:** Custom optimized Spark (not YARN) with Photon engine
* **Google Dataproc:** Standard YARN with Google Cloud integration
* **Cloudera/CDH:** Enterprise YARN with management tools
* **Hortonworks:** Pure Apache YARN distribution

**Interview Tip:** Understand that managed services often use customized Spark versions with proprietary optimizations.

### 2.3 Deployment Modes

#### 2.3.1 What are the different deployment modes in Spark (client mode vs cluster mode)?

**Definition:**

* **Client Mode:** Driver runs on the machine where spark-submit is executed
* **Cluster Mode:** Driver runs inside the cluster (on a worker node)

**Comparison Table:**

| Aspect                       | Client Mode                       | Cluster Mode      |
| ---------------------------- | --------------------------------- | ----------------- |
| **Driver Location**    | Client machine                    | Cluster worker    |
| **Network Dependency** | High (client must stay connected) | Low               |
| **Production Use**     | Development/Testing               | Production        |
| **Resource Usage**     | Client machine resources          | Cluster resources |

#### 2.3.2 What are the implications of each mode for driver placement and resource allocation?

**Client Mode Implications:**

* **Driver Resources:** Use client machine CPU/memory
* **Network:** Client must remain connected to cluster
* **Firewall:** Client needs network access to executors
* **Use Case:** Interactive development, debugging

**Cluster Mode Implications:**

* **Driver Resources:** Allocated from cluster resources
* **Reliability:** Client can disconnect after submission
* **Production:** Recommended for production workloads
* **Resource Management:** Unified with cluster scheduler

#### 2.3.3

 In a high-availability setup for Spark on Kubernetes, how is a driver
or executor failure handled differently compared to YARN?

**Failure Handling Comparison:**

| Component                  | Kubernetes                         | YARN                      |
| -------------------------- | ---------------------------------- | ------------------------- |
| **Driver Failure**   | Pod restart policy, new driver pod | ApplicationMaster restart |
| **Executor Failure** | New executor pod scheduled         | New container allocated   |
| **Node Failure**     | Pods rescheduled on healthy nodes  | Containers reassigned     |
| **State Recovery**   | External shuffle service needed    | Built-in shuffle handling |

**Key Points:**

* Kubernetes: More granular control but requires more configuration
* YARN: Battle-tested failure recovery mechanisms
* Both support driver high-availability with external services

### 2.4 Dynamic Resource Allocation

#### 2.4.1 What is dynamic allocation in Spark? How do you configure it?

**Definition:** Dynamic allocation automatically scales the number of executors up/down based on workload requirements.

**Configuration:**

**bash**

```
spark-submit \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=1 \
  --conf spark.dynamicAllocation.maxExecutors=50 \
  --conf spark.dynamicAllocation.initialExecutors=2
```

**Key Parameters:**

* `spark.dynamicAllocation.enabled=true`
* `spark.dynamicAllocation.minExecutors` / `maxExecutors`
* `spark.dynamicAllocation.initialExecutors`
* `spark.shuffle.service.enabled=true` (for shuffle data preservation)

#### 2.4.2 What does spark.dynamicAllocation.initialExecutors control?

**Definition:**`spark.dynamicAllocation.initialExecutors` sets the number of executors to allocate when the application starts, before dynamic scaling begins.

**Usage Guidelines:**

* Set based on expected initial workload
* Too high: Wasted resources during startup
* Too low: Slow initial processing
* Typical range: 2-10 executors depending on data size

**Interview Tip:** Mention that this helps avoid cold-start performance issues.

#### 2.4.3 How do AWS Glue and Databricks implement autoscaling on top of Apache Spark?

**AWS Glue Autoscaling:**

* Proprietary scaling based on DPU (Data Processing Unit) usage
* Scales both number and size of compute resources
* Billed per DPU-hour with minute-level granularity

**Databricks Autoscaling:**

* **Standard:** Terminate nodes after inactivity period
* **Optimized:** Scale workers based on queue depth and task backlog
* Photon engine provides additional performance scaling

**Key Differentiator:** Managed services provide simpler autoscaling without complex configuration.

#### 2.4.4 How does dynamic scaling work with Kubernetes?

**Kubernetes Dynamic Scaling:**

1. Spark requests executor pods via Kubernetes API
2. Horizontal Pod Autoscaler (HPA) can scale based on metrics
3. Cluster Autoscaler adds/removes nodes based on pod demands
4. Custom metrics can trigger scaling decisions

**Configuration Example:**

**bash**

```
spark-submit \
  --conf spark.kubernetes.allocation.batch.size=10 \
  --conf spark.kubernetes.executor.deleteOnTermination=false
```

**Advantage:** Tight integration with Kubernetes ecosystem and monitoring.

### 2.5 Cloud Platform Specifics

#### 2.5.1 What is a DPU (Data Processing Unit) in AWS Glue?

**Definition:** DPU is a relative measure of processing power in AWS Glue, representing a combination of 4 vCPUs and 16 GB of memory.

**DPU Characteristics:**

* **Compute:** 4 vCPUs + 16 GB memory per DPU
* **Pricing:** Billed per DPU-hour
* **Scaling:** Auto-scaling from 2 to 100 DPUs (higher with limit increase)
* **Use Case:** Determines parallelism and job execution speed

**Interview Tip:** Calculate DPU requirements based on data volume and time constraints.

#### 2.5.2 What are AWS Spot Instances and how are they relevant for cost-effective Spark deployments?

**Spot Instances Definition:** AWS spare compute capacity available at up to 90% discount, but can be terminated with 2-minute warning.

**Spark Usage Strategy:**

* **Cost Savings:** 60-90% reduction vs On-Demand instances
* **Fault Tolerance:** Design jobs to handle executor failures
* **Mixed Fleets:** Combine Spot (cost) + On-Demand (reliability)
* **Best For:** Batch processing, non-time-critical workloads

**Configuration:**

**bash**

```
--conf spark.executor.instances=10 \
--conf spark.ec2.spot.instance.count=8 \
--conf spark.ec2.onDemand.instance.count=2
```

#### 2.5.3 What are the latest Spark versions available in Databricks, Google Dataproc, and AWS Glue?

**Platform Spark Versions (as of 2024):**

| Platform                  | Latest Spark Version   | Key Features                      |
| ------------------------- | ---------------------- | --------------------------------- |
| **Databricks**      | Spark 3.4+ with Photon | Native vectorized engine          |
| **Google Dataproc** | Spark 3.3+             | Integrated with GCP services      |
| **AWS Glue**        | Spark 3.3+             | Serverless, minimal configuration |
| **Azure Synapse**   | Spark 3.3+             | Deep Azure integration            |

**Version Adoption Pattern:**

* Managed services typically lag 6-12 months behind Apache releases
* Focus on stability and integration over latest features
* Backport critical performance and security fixes

**Interview Tip:** Check current versions before interviews as these change frequently.
