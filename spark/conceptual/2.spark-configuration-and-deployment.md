# Spark Configuration & Deployment - Interview Q&A

## 2.1 SparkSession & Context

### 2.1.1 What is the difference between SparkSession and SparkContext?

**SparkContext Definition:** The entry point and central orchestrator for Spark functionality in earlier versions (Spark 1.x), primarily for RDD operations.

**SparkSession Definition:** The unified entry point for Spark functionality introduced in Spark 2.0, encapsulating SparkContext, SQLContext, HiveContext, and StreamingContext.

**Key Differences:**
- **SparkContext:** Manages RDD operations and low-level Spark features
- **SparkSession:** Provides unified interface for DataFrame, Dataset, SQL, and Streaming operations
- **Compatibility:** SparkSession internally contains SparkContext for backward compatibility

### 2.1.2 How do DataFrames relate to SparkSession and RDDs to SparkContext?

**Architectural Relationship:**
- **DataFrames → SparkSession:** Created and manipulated through SparkSession APIs (`spark.read.csv()`, `spark.sql()`)
- **RDDs → SparkContext:** Created and processed through SparkContext APIs (`sc.parallelize()`, `sc.textFile()`)

**Evolution Path:** Spark 1.x (RDDs + SparkContext) → Spark 2.x+ (DataFrames/Datasets + SparkSession)

### 2.1.3 Where is the SparkSession configuration defined?

**Configuration Sources:**
1. **Programmatic:** `SparkSession.builder().config("key", "value")`
2. **spark-submit:** `--conf "spark.sql.adaptive.enabled=true"`
3. **Configuration Files:** `spark-defaults.conf`
4. **Environment Variables:** `SPARK_HOME`, `SPARK_MASTER_HOST`

### 2.1.4 What are the most important Spark configuration parameters?

**Critical Configuration Categories:**

**Memory Management:**
- `spark.executor.memory`: Amount of memory to use per executor process
- `spark.driver.memory`: Amount of memory to use for the driver process
- `spark.memory.fraction`: Fraction of heap space used for execution and storage

**Parallelism & Cores:**
- `spark.executor.cores`: Number of cores to use on each executor
- `spark.default.parallelism`: Default number of partitions in RDDs
- `spark.sql.shuffle.partitions`: Number of partitions to use when shuffling data for joins/aggregations

**Dynamic Allocation:**
- `spark.dynamicAllocation.enabled`: Whether to use dynamic resource allocation
- `spark.dynamicAllocation.minExecutors`: Lower bound for number of executors
- `spark.dynamicAllocation.maxExecutors`: Upper bound for number of executors

**Shuffle & I/O:**
- `spark.sql.adaptive.enabled`: Whether to enable adaptive query execution
- `spark.serializer`: Class used to serialize objects

## 2.2 Cluster Managers

### 2.2.1 What are the different cluster managers available for Spark?

**Cluster Manager Definition:** External services that manage resource allocation and executor lifecycle in Spark deployments.

**Available Options:**
- **Apache YARN:** Hadoop ecosystem resource manager
- **Kubernetes:** Container orchestration platform
- **Apache Mesos:** General-purpose cluster manager
- **Standalone:** Spark's built-in simple cluster manager

### 2.2.2 Which cluster manager is preferred for production environments and why?

**Enterprise Preferences:**
- **On-premise Hadoop:** YARN (integration with HDFS, mature ecosystem)
- **Cloud-native:** Kubernetes (containerization, scalability, cloud integration)
- **Legacy:** Mesos (declining adoption)

**Kubernetes Advantages:**
- Container isolation and portability
- Native cloud provider integration
- Fine-grained resource management

### 2.2.3 Which cluster managers are used in AWS Glue and Databricks platforms?

**Managed Service Architectures:**
- **AWS Glue:** Proprietary serverless engine (not traditional cluster managers)
- **Databricks:** Optimized Spark engine with proprietary resource management
- **Google Dataproc:** YARN-based managed Hadoop/Spark service

### 2.2.4 How is YARN used in Databricks, Google Dataproc, and on-premise setups like Cloudera?

**Deployment Patterns:**
- **Databricks:** Custom optimized runtime (YARN-like internal scheduler)
- **Google Dataproc:** Managed YARN clusters with auto-scaling
- **Cloudera/Hortonworks:** Traditional YARN deployment with enterprise features

## 2.3 Deployment Modes

### 2.3.1 What are the different deployment modes in Spark?

**Deployment Mode Definition:** Determines the placement and lifecycle management of the Spark driver process.

**Client Mode:**
- Driver runs on the client machine (where spark-submit is executed)
- Client machine must remain active for job duration
- Suitable for interactive sessions and development

**Cluster Mode:**
- Driver runs inside the cluster (on a worker node)
- Client can disconnect after job submission
- Recommended for production workloads

### 2.3.2 What are the implications of each mode for driver placement and resource allocation?

**Client Mode Implications:**
- **Driver Placement:** Local machine
- **Network Dependency:** Continuous client-cluster connectivity required
- **Resource Impact:** Client machine resources used for driver

**Cluster Mode Implications:**
- **Driver Placement:** Cluster worker node
- **Fault Tolerance:** Cluster manager can restart failed drivers
- **Resource Management:** Unified resource allocation within cluster

### 2.3.3 In a high-availability setup for Spark on Kubernetes, how is a driver or executor failure handled differently compared to YARN?

**Kubernetes Failure Handling:**
- **Driver Failure:** Kubernetes restarts driver pod based on restart policy
- **Executor Failure:** Kubernetes recreates executor pods automatically
- **State Management:** External shuffle service required for shuffle data persistence

**YARN Failure Handling:**
- **Driver Failure:** ApplicationMaster restart with recovery from checkpoint
- **Executor Failure:** NodeManager respawns executors
- **Shuffle Management:** Built-in shuffle service in NodeManager

## 2.4 Dynamic Resource Allocation

### 2.4.1 What is dynamic allocation in Spark? How do you configure it?

**Dynamic Allocation Definition:** Feature that automatically scales the number of executors up/down based on workload requirements.

**Configuration:**
```properties
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=50
spark.dynamicAllocation.initialExecutors=2
spark.shuffle.service.enabled=true

2.4.2 What does spark.dynamicAllocation.initialExecutors control?
Parameter Definition: Specifies the initial number of executors to allocate when the application starts, before dynamic scaling logic applies.

Use Case: Balances between quick startup (low value) and immediate processing capacity (high value)

2.4.3 How do AWS Glue and Databricks implement autoscaling on top of Apache Spark?
AWS Glue Autoscaling:

Proprietary scaling based on DPU (Data Processing Unit) utilization

Scales between minimum and maximum DPU limits

Continuous workload monitoring and adjustment

Databricks Autoscaling:

Enhanced dynamic allocation with cluster manager integration

Spot instance optimization for cost savings

Workload-aware scaling policies

2.4.4 How does dynamic scaling work with Kubernetes?
Kubernetes Integration:

Spark requests executor pods through Kubernetes API

Horizontal Pod Autoscaler (HPA) can complement Spark's dynamic allocation

Resource quotas and limits enforced at container level

Pod lifecycle managed by Kubernetes controller

2.5 Cloud Platform Specifics
2.5.1 What is a DPU (Data Processing Unit) in AWS Glue?
DPU Definition: AWS Glue's proprietary unit of measure representing compute capacity, comprising 4 vCPUs and 16 GB of memory.

Pricing Model: Customers pay per DPU-hour consumed

Standard: 2-100 DPU range

G.1X/G.2X: Memory-optimized variants for specific workloads

2.5.2 What are AWS Spot Instances and how are they relevant for cost-effective Spark deployments?
Spot Instances Definition: AWS's unused EC2 capacity available at significant discounts (up to 90% compared to On-Demand).

Spark Relevance:

Cost-effective for fault-tolerant workloads

Ideal for executor nodes in cluster mode

Requires handling of instance termination gracefully

Best Practices:

Use for executors, not drivers

Implement checkpointing for long-running jobs

Combine with On-Demand instances for stability

2.5.3 What are the latest Spark versions available in Databricks, Google Dataproc, and AWS Glue?
Platform-Specific Versions (as of 2024):

Databricks: Spark 3.5.x with proprietary optimizations and Delta Lake integration

Google Dataproc: Spark 3.4.x/3.5.x with Hadoop ecosystem components

AWS Glue: Spark 3.3.x/3.4.x with AWS-specific enhancements and connectors

Version Lag: Managed services typically lag 6-12 months behind Apache Spark releases for stability testing

text
