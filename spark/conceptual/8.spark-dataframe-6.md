## 3.15 JSON Functions

### 3.15.1 What is the difference between JSON as a file vs JSON as a column value?

**Difference:**

- **JSON File**: Entire file in JSON format, read with `spark.read.json()`
- **JSON Column**: String column containing JSON data, processed with JSON functions

### 3.15.2 Why does Spark convert JSON to Structs (not Maps) by default when parsing?

**Reason:** Structs provide:

- Type safety and schema enforcement
- Better performance through columnar access
- Catalyst optimizer benefits
- SQL compatibility

### 3.15.3 What are the performance implications of Struct vs Map for JSON data?

**Performance:**

- **Struct**: Faster access, better optimization, fixed schema
- **Map**: Flexible schema, slower access, less optimization

### 3.15.4 How do you parse JSON strings using from_json()?

**Usage:**

```python
from pyspark.sql.functions import from_json
from pyspark.sql.types import StructType, StructField, StringType

schema = StructType([StructField("name", StringType()), StructField("age", IntegerType())])
df.withColumn("parsed_json", from_json(col("json_string"), schema))
```

### 3.15.5 What schema do you need to provide for from_json()?

**Requirement:** Must provide a `StructType` schema defining the JSON structure.

### 3.15.6 How do you convert structs to JSON using to_json()?

**Usage:**`df.withColumn("json_output", to_json(col("struct_column")))`

### 3.15.7 What is the difference between from_json() and to_json()?

**Direction:**

* **from_json()** : JSON string → Struct
* **to_json()** : Struct → JSON string

### 3.15.8 What does get_json_object() do?

**Purpose:** Extracts JSON field using JSONPath expression.

**Example:**`get_json_object(col("json"), "$.user.name")`

### 3.15.9 How do you use json_tuple() to extract multiple fields?

**Usage:**`json_tuple(col("json"), "field1", "field2", "field3")`

### 3.15.10 What is the difference between from_json() and json_tuple()?

**Comparison:**

* **from_json()** : Returns struct with full schema
* **json_tuple()** : Returns multiple separate columns

### 3.15.11 What is the difference between from_json() and get_json_object() in terms of efficiency?

**Efficiency:**

* **from_json()** : More efficient for multiple field access (parses once)
* **get_json_object()** : Efficient for single field extraction

### 3.15.12 When should you use from_json() vs get_json_object()?

**Guidelines:**

* **from_json()** : Multiple fields, structured access, type safety
* **get_json_object()** : Single field, JSONPath queries, dynamic access

### 3.15.13 What does schema_of_json() function do?

**Purpose:** Infers schema from JSON string example.

### 3.15.14 What does json_array_length() return?

**Returns:** Number of elements in JSON array.

### 3.15.15 What does json_object_keys() return?

**Returns:** Array of keys from JSON object.

## 3.16 Advanced Column Operations

### 3.16.1 What does lit() function do? When do you use it?

**Purpose:** Creates a column with literal (constant) value.

**Use Case:** When you need constant values in expressions.

**Example:**`lit("constant_value")`, `lit(42)`

### 3.16.2 How do you create a column with constant values across all rows?

**Method:** Use `lit()` function:

**python**

```
df.withColumn("constant_col", lit("same_value_for_all_rows"))
```

### 3.16.3 What is input_file_name() function used for?

**Purpose:** Returns the source file path for each row.

**Use Case:** Data lineage, debugging, partitioning analysis.

### 3.16.4 How do you use spark_partition_id() to see data distribution?

**Usage:**`df.withColumn("partition_id", spark_partition_id())`

### 3.16.5 What does hash() function compute?

**Purpose:** Computes hash code for column values.

**Use Case:** Partitioning, sampling, duplicate detection.

### 3.16.6 What is md5() and sha1() used for?

**Cryptographic Hashes:**

* **md5()** : 128-bit hash (weaker, faster)
* **sha1()** : 160-bit hash (stronger, slower)

**Use Case:** Data fingerprinting, checksums.

### 3.16.7 How do you use crc32() for checksums?

**Usage:**`crc32(col("data"))` for cyclic redundancy check.

### 3.16.8 What does base64() and unbase64() do?

**Encoding:**

* **base64()** : Encodes binary to base64 string
* **unbase64()** : Decodes base64 string to binary

### 3.16.9 How do you generate random values using rand() and randn()?

**Functions:**

* **rand()** : Uniform distribution (0-1)
* **randn()** : Standard normal distribution

## 3.17 Set Operations on DataFrames

### 3.17.1 What is the difference between union() and unionAll()?

**Critical Answer:** In DataFrame API, they are identical - both keep duplicates.

### 3.17.2 What is the critical inconsistency between DataFrame API and Spark SQL for union operations?

**Inconsistency:**

* **DataFrame API** : `union()` keeps duplicates
* **Spark SQL** : `UNION` removes duplicates

### 3.17.3 In DataFrame API, do union() and unionAll() keep or remove duplicates?

**DataFrame API:** Both keep duplicates (identical behavior).

### 3.17.4 In Spark SQL, does UNION keep or remove duplicates?

**Spark SQL:**`UNION` removes duplicates (SQL standard).

### 3.17.5 In Spark SQL, does UNION ALL keep or remove duplicates?

**Spark SQL:**`UNION ALL` keeps duplicates (SQL standard).

### 3.17.6 Why is this inconsistency important to remember?

**Importance:** Can lead to different results between DataFrame API and Spark SQL queries.

### 3.17.7 What does unionByName() do? How is it different from union()?

**Difference:**

* **union()** : Matches columns by position
* **unionByName()** : Matches columns by name

### 3.17.8 What does unionByName() do when schemas differ between DataFrames?

**Behavior:** Matches columns with same names, ignores others by default.

### 3.17.9 What is the allowMissingColumns parameter in unionByName()?

**Parameter:** When True, allows schemas with different columns.

### 3.17.10 What does unionByName(allowMissingColumns=True) enable?

**Enables:** Union of DataFrames with different schemas, filling missing columns with nulls.

### 3.17.11 When would you use union() vs unionByName()?

**Guidelines:**

* **union()** : Same schema (position-based)
* **unionByName()** : Different schemas or column order changes

### 3.17.12 What is "strict mode" vs "flexible mode" vs "forgiving mode" for unions?

**Modes:**

* **Strict** : `union()` - exact schema match required
* **Flexible** : `unionByName()` - column name matching
* **Forgiving** : `unionByName(allowMissingColumns=True)` - schema differences allowed

### 3.17.13 How do you use intersect() to find common rows?

**Usage:**`df1.intersect(df2)`

### 3.17.14 What is the difference between intersect() and intersectAll()?

**Duplicate Handling:**

* **intersect()** : Returns distinct common rows
* **intersectAll()** : Returns all common rows including duplicates

### 3.17.15 Does intersect() show unique or all common records?

**intersect():** Shows unique common records only.

### 3.17.16 Does intersectAll() show unique or all common records?

**intersectAll():** Shows all common records including duplicates.

### 3.17.17 How does intersectAll() handle duplicate counts?

**Behavior:** If row appears 3 times in df1 and 2 times in df2, appears 2 times in result.

### 3.17.18 What does intersect() answer vs intersectAll()?

**Questions:**

* **intersect()** : "Which distinct values are in both datasets?"
* **intersectAll()** : "Which values (with counts) are in both datasets?"

### 3.17.19 When would you use intersect() vs intersectAll()?

**Use Cases:**

* **intersect()** : Set membership checking
* **intersectAll()** : Multi-set operations with counts

### 3.17.20 What does subtract() (or exceptAll()) do?

**Purpose:** Returns rows in first DataFrame but not in second.

### 3.17.21 Do set operations require the same schema in both DataFrames?

**Yes:** All set operations require identical schemas (column names and types).

### 3.17.22 How do set operations handle duplicates?

**Handling:**

* **union()/unionAll()** : Keep duplicates
* **intersect()** : Remove duplicates
* **intersectAll()** : Keep duplicate counts
* **except()** : Remove duplicates
* **exceptAll()** : Keep duplicate counts
