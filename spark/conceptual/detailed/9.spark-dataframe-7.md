# PySpark Interview Preparation Guide

## 3.18 Comparison & Logical Operators

### 3.18.1 What is the difference between = and <=> (null-safe equality)?

**Critical Differences:**

| Aspect                 | = (Standard Equality) | <=> (Null-Safe Equality)       |
| ---------------------- | --------------------- | ------------------------------ |
| **NULL = NULL**  | Returns NULL          | Returns TRUE ✅                |
| **NULL = value** | Returns NULL          | Returns FALSE ❌               |
| **value = NULL** | Returns NULL          | Returns FALSE ❌               |
| **Use Case**     | Standard comparisons  | Joins/filters with NULL values |

### 3.18.2 How does = handle NULL comparisons?

**Behavior:** ANY comparison involving NULL returns NULL (SQL standard three-valued logic)

### 3.18.3 How does <=> handle NULL comparisons?

**Behavior:** Treats NULL as comparable value - NULL <=> NULL returns TRUE

### 3.18.4 When should you use <=> instead of =?

**Use Cases:**

- **Joins** on columns that may contain NULLs
- **Filters** where you want NULL = NULL to match
- **Deduplication** considering NULLs as equal

### 3.18.5 What is the DataFrame API equivalent of <=>?

**Equivalent:** `col("a").eqNullSafe(col("b"))`

### 3.18.6 Explain the AND/OR truth table with NULL values.

**Truth Tables:**

AND Truth Table:
TRUE AND TRUE = TRUE
TRUE AND FALSE = FALSE
TRUE AND NULL = NULL ❗
FALSE AND ANY = FALSE ✅ (short-circuits)

OR Truth Table:
TRUE OR ANY = TRUE ✅ (short-circuits)
FALSE OR FALSE = FALSE
FALSE OR TRUE = TRUE
FALSE OR NULL = NULL ❗

**text**

```
### 3.18.7 What does TRUE AND NULL return?

**Returns:** NULL (because NULL could be TRUE or FALSE)

### 3.18.8 What does FALSE OR NULL return?

**Returns:** NULL (because NULL could make it TRUE or keep it FALSE)

### 3.18.9 What is short-circuit evaluation in AND/OR operations?

**Definition:** Stops evaluating remaining conditions once final result is determined

**Examples:**
- `FALSE AND expensive_function()` → expensive_function() never runs
- `TRUE OR expensive_function()` → expensive_function() never runs

## 3.19 Window Functions

### 3.19.1 What is rowsBetween in window functions? Provide examples.

**Definition:** Defines window frame by physical row positions (row numbers)

**Examples:**
```python
# Current row + 1 preceding + 1 following
Window.rowsBetween(-1, 1)

# All rows from start to current row
Window.rowsBetween(Window.unboundedPreceding, Window.currentRow)

# Current row to end
Window.rowsBetween(Window.currentRow, Window.unboundedFollowing)
```

### 3.19.2 What is rangeBetween in window functions? How does it differ from rowsBetween?

**Critical Differences:**

| Aspect                | rowsBetween               | rangeBetween                       |
| --------------------- | ------------------------- | ---------------------------------- |
| **Basis**       | Physical row positions    | Logical value ranges               |
| **Example**     | 2 rows before/after       | Values within ±10 units           |
| **Performance** | Faster (no value sorting) | Slower (requires value comparison) |
| **Use Case**    | Fixed window sizes        | Value-based windows                |

### 3.19.3 Does data shuffling occur during window function operations? Why or why not?

**Answer:** YES, if window partitioning doesn't match existing data distribution

**Reason:** Data must be reshuffled to group rows by window partition keys

### 3.19.4 What is the difference between CUME_DIST() and PERCENT_RANK()?

**Critical Differences:**

| Function                 | Formula                        | Range  | Use Case              |
| ------------------------ | ------------------------------ | ------ | --------------------- |
| **CUME_DIST()**    | (rows ≤ current) / total_rows | (0, 1] | Distribution analysis |
| **PERCENT_RANK()** | (rank - 1) / (total_rows - 1)  | [0, 1] | Competitive ranking   |

### 3.19.5 What does CUME_DIST() calculate?

**Calculates:** Cumulative distribution - proportion of rows with values ≤ current row

### 3.19.6 What does PERCENT_RANK() calculate?

**Calculates:** Relative rank position between 0 (first) and 1 (last)

### 3.19.7 When would you use CUME_DIST() vs PERCENT_RANK()?

**CUME_DIST:** "What percentage of students scored ≤ my score?"
**PERCENT_RANK:** "What's my rank position relative to others?"

### 3.19.8 What is asc_nulls_first vs asc_nulls_last in ordering?

**NULL Placement:**

* **asc_nulls_first** : NULLs → Lowest values → Highest values
* **asc_nulls_last** : Lowest values → Highest values → NULLs

### 3.19.9 Where do NULLs appear with asc_nulls_first?

**Position:** NULLs appear BEFORE all non-NULL values

### 3.19.10 Where do NULLs appear with asc_nulls_last?

**Position:** NULLs appear AFTER all non-NULL values

## 3.20 DataFrame Gotchas & Common Pitfalls

### 3.20.1 Why does chaining multiple withColumn() calls have performance implications?

**Reason:** Each `withColumn()` creates a NEW DataFrame object, increasing:

* Plan complexity in Catalyst optimizer
* Memory usage for plan storage
* Debugging difficulty

### 3.20.2 What is the better alternative to multiple withColumn() calls?

**Better Approach:** Single `select()` with all transformations

**python**

```
# ❌ Bad: Multiple withColumn()
df.withColumn("col1", ...).withColumn("col2", ...).withColumn("col3", ...)

# ✅ Good: Single select()
df.select("*", expr("...").alias("col1"), expr("...").alias("col2"), expr("...").alias("col3"))
```

### 3.20.3

 When joining two tables with the same column name (e.g., 'id'), why
does select("*") work but select("id") throws an "ambiguous column"
error?

**Reason:**

* `select("*")` → Expands to `table1.id, table2.id` (explicit)
* `select("id")` → Cannot determine which `id` column (ambiguous)

### 3.20.4 How do you resolve column name ambiguity after joins?

**Solutions:**

**python**

```
# Method 1: DataFrame-specific references
df1.join(df2).select(df1["id"], df2["id"])

# Method 2: Column aliases before join
df1.alias("a").join(df2.alias("b")).select(col("a.id"), col("b.id"))

# Method 3: Rename columns
df1.join(df2.withColumnRenamed("id", "id2")).select("id", "id2")
```

### 3.20.5 What happens when you call an action multiple times on the same DataFrame? Is it recomputed?

**Answer:** YES, recomputed every time (unless cached)

**Example:**`df.count()` called 3 times = 3 full computations

### 3.20.6 Why should you be careful with collect() on large datasets?

**Dangers:**

* Driver OutOfMemoryError (all data collected to driver)
* Network saturation
* Single point of failure

### 3.20.7 What is the difference between df.count() and df.select(count("*"))?

**Critical Differences:**

| Aspect                | df.count()            | df.select(count("*"))         |
| --------------------- | --------------------- | ----------------------------- |
| **Return Type** | Integer (action)      | DataFrame (transformation)    |
| **Execution**   | Immediate computation | Lazy (computed later)         |
| **Use Case**    | Get row count now     | Build transformation pipeline |

### 3.20.8 Can you modify a DataFrame in place? Why or why not?

**Answer:** NO, DataFrames are IMMUTABLE

**Reason:** Functional programming paradigm - operations return new DataFrames

### 3.20.9 What happens when you try to access a column that doesn't exist?

**Result:** AnalysisException at ANALYSIS phase (before execution)

### 3.20.10 Why might df.show() show different results than the actual data?

**Reasons:**

* Sampling in `show()` (first N rows only)
* No ordering guarantee without `orderBy()`
* Concurrent data modifications
* Caching inconsistencies

### 3.20.11 What is the behavior of limit() - does it guarantee which rows are returned?

**Answer:** NO guarantee without `orderBy()`

**Behavior:** Returns arbitrary N rows from current partitioning

### 3.20.12 How do column name case sensitivity work in Spark (spark.sql.caseSensitive)?

**Default Behavior:**

* **Spark SQL** : Case INSENSITIVE ✅
* **DataFrame API** : Case SENSITIVE ✅
* **Config** : `spark.sql.caseSensitive` (default: false)

## 3.21 Performance Tips for DataFrame Operations

### 3.21.1 Why is it better to filter data early in your transformation pipeline?

**Performance Impact:** Reduces data volume for ALL subsequent operations

**Example:** Filter 1GB → 100MB before join = 90% less shuffle data

### 3.21.2 What is the performance difference between filter() and where() (trick question)?

**Answer:** They are ALIASES - IDENTICAL performance

### 3.21.3 When should you use repartition() vs coalesce()?

**Decision Matrix:**

| Scenario                        | Use repartition() | Use coalesce()  |
| ------------------------------- | ----------------- | --------------- |
| **Increasing partitions** | ✅ YES            | ❌ NO           |
| **Decreasing partitions** | ✅ YES (balanced) | ✅ YES (faster) |
| **Need perfect balance**  | ✅ YES            | ❌ NO           |
| **Minimize shuffle**      | ❌ NO             | ✅ YES          |

### 3.21.4 What is the performance impact of using UDFs vs built-in functions?

**Performance Impact:** UDFs are 2-10x SLOWER due to:

* No Catalyst optimization
* Python-JVM serialization overhead
* Row-by-row processing vs vectorized

### 3.21.5 Why is reduceByKey() preferred over groupByKey() in RDD operations?

**Key Difference:**`reduceByKey()` performs MAP-SIDE COMBINE

**Data Reduction:** Can reduce shuffle data by 90%+ for aggregations

### 3.21.6 How does column pruning (selecting only needed columns) improve performance?

**Benefits:**

* Less I/O from storage
* Less memory usage
* Faster processing (less data to process)

### 3.21.7 What is predicate pushdown and how does it improve query performance?

**Definition:** Pushing filters DOWN to data source level

**Benefit:** Data files read ONLY relevant rows/columns

### 3.21.8 Why should you avoid using count() unnecessarily in your code?

**Reason:** Triggers FULL DATA SCAN and computation

**Alternative:** Use approximate counts or sampling when exact count isn't critical

## 3.22 Performance Optimization Best Practices

### 3.22.1 What is the optimal partition size for Spark processing (128MB-256MB)?

**Optimal Range:** 128MB to 256MB per partition

**Reason:** Balances parallelism vs overhead

### 3.22.2 How do you determine the right number of partitions for your data?

**Formula:**`optimal_partitions = total_data_size / target_partition_size`

**Example:** 10GB data ÷ 128MB = ~80 partitions

### 3.22.3 Why is it important to avoid small files in distributed processing?

**Small File Problem:**

* Excessive metadata overhead
* Poor cluster utilization
* Many small, inefficient tasks

### 3.22.4 What is the small file problem and its performance impact?

**Impact:** 1000 × 1MB files = 1000 tasks vs 10 × 100MB files = 10 tasks

### 3.22.5 How do you consolidate small files before processing?

**Methods:**

* `df.coalesce(N)` - merge partitions without shuffle
* `df.repartition(N)` - full shuffle for balanced files
* File compaction tools

### 3.22.6 How does data skewness affect query performance?

**Effect:** Slowest task determines job completion time

### 3.22.7 What are the symptoms of data skew in Spark UI?

**Symptoms:**

* Few very long-running tasks
* Uneven task durations
* Some executors idle while others overloaded

### 3.22.8 What is salting technique for handling data skew?

**Technique:** Add random salt to keys to distribute data evenly

### 3.22.9 How do you implement salting for skewed join keys?

**Implementation:**

**python**

```
# Add salt to skewed keys
salted_df = df.withColumn("salted_key", concat(col("key"), lit("_"), (rand() * N).cast("int")))

# Join on salted keys, then aggregate
```

### 3.22.10 What is the performance cost of salting?

**Cost:** Additional shuffle + aggregation overhead

### 3.22.11 What is adaptive execution and how does it handle skew automatically?

**Feature:** Spark 3.0+ automatically splits skewed partitions during sort-merge joins

### 3.22.12 How does reduceByKey perform better than groupByKey?

**Advantage:** Map-side combine reduces shuffle data volume significantly

### 3.22.13 What is the relationship between parallelism and shuffle partitions?

**Relationship:** More partitions = more parallelism but more overhead

**Balance:** 2-4 tasks per CPU core typically optimal

### 3.22.14 What happens if you have too many partitions?

**Problems:**

* Excessive task scheduling overhead
* Many small, inefficient tasks
* Memory pressure from many concurrent tasks

### 3.22.15 How does broadcast join eliminate shuffle?

**Mechanism:** Small table broadcast to ALL executors → no data movement during join

### 3.22.16 What is the maximum size for broadcast (driver memory constraint)?

**Default:** 10MB, configurable up to driver memory limits

### 3.22.17 How does bucketing eliminate shuffle in joins?

**Benefit:** Pre-shuffled data → no shuffle needed during join operations

### 3.22.18 What is partition pruning and how does it reduce data scanning?

**Technique:** Skip reading partitions that don't satisfy filter conditions

### 3.22.19 How does caching improve performance for iterative algorithms?

**Benefit:** Avoid recomputing same transformations multiple times

### 3.22.20 What is checkpointing and when should you use it vs caching?

**Checkpointing:** Breaks lineage to avoid stack overflow in very long transformation chains

### 3.22.21 How do you optimize filter ordering in chained operations?

**Strategy:** Apply most selective filters FIRST

**Example:**`country = 'US' AND city = 'NYC'` → filter by city first if more selective

### 3.22.22 Why should you select only required columns early?

**Benefits:** Less I/O, less memory, faster processing

### 3.22.23 How do you optimize expensive string operations?

**Approaches:**

* Use built-in functions over UDFs
* Pre-process strings if possible
* Avoid regex when simpler functions suffice

### 3.22.24 How does avoiding UDFs improve performance (2-10x faster)?

**Reasons:**

* Catalyst optimizer can't optimize UDFs
* Serialization overhead between JVM and Python
* No vectorized execution

### 3.22.25 What is Pandas UDF (vectorized UDF) and how is it faster?

**Advantage:** Processes data in batches using Pandas → 3-100x faster than row-wise UDFs

### 3.22.26 How much faster is Kryo serialization (2-10x)?

**Improvement:** 2-10x faster serialization with smaller footprint

### 3.22.27 What compression codecs are available (snappy, gzip, lzo, lz4)?

**Options:**

* **Snappy** : Balanced (default)
* **Gzip** : High compression
* **LZ4** : Fastest
* **LZO** : Legacy

### 3.22.28 Why is Parquet preferred for analytics (columnar, compressed)?

**Benefits:** Columnar storage + efficient compression + predicate pushdown

### 3.22.29 What is the optimal file size for Spark (128MB-1GB)?

**Range:** 128MB to 1GB balances I/O efficiency and parallelism

### 3.22.30 How do you control output file size?

**Methods:**

* `df.coalesce(N)` / `df.repartition(N)`
* `maxRecordsPerFile` option
* Control partition count before write

### 3.22.31 What is the difference between repartition() and coalesce() for file output?

**Critical Differences:**

| Aspect                    | repartition()               | coalesce()                      |
| ------------------------- | --------------------------- | ------------------------------- |
| **Shuffle**         | ✅ ALWAYS full shuffle      | ❌ NO shuffle when possible     |
| **Partition Count** | Increase or decrease        | ONLY decrease                   |
| **Data Balance**    | Perfectly balanced          | May be unbalanced               |
| **Performance**     | Slower (shuffle cost)       | Faster (minimal data movement)  |
| **Use Case**        | Need exact N balanced files | Reducing partitions efficiently |
