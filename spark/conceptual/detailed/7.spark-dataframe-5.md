# PySpark Interview Preparation Guide

## 3.11 Type Conversion & Casting

### 3.11.1 How do you cast columns using cast() function?

**Usage:** `col("column_name").cast("target_type")` or `col("column_name").cast(IntegerType())`

**Example:**

```python
from pyspark.sql.types import IntegerType

df.select(col("age").cast("int"))
df.select(col("age").cast(IntegerType()))
```

### 3.11.2 What is the difference between cast() and astype()?

**Answer:**`astype()` is an alias for `cast()` - they are functionally identical.

### 3.11.3 What happens when casting fails (e.g., string "abc" to integer)?

**Behavior:** Returns null for the failed conversion by default.

### 3.11.4 How do you handle casting errors gracefully?

**Approaches:**

* Use `try_cast()` (if available in your Spark version)
* Use `when().otherwise()` logic
* Pre-validate data with regex or conditions

### 3.11.5 What does try_cast() do in Spark SQL?

**Purpose:** Returns null instead of error when casting fails (Spark 3.0+).

## 3.11.1 Numeric Type Casting Functions

### 3.11.1.1 What is tinyint() function and what data type does it cast to?

**Casts to:** 1-byte integer (-128 to 127)

### 3.11.1.2 What is smallint() function and when would you use it?

**Casts to:** 2-byte integer (-32,768 to 32,767)

**Use Case:** When you need small integers with memory efficiency.

### 3.11.1.3 What is the difference between int() and bigint() casting?

**Difference:**

* **int()** : 4-byte integer (-2^31 to 2^31-1)
* **bigint()** : 8-byte integer (-2^63 to 2^63-1)

### 3.11.1.4 When should you use tinyint vs smallint vs int vs bigint?

**Guidelines:**

* **tinyint** : Very small ranges (age, flags, counters < 128)
* **smallint** : Medium ranges (year, IDs < 32K)
* **int** : Most common use cases (default choice)
* **bigint** : Large numbers (timestamps, big counters)

### 3.11.1.5 What are the value ranges for tinyint, smallint, int, and bigint?

**Ranges:**

* **tinyint** : -128 to 127
* **smallint** : -32,768 to 32,767
* **int** : -2,147,483,648 to 2,147,483,647
* **bigint** : -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807

## 3.11.2 Other Specific Casting Functions

### 3.11.2.1 What does binary() function do?

**Purpose:** Casts to binary data type (byte array).

### 3.11.2.2 What is boolean() casting function used for?

**Purpose:** Casts to boolean (true/false) type.

### 3.11.2.3 How do you use date() function for type casting?

**Usage:**`date(col("date_string"))` converts string to date.

### 3.11.2.4 What does decimal() function do?

**Purpose:** Casts to decimal type with specified precision and scale.

**Example:**`decimal(10, 2)` for numbers like 12345678.90

### 3.11.2.5 What is the difference between double() and float() casting?

**Precision:**

* **float()** : 4-byte floating point (single precision)
* **double()** : 8-byte floating point (double precision)

### 3.11.2.6 What does string() function do for type conversion?

**Purpose:** Casts any type to string representation.

### 3.11.2.7 How do you use timestamp() function?

**Usage:**`timestamp(col("timestamp_string"))` converts to timestamp.

## 3.11.3 Type Conversion (to_ Functions)

### 3.11.3.1 What is to_char() function? What does it convert from and to?

**Purpose:** Converts numbers/dates to formatted strings (Oracle compatibility).

### 3.11.3.2 What is to_varchar() function used for?

**Purpose:** Similar to to_char(), converts to variable character string.

### 3.11.3.3 What does to_number() function do? When do you use it?

**Purpose:** Converts string to number with format specification.

### 3.11.3.4 What is the difference between to_date() and date() casting?

**Format Handling:**

* **date()** : Uses default format (yyyy-MM-dd)
* **to_date()** : Allows custom format specification

### 3.11.3.5 What is the difference between to_timestamp() and timestamp() casting?

**Format Handling:**

* **timestamp()** : Uses default format
* **to_timestamp()** : Allows custom format specification

### 3.11.3.6 What does to_json() function do? What data types can it convert?

**Purpose:** Converts structs/maps/arrays to JSON string.

### 3.11.3.7 What is to_binary() function used for?

**Purpose:** Converts string to binary using specified charset.

### 3.11.3.8 When would you use to_ functions vs direct casting with cast()?

**Guidelines:**

* **to_ functions** : When you need format specification or specific behavior
* **cast()** : For simple type conversions

## 3.11.4 FLOAT vs DOUBLE vs DECIMAL

### 3.11.4.1 What is the precision difference between FLOAT, DOUBLE, and DECIMAL?

**Precision:**

* **FLOAT** : ~7 decimal digits
* **DOUBLE** : ~15 decimal digits
* **DECIMAL** : Exact precision (user-defined)

### 3.11.4.2 How much storage does each numeric type use (FLOAT, DOUBLE, DECIMAL)?

**Storage:**

* **FLOAT** : 4 bytes
* **DOUBLE** : 8 bytes
* **DECIMAL** : Variable (depends on precision)

### 3.11.4.3 What types of arithmetic do FLOAT and DOUBLE use (approximate vs exact)?

**Arithmetic:**

* **FLOAT/DOUBLE** : Approximate (IEEE 754 floating point)
* **DECIMAL** : Exact (fixed-point arithmetic)

### 3.11.4.4 Do FLOAT and DOUBLE have rounding errors? What about DECIMAL?

**Rounding:**

* **FLOAT/DOUBLE** : Yes, due to binary representation
* **DECIMAL** : No rounding errors for defined precision

### 3.11.4.5 Which numeric type is fastest for computations?

**Performance:** FLOAT/DOUBLE are faster than DECIMAL for mathematical operations.

### 3.11.4.6 When should you use FLOAT or DOUBLE for data processing?

**Use Cases:** Scientific computing, ML algorithms, large-scale analytics where exact precision isn't critical.

### 3.11.4.7 When should you ALWAYS use DECIMAL instead of FLOAT/DOUBLE?

**Always Use DECIMAL for:** Financial calculations, monetary values, exact arithmetic requirements.

### 3.11.4.8 Why is DECIMAL the only choice for financial and monetary data?

**Reason:** Avoids rounding errors that can cause financial discrepancies.

### 3.11.4.9 What is the maximum precision supported by DECIMAL in Spark?

**Maximum:** 38 digits of precision.

### 3.11.4.10 What are the performance trade-offs between DECIMAL and FLOAT/DOUBLE?

**Trade-offs:**

* **DECIMAL** : Slower computation, exact results
* **FLOAT/DOUBLE** : Faster computation, approximate results

### 3.11.4.11 Provide examples where using FLOAT/DOUBLE would cause problems in financial calculations.

**Example:**

**python**

```
# FLOAT/DOUBLE problem
0.1 + 0.2 = 0.30000000000000004  # Not exactly 0.3

# DECIMAL solution  
Decimal("0.1") + Decimal("0.2") = Decimal("0.3")  # Exact
```

## 3.12 Null Handling & Data Cleaning

### 3.12.1 What is the difference between dropna() and fillna()?

**Difference:**

* **dropna()** : Removes rows/columns with nulls
* **fillna()** : Replaces nulls with specified values

### 3.12.2 How do you drop rows with nulls in specific columns using dropna(subset=[])?

**Usage:**`df.dropna(subset=["col1", "col2"])`

### 3.12.3 What are the different threshold options in dropna()?

**Parameters:**

* `how`: "any" or "all"
* `thresh`: Minimum non-null values required
* `subset`: Specific columns to check

### 3.12.4 How do you fill nulls with different values for different columns?

**Usage:**

**python**

```
df.fillna({
    "col1": "default_value",
    "col2": 0,
    "col3": -1
})
```

### 3.12.5 What does na.replace() do?

**Purpose:** Replaces specific values (not necessarily nulls) with new values.

### 3.12.6 What is the critical difference between na.replace() and na.fill()?

**Critical Difference:**

* **na.fill()** : Only replaces null values
* **na.replace()** : Replaces any specified values

### 3.12.7 Is na.replace() used for handling missing data or replacing existing values?

**Both:** Can handle missing data (nulls) and replace existing non-null values.

### 3.12.8 Can you use na.replace() to replace NULL values? Why or why not?

**Yes:**`na.replace()` can replace nulls along with other values.

### 3.12.9 How do you use isNull() and isNotNull() for filtering?

**Usage:**

**python**

```
df.filter(col("age").isNull())
df.filter(col("age").isNotNull())
```

### 3.12.10 What is nanvl() used for (NaN value handling)?

**Purpose:** Returns second value if first is NaN, otherwise returns first value.

### 3.12.11 How do you distinguish between null and NaN in Spark?

**Distinction:**

* **NULL** : Missing/unknown value
* **NaN** : "Not a Number" - specific floating-point value

### 3.12.12 What is the difference between NULL and NaN in terms of meaning and data types?

**Differences:**

* **NULL** : Can occur in any data type, represents absence of value
* **NaN** : Only in floating-point types, represents invalid mathematical operation

### 3.12.13 How do NULL and NaN behave differently in comparisons?

**Behavior:**

* **NULL** : Any comparison returns NULL (unknown)
* **NaN** : NaN = NaN returns false, need isnan() to detect

### 3.12.14 What does dropDuplicates() do? How do you specify subset of columns?

**Usage:** Removes duplicate rows.

**python**

```
df.dropDuplicates()  # All columns
df.dropDuplicates(["col1", "col2"])  # Specific columns
```

### 3.12.15 Does dropDuplicates() preserve the order of rows?

**No:** Order is not guaranteed to be preserved.

## 3.12.1 COALESCE, NVL, and NVL2 Functions

### 3.12.1.1 What does COALESCE() function do?

**Purpose:** Returns first non-null value from the argument list.

### 3.12.1.2 How many arguments can COALESCE() accept?

**Multiple:** Can accept any number of arguments.

### 3.12.1.3 What does NVL() function do? How is it different from COALESCE()?

**NVL:** Two-argument version of COALESCE (Oracle compatibility).

### 3.12.1.4 How many arguments does NVL() accept?

**Exactly two:**`NVL(expr1, expr2)`

### 3.12.1.5 What does NVL2() function do?

**Purpose:** Returns expr2 if expr1 is not null, else expr3.

### 3.12.1.6 What are the three arguments in NVL2() and what do they represent?

**Arguments:**`NVL2(expr1, expr2, expr3)`

* expr1: Value to check
* expr2: Return if expr1 not null
* expr3: Return if expr1 null

### 3.12.1.7 Compare COALESCE() vs NVL() vs NVL2() - when to use each?

**Usage:**

* **COALESCE** : Multiple fallback options
* **NVL** : Simple two-value fallback (Oracle environments)
* **NVL2** : Conditional return based on null check

### 3.12.1.8 Is NVL() a SQL standard function or Oracle compatibility function?

**Answer:** Oracle compatibility function, not SQL standard.

### 3.12.1.9 Can you use COALESCE() with more than 2 arguments? Provide an example.

**Yes:**`COALESCE(col1, col2, col3, "default")`

### 3.12.1.10 What does NVL2(NULL, 'Y', 'N') return?

**Returns:** 'N' (since first argument is null)

### 3.12.1.11 What does NVL(NULL, 'X') return?

**Returns:** 'X' (fallback value)

### 3.12.1.12 How would you replicate NVL2() behavior using CASE WHEN?

**Equivalent:**

**sql**

```
CASE WHEN expr1 IS NOT NULL THEN expr2 ELSE expr3 END
```

## 3.13 Column Expressions & SQL Functions

### 3.13.1 What is the difference between using column names as strings vs Column objects (col(), F.col())?

**Difference:**

* **String names** : Simple but limited to current DataFrame columns
* **Column objects** : Enable complex expressions, referencing joined tables

### 3.13.2 When must you use col() or F.col() instead of string column names?

**When:**

* Creating complex expressions
* Referencing columns after joins
* Using functions in expressions

### 3.13.3 What does expr() function allow you to do?

**Purpose:** Evaluate SQL expressions as strings.

**Example:**`expr("age * 2 + 1")`

### 3.13.4 How do you reference columns from different DataFrames after a join?

**Method:** Use DataFrame-specific column references:

**python**

```
df1.join(df2).select(df1["col"], df2["col"])
```

### 3.13.5 What is the alias() method used for?

**Purpose:** Renames a column in the result.

### 3.13.6 What does name() method return for a Column object?

**Returns:** The column name as string.

## 3.14 Conditional Logic & Case Statements

### 3.14.1 How do you create complex conditional logic using when().when().otherwise()?

**Usage:**

**python**

```
from pyspark.sql.functions import when

df.withColumn("category",
    when(col("age") < 18, "child")
    .when(col("age") < 65, "adult")
    .otherwise("senior"))
```

### 3.14.2 What happens if you don't provide an otherwise() clause?

**Result:** Returns null for rows that don't match any condition.

### 3.14.3 How do you implement SQL CASE WHEN logic in PySpark?

**Methods:**

* Using `when().otherwise()` chain
* Using `expr()` with SQL CASE statement

### 3.14.4 Can you nest when() conditions? Provide an example?

**Yes:** You can nest conditions within when() clauses.

**Example:**

**python**

```
df.withColumn("complex_category",
    when(col("age") < 18, 
         when(col("grade") > 90, "gifted_child")
         .otherwise("regular_child"))
    .when(col("age") >= 65, "senior")
    .otherwise("adult"))\
```
