# Section 14: Monitoring & Troubleshooting

## 14.1 Spark UI

**1780. How do you explore and navigate the Spark UI for performance analysis?**

The Spark UI is your performance investigation headquarters. Access it at `http://<driver-node>:4040` while your application runs.

**Key tabs to navigate:**

* **Jobs** - See which jobs ran, their duration, stages breakdown
* **Stages** - Dive into individual stages, see task distribution, identify stragglers
* **Storage** - Check cached DataFrames, memory usage, spill to disk
* **Environment** - Review all Spark configurations applied
* **Executors** - Monitor resource usage per executor, GC time, shuffle metrics
* **SQL** - See DataFrame/SQL query plans, execution times, data scanned

**Pro tip:** Start with the Jobs tab to find slow jobs, then drill into Stages to see which specific stage is problematic, then examine individual tasks to spot skew or bottlenecks.

**1781. Is Spark UI only available during active Spark sessions?**

By default,  **yes** —the UI disappears when your application ends. But you can preserve it using the  **Spark History Server** .

Enable history server:

```python
spark.conf.set("spark.eventLog.enabled", "true")
spark.conf.set("spark.eventLog.dir", "hdfs:///spark-logs")
```

Then start the history server:

```bash
$SPARK_HOME/sbin/start-history-server.sh
```

Now you can browse completed applications at `http://<history-server>:18080`. Essential for post-mortem analysis—investigate what went wrong after failures without having to reproduce the issue.

**1782. How can you preserve execution history and logs using log4j?**

Log4j configuration lets you control what gets logged and where. The key file is `log4j.properties` in your Spark conf directory.

**Common configurations:**

```properties
# Set root logger level (INFO is default, DEBUG for troubleshooting)
log4j.rootCategory=INFO, console

# Direct logs to file for persistence
log4j.appender.file=org.apache.log4j.RollingFileAppender
log4j.appender.file.File=/var/log/spark/spark.log
log4j.appender.file.MaxFileSize=10MB
log4j.appender.file.MaxBackupIndex=5

# Quiet down noisy libraries
log4j.logger.org.apache.spark.repl.Main=WARN
log4j.logger.org.apache.hadoop=WARN
```

This preserves logs to files that survive beyond application lifetime. Crucial for debugging issues that only appear in production, especially intermittent failures.

---

## 14.2 Metrics & Accumulators

**1783. What are accumulators in Spark?**

Accumulators are **write-only variables** that allow executors to safely add values that only the driver can read. Think of them as distributed counters—perfect for tracking metrics across your cluster without breaking Spark's functional programming model.

Key property:  **add-only from workers, read-only from driver** . This one-way communication avoids the complexity and performance hit of two-way communication.

**1784. How are accumulators used for distributed counting and metrics collection?**

Classic use case: counting records that meet certain conditions without collecting all data to driver.

```python
# Create accumulator
error_count = spark.sparkContext.longAccumulator("Error Records")

def process_with_tracking(row):
    if row['status'] == 'error':
        error_count.add(1)  # Each executor increments
    return row

df.foreach(process_with_tracking)

# Only driver can read the total
print(f"Total errors: {error_count.value}")
```

**Common uses:**

* Count malformed records during ETL
* Track how many records hit each branch in conditional logic
* Monitor cache hits vs misses
* Debug data quality issues (nulls, outliers, duplicates)

**Critical gotcha:** Accumulators in transformations can double-count if Spark retries failed tasks. Use them in actions for accurate counts.

**1785. What metrics indicate performance bottlenecks (skew, spilling, GC time)?**

Watch these red flags in Spark UI:

**Data Skew Indicators:**

* **Task Duration Variance** - Some tasks take 10x longer than median
* **Data Read Variance** - One task reads 1GB while others read 100MB
* **Stragglers** - 95% of tasks finish fast, then you wait for a few slow ones
* Location: Stages tab → Task Metrics → look for outliers in duration/input size

**Memory Pressure Indicators:**

* **Spill to Disk** - "Shuffle Spill (Memory)" > 0 in Stage metrics
* **Storage Memory Usage** - Consistently near 100% in Executors tab
* **GC Time > 10%** - "GC Time" column in Executors tab shows high percentage
* **OOM Errors** - Check executor logs for "java.lang.OutOfMemoryError"

**Shuffle Bottlenecks:**

* **Shuffle Read/Write Size** - Terabytes shuffled = slow job guaranteed
* **Shuffle Read Blocked Time** - Tasks waiting for shuffle data to arrive
* Location: Stages tab → "Shuffle Read" and "Shuffle Write" columns

**Rule of thumb:** If GC time > 10% of execution time, you have memory issues. If max task time > 3x median, you have skew. If shuffle size is TBs, you need to reduce data movement.

---

## 14.3 Common Errors & Debugging

**1786. What is the "out of memory" error and common causes in Spark?**

The dreaded OOM error means an executor ran out of heap memory. There are different flavors:

**Driver OOM:**

* **Cause:** Collecting too much data to driver (`collect()`, `toPandas()` on huge DataFrame)
* **Symptom:** "java.lang.OutOfMemoryError: Java heap space" on driver
* **Fix:** Don't collect large datasets. Use `take(n)` or aggregate before collecting. Increase driver memory with `spark.driver.memory`.

**Executor OOM:**

* **Cause:** Processing partitions too large for executor memory
* **Symptom:** Task failures with OOM in executor logs
* **Fix:** Increase partitions (`repartition()` to split data), increase executor memory (`spark.executor.memory`), or reduce memory-intensive operations (fewer cached DataFrames).

**Broadcast OOM:**

* **Cause:** Broadcasting table too large for executor memory
* **Symptom:** OOM during broadcast join
* **Fix:** Disable broadcast for that join (`spark.sql.autoBroadcastJoinThreshold = -1`), or reduce broadcast table size with filtering/aggregation.

**Common root causes:**

1. **Data skew** - One partition has 100x more data
2. **Too few partitions** - Each partition is huge
3. **Memory leak** - Caching too many DataFrames
4. **Collection to driver** - Trying to collect 1TB to driver with 4GB heap

**1787. Why do you get "Task not serializable" errors? How do you fix them?**

This error means Spark tried to send your code to executors but couldn't serialize (convert to bytes) some object in your closure.

**Common culprits:**

**1. Non-serializable objects in closures:**

```python
# BAD - db_connection isn't serializable
db_connection = create_connection()
df.map(lambda x: db_connection.query(x))  # ERROR!

# GOOD - create connection inside the function
def query_with_connection(x):
    db_connection = create_connection()  # Created on executor
    return db_connection.query(x)

df.map(query_with_connection)
```

**2. Referencing self/class in non-serializable way:**

```python
# BAD - captures entire class instance
class MyProcessor:
    def process(self, df):
        return df.map(lambda x: self.helper(x))  # ERROR - self not serializable

# GOOD - make method static or extract value
class MyProcessor:
    def process(self, df):
        helper_func = self.helper  # Extract method reference
        return df.map(lambda x: helper_func(x))
```

**3. SparkSession/SparkContext in closure:**

```python
# BAD - trying to send spark to executors
df.foreach(lambda x: spark.createDataFrame([x]))  # ERROR!

# GOOD - spark operations stay on driver
results = df.collect()
spark.createDataFrame(results)
```

**Quick fix:** If you see this error, look at your lambda/function—anything it references must be serializable. Extract only what you need before the closure.

**1788. What causes "Stage X has Y failed attempts" and how do you debug it?**

This means a stage failed repeatedly—Spark tried multiple times but kept hitting problems.

**Common causes:**

**1. Executor failures (OOM, node crashes):**

* Check executor logs in Spark UI → Executors tab
* Look for OOM errors or "lost executor" messages
* Fix: Increase executor memory, reduce partition size

**2. Data skew causing timeouts:**

* One task processes 100x more data, times out
* Spark retries → same task fails again
* Fix: Repartition on skewed key with salting

**3. Network issues (shuffle fetch failures):**

* Tasks can't fetch shuffle data from dead executors
* Errors like "shuffle fetch failed" in logs
* Fix: Increase `spark.shuffle.io.retryWait`, reduce shuffle size

**4. Bad data causing exceptions:**

* Some records cause code to crash (division by zero, null pointer)
* Same bad record causes failure every retry
* Fix: Add defensive null checks, use `try/except` in UDFs

**Debugging strategy:**

1. Go to Stages tab → find failed stage
2. Click stage → scroll to failed tasks
3. Look at error messages in task logs
4. Check if same task ID fails repeatedly (data issue) or different tasks fail (resource issue)

**1789. What is data skew and what are the symptoms in Spark UI?**

Data skew means your data is unevenly distributed across partitions—some partitions have way more data than others.

**Visual symptoms in Spark UI:**

**Stages tab:**

* **Task Duration** - Min: 5 seconds, Median: 10 seconds, Max: 5 minutes (huge variance)
* **Input Size** - Most tasks read 100MB, one task reads 10GB
* **Timeline view** - Most tasks finish quickly, then you wait forever for 1-2 stragglers

**SQL tab (if using DataFrames):**

* Look at "Exchange" (shuffle) nodes
* Hover over them to see partition size distribution
* One or few partitions significantly larger

**Executors tab:**

* One executor has 10x more "Shuffle Write" than others
* That executor's tasks take 10x longer

**Real-world example:**
Processing user events where 1% of users generate 80% of events. When you group by user_id, those "power users" all land in few partitions → massive skew.

**How to spot:** If your job has 1000 tasks, 999 finish in 1 minute, and you wait 30 more minutes for the last task—that's skew.

**1790. How do you identify and fix shuffle spill to disk issues?**

**Identification:**

**In Spark UI Stages tab, look for:**

* "Shuffle Spill (Memory)" > 0 - Data didn't fit in memory during shuffle
* "Shuffle Spill (Disk)" > 0 - Data was written to disk temporarily
* High "Shuffle Read Blocked Time" - Tasks waiting for data to be read from disk

**Bad signs:**

* Spill size approaching input size = most data spilled
* Many GBs spilled per task = serious memory pressure

**Root causes:**

1. **Too much data per partition** during shuffle operations (join, groupBy, repartition)
2. **Too little memory** allocated per executor
3. **Too many cached DataFrames** competing for memory

**Fixes:**

**Increase partitions to reduce per-partition data:**

```python
# Before: 200 partitions (default), each has 5GB
df.repartition(1000)  # Now each has 1GB

# Or let Spark handle it
spark.conf.set("spark.sql.shuffle.partitions", "1000")
```

**Increase executor memory:**

```python
spark.conf.set("spark.executor.memory", "8g")  # More heap space
spark.conf.set("spark.memory.fraction", "0.8")  # More for execution/storage
```

**Reduce shuffle size:**

* Filter before shuffle: `df.filter(...).groupBy(...)` not `df.groupBy(...).filter(...)`
* Aggregate earlier: pre-aggregate before join
* Broadcast small tables: avoid shuffle entirely for small dimension tables

**Uncache unused DataFrames:**

```python
df.unpersist()  # Free up memory
```

**1791. What are best practices for naming columns to avoid conflicts?**

**Use descriptive, collision-free names:**

**Avoid these common pitfalls:**

**1. Same column name in both join tables:**

```python
# BAD - both have 'id' and 'name'
left.join(right, "id").select("id", "name")  # Which name?

# GOOD - prefix with table context
left.select(
    col("id").alias("user_id"),
    col("name").alias("user_name")
).join(right.select(
    col("id").alias("order_id"),
    col("name").alias("product_name")
), left.user_id == right.order_id)
```

**2. Special characters causing parsing issues:**

```python
# Problematic names
"order.date"      # Dot looks like nested field
"user-name"       # Dash can cause issues
"count(*)"        # Parentheses/asterisk are special

# Safe names
"order_date"
"user_name"
"total_count"
```

**3. Reserved keywords:**
Avoid SQL reserved words as column names: `select`, `from`, `where`, `join`, `group`, `order`, etc.

**Best practices:**

* Use **snake_case** consistently: `order_date`, `customer_id`
* **Prefix** columns after joins: `src_id`, `tgt_id` or `left_id`, `right_id`
* **Be specific** : Not `id`, but `customer_id`, `order_id`, `product_id`
* **Avoid abbreviations** that could collide: `cust` vs `customer`, `ord` vs `order`

**1792. How do you handle special characters in column names?**

If you're stuck with column names containing spaces, dots, or special characters (often from source systems), use backticks:

```python
# Column name with spaces/special chars
df.select("`order date`", "`customer.name`", "`amount($)`")

# In SQL
spark.sql("SELECT `order date`, `customer.name` FROM table")

# Better: rename them immediately
df = df.withColumnRenamed("order date", "order_date") \
       .withColumnRenamed("customer.name", "customer_name") \
       .withColumnRenamed("amount($)", "amount_usd")
```

**When to use backticks:**

* Reading from external sources with bad naming (CSV headers, database tables)
* One-off operations before cleanup

**When to rename:**

* Any data you'll work with repeatedly
* Before any joins or complex transformations
* At ingestion time as part of standardization

**Pro tip:** Make column renaming part of your data ingestion pipeline—clean names from the start saves headaches later.

**1793. What is the impact of data types on performance (e.g., StringType vs IntegerType)?**

Data types dramatically affect performance—wrong types waste memory and slow processing.

**Storage size comparison:**

* **IntegerType** : 4 bytes per value
* **LongType** : 8 bytes per value
* **StringType** : Variable (typically 10-50+ bytes for IDs stored as strings)
* **Example:** 1 billion user IDs: 4GB as integers vs ~20GB as strings

**Processing speed:**

* **Numeric comparisons** : Extremely fast (single CPU instruction)
* **String comparisons** : Much slower (byte-by-byte comparison)
* **Example:** Filtering 100M rows on integer column: seconds. Same on string column: minutes.

**Common performance killers:**

**1. Storing IDs as strings:**

```python
# BAD - user_id as string "123456"
df_slow = df.withColumn("user_id", col("id").cast("string"))

# GOOD - user_id as long
df_fast = df.withColumn("user_id", col("id").cast("long"))
```

Joins and groupBys on integer IDs are **5-10x faster** than on string IDs.

**2. Using strings for boolean flags:**

```python
# BAD - "true"/"false" strings
df.withColumn("is_active", lit("true"))  # 4-8 bytes per value

# GOOD - boolean type
df.withColumn("is_active", lit(True))  # 1 byte per value
```

**3. Not using appropriate numeric types:**

```python
# BAD - everything as double
df.withColumn("age", col("age").cast("double"))  # 8 bytes

# GOOD - use smallest type that fits
df.withColumn("age", col("age").cast("byte"))  # 1 byte (sufficient for ages)
```

**When to use strings:** Only for actual text data (names, descriptions, addresses) or categorical data with hundreds+ of distinct values where mapping to integers isn't worth it.

**Rule of thumb:** If it's a number, store it as a number. If it's a flag, use boolean. Reserve strings for actual text.

**1794. Why should you avoid using `count()` multiple times on the same DataFrame?**

Every `count()` triggers a full scan of the data—it's an **action** that reads the entire DataFrame.

**The problem:**

```python
# BAD - three full scans!
total = df.count()
nulls = df.filter(col("age").isNull()).count()
valid = df.filter(col("age").isNotNull()).count()
```

Each `count()` re-reads all data from source, re-applies all transformations. If your DataFrame represents 1TB of data, you just scanned 3TB.

**The fix - single pass with aggregation:**

```python
# GOOD - one scan!
stats = df.agg(
    count("*").alias("total"),
    count(when(col("age").isNull(), 1)).alias("nulls"),
    count(when(col("age").isNotNull(), 1)).alias("valid")
).collect()[0]

total = stats["total"]
nulls = stats["nulls"]
valid = stats["valid"]
```

**Or use cache if you must count multiple times:**

```python
df.cache()  # Materialize once
total = df.count()
nulls = df.filter(col("age").isNull()).count()
valid = df.filter(col("age").isNotNull()).count()
df.unpersist()  # Clean up
```

But caching has memory cost—single-pass aggregation is better.

**1795. What happens when you mix transformation logic with actions improperly?**

Mixing transformations and actions carelessly causes redundant computation and confusing code flow.

**Problem 1: Actions inside transformations**

```python
# BAD - count() action called once per partition!
def process(partition):
    records = list(partition)
    total = df.count()  # This triggers full job... every partition!
    return records

df.mapPartitions(process)  # Disaster
```

**Problem 2: Multiple actions on non-cached DataFrame**

```python
# BAD - re-computes expensive transformation twice
expensive_df = df.join(large_table, "id").groupBy("category").agg(...)
count = expensive_df.count()        # Computes join + groupBy
results = expensive_df.collect()    # Computes join + groupBy AGAIN
```

**Problem 3: Unnecessary early materialization**

```python
# BAD - premature collection
intermediate = df.filter(...).collect()  # Brings to driver
final_df = spark.createDataFrame(intermediate).groupBy(...)  # Back to executors

# GOOD - keep transformations lazy
final_df = df.filter(...).groupBy(...)  # All distributed
```

**Best practices:**

* **Keep transformations pure** - no actions inside map/filter/etc
* **Cache before multiple actions** - if you'll trigger multiple actions on same DataFrame
* **Combine actions when possible** - use single `agg()` instead of multiple `count()`/`sum()`/etc
* **Minimize driver-executor round trips** - avoid collect → createDataFrame patterns

---

## 14.4 Data Sampling & Debugging

**1796. How do you use `sample()` for testing on subset of data?**

`sample()` is your friend for development and testing—work with 1% of data while building logic, then run on 100% in production.

```python
# Take 10% random sample
sample_df = df.sample(fraction=0.1, seed=42)

# Work with sample during development
sample_df.filter(...).groupBy(...).agg(...)  # Fast iteration
```

**Key use cases:**

* **Development** - Test transformations on small subset before running on TBs
* **Debugging** - Reproduce issues with manageable data size
* **Data profiling** - Quick statistics on sample instead of full scan
* **Model training** - Sample for faster training iterations

**Critical:** Use a **seed** for reproducibility—same seed gives same sample every time. Essential for debugging and testing.

**1797. What does `sample(withReplacement, fraction, seed)` mean?**

Three parameters control sampling behavior:

**1. withReplacement (boolean):**

* `False` (default): Each record appears at most once—simple random sample
* `True`: Records can appear multiple times—for bootstrap sampling

**2. fraction (float 0.0-1.0):**

* Expected proportion of data to sample
* `0.1` = ~10% of rows, `0.5` = ~50% of rows
* Not exact count—statistical sampling, might get 9.8% or 10.3%

**3. seed (integer):**

* Random seed for reproducibility
* Same seed = same sample every run
* Different seed = different random sample

```python
df.sample(False, 0.1, seed=42)  # Always same 10%
df.sample(False, 0.1, seed=99)  # Different 10%
```

**1798. What is stratified sampling using `sampleBy()`?**

`sampleBy()` samples different fractions from different groups—ensures representation of rare categories.

**Why it matters:**
Simple random sampling might miss rare categories. If you have 99% "regular" customers and 1% "VIP", a 10% sample might have zero VIPs.

**Stratified sampling ensures balance:**

```python
# Sample different fractions per customer type
fractions = {
    "regular": 0.05,  # 5% of regular customers
    "premium": 0.20,  # 20% of premium customers
    "vip": 1.0        # 100% of VIPs (they're rare)
}

stratified = df.sampleBy("customer_type", fractions, seed=42)
```

**Use cases:**

* **Imbalanced data** - Ensure minority classes represented
* **A/B testing** - Different sample rates for control vs treatment
* **Cost optimization** - Heavy sampling of cheap queries, light sampling of expensive ones
* **Quality checks** - 100% of suspicious records, 1% of normal records

**1799. How do you use `limit()` for quick data inspection?**

`limit(n)` returns first n rows—fastest way to peek at data structure without processing everything.

```python
# Quick peek at first 10 rows
df.limit(10).show()

# Check schema and few sample values
df.limit(5).printSchema()
df.limit(5).show(truncate=False)
```

**When to use limit vs sample:**

* **limit()** : Fast, deterministic, first N rows—good for schema inspection
* **sample()** : Random selection, representative of full data—good for profiling

**Critical gotcha:** `limit()` gives you whatever rows Spark processes first—often from first partition only. Not representative of full dataset.

**Use limit for:**

* Schema inspection: `df.limit(1).printSchema()`
* Quick existence check: `df.limit(1).count() > 0`
* Fast failure during development
* Testing transformations: `df.filter(...).limit(100).show()`

**1800. What does `show(n, truncate)` do? What are the parameters?**

`show()` displays DataFrame contents in tabular format—your primary data inspection tool.

**Parameters:**

**1. n (default=20):** Number of rows to display

```python
df.show()         # First 20 rows
df.show(5)        # First 5 rows
df.show(100)      # First 100 rows
```

**2. truncate (default=True):**

* `True` or number: Truncate strings to 20 characters (or specified width)
* `False`: Show full strings without truncation

```python
# Truncated (default)
df.show()
# +----+--------------------+
# | id |                name|
# +----+--------------------+
# |  1 |    This is a ver...|  # Truncated!

# Full content
df.show(truncate=False)
# +----+------------------------------------------+
# | id |                                      name|
# +----+------------------------------------------+
# |  1 |This is a very long string that shows ...|

# Custom truncation width
df.show(10, truncate=50)  # Show 10 rows, truncate at 50 chars
```

**3. vertical (optional):** Show records vertically for detailed inspection

```python
df.show(n=1, vertical=True)
# -RECORD 0------------------
#  id             | 1
#  name           | John
#  email          | john@example.com
```

**1801. How do you use `printSchema()` for debugging?**

`printSchema()` displays DataFrame structure—column names, data types, nullability. Essential first step when debugging data issues.

```python
df.printSchema()

# Output:
# root
#  |-- id: long (nullable = true)
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)
#  |-- metadata: struct (nullable = true)
#  |    |-- source: string (nullable = true)
```

**What it reveals:**

**1. Data types:** Catch type mismatches early

```python
# Debugging joins failing silently
left_df.printSchema()   # id: string
right_df.printSchema()  # id: long
# Aha! Need to cast for join to work
```

**2. Nullability:** Understand which columns can be null

**3. Nested structures:** See complex types (structs, arrays)

```python
# How do I access nested fields?
df.printSchema()
# |-- address: struct
# |    |-- city: string
# Access: df.select("address.city")
```

**Debugging workflow:**

1. Check schema immediately after reading
2. Verify schema after transformations
3. Compare schemas before joins

**1802. What does `explain()` show? How do you read the physical plan?**

`explain()` shows Spark's execution plan—how it will actually execute your query. Essential for performance debugging.

```python
df.explain()
```

**Output shows plans:**

1. **Parsed Logical Plan** - Your code, as Spark understands it
2. **Analyzed Logical Plan** - After resolving column names, types
3. **Optimized Logical Plan** - After Catalyst optimizer improvements
4. **Physical Plan** - Actual execution strategy

**How to read the physical plan (bottom-up):**

```python
result = df.filter(col("age") > 25).select("name", "age")
result.explain()

# == Physical Plan ==
# *(1) Project [name#1, age#2]              ← 3. Finally, project columns
# +- *(1) Filter (age#2 > 25)               ← 2. Then filter
#    +- FileScan parquet [name#1, age#2]    ← 1. Start: scan file
```

**Read from bottom to top** (data flows upward).

**Key things to look for:**

**1. Exchange (shuffle) operations:**

```
Exchange hashpartitioning(id#1, 200)  ← SHUFFLE! Expensive!
```

Too many shuffles = slow job.

**2. Broadcast joins:**

```
BroadcastHashJoin [id#1], [id#2]  ← Good! Small table broadcasted
```

vs regular shuffle join:

```
SortMergeJoin [id#1], [id#2]  ← Requires shuffle
```

**3. Partition pruning:**

```
PartitionFilters: [year#10 = 2024]  ← Good! Skipping partitions
PushedFilters: [age > 25]            ← Good! Filter pushed to scan
```

**4. Whole stage code generation (*):**

```
*(1) Project ...  ← Asterisk means optimized code generation
```

**1803. What does `explain(extended=True)` reveal?**

`explain(True)` or `explain(extended=True)` shows **all optimization stages** in detail—the full journey from your code to execution plan.

```python
df.explain(extended=True)
```

**Shows five plans:**

**1. Parsed Logical Plan** - Raw translation of your DataFrame code

```
'Project ['name, 'age]
+- 'Filter ('age > 25)
   +- 'UnresolvedRelation [users]
```

Your code, literally translated. Still has unresolved references (the quotes).

**2. Analyzed Logical Plan** - After Spark resolves all references

```
Project [name#1, age#2]
+- Filter (age#2 > 25)
   +- Relation[name#1, age#2] parquet
```

All columns resolved to actual field IDs. Types validated.

**3. Optimized Logical Plan** - After Catalyst optimizer's transformations

```
Project [name#1, age#2]
+- Filter (age#2 > 25)
   +- FileScan parquet [name#1, age#2] 
      PushedFilters: [age > 25]
```

Optimizations applied: filter pushdown, column pruning, predicate pushdown.

**4. Physical Plan** - Actual execution strategy with concrete operators

**When to use extended explain:**

* Debugging why filter isn't being pushed down
* Understanding what optimizations Catalyst applied
* Verifying partition pruning is working
* Learning what Spark does automatically vs what you need to help with

**Pro tip:** Usually regular `explain()` is enough. Use `extended=True` when you need to debug **why** Spark made certain optimization decisions or **why** an expected optimization didn't happen.

---

## 14.5 Performance Validation

### Verification & Monitoring

**1804. How do you verify partitionBy is working correctly in Spark UI?**

Look for these telltale signs in Spark UI during and after the write:

**In the Stages tab during write:**

* **Task count matches partition combinations** - Writing 2 years × 12 months should show ~24 tasks, not just 1-2
* **"Partition" metrics** visible in task details showing actual partition values being written
* **Even distribution** of "Bytes Written" across tasks

**After the write, verify the physical output:**
Check if you actually got separate directories. Read the data back and look at `input_file_name()` - you should see paths like `/data/year=2024/month=01/part-00000.parquet`, not a flat structure.

**The real test - partition pruning:**
Query with a filter on your partition column and run `explain(True)`. Look for `PartitionFilters: [year = 2024]` in the optimized plan. Then check Spark UI - "number of files read" should be a fraction of total files.

**Red flags that partitioning failed:**

* Only 1-2 write tasks (should be many)
* Flat directory structure after write
* Filtered queries still scan all files

**1805. What metrics indicate successful bucketing in Spark UI?**

Bucketing has two verification points - write time and read time.

**During the write:**

Must see these in Spark UI:

* **Exchange operation with hash partitioning** on your bucket column
* **Exactly N output files** where N = your bucket count (100 buckets = exactly 100 files)
* **Optional Sort stage** if you used `sortBy()`

**The payoff - during joins (read time):**

This is where you see if bucketing actually worked. Join two bucketed tables on the bucket key:

**What you want to see:**

* **NO Exchange operation before the join** - no shuffle! This is the whole point
* **SortMergeJoin** with task count matching bucket count
* **Dramatically lower "Shuffle Read/Write"** metrics (should be zero)

**How to verify metadata:**

```python
spark.sql("DESCRIBE EXTENDED bucketed_table").show()
# Look for: Num Buckets: 100, Bucket Columns: [user_id]
```

**1806. How do you confirm sortBy ordering in written data files?**

The simple test: read the data back without any `orderBy()` and check if it's already sorted.

**Quick visual check:**

```python
spark.read.parquet("/sorted_output").show(50)
# Eyeball the sort column - should be in order
```

**Programmatic verification:**
Compare each row's sort column to the previous row. Use window functions with `lag()` to check violations across the dataset.

**Important distinction:**

* `sortBy()` sorts  **within each output file** , not globally across files
* File 1 might be [1, 2, 3], File 2 might be [50, 51, 52] - each sorted internally
* This is different from global `orderBy()` which is much more expensive

**Check the physical plan during write:**
Look for a Sort operator in Spark UI. You should see "Sort Time" in task metrics if sorting actually happened.

**1807. How do you verify write mode 'ignore' actually skipped the write operation?**

The key insight: if `mode("ignore")` works, there should be  **zero write activity** .

**Spark UI evidence:**

* Job completes in milliseconds (just checking if path exists)
* No write stage at all, or write stage shows 0 tasks
* "Bytes Written" metric = 0

**File system checks:**

* File modification timestamps unchanged on the directory
* File count identical before and after
* Record count when reading the data is unchanged

If any of these change, the write didn't actually ignore - something was written.

**1808. What Spark UI metrics show V2 writer is being used?**

V2 writer (DataSource V2) has a different execution pattern than V1.

**Job naming clues:**

* V1: Simple "save at `<location>`"
* V2: "Write to DataSourceV2" or "Execute CreateTableAsSelectExec"

**Stage structure differences:**

* V2 shows **additional coordination stages** - commit coordination, transaction logging
* Look for "Job commit" as a separate stage after the main write
* With Delta Lake specifically: "TransactionalWrite", "OptimisticTransaction" in job descriptions

**Why V2 matters:**
It's required for transactional table formats (Delta, Iceberg). You'll see extra stages handling ACID guarantees - that's the transaction log being updated and committed.

**Comparison:**

* V1 (plain Parquet): 1-2 stages, straightforward write
* V2 (Delta/Iceberg): 3-5 stages including transaction coordination

**1809. How do you measure the performance improvement from bucketing?**

Run the same join twice - once with bucketed tables, once without.

**The baseline (no bucketing):**

```python
# Regular tables
result = users_normal.join(orders_normal, "user_id").count()
# Time this. Note the shuffle size in Spark UI (could be hundreds of GBs)
```

**The optimized version (bucketed):**

```python
# Bucketed tables  
result = users_bucketed.join(orders_bucketed, "user_id").count()
# Time this. Shuffle size should be ZERO
```

**What to compare:**

* **Execution time** - bucketed should be 2-5x faster for large joins
* **Shuffle metrics** - baseline has massive shuffle read/write, bucketed has zero
* **Network I/O** - eliminated entirely with bucketing

**The trade-off analysis:**
Bucketing adds overhead during the initial write. Calculate break-even: if your write takes 10 minutes extra but each of your 100 daily queries saves 5 minutes, you break even on day 1.

**When bucketing wins:**

* Large tables joined repeatedly (fact-dimension joins)
* Join key has decent cardinality (not too skewed)
* Read-heavy workloads where you query more than you write

**1810. How do you validate that Dynamic Partition Overwrite worked correctly?**

The whole point: overwrite **only** the partitions you're writing to, leave others untouched.

**The verification:**
Count records per partition before and after:

```python
# Before: Year 2022: 1000, 2023: 1000, 2024: 1000
# Overwrite only 2024 data with dynamic mode
# After: Year 2022: 1000 (unchanged ✓), 2023: 1000 (unchanged ✓), 2024: 500 (changed ✓)
```

**File-level proof:**
Check file modification timestamps. The 2022 and 2023 partition directories should have old timestamps (untouched), while 2024 should be fresh.

**What would happen without dynamic mode:**
Regular `mode("overwrite")` would nuke ALL partitions, then write only 2024. You'd lose your 2022 and 2023 data.

**The configuration:**

```python
.option("partitionOverwriteMode", "dynamic")
```

**1811. What logs confirm all executors accessed distributed storage successfully?**

You're looking for success indicators across the cluster, not just from the driver.

**In Spark UI → Executors tab:**

* Click into individual executor logs (stdout/stderr links)
* Look for messages like "Successfully opened file s3://...", "Found block locally"
* Should see these across **all** executors, not just one

**Common success patterns in logs:**

```
INFO FileSourceStrategy: Successfully opened file
INFO ParquetFileReader: Successfully read footer
INFO BlockManager: Found block rdd_1_0 locally
```

**Failure patterns to watch for:**

```
ERROR S3NativeFileSystem: AccessDeniedException
WARN TaskSetManager: Lost task - Connection timeout
ERROR CredentialProvider: Unable to load credentials
```

**Quick programmatic check:**
Use `spark_partition_id()` and `input_file_name()` to see which executors processed which files. If you have good distribution across executor IDs, storage access is working cluster-wide.

**The real test:** If your job completes successfully with data distributed across partitions, executors are accessing storage. Storage access failures typically cause immediate task failures.

### Performance Testing

**1812. How do you benchmark partitionBy vs no partitioning for your workload?**

Create two identical datasets - one partitioned, one not. Run your typical queries against both.

**What to measure:**

**Write performance:**
Partitioned writes are slower (Spark needs to organize data into separate directories). Measure the overhead - typically 10-30% slower writes.

**Full scan reads:**
Should be roughly equivalent. Partitioning doesn't help if you're reading everything anyway.

**Filtered reads (the payoff):**
Query with a filter on the partition column. This is where partitioning shines:

* Non-partitioned: scans entire dataset
* Partitioned: **only scans relevant partition directories** (partition pruning)

**The typical result:** 5-10x speedup on filtered queries, but at the cost of slower writes and more files to manage.

**The decision:** If your workload is 80% filtered queries (reading specific date ranges, specific categories), partitioning wins. If you mostly do full scans, skip it.

**1813. How do you measure the write performance cost of sortBy?**

Time four scenarios to understand the overhead:

1. **Baseline:** Write with no sorting
2. **Single column sort:** Write with `sortBy("id")`
3. **Multi-column sort:** Write with `sortBy("category", "id")`
4. **Partition + sort:** Write with `partitionBy(...).sortBy(...)`

**Typical overhead:** Sorting adds 30-70% to write time because Spark must shuffle and sort all data before writing.

**Check Spark UI for:**

* Exchange + Sort stages (the expensive part)
* Spill metrics - if sort is spilling to disk, it's really hurting performance
* Task duration - should be more uniform with sort

**The trade-off question:** Does your read workload benefit from pre-sorted data? If you frequently query with ORDER BY, pre-sorting can save time on every read. Otherwise, it's wasted overhead.

**1814. How do you test if bucketing improves join performance for your data?**

The definitive test: same join, two ways.

**Setup:**

* Create regular tables: `users_normal`, `orders_normal`
* Create bucketed tables: `users_bucketed(100, "user_id")`, `orders_bucketed(100, "user_id")`
* Run identical join: `users JOIN orders ON user_id`

**Expected results for well-suited data:**

* **Regular join:** Takes 10 minutes, shuffles 500GB
* **Bucketed join:** Takes 3 minutes, shuffles 0GB

**When bucketing doesn't help:**

* Join key is heavily skewed (few values have most of the data)
* Tables are small (broadcast join is better)
* You rarely join these tables (setup cost not worth it)

**The sweet spot:** Large fact-dimension joins that run frequently, with decent cardinality in join keys.

**1815. How do you compare 'overwrite' vs 'append' mode performance?**

Both are writes, but the mechanics differ:

**Overwrite:**

* Deletes existing data first (or marks for deletion in cloud storage)
* Then writes new data
* Slightly slower due to cleanup phase

**Append:**

* Just writes new files alongside existing ones
* Faster - no cleanup needed
* But accumulates files over time (can lead to small file problem)

**The benchmark:** Write the same dataset with both modes, time them. Overwrite is typically 10-20% slower.

**Hidden cost of append:** Over time, you'll have hundreds or thousands of small files. This makes **reads** progressively slower (file listing overhead, per-file open/close costs). You'll need periodic compaction.

**1816. How do you measure the overhead of creating temp views?**

Creating a temp view is essentially free - it's just registering a DataFrame reference in Spark's catalog. No data is moved or copied.

**The test:**

```python
# Time this:
df.createOrReplaceTempView("my_view")  
# Should complete in milliseconds
```

**Where "cost" appears:** When you **query** the view, that's when computation happens. The view itself is just a pointer to your DataFrame's lazy transformations.

**Common confusion:** People think views materialize data. They don't. Views are aliases. The performance cost is in the underlying DataFrame's computation, not the view creation.

**1817. What is the performance difference between V1 and V2 writer for Delta Lake?**

Delta Lake **requires** V2 writer - you can't use V1.

The question becomes: how much overhead does V2's transactional protocol add?

**V2 overhead:**

* Transaction log updates (writing JSON metadata files)
* Optimistic concurrency control (checking for conflicts)
* Checkpoint file creation (if crossing checkpoint threshold)

**Typical overhead:** 10-30% slower than plain Parquet V1 writes, but you gain ACID transactions, time travel, schema enforcement.

**When V2/Delta is worth it:**

* Need concurrent writes (multiple jobs writing safely)
* Need to update/delete/merge (not just append)
* Need audit trail (time travel)
* Need schema enforcement

**When plain Parquet V1 is better:**

* Write-once, read-many workloads
* No concurrent writers
* No update/delete requirements

### Cost Analysis

**1818. How do you calculate storage cost savings from partitionBy?**

Partitioning doesn't typically **reduce** storage size - you have the same data. The savings come from  **reading less data** .

**The calculation:**

**Without partitioning:**

* Query for one month's data scans entire year: 120GB scanned
* At $0.01/GB scan cost = $1.20 per query
* 1000 queries/day = $1,200/day

**With partitioning by month:**

* Query for one month's data scans only that month: 10GB scanned
* At $0.01/GB scan cost = $0.10 per query
* 1000 queries/day = $100/day
* **Savings: $1,100/day or $400K/year**

**The caveat:** This assumes your queries actually filter on the partition column. If you do full scans, partitioning costs you money (more files = more file listing overhead).

**1819. How do you estimate the cost of over-partitioning (too many small files)?**

Small file problem: each file has overhead in cloud storage systems.

**The math:**

**Listing overhead:**

* S3 LIST operations: $0.005 per 1000 requests
* 1 million tiny files = 1000 LIST calls = $5 just to list
* This adds up fast with many queries

**Open/close overhead:**

* Each file requires separate open, metadata read, close
* 1000 files vs 100 files = 10x more open/close operations
* Translates to slower queries (I/O latency adds up)

**Compute cost:**

* More tasks needed to process more files
* Task scheduling overhead
* Can add 20-50% to compute costs for same data volume

**The sweet spot:** Target 128MB-1GB per file. Smaller = too many files, larger = tasks can't parallelize well.

**1820. What is the compute cost of bucketing during write vs savings during read?**

**Upfront write cost:**
Bucketing requires shuffling all data into buckets, then sorting within buckets.

* One-time write: 30 minutes for shuffle + sort
* Compute cost: Maybe $10 in cluster time

**Read savings:**
Every join on bucketed tables eliminates shuffle.

* Normal join: 10 minutes, shuffle 200GB
* Bucketed join: 3 minutes, no shuffle
* Savings: 7 minutes per query

**Break-even calculation:**

* Write overhead: $10
* Per-query savings: $2
* Break-even: 5 queries
* After 5 queries, all savings are pure profit

**For daily ETL with 100+ joins:** Bucketing pays for itself many times over.

**1821. How do you measure the ROI of migrating to V2 writer?**

V2 writer (for Delta/Iceberg) has costs and benefits.

**Costs:**

* 10-30% slower writes (transaction overhead)
* Storage overhead for transaction logs (~1-5% extra)
* Learning curve and migration effort

**Benefits:**

* Eliminate data corruption from concurrent writes
* Enable UPDATE/DELETE/MERGE (save rewriting entire tables)
* Time travel (avoid maintaining multiple copies of data)
* Schema enforcement (catch errors before they corrupt data lake)

**The ROI calculation:**

**Before (Parquet V1):**

* 2 data corruption incidents/year requiring manual fixes: $4,000
* Full table rewrites for updates: $6,000/year
* Maintaining dated copies for "time travel": extra $15,000/year
* **Total cost: $25,000/year**

**After (Delta V2):**

* Slight write overhead: +10% compute = $2,000/year
* Transaction log storage: +2% storage = $1,000/year
* **Total cost: $3,000/year**
* **Savings: $22,000/year**

Plus intangibles: better data quality, easier operations, happier data engineers.

---

## Summary

**Key Debugging Tools:**

* **Spark UI** - Visual performance analysis (Jobs → Stages → Tasks)
* **Accumulators** - Distributed metrics without collecting data
* **explain()** - Understand execution plans and optimization
* **printSchema()** - Verify data types and structure
* **show()** - Inspect actual data values

**Common Issues & Quick Fixes:**

* **OOM errors** → Increase partitions or memory, avoid collecting large data
* **Task not serializable** → Keep closures simple, don't reference non-serializable objects
* **Data skew** → Repartition with salting, broadcast small tables
* **Shuffle spill** → Increase partitions, increase memory, reduce shuffle size

**Performance Best Practices:**

* Use appropriate data types (integers not strings for IDs)
* Cache before multiple actions
* Single-pass aggregations instead of multiple counts
* Sample for development, full data for production
* Read physical plans to understand and optimize execution
* Verify partitioning with partition pruning tests
* Benchmark bucketing before committing to it
* Measure ROI of storage strategies against your workload
