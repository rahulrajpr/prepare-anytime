# PySpark Interview Preparation Guide

## 3.28 Caching and Persistence - Deep Dive

### 3.28.1 What is cache() method in Spark?

**Definition:** `cache()` is a performance optimization method that persists a DataFrame/RDD in memory (and optionally disk) so that subsequent operations on the same data can be executed much faster.

**Detailed Explanation:**
When you call `cache()` on a DataFrame, Spark marks it for persistence. The actual caching happens lazily - when an action is triggered. Once cached, the data stays in memory across multiple operations, eliminating the need to recompute the entire transformation chain.

### 3.28.2 What storage level does cache() use by default?

**Default Storage Level:** `MEMORY_AND_DISK`

**Detailed Behavior:**

- **Priority 1:** Store partitions in memory as deserialized Java objects
- **Priority 2:** If memory fills up, spill remaining partitions to disk
- **Benefit:** Balances speed and reliability - fast memory access with disk fallback

### 3.28.3 Is cache() lazy or eager evaluation?

**Answer:** LAZY evaluation

**Detailed Explanation:**
Like all Spark transformations, `cache()` is lazy. Calling `df.cache()` doesn't actually persist anything immediately. It just adds a "cache this" instruction to the execution plan. The real caching happens when you trigger an ACTION like `count()`, `show()`, or `collect()`.

### 3.28.4 When does cached data actually get stored - at cache() call or first action?

**Storage Timing:** During the FIRST ACTION after the `cache()` call

**Example Flow:**

```python
df = spark.read.parquet("large_dataset.parquet")
transformed_df = df.filter(col("year") == 2023).cache()  # Nothing happens yet

# First action - caching occurs here
transformed_df.count()  # ← Data gets cached during this computation

# Subsequent actions use cached data
transformed_df.show()   # ← Uses cached data, no recomputation
```

### 3.28.5 What happens to cached data when memory is full?

**Behavior:** Least Recently Used (LRU) eviction policy

**Detailed Process:**

1. Spark tries to cache new partitions in available memory
2. If memory is full, it identifies least-recently-used cached partitions
3. Those partitions are evicted (removed from memory)
4. If those partitions were originally from `MEMORY_AND_DISK` level, they remain on disk
5. If they were `MEMORY_ONLY`, they're completely removed and must be recomputed

### 3.28.6 What is LRU eviction in caching?

**LRU Definition:**
 "Least Recently Used" - a cache management algorithm that removes the
items that haven't been accessed for the longest time when space is
needed.

**Real-world Analogy:** Like a library shelf - books that haven't been checked out recently get moved to storage to make space for new popular books.

### 3.28.7 When should you use cache() for multiple actions on same DataFrame?

**Optimal Use Case:** When you perform multiple different actions on the same transformed DataFrame

**Example Scenario:**

**python**

```
# Good caching scenario
feature_engineered_df = (
    raw_df
    .filter(col("active") == True)
    .withColumn("new_feature", complex_calculation())
    .cache()  # Worth caching because we use it multiple times
)

# Multiple actions on same data
feature_engineered_df.count()           # Action 1
feature_engineered_df.show(10)          # Action 2  
feature_engineered_df.write.parquet(...) # Action 3
```

### 3.28.8 When should you use cache() for iterative algorithms?

**Use Case:** Machine learning algorithms, graph processing, any computation that reuses the same data multiple times in a loop

**ML Pipeline Example:**

**python**

```
training_data = preprocessed_df.cache()  # Cache before iterative training

for epoch in range(100):
    # Reuse same training data in each epoch
    model.fit(training_data)
    # Without caching, Spark would recompute preprocessing 100 times!
```

### 3.28.9 When should you avoid cache() for single-use DataFrames?

**Avoid When:** DataFrame is used only once in the entire pipeline

**Example of Wasteful Caching:**

**python**

```
# ❌ Don't cache - used only once
df = spark.read.csv("data.csv").cache()
df.write.parquet("output.parquet")  # Single use - caching wasted

# ✅ Better - no caching needed
df = spark.read.csv("data.csv")
df.write.parquet("output.parquet")
```

### 3.28.10 When should you avoid cache() in memory-constrained environments?

**Avoid When:** Limited executor memory and working with large datasets

**Risk Scenario:**

* You have 4GB executor memory
* Dataset is 10GB after transformations
* Caching would cause excessive disk spilling, making performance WORSE than no caching

### 3.28.11 What is the difference between cache() and persist() in terms of flexibility?

**Critical Difference:**

* `cache()` → Always uses `MEMORY_AND_DISK` (no choices)
* `persist()` → Lets you choose specific storage level based on your needs

### 3.28.12 Can you customize storage level with cache()?

**Answer:** NO - `cache()` always uses `MEMORY_AND_DISK`

### 3.28.13 Can you customize storage level with persist()?

**Answer:** YES - `persist()` accepts custom storage levels

**Flexibility Examples:**

**python**

```
from pyspark import StorageLevel

# Different storage strategies
df.persist(StorageLevel.MEMORY_ONLY)        # Fastest, no disk spill
df.persist(StorageLevel.MEMORY_ONLY_SER)    # More compact, serialized
df.persist(StorageLevel.DISK_ONLY)          # Large data, memory conservation
```

### 3.28.14 What are the available storage levels in Spark?

**Complete Storage Level Hierarchy:**

| Storage Level           | Memory | Disk | Serialized | Replicated | Use Case                           |
| ----------------------- | ------ | ---- | ---------- | ---------- | ---------------------------------- |
| `MEMORY_ONLY`         | ✅     | ❌   | ❌         | ❌         | Small datasets, maximum speed      |
| `MEMORY_AND_DISK`     | ✅     | ✅   | ❌         | ❌         | Default, balanced approach         |
| `MEMORY_ONLY_SER`     | ✅     | ❌   | ✅         | ❌         | Memory efficiency needed           |
| `MEMORY_AND_DISK_SER` | ✅     | ✅   | ✅         | ❌         | Large datasets, memory constrained |
| `DISK_ONLY`           | ❌     | ✅   | ❌         | ❌         | Very large data, minimal memory    |
| `OFF_HEAP`            | ✅*    | ❌   | ✅         | ❌         | Large memory, avoid GC pauses      |

*Off-heap memory is outside JVM control

### 3.28.15 What is MEMORY_ONLY storage level?

**Definition:** Stores data ONLY in memory as deserialized Java objects

**Pros:** Fastest access speed
**Cons:** Risk of eviction if memory pressure

### 3.28.16 What is MEMORY_AND_DISK storage level?

**Definition:** Stores in memory first, spills to disk when memory full

**Pros:** Reliable, won't lose data under memory pressure
**Cons:** Slower than MEMORY_ONLY when spilling occurs

### 3.28.17 What is MEMORY_ONLY_SER storage level?

**Definition:** Stores serialized data in memory

**Key Insight:** Serialization makes data more compact (2-5x space savings) but requires CPU to deserialize

### 3.28.18 What is MEMORY_AND_DISK_SER storage level?

**Definition:** Serialized in memory, spills serialized to disk

**Best For:** Large datasets where memory conservation is critical

### 3.28.19 What is DISK_ONLY storage level?

**Definition:** Stores data only on disk - no memory usage

**Use Case:** When you want to avoid memory usage entirely but still break lineage

### 3.28.20 What is OFF_HEAP storage level?

**Definition:** Stores data in memory outside the JVM heap

**Advantage:** Avoids garbage collection pauses, can use more memory
**Disadvantage:** More complex setup, manual memory management

### 3.28.21 What storage levels support replication (_2 suffix)?

**Replication Levels:** All storage levels with `_2` suffix store two replicas

**Examples:**`MEMORY_ONLY_2`, `DISK_ONLY_2`
**Benefit:** Fault tolerance - if one replica is lost, another exists

### 3.28.22 How do you unpersist cached data?

**Method:**`df.unpersist()` or `df.unpersist(blocking=True)`

**Blocking vs Non-blocking:**

* `unpersist()` - non-blocking, returns immediately
* `unpersist(blocking=True)` - waits until data is actually removed

### 3.28.23 Is there a separate uncache() method?

**Answer:** NO - `unpersist()` is used for both cache() and persist()

### 3.28.24 Does unpersist() work for both cache() and persist()?

**Answer:** YES - works for any persisted DataFrame regardless of storage level

## 3.29 Checkpoint - Reliability and Fault Tolerance

### 3.29.1 What is checkpoint() in Spark?

**Definition:**`checkpoint()` is a reliability mechanism that saves a DataFrame to reliable distributed storage and completely breaks the lineage graph.

**Key Concept:** It creates a "save point" in your computation that can be recovered from without recomputing all previous transformations.

### 3.29.2 What is the main purpose of checkpointing?

**Main Purpose:** To truncate excessively long lineage chains that can cause stack overflow errors or make fault recovery too expensive.

**Lineage Problem Example:**

**text**

```
df1 → filter → join → groupBy → window → join → filter → ... [100 steps]
```

If any executor fails, Spark must recompute ALL 100 steps from df1.

**With Checkpoint:**

**text**

```
df1 → filter → join → checkpoint() → groupBy → window → join
```

After checkpoint, only steps after checkpoint need recomputation on failure.

### 3.29.3 Where does checkpoint() save data?

**Storage Location:** Reliable distributed storage (HDFS, S3, DBFS, etc.)

**Requirement:** You must configure a checkpoint directory first:

**python**

```
spark.sparkContext.setCheckpointDir("hdfs:///checkpoint/dir")
```

### 3.29.4 Does checkpoint() break the lineage graph?

**Answer:** YES - completely breaks the lineage and creates a fresh starting point

**Visualization:**

**text**

```
Before: A → B → C → D [Lineage: A→B→C→D]
After checkpoint: A → B → C → [CHECKPOINT] → D [Lineage: checkpoint→D]
```

### 3.29.5 What is the difference between checkpoint() and localCheckpoint()?

**Critical Differences Table:**

| Aspect                     | checkpoint()                                | localCheckpoint()                       |
| -------------------------- | ------------------------------------------- | --------------------------------------- |
| **Storage Location** | Reliable distributed storage (HDFS/S3)      | Local executor storage                  |
| **Fault Tolerance**  | ✅ Survives executor failures               | ❌ Lost on executor failure             |
| **Performance**      | Slower (network I/O to distributed storage) | Faster (local disk/SSD)                 |
| **Data Persistence** | Permanent until manually deleted            | Temporary, cleaned up automatically     |
| **Use Case**         | Production pipelines, critical data         | Development, testing, non-critical data |

### 3.29.6 Where does checkpoint() save data - reliable storage or local?

**checkpoint():** Reliable distributed storage (HDFS, S3, etc.)

### 3.29.7 Where does localCheckpoint() save data - reliable storage or local?

**localCheckpoint():** Local executor storage (local disks)

### 3.29.8 Does data survive driver/executor failures with checkpoint()?

**Answer:** YES - data survives both driver and executor failures with checkpoint()

**Recovery Process:** On failure, Spark can read checkpoint data from reliable storage and continue computation from that point.

### 3.29.9 Does data survive driver/executor failures with localCheckpoint()?

**Answer:** NO - data is lost on executor failure with localCheckpoint()

**Limitation:** If an executor crashes, its locally checkpointed data is gone forever.

### 3.29.10 When should you use checkpoint() for long transformation chains?

**Use Case:** When you have very deep transformation chains (50+ steps) that risk:

* StackOverflowError in Spark scheduler
* Expensive recomputation on failures
* Debugging difficulties due to complex lineage

**Example:** Complex ETL pipelines with multiple joins, aggregations, and window functions.

### 3.29.11 When should you use checkpoint() for iterative algorithms?

**Use Case:** Machine learning algorithms with many iterations

**ML Example:**

**python**

```
for i in range(1000):
    # Each iteration adds to lineage
    model.update(training_data)
    if i % 100 == 0:
        training_data.checkpoint()  # Break lineage every 100 iterations
```

### 3.29.12 When should you avoid checkpoint() for simple, fast transformations?

**Avoid When:** Transformations are simple and complete quickly

**Examples to Avoid:**

* Simple filter operations
* Column selections
* Basic aggregations on small data
* Any transformation that takes seconds rather than minutes

### 3.29.13 When should you avoid checkpoint() when storage is limited?

**Avoid When:** Limited storage availability or expensive storage costs

**Considerations:**

* Checkpointing doubles storage usage temporarily
* In cloud environments, storage costs money
* If storage I/O is slow, checkpointing becomes bottleneck

### 3.29.14 What is the performance cost of checkpointing?

**Significant Costs:**

1. **Computation Cost:** Full recomputation up to checkpoint point
2. **I/O Cost:** Writing entire dataset to distributed storage
3. **Network Cost:** Data transfer across network
4. **Time Cost:** Checkpointing can take minutes for large datasets

**Rule of Thumb:** Checkpointing typically adds 20-50% overhead to job runtime.

### 3.29.15 Where should you configure the checkpoint directory?

**Configuration:** Before any checkpoint operations

**python**

```
# Configure at application start
spark.sparkContext.setCheckpointDir("hdfs:///user/spark/checkpoints")

# Or for cloud storage
spark.sparkContext.setCheckpointDir("s3a://bucket/checkpoints/")
```

**Best Practice:** Use fast, reliable storage with good I/O performance.

## 3.30 Pandas Integration

### 3.30.1 What does toPandas() do in Spark?

**Definition:**`toPandas()` converts a distributed Spark DataFrame to a single-node pandas DataFrame.

**Critical Warning:** This collects ALL data from all executors to the driver node.

### 3.30.2 Where does data move when you call toPandas()?

**Data Movement Path:**

1. Each executor serializes its partition data
2. Data transferred over network to driver
3. Driver deserializes and assembles into pandas DataFrame
4. Entire dataset now lives in driver memory

### 3.30.3 What are the memory implications of toPandas()?

**Memory Risk:** HIGH - entire dataset must fit in driver JVM memory

**Example Disaster Scenario:**

* 100GB Spark DataFrame distributed across cluster
* `df.toPandas()` tries to load 100GB into driver (typically 4-16GB)
* Result: Driver crashes with OutOfMemoryError

### 3.30.4 When should you use toPandas() for visualization?

**Safe Use Case:** After aggregations that produce small results

**Good Example:**

**python**

```
# Safe - aggregation produces small result
summary_df = large_df.groupBy("category").agg(
    count("*").alias("count"),
    avg("value").alias("average")
)
pandas_df = summary_df.toPandas()  # Small result, safe

# Now use with visualization libraries
import matplotlib.pyplot as plt
plt.bar(pandas_df["category"], pandas_df["count"])
```

### 3.30.5 When should you use toPandas() for small results after aggregation?

**Use Case:** When your aggregation reduces data size significantly

**Safe Pattern:** Always aggregate in Spark first, then convert small results

### 3.30.6 When should you avoid toPandas() for large datasets?

**Avoid When:** Dataset size exceeds ~1GB or is larger than driver memory

**Danger Signs:**

* More than 10 million rows
* Wide schemas with many columns
* No aggregation before conversion

### 3.30.7 When should you avoid toPandas() in production pipelines?

**Avoid When:** Production environments where reliability and scalability are critical

**Production Risk:** Driver OOM crashes entire application, affecting all users

### 3.30.8 What is the risk of out-of-memory errors with toPandas()?

**Risk Level:** VERY HIGH for large datasets

**Consequences:**

* Driver JVM crashes
* Spark application fails
* Lost computation progress
* Need to restart entire job

### 3.30.9 What is pandas_api() in Spark?

**Definition:** A distributed pandas-compatible API that operates on Spark DataFrames WITHOUT collecting data to driver.

**Key Advantage:** You get pandas-like syntax but distributed execution.

### 3.30.10 Where does data stay with pandas_api() - local or distributed?

**Data Location:** Stays DISTRIBUTED across executors - no data movement to driver

### 3.30.11 What is the difference between toPandas() and pandas_api() in terms of data location?

**Critical Difference Visualization:**

**text**

```
toPandas():
[Executor1] → [Data] → [Network] → [Driver] → pandas DataFrame
[Executor2] → [Data] → [Network] → [Driver] → (All data in driver memory)
[Executor3] → [Data] → [Network] → [Driver]

pandas_api():
[Executor1] → [Data stays here] → Distributed processing
[Executor2] → [Data stays here] → Distributed processing  
[Executor3] → [Data stays here] → Distributed processing
```

### 3.30.12 What is the difference between toPandas() and pandas_api() in terms of processing?

**Processing Architecture:**

**toPandas():**

* Single-node processing on driver
* Full pandas functionality
* Limited by driver memory
* Good for small data

**pandas_api():**

* Distributed processing across cluster
* Subset of pandas API
* Scales to terabytes
* Good for big data

### 3.30.13 What is the maximum data size for toPandas() - RAM limited or terabytes?

**toPandas():** Limited by DRIVER RAM (typically 4-64GB)

**Practical Limit:** ~1-10GB depending on driver memory configuration

### 3.30.14 What is the maximum data size for pandas_api() - RAM limited or terabytes?

**pandas_api():** Limited by CLUSTER RAM (can handle TBs+)

**Scalability:** Grows with cluster size - add more executors for more data

### 3.30.15 Does toPandas() have full pandas compatibility?

**Answer:** YES - 100% pandas compatibility

**Benefit:** All pandas functions, methods, and third-party libraries work

### 3.30.16 Does pandas_api() have full pandas compatibility?

**Answer:** NO - subset of pandas API optimized for distributed processing

**Limitation:** Some pandas functions that assume single-node data don't work

### 3.30.17 When should you use toPandas() for Python ML libraries?

**Use Case:** When using single-node ML libraries like scikit-learn, statsmodels, XGBoost (single node)

**Pattern:** Process big data in Spark → aggregate to small results → convert to pandas → use scikit-learn

### 3.30.18 When should you use pandas_api() for big data processing?

**Use Case:** Large datasets that need pandas-like operations but must stay distributed

**Examples:** Feature engineering on TB-scale data, distributed data cleaning

### 3.30.19 What is the strategic usage pattern: Process in Spark → Convert to Pandas → Use Python ecosystem?

**Optimal Big Data Strategy:**

**python**

```
# Step 1: Big data processing in Spark (distributed)
big_data = spark.read.parquet("tb_of_data/")
aggregated = big_data.groupBy("user_id").agg(
    count("*").alias("activity_count"),
    sum("value").alias("total_value")
).filter(col("activity_count") > 1000)

# Step 2: Convert small results to pandas (safe)
small_results = aggregated.toPandas()  # Now small enough

# Step 3: Rich Python ecosystem (single node)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(small_results[["activity_count"]], small_results["label"])

import matplotlib.pyplot as plt
plt.hist(small_results["total_value"])
```

## 3.31 Temporary Views and SQL Integration

### 3.31.1 What is a temporary view in Spark?

**Definition:** A DataFrame registered as a SQL table that can be queried using Spark SQL syntax.

**Key Point:** It's a virtual table - no data is copied or moved.

### 3.31.2 How do you create a temporary view?

**Method:**`df.createOrReplaceTempView("view_name")`

**Example:**

**python**

```
df = spark.read.parquet("sales_data/")
df.createOrReplaceTempView("sales")  # Now queryable as SQL table
```

### 3.31.3 What is the scope of a temporary view - session or application?

**Scope:** CURRENT SparkSession only

**Implication:** If you create a new SparkSession, the temporary view won't be available.

### 3.31.4 How long does a temporary view last?

**Lifetime:** Until the SparkSession ends or you explicitly drop it

**Automatic Cleanup:** When SparkSession stops, all its temporary views are automatically removed.

### 3.31.5 What is a global temporary view in Spark?

**Definition:** A temporary view accessible across multiple SparkSessions within the same Spark application.

**Use Case:** Sharing data between different notebooks, scripts, or sessions.

### 3.31.6 How do you create a global temporary view?

**Method:**`df.createOrReplaceGlobalTempView("view_name")`

### 3.31.7 What is the scope of a global temporary view - session or application?

**Scope:** Entire Spark application (all SparkSessions)

**Database Context:** Global views live in `global_temp` database.

### 3.31.8 How do you reference a temporary view in SQL?

**Reference:** Directly by name in SQL queries

**Example:**

**python**

```
spark.sql("SELECT * FROM sales WHERE amount > 1000")
```

### 3.31.9 How do you reference a global temporary view in SQL?

**Reference:** Must prefix with `global_temp` database

**Example:**

**python**

```
spark.sql("SELECT * FROM global_temp.sales")
```

### 3.31.10 What is the SQL syntax for accessing a global temporary view (global_temp.view_name)?

**Syntax:**`global_temp.view_name` - always use this prefix

**Why:** Global views are stored in a separate namespace to avoid conflicts with session-local views.

### 3.31.11 When should you use temporary views for single-session work?

**Use Case:** SQL queries within a single notebook, script, or session

**Examples:**

* Data exploration in Jupyter notebook
* SQL-based transformations in a single ETL script
* Ad-hoc analysis sessions

### 3.31.12 When should you use global temporary views for cross-session sharing?

**Use Case:** Sharing data between different parts of your application

**Examples:**

* Multiple notebooks analyzing the same dataset
* Different services in same application needing shared data
* Staging data for multiple downstream processes

### 3.31.13 Can multiple sessions access a temporary view?

**Answer:** NO - only the creating SparkSession can access it

**Isolation:** Each SparkSession has its own completely separate temporary view namespace.

### 3.31.14 Can multiple sessions access a global temporary view?

**Answer:** YES - all SparkSessions in the same application can access it

**Shared Access:** Perfect for collaborative analysis or multi-step pipelines.

## 3.32 Miscellaneous DataFrame Operations

### 3.32.1 What does freqItems() function do in Spark?

**Definition:**`freqItems()` finds frequent items (values that appear often) in specified columns using approximate frequency counting.

**Statistical Concept:** It identifies values whose frequency exceeds a minimum support threshold.

### 3.32.2 What is the default threshold for freqItems()?

**Default:** Support = 0.01 (1%)

**Meaning:** Values that appear in at least 1% of rows are considered "frequent"

### 3.32.3 What does support = 0.01 mean in freqItems()?

**Statistical Meaning:** A value has "support" of 1% if it appears in 1 out of every 100 rows.

**Example:** In a dataset of 1 million rows, values appearing in at least 10,000 rows are frequent.

### 3.32.4 What values are included when threshold is 1%?

**Included:** Values appearing in ≥1% of rows

**Example:** If "category_A" appears in 15,000 out of 1,000,000 rows (1.5%), it will be included.

### 3.32.5 What values are excluded when threshold is 1%?

**Excluded:** Values appearing in <1% of rows

**Example:** If "category_Z" appears in 500 out of 1,000,000 rows (0.05%), it will be excluded.

### 3.32.6 When would you use freqItems() for pattern mining?

**Practical Use Cases:**

1. **Data Quality Analysis:**
   **python**

```
# Find common values in categorical columns
frequent_values = df.freqItems(["category", "status", "type"])
```

**Feature Engineering:**

**python**

```
# Identify common categories for one-hot encoding
common_cats = df.freqItems(["product_category"], 0.05)
```

**Anomaly Detection:**

**python**

```
# Rare values (not in freqItems) might be anomalies
frequent = df.freqItems(["user_behavior"], 0.02)
```

**Market Basket Analysis:**

**python**

```
# Find frequently co-purchased products
transaction_freq = df.freqItems(["product_ids"], 0.01)
```

**Business Applications:**

* Identify popular products in e-commerce
* Find common error patterns in log data
* Discover frequent customer segments
* Detect data quality issues (unexpected frequent values)
