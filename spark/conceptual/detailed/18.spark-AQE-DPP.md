# Spark Joins & Query Optimization - Interview Guide

## Table of Contents

### 8. Adaptive Query Execution (AQE)
- [8.6.1 What is AQE?](#861-what-is-adaptive-query-execution-aqe)
- [8.6.2 Dynamic Join Strategy Switching](#862-how-does-aqe-improve-join-performance-through-dynamic-switching)
- [8.6.3 Skew Join Optimization](#863-what-is-skew-join-optimization-and-how-does-it-work)
- [8.6.4 Dynamic Partition Coalescing](#864-what-is-dynamic-partition-coalescing-and-how-does-it-work)
- [8.6.5 Local Shuffle Reader](#865-local-shuffle-reader-optimization)
- [8.6.6 Prerequisites](#866-prerequisites-for-effective-aqe)
- [8.6.7 Maximum Benefit Scenarios](#867-when-aqe-provides-maximum-benefit)
- [8.6.8 Limitations](#868-when-aqe-might-not-help-or-hurt)
- [8.6.9 Verification](#869-how-to-verify-aqe-is-working)
- [8.6.10 Combining Optimizations](#8610-can-aqe-optimizations-be-combined)
- [8.6.11 AQE with DPP](#8611-aqe-with-dpp-dynamic-partition-pruning)

### 8.7 Dynamic Partition Pruning (DPP)
- [8.7.1 What is DPP?](#871-what-is-dynamic-partition-pruning-dpp)
- [8.7.2 DPP vs Static Pruning](#872-how-does-dpp-differ-from-static-partition-pruning)
- [8.7.3 Enabling DPP](#873-how-do-you-enable-dynamic-partition-pruning)
- [8.7.4 Requirements](#874-what-conditions-must-be-met-for-dpp-to-work)
- [8.7.5 Best Scenarios](#875-ideal-scenarios-for-maximum-dpp-benefit)
- [8.7.6 Fact-Dimension Joins](#876-how-does-dpp-work-in-fact-dimension-joins)
- [8.7.7 Verification](#877-how-to-verify-dpp-is-working)
- [8.7.8 Performance Impact](#878-performance-impact-of-dpp)
- [8.7.9 Limitations](#879-limitations-of-dpp)
- [8.7.10 Bucketed Tables](#8710-does-dpp-work-with-bucketed-tables)
- [8.7.11 Non-Partitioned Tables](#8711-does-dpp-work-with-non-partitioned-tables)
- [8.7.12 DPP & Predicate Pushdown](#8712-relationship-between-dpp-and-predicate-pushdown)
- [8.7.13 DPP & AQE](#8713-can-dpp-and-aqe-be-combined)
- [8.7.14 Shuffle Impact](#8714-how-does-dpp-affect-shuffle-operations)
- [8.7.15 Statistics](#8715-what-statistics-are-needed-for-dpp)

### 9. Shuffle & Partitioning
#### 9.1 Shuffle Fundamentals
- [9.1.1 Shuffle Operations](#911-what-are-shuffle-operations-in-spark)
- [9.1.2 GROUP BY Shuffle](#912-shuffle-sort-in-group-by-context)
- [9.1.3 JOIN Shuffle](#913-shuffle-sort-in-join-context)

#### 9.2 Partition Configuration
- [9.2.1 spark.sql.shuffle.partitions](#921-what-is-sparksqlshufflepartitions)
- [9.2.2 Recommended Values](#922-recommended-values-for-different-workload-sizes)
- [9.2.3 Shuffle Buffer](#923-what-is-the-shuffle-buffer-and-what-does-it-control)

#### 9.3 Partition Tuning
- [9.3.1 What is Partition Tuning?](#931-what-is-partition-tuning-and-why-is-it-crucial)
- [9.3.2 Balancing Parallelism](#932-finding-the-balance-parallelism-vs-overhead)

#### 9.4 Data Skewness
- [9.4.1 What is Data Skewness?](#941-what-is-data-skewness-in-spark)
- [9.4.2 Causes of Skew](#942-what-causes-uneven-distribution-across-partitions)
- [9.4.3 Shuffle Metrics](#943-interpreting-uneven-shuffle-metrics)

#### 9.5 Handling Data Skew
- [9.5.1 Salting Techniques](#951-salting-techniques-for-skewed-datasets)
- [9.5.2 Performance Impact](#952-how-data-skewness-affects-performance)

#### 9.6 Advanced Operations
- [9.6.1 Custom Partitioning](#961-what-is-custom-partitioning-and-when-to-use-it)
- [9.6.2 reduceByKey vs groupByKey](#962-reducebykey-vs-groupbykey)

---

## Adaptive Query Execution (AQE)

### Overview & Core Concepts

#### 8.6.1 What is Adaptive Query Execution (AQE)?

**Answer:** Adaptive Query Execution (AQE) is Spark's runtime optimization framework that re-optimizes query execution plans based on actual execution statistics rather than relying solely on compile-time estimates.

**Key Features:**
- Introduced in Spark 3.0 (June 2020)
- Enabled by default in Spark 3.2+
- Works at runtime, not just compile time
- Addresses limitations of static query planning

**Why AQE is Important:**
Traditional query planning relies on table statistics that may be outdated or inaccurate. AQE collects real-time metrics during execution and dynamically adjusts the plan for better performance.

**Configuration:**
```python
# Enable AQE (default in Spark 3.2+)
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
```

**Static vs Adaptive Optimization:**

| Aspect | Static Optimization | Adaptive Optimization |
|--------|-------------------|---------------------|
| **Timing** | Before query execution | During query execution |
| **Data Awareness** | Based on table statistics | Based on actual runtime metrics |
| **Plan Flexibility** | Fixed plan | Dynamic plan adjustments |
| **Skew Handling** | Manual or none | Automatic detection and handling |
| **Partition Tuning** | Static partitioning | Dynamic coalescing |
| **Join Strategy** | Fixed at planning time | Can switch at runtime |

**Interview Tips:**
- AQE represents a paradigm shift from static to adaptive planning
- Three main optimizations: join strategy switching, skew handling, partition coalescing
- Always mention it's enabled by default in Spark 3.2+

---

### 1. Dynamic Join Strategy Switching

#### 8.6.2 How does AQE improve join performance through dynamic switching?

**Answer:** AQE can change join strategies during execution based on runtime statistics about actual data sizes, potentially converting sort-merge joins to broadcast joins when appropriate.

**Switching Scenarios:**
1. **Sort-Merge → Broadcast Join**: When filtered data is smaller than expected
2. **Broadcast → Sort-Merge**: When broadcast table exceeds memory
3. **Shuffle Hash → Sort-Merge**: Based on memory pressure

**Example:**
```python
# Initial plan might choose sort-merge join based on table statistics
# But after filtering, the right side becomes small enough for broadcast

large_table = spark.table("user_events")  # Estimated 50GB
small_table = spark.table("campaigns").filter("status = 'active'")  
# Estimated 5GB, actual 50MB after filter

# AQE detects actual 50MB size and switches to broadcast join
result = large_table.join(small_table, "campaign_id")
```

**Conversion Process:**
1. **Initial Planning**: Query planner chooses sort-merge based on statistics
2. **Partial Execution**: AQE executes parts up to shuffle boundaries
3. **Statistics Collection**: Collects actual data size metrics
4. **Strategy Reevaluation**: If one side < broadcast threshold, switches strategy
5. **Plan Reoptimization**: Replaces sort-merge with broadcast join

**Configuration:**
```python
# Adaptive broadcast threshold (default: 10MB)
spark.conf.set("spark.sql.adaptive.autoBroadcastJoinThreshold", "10485760")

# This differs from static threshold:
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760")
```

**Key Differences: Adaptive vs Static Threshold:**

| Aspect | Static Threshold | Adaptive Threshold |
|--------|-----------------|-------------------|
| **Timing** | Compile-time decision | Runtime decision |
| **Basis** | Table statistics | Actual data size after partial execution |
| **Accuracy** | Based on estimates | Based on real metrics |
| **Use Case** | Simple cases with accurate stats | Complex queries with filtering/aggregation |

**What Triggers Strategy Switching:**
- Data size < broadcast threshold after filtering
- Memory pressure indicators
- Performance metrics showing suboptimal strategy
- Skew detection requiring different approach

**Verification in Spark UI:**
```python
# Check query plan for switching
result.explain()  # Look for "AdaptiveSparkPlan"

# In Spark UI SQL tab, look for:
# - "BroadcastHashJoin" instead of "SortMergeJoin"
# - "Adaptive execution" indicators
# - Message: "BroadcastHashJoin after adaptive planning"
```

**Interview Tips:**
- This is one of the most impactful AQE optimizations
- Requires shuffle boundaries in query plan to work
- Works best when filters significantly reduce data size
- Can provide 2-10x performance improvement

---

### 2. Skew Join Optimization

#### 8.6.3 What is skew join optimization and how does it work?

**Answer:** Skew join optimization is AQE's automatic mechanism to handle data skew by detecting unevenly distributed partitions and splitting them for balanced processing.

**The Problem:**
```python
# Skewed data scenario - one key dominates
skewed_data = spark.range(1000000).withColumn(
    "key", 
    when(col("id") < 100, 1)  # 100 rows with key=1
    .otherwise(col("id") % 1000)  # 999,900 rows distributed across other keys
)

# Without AQE: One task processes most data for key=1 (straggler)
# With AQE: Splits the skewed partition into multiple tasks
```

**Detection Algorithm:**

A partition is considered skewed if **BOTH** conditions are met:
1. `size > median_partition_size * skewedPartitionFactor` (default: 5x)
2. `size > skewedPartitionThresholdInBytes` (default: 256MB)

**Configuration:**
```python
# Enable skew join optimization
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# Detection thresholds
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionFactor", "5")
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "268435456")  # 256MB

# Control splitting
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionMaxSplits", "10")
```

**Detection Example:**
```python
# Partition sizes: [10MB, 15MB, 12MB, 450MB, 11MB, 13MB]
# Median: ~12.5MB
# Threshold: max(12.5MB * 5, 256MB) = max(62.5MB, 256MB) = 256MB
# Result: 450MB > 256MB → Partition is skewed
```

**How Skew is Handled:**

1. **Detection Phase:**
   - Monitors shuffle partition sizes
   - Identifies partitions exceeding thresholds
   
2. **Optimization Phase:**
   - Splits large partitions into smaller chunks
   - Distributes work across multiple tasks
   - Maintains data locality

3. **Execution:**
   ```
   Before Splitting:
   Partition 1: 10MB  [====]
   Partition 2: 15MB  [======]
   Partition 3: 800MB [========================================...]
   Partition 4: 12MB  [=====]
   
   After Splitting:
   Partition 1: 10MB  [====]
   Partition 2: 15MB  [======]
   Partition 3a: 200MB [===========]
   Partition 3b: 200MB [===========]
   Partition 3c: 200MB [===========]
   Partition 3d: 200MB [===========]
   Partition 4: 12MB  [=====]
   ```

**Performance Impact:**

| Scenario | Without AQE | With AQE | Improvement |
|----------|------------|----------|-------------|
| Mild skew (2-3x) | Minimal impact | Small improvement | 10-20% faster |
| Moderate skew (5-10x) | Noticeable stragglers | Balanced processing | 2-5x faster |
| Severe skew (>10x) | Single task dominates | Parallel processing | 5-20x faster |
| Uniform data | No issue | Minimal overhead | Neutral |

**Splitting Strategy:**
```python
# Example: 800MB skewed partition
# AQE splits into N smaller chunks:
# - Number of splits based on data size and max splits config
# - Each split processed by separate task in parallel
# - Results combined transparently

# Maximum splits per partition (default: 10)
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionMaxSplits", "10")
```

**Supported Join Types:**

| Join Type | AQE Skew Support | Notes |
|-----------|-----------------|-------|
| **INNER JOIN** | ✅ Full support | Both sides can be optimized |
| **LEFT OUTER JOIN** | ✅ Partial support | Left side skew optimized |
| **RIGHT OUTER JOIN** | ✅ Partial support | Right side skew optimized |
| **LEFT SEMI JOIN** | ✅ Full support | Strategy switching, skew handling |
| **LEFT ANTI JOIN** | ✅ Full support | Strategy switching, skew handling |
| **FULL OUTER JOIN** | ⚠️ Limited | Some optimizations, no broadcast |
| **CROSS JOIN** | ❌ Limited | Minimal AQE optimization |

**Verification:**
```python
# Enable detailed logging
spark.conf.set("spark.sql.adaptive.logLevel", "INFO")

# Check plan for skew optimization
result.explain()  # Look for "SkewJoin" or "OptimizedSkewedJoin"

# Spark UI indicators:
# - SQL Tab: "SkewJoin" in optimized plan
# - Stages Tab: Even task distribution (no stragglers)
# - Logs: "INFO AdaptiveSparkPlanExec: Applied skew join optimization"
```

**Interview Tips:**
- Completely automatic - no manual tuning needed
- One of the major benefits of AQE
- Look for "SkewJoin" in query plans
- Can provide 10x+ improvements for highly skewed data

---

### 3. Dynamic Partition Coalescing

#### 8.6.4 What is dynamic partition coalescing and how does it work?

**Answer:** Dynamic partition coalescing is an AQE optimization that merges small shuffle partitions into larger, more efficient partitions to reduce overhead and improve performance.

**The Problem:**
```python
# After shuffle operations, many small partitions create overhead
# Before Coalescing:
# 200 partitions: [1MB, 2MB, 0.5MB, 3MB, 0.8MB, ...]
# Many small partitions causing scheduling overhead

# After Coalescing:
# 50 partitions: [64MB, 65MB, 63MB, 62MB, ...]
# Optimally sized partitions for processing
```

**How It Works:**

1. **Analysis**: Examine shuffle partition sizes after shuffle write
2. **Identification**: Find partitions smaller than target size
3. **Merging**: Combine adjacent small partitions
4. **Optimization**: Create balanced partitions for next stage

**Configuration:**
```python
# Enable partition coalescing (default: true)
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# Target partition size (default: 64MB)
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "67108864")

# Minimum partition size after coalescing (default: 1MB)
spark.conf.set("spark.sql.adaptive.coalescePartitions.minPartitionSize", "1048576")

# Initial shuffle partitions (before coalescing)
spark.conf.set("spark.sql.shuffle.partitions", "200")
```

**Coalescing Algorithm:**
```python
# Example: Shuffle produces 200 partitions with total size 1GB
# Average partition size: 5MB (too small)

total_size = 1024MB  # 1GB
target_partition_size = 64MB  # advisory size
optimal_partitions = ceil(1024MB / 64MB) = 16 partitions

# AQE merges 200 partitions into ~16 optimally sized partitions
```

**Benefits for Joins:**
- Reduced task scheduling overhead
- Better I/O efficiency
- Improved memory utilization
- Faster shuffle read operations
- Eliminates "small file problem"

**Target Partition Size Tuning:**

| Cluster Size | Data Volume | Recommended Size | Reasoning |
|-------------|-------------|-----------------|-----------|
| Small | < 10GB | 32MB | Memory efficiency |
| Medium | 10GB-100GB | 64MB (default) | Balanced approach |
| Large | 100GB-1TB | 128MB | I/O optimization |
| Very Large | > 1TB | 256MB | Reduced overhead |

**Configuration Guidelines:**
```python
# Smaller partitions for memory-constrained environments
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "33554432")  # 32MB

# Larger partitions for better I/O efficiency
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "134217728")  # 128MB

# Minimum size prevents extremely small partitions
spark.conf.set("spark.sql.adaptive.coalescePartitions.minPartitionSize", "1048576")  # 1MB
```

**How Reducers are Optimized:**
```python
# Scenario: Shuffle produces many small partitions
# Initial: 200 partitions → Final: 16-50 partitions (depending on data)

# Impact:
# - Fewer small tasks → Reduced scheduling overhead
# - Better data locality → Improved I/O
# - Reduced memory pressure → Fewer concurrent tasks per executor
```

**When to Disable:**

| Scenario | Recommendation | Reasoning |
|----------|---------------|-----------|
| General workloads | Enable (default) | Automatic optimization |
| Known uniform large partitions | Enable | No overhead, handles variations |
| Debugging partition issues | Temporarily disable | Isolate coalescing effects |
| Very specific partition needs | Disable | Manual control required |

**Verification:**
```python
# Check AQE coalescing in Spark UI
df = spark.range(10000000).groupBy(col("id") % 1000).count()

# In Spark UI, look for:
# - "CustomShuffleReader" in query plan
# - Reduced number of tasks in later stages
# - Message: "Coalesced 200 partitions to 50 partitions"
# - Even partition sizes in task metrics
```

**Interview Tips:**
- Addresses the "many small tasks" problem
- Particularly valuable when `spark.sql.shuffle.partitions` is misconfigured
- Works automatically without manual repartitioning
- One of three main AQE optimizations

---

### AQE Additional Features

#### 8.6.5 Local Shuffle Reader Optimization

**Configuration:**
```python
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
```

**How it Works:**
- When map and reduce tasks are on same executor, reads data locally
- Avoids network transfer for co-located partitions
- Converts shuffle read to local read when possible

**Benefits:**
- Reduced network I/O
- Faster data access
- Lower resource usage

---

### AQE Prerequisites & Best Practices

#### 8.6.6 Prerequisites for Effective AQE

1. **Spark Version**: Spark 3.0 or later
2. **Configuration**: AQE enabled (default in 3.2+)
3. **Statistics**: Table statistics help initial planning
4. **Shuffle Operations**: Queries need shuffle operations
5. **Memory Resources**: Adequate executor memory
6. **Query Characteristics**: Joins with optimization potential

**Setup Example:**
```python
# Optimal AQE setup
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
spark.conf.set("spark.sql.adaptive.autoBroadcastJoinThreshold", "10485760")

# Ensure table statistics
spark.sql("ANALYZE TABLE fact_table COMPUTE STATISTICS")
spark.sql("ANALYZE TABLE dim_table COMPUTE STATISTICS")
```

#### 8.6.7 When AQE Provides Maximum Benefit

**High-Benefit Scenarios:**
- Join-heavy queries with potential for strategy switching
- Aggregation queries with shuffle operations
- Queries with data skew in joins or groupings
- Queries where table statistics are stale or inaccurate
- Multi-stage ETL pipelines with multiple shuffles

**Example:**
```python
# Query benefiting from multiple AQE optimizations
result = (spark.table("sales_fact")
    .filter("date >= '2024-01-01'")
    .join(spark.table("dim_products"), "product_id")  # Strategy switching
    .join(spark.table("dim_customers"), "customer_id")  # Skew handling
    .groupBy("category", "region")  # Partition coalescing
    .agg(sum("revenue").alias("total_revenue"))
)
```

#### 8.6.8 When AQE Might Not Help (or Hurt)

**Potential Issues:**

| Scenario | Impact | Mitigation |
|----------|--------|-----------|
| Very simple queries | Minor overhead may outweigh benefits | Disable for trivial queries |
| Extremely large metadata | Expensive statistics collection | Monitor and tune |
| Frequently changing data | Runtime stats may become stale | Use conservative settings |
| Memory-constrained clusters | Broadcasting may cause OOM | Reduce broadcast thresholds |

**Overhead Analysis:**

| Overhead Type | Typical Impact | Management |
|--------------|---------------|------------|
| **Planning Time** | 5-10% increase | Usually offset by execution gains |
| **Memory Usage** | Minor for statistics | Monitor executor memory |
| **CPU Usage** | Minimal for adaptive logic | Negligible in most cases |
| **Network I/O** | Statistics collection | Very small vs shuffle |

---

### AQE Debugging & Verification

#### 8.6.9 How to Verify AQE is Working

**1. Query Plan Examination:**
```python
# Check for AQE indicators
df.explain("formatted")

# Look for:
# - "AdaptiveSparkPlan isFinalPlan=true/false"
# - "CustomShuffleReader"
# - "SkewJoin"
# - "BroadcastHashJoin" (after conversion)
```

**2. Spark UI Analysis:**
- **SQL Tab**: Look for "AdaptiveSparkPlan"
- **Details Tab**: Check "isFinalPlan" flag
- **Stages Tab**: Even task distribution
- **Metrics**: Reduced shuffle, balanced execution

**3. Enable Debug Logging:**
```python
spark.conf.set("spark.sql.adaptive.logLevel", "DEBUG")

# Check logs for messages like:
# "INFO AdaptiveSparkPlan: Final plan: BroadcastHashJoin ..."
# "INFO AdaptiveSparkPlan: Applied skew join optimization"
# "INFO AdaptiveSparkPlan: Coalesced 200 partitions to 50"
```

**4. Comparison Method:**
```python
# Run query with and without AQE to compare
import time

# Without AQE
spark.conf.set("spark.sql.adaptive.enabled", "false")
start = time.time()
result1 = query.count()
time_without_aqe = time.time() - start

# With AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")
start = time.time()
result2 = query.count()
time_with_aqe = time.time() - start

print(f"Without AQE: {time_without_aqe:.2f}s")
print(f"With AQE: {time_with_aqe:.2f}s")
print(f"Improvement: {time_without_aqe/time_with_aqe:.1f}x")
```

**Key Success Metrics:**

| Metric | Indicates Success When |
|--------|----------------------|
| Reduced task count | After partition coalescing |
| Balanced task times | After skew handling |
| Join strategy changes | Sort-merge → broadcast |
| Reduced shuffle metrics | Less data movement |
| Improved query duration | Faster overall execution |

---

### AQE with Other Optimizations

#### 8.6.10 Can AQE Optimizations Be Combined?

**Answer:** Yes, AQE optimizations work together synergistically.

**Combination Examples:**

1. **Coalescing + Skew Handling:**
```python
# AQE can first coalesce many small partitions
# Then handle any remaining skewed partitions
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

2. **Join Switching + Coalescing:**
```python
# AQE can switch join strategy AND coalesce partitions
# Sort-merge → Broadcast join + partition coalescing
```

3. **All Optimizations Together:**
```python
# Enable all AQE features
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")

# Complex query benefiting from multiple optimizations
large_fact = spark.table("sales")  # 100GB, partitioned by date
small_dim = spark.table("products").filter("category = 'Electronics'")  # 50MB

result = (large_fact
    .filter("date >= '2024-01-01'")  # Reduces to 10GB
    .join(small_dim, "product_id")   # Strategy switching
    .groupBy("store_id", "product_id")  # Partition coalescing, skew handling
    .agg(sum("amount").alias("total_sales"))
)

# AQE optimizations applied:
# 1. Join strategy switching: Sort-merge → Broadcast
# 2. Partition coalescing: After groupBy shuffle
# 3. Skew handling: If any keys are skewed
```

**Synergistic Benefits:**
- Faster execution (each optimization contributes)
- Better resource usage (balanced workload)
- Automatic adaptation (handles varying data characteristics)

#### 8.6.11 AQE with DPP (Dynamic Partition Pruning)

**Interaction:**
- DPP happens first: Filters partitions at scan time
- AQE optimizes second: Optimizes the pruned dataset
- Synergistic effect: DPP reduces volume, AQE optimizes reduced data

**Configuration:**
```python
# Both enabled by default in Spark 3.0+
spark.conf.set("spark.sql.adaptive.enabled", "true")  # AQE
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")  # DPP
```

**Execution Order:**
```
Query → DPP (Partition Pruning) → Reduced Data → AQE (Runtime Optimization) → Result
```

---

### AQE Interview Tips Summary

**Key Points to Remember:**
1. AQE is enabled by default in Spark 3.2+
2. Three main optimizations: join switching, skew handling, partition coalescing
3. Works at runtime using actual statistics
4. Minimal overhead, significant benefits
5. Combines well with other optimizations (DPP, predicate pushdown)

**Common Interview Questions:**
- "What's the difference between static and adaptive optimization?"
- "How does AQE detect and handle data skew?"
- "When would you disable AQE?"
- "How do you verify AQE is working?"

**Real-World Impact:**
- 2-10x performance improvements common
- Particularly valuable for complex analytical queries
- Reduces need for manual query tuning
- Handles outdated statistics automatically

---

## Dynamic Partition Pruning (DPP)

### Overview & Core Concepts

#### 8.7.1 What is Dynamic Partition Pruning (DPP)?

**Answer:** Dynamic Partition Pruning (DPP) is a Spark optimization that uses join conditions to eliminate unnecessary partitions from scanned tables at runtime, significantly reducing I/O and processing overhead.

**How It Works:**
DPP leverages the relationship between dimension and fact tables in star schemas. When a dimension table is filtered, DPP uses those filters to determine which fact table partitions are relevant, skipping the rest during scan.

**Classic Example:**
```python
# Star schema pattern
fact_sales = spark.table("sales")  # Partitioned by date_id, store_id (1TB)
dim_date = spark.table("date_dim").filter("year = 2024 AND quarter = 'Q1'")  # 1000 rows

# Without DPP: Scans ALL sales partitions (1TB)
# With DPP: Only scans Q1 2024 partitions (~250GB)
result = fact_sales.join(dim_date, "date_id")
```

**Optimization Process:**

1. **Dimension Processing**: Execute filters on dimension table
2. **Key Extraction**: Extract distinct partition keys from filtered dimension
3. **Partition Pruning**: Use keys to skip irrelevant fact partitions
4. **Efficient Join**: Join only the relevant subset

**Visual Example:**
```
Fact Table Partitions:
├── date_id=2023-12-31 ❌ Skipped (not in 2024)
├── date_id=2024-01-01 ✅ Kept (Q1 2024)
├── date_id=2024-01-02 ✅ Kept (Q1 2024)
├── ...
├── date_id=2024-03-31 ✅ Kept (Q1 2024)
├── date_id=2024-04-01 ❌ Skipped (not Q1)
└── date_id=2024-12-31 ❌ Skipped (not Q1)
```

**Key Benefits:**
- **I/O Reduction**: Skips reading entire partition files from storage
- **Memory Savings**: Less data loaded into memory
- **Faster Processing**: Fewer records in downstream operations
- **Network Efficiency**: Less data shuffled across cluster

---

### DPP vs Static Partition Pruning

#### 8.7.2 How does DPP differ from static partition pruning?

**Comparison:**

| Aspect | Static Partition Pruning | Dynamic Partition Pruning |
|--------|------------------------|--------------------------|
| **Trigger** | Explicit WHERE clauses | Join conditions with filtered dimension |
| **Timing** | Query compilation | Runtime optimization |
| **Scope** | Single table | Cross-table inference |
| **Data Source** | Query predicates | Dimension table filters |
| **Example** | `WHERE date = '2024-01-01'` | Uses dim filters to prune fact partitions |

**Code Examples:**
```python
# Static Partition Pruning (explicit filter)
static_result = spark.table("sales").filter("sale_date = '2024-01-01'")
# Prunes based on explicit predicate

# Dynamic Partition Pruning (inferred from join)
dim_filtered = spark.table("stores").filter("region = 'West'")
dynamic_result = spark.table("sales").join(dim_filtered, "store_id")
# DPP infers: only read sales partitions with West region stores
```

**Both Can Work Together:**
```python
# Static pruning on date + DPP on store
result = (spark.table("sales")
    .filter("date >= '2024-01-01'")  # Static pruning
    .join(dim_stores.filter("region = 'West'"), "store_id")  # DPP
)
```

---

### DPP Configuration & Enablement

#### 8.7.3 How do you enable Dynamic Partition Pruning?

**Master Switch:**
```python
# Enable DPP (enabled by default in Spark 3.0+)
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
```

**Complete Configuration:**
```python
# Core DPP settings
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.useStats", "true")

# Tuning parameters
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio", "0.5")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly", "true")

# Related optimizations
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760")  # 10MB
```

**Configuration Parameters Explained:**

**1. spark.sql.optimizer.dynamicPartitionPruning.useStats**
- **Default**: `true`
- **Purpose**: Use table statistics for cost-based DPP decisions
- **Impact**: If statistics indicate low selectivity, might skip DPP

```python
# With useStats=true, Spark checks:
# - Dimension table size after filtering
# - Fact table partition statistics
# - Selectivity of the partition filter
# Then decides if DPP is worth applying

# Disable if statistics are outdated
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.useStats", "false")
```

**2. spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio**
- **Default**: `0.5` (50%)
- **Purpose**: Default selectivity when statistics unavailable
- **Tuning**:
  - Lower (0.3): More conservative DPP
  - Higher (0.8): More aggressive DPP

```python
# Conservative: Apply DPP less often
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio", "0.3")

# Aggressive: Apply DPP more often
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio", "0.8")
```

**3. spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly**
- **Default**: `true` (in some versions)
- **Purpose**: Restrict DPP to scenarios with broadcast joins
- **Why**: Reduces overhead in shuffle-based joins

```python
# Only apply DPP with broadcast joins (more efficient)
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly", "true")

# Allow DPP in all join types (more coverage)
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly", "false")
```

---

### DPP Requirements & Scenarios

#### 8.7.4 What conditions must be met for DPP to work?

**Mandatory Conditions:**

1. **Partitioned Table**: At least one table must be partitioned
2. **Join Condition**: Equi-join on partition key or related column
3. **Selective Filter**: Dimension table has filters that eliminate partitions
4. **Statistics Available**: Table statistics help optimization decisions

**Example Setup:**
```python
# Good candidate for DPP
fact_table = spark.table("sales")  # Partitioned by (date_id, store_id)
dim_table = spark.table("stores").filter("region = 'West'")  # Selective filter

# Join on partition key
result = fact_table.join(dim_table, "store_id")  # store_id is partition key
```

**When DPP Won't Work:**
- ❌ Non-partitioned tables
- ❌ Non-equi joins (`>`, `<`, `BETWEEN`)
- ❌ No selective filters on dimension
- ❌ Disabled configuration
- ❌ Complex join conditions (OR, expressions)

#### 8.7.5 Ideal Scenarios for Maximum DPP Benefit

**1. Star Schema Data Warehouse:**
```python
# Classic data warehouse pattern
fact_sales = spark.table("sales_fact")  # 100GB, partitioned by date_id
dim_date = spark.table("date_dim").filter("year = 2024 AND quarter = 'Q1'")
dim_stores = spark.table("store_dim").filter("region = 'Northeast'")

# DPP benefits:
# - date_dim filter prunes sales partitions outside Q1 2024
# - store_dim filter prunes sales partitions for other regions
result = (fact_sales
    .join(dim_date, "date_id")
    .join(dim_stores, "store_id")
)
```

**2. Time-Series Analysis:**
```python
# Analyzing specific time periods
events = spark.table("user_events")  # Partitioned by event_date
date_range = spark.table("dates").filter("date BETWEEN '2024-01-01' AND '2024-01-07'")

# DPP: Only scan events from specific week
weekly_events = events.join(date_range, events.event_date == date_range.date)
```

**3. Selective Business Queries:**
```python
# Filtering on specific business entities
transactions = spark.table("transactions")  # Partitioned by customer_id
premium_customers = spark.table("customers").filter("tier = 'premium'")

# DPP: Only scan transactions for premium customers
premium_txns = transactions.join(premium_customers, "customer_id")
```

**Performance Impact Estimation:**
```python
def estimate_dpp_benefit(selectivity_ratio):
    """
    Estimate DPP performance benefit
    selectivity_ratio: fraction of partitions scanned (0.0 to 1.0)
    """
    improvement = 1.0 / selectivity_ratio
    print(f"Filter selectivity: {selectivity_ratio:.1%}")
    print(f"Partitions scanned: {selectivity_ratio:.1%}")
    print(f"Performance improvement: {improvement:.1f}x")
    print(f"Speedup: {(improvement - 1) * 100:.0f}% faster")
    return improvement

# Examples:
estimate_dpp_benefit(0.05)   # 5% partitions → 20x improvement
estimate_dpp_benefit(0.25)   # 25% partitions → 4x improvement
estimate_dpp_benefit(0.75)   # 75% partitions → 1.3x improvement
```

---

### DPP Technical Implementation

#### 8.7.6 How does DPP work in fact-dimension joins?

**Step-by-Step Process:**

1. **Dimension Processing:**
```python
# Execute dimension query first
dim_filtered = dim_table.filter("region = 'West'")
dim_keys = dim_filtered.select("store_id").distinct()
# Result: [101, 102, 105, 107, ...]
```

2. **Filter Creation:**
```python
# Create dynamic filter from dimension keys
# Spark generates subquery like:
# SELECT DISTINCT store_id FROM stores WHERE region = 'West'
```

3. **Fact Table Scan:**
```python
# Apply filter during fact table scan
# Only read partitions containing these store_ids
fact_filtered = fact_table.filter(fact_table.store_id.isin(dim_keys))
```

4. **Join Execution:**
```python
# Perform join on pre-filtered data
result = fact_filtered.join(dim_filtered, "store_id")
```

**Under the Hood - Generated Subquery:**
```sql
-- Spark internally creates:
SELECT f.*, d.* 
FROM sales_fact f
JOIN store_dim d ON f.store_id = d.store_id
WHERE f.store_id IN (
    SELECT store_id FROM store_dim WHERE region = 'West'
)
AND f.date_id IN (
    SELECT date_id FROM date_dim WHERE year = 2024
)
```

**Multi-Level Partitioning:**
```python
# Table partitioned by year → month → day
fact_table = spark.table("sales")  # Partitioned by (year, month, day)
dim_date = spark.table("date_dim").filter("year = 2024 AND quarter = 'Q1'")

result = fact_table.join(dim_date, "date_id")

# DPP prunes at multiple levels:
# - year != 2024 → Skip entire year directories
# - month not in [1,2,3] → Skip non-Q1 months
# - day filters within Q1 months
```

**Partition Directory Structure:**
```
sales/
├── year=2023/          ❌ Skipped (not 2024)
│   └── month=12/
├── year=2024/
│   ├── month=1/        ✅ Kept (Q1)
│   ├── month=2/        ✅ Kept (Q1)
│   ├── month=3/        ✅ Kept (Q1)
│   ├── month=4/        ❌ Skipped (not Q1)
│   └── month=5/        ❌ Skipped (not Q1)
└── year=2025/          ❌ Skipped (not 2024)
```

---

### DPP Verification & Debugging

#### 8.7.7 How to verify DPP is working?

**Method 1: Query Plan Examination**
```python
df = fact_table.join(dim_table.filter("region = 'West'"), "store_id")
df.explain("formatted")

# Look for:
# - "DynamicFileSourceFilter"
# - "PartitionFilters: [isnotnull(...), ...]"
# - "PushedFilters: [In(...), ...]"
# - Subquery indicators
```

**Method 2: Spark UI Analysis**
- **SQL Tab**: Look for "DynamicPartitionPruning"
- **Details Tab**: Check scan metrics for reduced partitions
- **Storage Tab**: Fewer partitions scanned than available

**Method 3: Enable Detailed Logging**
```python
spark.conf.set("spark.sql.adaptive.logLevel", "DEBUG")

# Execute query
dim_filtered = dim_table.filter("region = 'West'")
result = fact_table.join(dim_filtered, "store_id")

# Check plan
result.explain()
```

**Method 4: Verification Function**
```python
def verify_dpp_application(query_df):
    """Verify DPP is being applied"""
    plan = query_df._jdf.queryExecution().toString()
    
    dpp_indicators = [
        "DynamicFileSourceFilter",
        "Subquery",
        "partitionFilters",
        "PushedFilters"
    ]
    
    applied = any(indicator in plan for indicator in dpp_indicators)
    
    if applied:
        print("✓ DPP optimization detected")
        for line in plan.split('\n'):
            if any(ind in line for ind in dpp_indicators):
                print(f"  {line.strip()}")
    else:
        print("✗ No DPP optimization detected")
    
    return applied

# Usage
dim_filtered = dim_table.filter("region = 'West'")
result = fact_table.join(dim_filtered, "store_id")
dpp_applied = verify_dpp_application(result)
```

**What "DynamicFileSourceFilter" Indicates:**
```
Physical Plan:
*(1) BroadcastHashJoin [store_id#10], [store_id#20]
├── *(1) Scan parquet default.sales
│   ├── DynamicFileSourceFilter: [isnotnull(store_id#10), 
│   │                              (store_id#10 IN (subquery#5))]
│   └── Subquery subquery#5
│       └── BroadcastExchange
│           └── *(1) Filter (region#22 = West)
│               └── *(1) Scan parquet default.stores
└── BroadcastExchange
    └── *(1) Filter (region#22 = West)
        └── *(1) Scan parquet default.stores
```

This clearly shows:
- DPP is active (`DynamicFileSourceFilter`)
- Subquery filtering fact table
- Reduced partition scan

---

### DPP Performance & Limitations

#### 8.7.8 Performance Impact of DPP

**Improvement Range:**

| Scenario | Partitions Before | Partitions After | Improvement |
|----------|------------------|-----------------|-------------|
| Daily partitions, monthly filter | 365 | 30 | 12x |
| Store partitions, regional filter | 1000 | 50 | 20x |
| Product partitions, category filter | 10000 | 100 | 100x |
| Single day from years | 1825 (5 years) | 1 | 1825x |

**Real-World Example:**
```python
import time

# Measure DPP impact
start = time.time()
result = fact_table.join(dim_table.filter("region = 'West'"), "store_id")
count = result.count()
elapsed = time.time() - start

# Typical results:
# Without DPP: 300 seconds (scan all partitions)
# With DPP: 15 seconds (scan only relevant partitions)
# Improvement: 20x faster
```

#### 8.7.9 Limitations of DPP

**Key Limitations:**

1. **Partitioning Requirement:**
```python
# DPP only works with partitioned tables
non_partitioned.join(dim_table, "key")  # ❌ No DPP benefit
```

2. **Equi-Join Restriction:**
```python
# Only works with equality conditions
fact.join(dim, fact.value > dim.min_value)  # ❌ No DPP
fact.join(dim, fact.key == dim.key)  # ✅ DPP works
```

3. **Complex Join Conditions:**
```python
# Limited support for complex conditions
fact.join(dim, 
    (fact.key1 == dim.key1) | (fact.key2 == dim.key2))  # ⚠️ Limited DPP
```

4. **Statistics Dependency:**
```python
# Outdated statistics can lead to poor decisions
# Solution: Regularly update statistics
spark.sql("ANALYZE TABLE sales COMPUTE STATISTICS")
```

5. **Selectivity Threshold:**
```python
# Low-selectivity filters provide minimal benefit
dim.filter("country = 'US'")  # If 90% data is US, limited benefit
```

**Workarounds:**
```python
# For non-partitioned tables: Consider partitioning
spark.sql("""
    CREATE TABLE sales_partitioned
    PARTITIONED BY (store_id)
    AS SELECT * FROM sales
""")

# For complex conditions: Break into multiple queries
step1 = fact.join(dim, "key1")
step2 = fact.join(dim, "key2")
result = step1.union(step2).distinct()
```

---

### DPP with Different Join Types

#### 8.7.10 Does DPP work with bucketed tables?

**Answer:** Yes, DPP can work with bucketed tables, though with different characteristics.

```python
# Create bucketed tables
fact_table.write.bucketBy(50, "store_id").saveAsTable("bucketed_sales")
dim_table.write.bucketBy(50, "store_id").saveAsTable("bucketed_stores")

# DPP can prune buckets during join
result = spark.table("bucketed_sales").join(
    spark.table("bucketed_stores").filter("region = 'West'"),
    "store_id"
)
```

**Comparison:**

| Aspect | Partitioned DPP | Bucketed DPP |
|--------|----------------|-------------|
| **Pruning Unit** | Entire partitions | Individual buckets |
| **I/O Benefit** | High (skip files) | Moderate (skip some files) |
| **Setup** | Physical directories | File organization |
| **Use Case** | Time-series data | Frequent join optimization |

#### 8.7.11 Does DPP work with non-partitioned tables?

**Answer:** No, DPP requires partitioned tables to provide meaningful benefits.

**Why Partitioning is Required:**
```python
# Non-partitioned: All data in few files
non_partitioned = spark.table("sales")  # 100GB in 10 files
# DPP cannot skip - must read all files

# Partitioned: Data split across many files
partitioned = spark.table("sales_partitioned")  # 100GB in 1000 partitions
# DPP can skip 950 partitions → read only 5GB
```

**Alternative for Non-Partitioned Tables:**
```python
# Consider bucketing for join optimization
df.write.bucketBy(50, "key").saveAsTable("bucketed_table")
```

---

### DPP with Other Optimizations

#### 8.7.12 Relationship Between DPP and Predicate Pushdown

**Comparison:**

| Optimization | Level | Mechanism | Benefit |
|--------------|-------|-----------|---------|
| **DPP** | Partition level | Skip entire partitions | I/O reduction |
| **Predicate Pushdown** | Row level | Filter during file scan | CPU reduction |

**Working Together:**
```python
fact_table = spark.table("sales")  # Partitioned by date_id
dim_table = spark.table("products").filter("category = 'Electronics'")

result = (fact_table
    .join(dim_table, "product_id")  # DPP eliminates partitions
    .filter("sale_amount > 1000")   # Predicate pushdown filters rows
)

# Optimizations applied:
# 1. DPP: Skip non-Electronics product partitions
# 2. Predicate Pushdown: Apply sale_amount filter during scan
# 3. Combined: Minimal I/O + efficient row filtering
```

#### 8.7.13 Can DPP and AQE Be Combined?

**Answer:** Yes, they are complementary and work well together.

**Execution Order:**
```
Query → DPP (Partition Pruning) → Reduced Data → AQE (Runtime Optimization) → Result
```

**Example:**
```python
# Enable both
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")

# Query benefits from both
result = (large_fact
    .join(small_dim.filter("active = true"), "key")  # DPP opportunity
    .groupBy("category")
    .agg(sum("value").alias("total"))
)

# Optimization sequence:
# 1. DPP: Prune fact table partitions
# 2. AQE: Switch join strategy if appropriate
# 3. AQE: Handle skew in aggregation
# 4. AQE: Coalesce partitions after shuffle
```

**Benefits:**
- DPP reduces data volume early
- AQE optimizes the reduced dataset
- Combined improvement > individual optimizations

#### 8.7.14 How does DPP affect shuffle operations?

**Impact on Shuffle:**

1. **Reduced Shuffle Write:**
```python
# Without DPP: Shuffle all fact data (100GB)
# With DPP: Shuffle only relevant fact data (5GB)
# Shuffle reduction: 95%
```

2. **Network Efficiency:**
- Less data transferred over network
- Faster shuffle phases
- Reduced network congestion

3. **Memory Benefits:**
- Smaller shuffle buffers needed
- Reduced spill to disk
- Better executor memory utilization

**Example:**
```python
fact_table = spark.table("sales")  # 100GB, 1000 partitions
dim_table = spark.table("stores").filter("region = 'West'")  # Filters to 50 stores

# Shuffle impact:
# Without DPP: ~100GB shuffled
# With DPP: ~5GB shuffled (50/1000 partitions)
```

---

### DPP Statistics & Maintenance

#### 8.7.15 What statistics are needed for DPP?

**Required Statistics:**

**1. Table Statistics:**
```python
# Basic table stats
spark.sql("ANALYZE TABLE sales COMPUTE STATISTICS")
# Provides: numRows, totalSize, etc.
```

**2. Column Statistics:**
```python
# Column-level stats for partition keys
spark.sql("""
    ANALYZE TABLE sales 
    COMPUTE STATISTICS FOR COLUMNS store_id, date_id
""")
# Provides: distinctCount, min, max, nullCount
```

**3. Partition Statistics:**
```python
# Partition-level statistics
spark.sql("""
    ANALYZE TABLE sales PARTITION (year=2024) 
    COMPUTE STATISTICS
""")
```

**Complete Statistics Strategy:**
```python
# Comprehensive setup
tables = ["sales_fact", "store_dim", "date_dim", "product_dim"]

for table in tables:
    # Table-level stats
    spark.sql(f"ANALYZE TABLE {table} COMPUTE STATISTICS")
    
    # Column-level stats for join keys
    spark.sql(f"""
        ANALYZE TABLE {table} 
        COMPUTE STATISTICS FOR COLUMNS key_col1, key_col2
    """)

# Regular maintenance (daily/weekly)
# Automated in production pipelines
```

**Statistics Usage:**
- Cost-based optimization decisions
- Selectivity estimation
- Join strategy selection
- DPP benefit prediction

---

### DPP Interview Tips Summary

**Key Points:**
1. DPP eliminates partitions at scan time using join filters
2. Requires partitioned tables and equi-joins
3. Works best in star schema data warehouses
4. Can provide 10-100x improvements for selective queries
5. Enabled by default in Spark 3.0+

**Common Questions:**
- "How does DPP differ from static pruning?"
- "What are the requirements for DPP?"
- "How do you verify DPP is working?"
- "When does DPP provide maximum benefit?"

**Real-World Impact:**
- Dramatic I/O reduction (50-99%)
- Particularly valuable for large fact tables
- Works synergistically with AQE
- Reduces query costs significantly

---

## Shuffle and Partitioning

### Shuffle Fundamentals

#### 9.1.1 What are shuffle operations in Spark?

**Answer:** Shuffle operations are data redistribution processes that involve moving data across partitions, typically required for operations that need to group or aggregate data by key.

**Why Shuffles are Expensive:**
- Data serialization and deserialization
- Network transfer between executors
- Disk I/O for spill operations
- Data sorting and merging

**Common Shuffle Operations:**
```python
# Operations that trigger shuffle
df.groupBy("department").agg(sum("salary"))        # GROUP BY
df1.join(df2, "key")                               # JOIN
df.distinct()                                      # DISTINCT
df.orderBy("salary")                               # ORDER BY
df.repartition(100)                                # REPARTITION
df.coalesce(10)                                    # COALESCE (sometimes)
```

**Shuffle Process Breakdown:**

1. **Map Phase**: Each task writes shuffle data to local disk
2. **Shuffle Write**: Data organized by partition, written to shuffle files
3. **Shuffle Read**: Subsequent tasks read their assigned partitions
4. **Reduce Phase**: Data processed after shuffle

**Visualization:**
```
Executor 1:                  Executor 2:
Partition 1 → [Shuffle Write] → Partition 1 → [Reduce]
Partition 2 → [Shuffle Write] → Partition 2 → [Reduce]
                ↓ Network ↓
Executor 3:                  Executor 4:
Partition 3 → [Shuffle Write] → Partition 3 → [Reduce]
Partition 4 → [Shuffle Write] → Partition 4 → [Reduce]
```

---

#### 9.1.2 Shuffle-Sort in GROUP BY Context

**Process:**

1. **Map Side (Partial Aggregation):**
```python
# Each partition groups data locally
# Partition 1: {IT: [5000, 6000], Sales: [4000]}
# Partition 2: {IT: [7000], Sales: [4500, 4800]}
```

2. **Shuffle Phase:**
```python
# Data shuffled by grouping key
# All IT salaries → Same partition
# All Sales salaries → Same partition
```

3. **Sort and Reduce:**
```python
# Each partition sorts data by key
# Then performs final aggregation
# Result: {IT: 18000, Sales: 13300}
```

**Visual Example:**
```
Before Shuffle:
Partition 1: [IT:5000, Sales:4000, IT:6000]
Partition 2: [IT:7000, Sales:4500, Sales:4800]

After Shuffle-Sort:
Partition 1: [IT:5000, IT:6000, IT:7000] → IT:18000
Partition 2: [Sales:4000, Sales:4500, Sales:4800] → Sales:13300
```

---

#### 9.1.3 Shuffle-Sort in JOIN Context

**Sort-Merge Join Process:**

1. **Shuffle Both Tables:**
```python
# Both tables shuffled by join key
# Same keys go to same partitions
```

2. **Sort Within Partitions:**
```python
# Each partition sorts data by join key
# Enables efficient merge join
```

3. **Merge Join:**
```python
# Sorted streams merged like zipper
# Matching records joined efficiently
```

**Example:**
```python
df1 = spark.createDataFrame([(1, "A"), (2, "B"), (3, "C")], ["id", "value1"])
df2 = spark.createDataFrame([(1, "X"), (2, "Y"), (4, "Z")], ["id", "value2"])

result = df1.join(df2, "id")
```

**Shuffle-Sort Visualization:**
```
df1 Before:           df2 Before:
P1: [1:A, 3:C]       P1: [1:X, 4:Z]
P2: [2:B]            P2: [2:Y]

After Shuffle-Sort:
Partition 1: df1:[1:A], df2:[1:X] → Join: (1, A, X)
Partition 2: df1:[2:B], df2:[2:Y] → Join: (2, B, Y)
Partition 3: df1:[3:C], df2:[]    → No match
Partition 4: df1:[],    df2:[4:Z] → No match
```

---

### Partition Configuration

#### 9.2.1 What is spark.sql.shuffle.partitions?

**Answer:** Controls the number of partitions created after shuffle operations, directly affecting parallelism and task distribution.

**Default Value:** 200

**Configuration:**
```python
# Set shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", 200)

# This affects:
# - Number of tasks in stages after shuffle
# - Data distribution across cluster
# - Memory usage per task
# - Task scheduling overhead
```

**Impact Analysis:**

| Partition Count | Pros | Cons |
|----------------|------|------|
| **Too Low (< cores)** | Less overhead | Potential skew, memory issues |
| **Optimal (2-4x cores)** | Balanced workload | Good resource utilization |
| **Too High (>> cores)** | Fine-grained parallelism | High overhead, many small tasks |

**Tuning Guidance:**

**Method 1: Based on Cluster Resources**
```python
# Rule of thumb: partitions = total_cores * 2-4
total_cores = spark.sparkContext.defaultParallelism
optimal_partitions = total_cores * 3
spark.conf.set("spark.sql.shuffle.partitions", optimal_partitions)

# Example: 10 executors × 4 cores = 40 cores
# Recommended: 80-160 partitions
```

**Method 2: Based on Data Size**
```python
# Aim for 100-200MB per partition
estimated_data_size_mb = 10 * 1024  # 10GB
target_partition_size_mb = 128
optimal_partitions = ceil(estimated_data_size_mb / target_partition_size_mb)

spark.conf.set("spark.sql.shuffle.partitions", optimal_partitions)
```

---

#### 9.2.2 Recommended Values for Different Workload Sizes

| Workload Size | Data Volume | Recommended Partitions | Reasoning |
|--------------|-------------|----------------------|-----------|
| **Small** | < 10GB | 50-100 | Avoid overhead, sufficient parallelism |
| **Medium** | 10GB-100GB | 100-200 | Balanced, default is reasonable |
| **Large** | 100GB-1TB | 200-500 | Increased parallelism |
| **Very Large** | > 1TB | 500-1000 | Maximum parallelism, avoid large partitions |

**Dynamic Tuning Example:**
```python
def calculate_optimal_partitions(estimated_data_gb, executor_cores):
    """Calculate optimal shuffle partitions"""
    # Method 1: Data-based
    target_partition_size_mb = 128
    data_based = ceil((estimated_data_gb * 1024) / target_partition_size_mb)
    
    # Method 2: Core-based
    total_cores = executor_cores
    core_based = total_cores * 3
    
    # Use the larger value, cap at 1000
    optimal = min(max(data_based, core_based), 1000)
    
    print(f"Data-based: {data_based}")
    print(f"Core-based: {core_based}")
    print(f"Recommended: {optimal}")
    
    return optimal

# Usage
partitions = calculate_optimal_partitions(50, 40)  # 50GB, 40 cores
spark.conf.set("spark.sql.shuffle.partitions", partitions)
```

---

### Shuffle Buffer & Memory

#### 9.2.3 What is the shuffle buffer and what does it control?

**Answer:** The shuffle buffer is memory used during shuffle operations to buffer data before writing to disk, affecting I/O efficiency and spill behavior.

**Key Configurations:**

**1. Shuffle File Buffer:**
```python
# Controls size of in-memory buffer for shuffle files
spark.conf.set("spark.shuffle.file.buffer", "32k")  # Default: 32KB
```

**2. Shuffle Memory Fraction:**
```python
# Fraction of heap for shuffle aggregation and storage
spark.conf.set("spark.shuffle.memoryFraction", "0.2")  # Default: 0.2 (20%)
```

**3. Shuffle Spill Thresholds:**
```python
# Controls when data spills to disk
spark.conf.set("spark.shuffle.spill.initialMemoryThreshold", "5m")
spark.conf.set("spark.shuffle.spill.numElementsForceSpillThreshold", "1000000")
```

**Buffer Size Impact:**

| Configuration | Small Buffer | Large Buffer | Optimal |
|--------------|--------------|--------------|---------|
| **Disk I/O** | More frequent | Less frequent | Balanced |
| **Memory Usage** | Lower | Higher | Efficient |
| **Performance** | Potential bottleneck | Better throughput | Best overall |

**Tuning Recommendations:**
```python
# For memory-rich environments
spark.conf.set("spark.shuffle.file.buffer", "64k")  # Double default
spark.conf.set("spark.shuffle.memoryFraction", "0.3")  # 30% for shuffle

# For shuffle-heavy workloads
spark.conf.set("spark.shuffle.memoryFraction", "0.35")
spark.conf.set("spark.memory.fraction", "0.6")  # Overall Spark memory
```

---

### Partition Tuning

#### 9.3.1 What is partition tuning and why is it crucial?

**Answer:** Partition tuning is the process of optimizing the number, size, and distribution of data partitions to achieve optimal performance, resource utilization, and parallelism.

**Why It's Crucial:**

**1. Performance Impact:**
```python
# Poor partitioning leads to:
# - Skewed workloads (straggler tasks)
# - Memory issues (large partitions → spills)
# - Resource waste (idle executors)
```

**2. Resource Efficiency:**
```python
# Optimal partitioning ensures:
# - All executors utilized
# - Tasks complete in similar timeframes
# - Memory usage balanced
```

**3. Cost Optimization:**
```python
# Better partitioning = faster jobs = lower cloud costs
# Reduced spill to disk = less I/O cost
```

**Partition Tuning Strategies:**
```python
# 1. Repartitioning for optimal size
df_optimized = df.repartition(200)

# 2. Partitioning by key for joins
df_partitioned = df.repartition("user_id")

# 3. Controlling shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", 200)

# 4. Monitoring partition metrics
num_partitions = df.rdd.getNumPartitions()
```

---

#### 9.3.2 Finding the Balance: Parallelism vs Overhead

**Balancing Factors:**

**1. Data Size Considerations:**
```python
def calculate_optimal_partitions(df, target_size_mb=128):
    """Calculate optimal partitions based on data size"""
    # Rough estimation
    row_count = df.count()
    sample_size = df.limit(1000).rdd.map(lambda x: len(str(x))).sum()
    avg_row_size = sample_size / 1000  # bytes
    total_size_mb = (row_count * avg_row_size) / (1024 * 1024)
    
    optimal = max(1, int(total_size_mb / target_size_mb))
    return min(optimal, 1000)  # Cap at reasonable limit

optimal_partitions = calculate_optimal_partitions(df)
df_optimized = df.repartition(optimal_partitions)
```

**2. Cluster Resource Analysis:**
```python
# Based on available resources
num_executors = int(spark.conf.get("spark.executor.instances", "1"))
cores_per_executor = int(spark.conf.get("spark.executor.cores", "1"))
total_cores = num_executors * cores_per_executor

# Optimal range: 2-4x total cores
min_partitions = total_cores * 2
max_partitions = total_cores * 4

print(f"Optimal partition range: {min_partitions}-{max_partitions}")
```

**Practical Balance Guidelines:**

| Scenario | Approach | Target Partition Size |
|----------|----------|---------------------|
| **Memory-constrained** | Fewer, larger partitions | 200-500MB |
| **I/O-heavy** | More, smaller partitions | 50-100MB |
| **CPU-intensive** | Balanced approach | 100-200MB |
| **General purpose** | Default tuning | 128MB |

**Monitoring for Optimal Balance:**
```python
def analyze_partition_quality(df):
    """Analyze if partitioning is optimal"""
    num_partitions = df.rdd.getNumPartitions()
    
    # Get partition size distribution
    partition_sizes = df.rdd.mapPartitions(
        lambda iter: [sum(1 for _ in iter)]
    ).collect()
    
    if not partition_sizes:
        return False
        
    avg_size = sum(partition_sizes) / num_partitions
    max_size = max(partition_sizes)
    min_size = min(partition_sizes)
    
    skew_ratio = max_size / avg_size if avg_size > 0 else float('inf')
    
    print(f"Partitions: {num_partitions}")
    print(f"Avg size: {avg_size:.1f} rows")
    print(f"Skew ratio: {skew_ratio:.2f}")
    print(f"Size range: {min_size}-{max_size} rows")
    
    # Good if skew < 2x
    return skew_ratio < 2.0

is_balanced = analyze_partition_quality(df)
```

---

### Data Skewness

#### 9.4.1 What is data skewness in Spark?

**Answer:** Data skewness occurs when data is unevenly distributed across partitions, causing some tasks to process significantly more data than others, creating performance bottlenecks.

**Visualization:**
```
Balanced Partitions:
Partition 1: ████ [100MB] - 2 min
Partition 2: ████ [105MB] - 2 min
Partition 3: ████ [ 95MB] - 2 min
Partition 4: ████ [102MB] - 2 min
Total: 8 minutes

Skewed Partitions:
Partition 1: █ [ 10MB] - 1 min
Partition 2: █ [ 15MB] - 1 min
Partition 3: █████████████████ [800MB] - 45 min ← Straggler
Partition 4: █ [ 12MB] - 1 min
Total: 45 minutes (dominated by one task)
```

**Common Causes:**

**1. Natural Data Distribution:**
```python
# Some keys are inherently more common
# - Popular products have most sales
# - Active users generate most events
# - Peak hours have more traffic
```

**2. Poor Key Selection:**
```python
# Using low-cardinality columns as keys
df.groupBy("status")  # Only 3 distinct values
df.repartition("category")  # 90% in one category
```

**3. Hash Partitioning Issues:**
```python
# Hash function collisions
# Certain key patterns hash to same partition
```

**4. Configuration Problems:**
```python
# Too few partitions for data volume
spark.conf.set("spark.sql.shuffle.partitions", 10)  # Too low for GBs
```

---

#### 9.4.2 What causes uneven distribution across partitions?

**Skew Detection Function:**
```python
def detect_skew(df, key_column):
    """Detect data skew in a DataFrame"""
    from pyspark.sql import functions as F
    
    # Analyze key distribution
    key_dist = df.groupBy(key_column).count().orderBy(F.desc("count"))
    
    # Calculate skew metrics
    total_rows = df.count()
    stats = key_dist.agg(
        F.max("count").alias("max_count"),
        F.min("count").alias("min_count"),
        F.avg("count").alias("avg_count"),
        F.count("*").alias("distinct_keys")
    ).collect()[0]
    
    max_count = stats["max_count"]
    avg_count = stats["avg_count"]
    distinct_keys = stats["distinct_keys"]
    
    skew_ratio = max_count / avg_count if avg_count > 0 else float('inf')
    dominance_ratio = max_count / total_rows if total_rows > 0 else 0
    
    print(f"Skew Analysis for {key_column}:")
    print(f"  Total rows: {total_rows:,}")
    print(f"  Distinct keys: {distinct_keys:,}")
    print(f"  Max key count: {max_count:,}")
    print(f"  Average: {avg_count:,.1f}")
    print(f"  Skew ratio: {skew_ratio:.2f}")
    print(f"  Dominance: {dominance_ratio:.2%}")
    
    print("\nTop 10 keys by count:")
    key_dist.show(10, False)
    
    return skew_ratio > 10.0  # Skewed if ratio > 10

# Usage
is_skewed = detect_skew(df, "user_id")
```

---

#### 9.4.3 Interpreting Uneven Shuffle Metrics

**Spark UI Indicators:**
```
Healthy Shuffle:
Task 1: Write 95MB, Read 102MB, Duration 2m
Task 2: Write 102MB, Read 98MB, Duration 2m
Task 3: Write 88MB, Read 105MB, Duration 2m
Task 4: Write 105MB, Read 95MB, Duration 2m

Skewed Shuffle:
Task 1: Write 15MB, Read 12MB, Duration 1m
Task 2: Write 20MB, Read 18MB, Duration 1m
Task 3: Write 800MB, Read 750MB, Duration 45m ← Straggler
Task 4: Write 12MB, Read 10MB, Duration 1m
```

**What to Monitor:**
- Task duration variance (should be < 2x)
- Shuffle read/write size variance (should be < 3x)
- GC time in specific tasks
- Spill metrics

---

### Handling Data Skew

#### 9.5.1 Salting Techniques for Skewed Datasets

**Answer:** Salting involves adding random prefixes to keys to distribute skewed data more evenly across partitions.

**Complete Salting Implementation:**
```python
from pyspark.sql import functions as F
from pyspark.sql.types import IntegerType

def handle_skewed_join(large_df, small_df, join_key, salt_buckets=10):
    """Handle skewed join using salting technique"""
    
    # Step 1: Add salt to large table (skewed side)
    salted_large = large_df.withColumn(
        "salted_key",
        F.concat(
            F.col(join_key),
            F.lit("_"),
            (F.rand() * salt_buckets).cast(IntegerType())
        )
    )
    
    # Step 2: Replicate small table for all salts
    salt_values = [F.lit(i) for i in range(salt_buckets)]
    salted_small = small_df.select(
        F.col(join_key).alias("original_key"),
        F.col("*")
    ).withColumn(
        "salt",
        F.explode(F.array(*salt_values))
    ).withColumn(
        "salted_key",
        F.concat(F.col("original_key"), F.lit("_"), F.col("salt"))
    ).drop("original_key", "salt")
    
    # Step 3: Perform salted join
    joined = salted_large.join(salted_small, "salted_key")
    
    # Step 4: Remove salt and clean up
    result = joined.withColumn(
        join_key,
        F.split(F.col("salted_key"), "_")[0]
    ).drop("salted_key")
    
    return result

# Usage
skewed_join_result = handle_skewed_join(large_table, small_table, "user_id", 10)
```

**Salting Process Visualization:**
```
Original Hot Key: user_id = 12345 (1M records)

After Salting:
user_id_0: 100K records → Partition 1
user_id_1: 100K records → Partition 2
user_id_2: 100K records → Partition 3
...
user_id_9: 100K records → Partition 10

Small Table Replicated 10 times (once for each salt value)
Join performed on salted keys
Results de-salted and combined
```

**Salting Variations:**

**1. Deterministic Salting:**
```python
# Use hash-based deterministic salting
salted_key = F.concat(
    F.col("key"),
    F.lit("_"),
    (F.hash(F.col("key")) % salt_buckets).cast(IntegerType())
)
```

**2. Two-Phase Aggregation:**
```python
# For skewed aggregations
# Phase 1: Aggregate with salt
phase1 = df.withColumn(
    "salted_key",
    F.concat(F.col("key"), F.lit("_"), (F.rand() * 10).cast(IntegerType()))
).groupBy("salted_key").agg(F.sum("value").alias("partial_sum"))

# Phase 2: Remove salt and aggregate again
phase2 = phase1.withColumn(
    "key",
    F.split(F.col("salted_key"), "_")[0]
).groupBy("key").agg(F.sum("partial_sum").alias("total"))
```

---

#### 9.5.2 How Data Skewness Affects Performance

**Performance Impacts:**

**1. Straggler Tasks:**
```python
# One task dominates execution time
# 99 tasks: 1 minute each
# 1 task: 60 minutes
# Total job time: 60 minutes (vs 1 minute if balanced)
```

**2. Resource Wastage:**
```python
# Most executors idle waiting for straggler
# Example: 10 executors
# - 9 idle for 59 minutes
# - 1 working for 60 minutes
# Effective utilization: 10%
```

**3. Memory Pressure:**
```python
# Large partitions may not fit in memory
# Causes:
# - Spill to disk (10-100x slower)
# - OOM errors (job failure)
# - Excessive GC (CPU waste)
```

**Quantitative Impact:**
```python
def calculate_skew_impact(task_times):
    """Calculate performance impact of skew"""
    avg_time = sum(task_times) / len(task_times)
    max_time = max(task_times)
    min_time = min(task_times)
    
    skew_ratio = max_time / avg_time
    efficiency = avg_time / max_time
    
    print(f"Average time: {avg_time:.2f}s")
    print(f"Max time: {max_time:.2f}s (straggler)")
    print(f"Min time: {min_time:.2f}s")
    print(f"Skew ratio: {skew_ratio:.2f}x")
    print(f"Efficiency: {efficiency:.2%}")
    print(f"Performance loss: {(1 - efficiency):.2%}")
    
    return efficiency

# Scenarios
balanced = [120, 125, 118, 122, 119]  # seconds
skewed = [120, 125, 600, 122, 119]    # one straggler

print("=== Balanced ===")
balanced_eff = calculate_skew_impact(balanced)

print("\n=== Skewed ===")
skewed_eff = calculate_skew_impact(skewed)

print(f"\nSkew causes {((1/skewed_eff) - (1/balanced_eff)):.1f}x slowdown")
```

---

### Advanced Operations

#### 9.6.1 What is custom partitioning and when to use it?

**Answer:** Custom partitioning allows defining your own logic for distributing data across partitions, providing fine-grained control over data locality.

**Custom Partitioner Implementation:**
```python
from pyspark import Partitioner

class DomainBasedPartitioner(Partitioner):
    """Custom partitioner that groups data by email domain"""
    
    def __init__(self, num_partitions):
        self.num_partitions = num_partitions
        # Map specific domains to partitions
        self.domain_map = {
            'hotmail.com': 0,
            'gmail.com': 1,
            'yahoo.com': 2,
        }
    
    def numPartitions(self):
        return self.num_partitions
    
    def getPartition(self, key):
        if '@' in key:
            domain = key.split('@')[-1]
            if domain in self.domain_map:
                return self.domain_map[domain]
        # Hash-based for other domains
        return hash(key) % self.numPartitions

# Usage with RDD
rdd_partitioned = rdd.partitionBy(
    num_partitions,
    DomainBasedPartitioner(num_partitions)
)
```

**When to Implement:**

**1. Known Data Skew:**
```python
# When certain keys are much more frequent
# Custom partitioner can distribute hot keys evenly
```

**2. Domain-Specific Optimization:**
```python
# Business logic requires specific data grouping
# e.g., all customer transactions in same partition
```

**3. Join Optimization:**
```python
# Pre-partition both datasets with same custom partitioner
# Enables partition-wise joins without shuffle
```

**4. Locality Requirements:**
```python
# Data needs to be collocated for processing
# e.g., time-series data from same period together
```

**DataFrame Custom Partitioning:**
```python
# For DataFrames, use repartition with expressions
df_custom = df.repartition(
    100,
    F.when(F.col("email").endswith("hotmail.com"), 0)
     .when(F.col("email").endswith("gmail.com"), 1)
     .when(F.col("email").endswith("yahoo.com"), 2)
     .otherwise(F.hash(F.col("email")) % 100)
)
```

---

#### 9.6.2 reduceByKey vs groupByKey

**Answer:** `reduceByKey` performs partial aggregation before shuffling, making it much more efficient than `groupByKey`.

**Comparison:**

**groupByKey (Inefficient):**
```python
# Shuffles ALL data
rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4)])
grouped = rdd.groupByKey()  # Shuffles: [("a", [1, 3]), ("b", [2, 4])]
result = grouped.mapValues(sum)  # Then sums: [("a", 4), ("b", 6)]
```

**reduceByKey (Efficient):**
```python
# Partial aggregation BEFORE shuffle
rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3), ("b", 4)])
result = rdd.reduceByKey(lambda x, y: x + y)  # Map-side combine + shuffle
# Shuffles: [("a", 4), ("b", 6)] (already aggregated)
```

**Why reduceByKey is Better:**

**1. Shuffle Data Reduction:**
```
groupByKey shuffle:
  Partition 1 → Partition 2: [("a", 1), ("a", 3)]
  Partition 2 → Partition 1: [("b", 2), ("b", 4)]

reduceByKey shuffle:
  Partition 1 → Partition 2: [("a", 4)]  # Already summed
  Partition 2 → Partition 1: [("b", 6)]  # Already summed
```

**2. Memory Usage:**
```python
# groupByKey: Must hold all values for each key in memory
# reduceByKey: Only holds aggregated results
```

**3. Network Transfer:**
```python
# groupByKey: Transfers all raw data
# reduceByKey: Transfers partially aggregated data
```

**Performance Example:**
```python
import time

# Large dataset with skew
data = [("hot_key", 1)] * 1000000 + [("normal_key", 1) for i in range(1000000)]
rdd = sc.parallelize(data, 100)

# groupByKey approach
start = time.time()
result1 = rdd.groupByKey().mapValues(lambda vals: sum(1 for _ in vals))
result1.collect()
groupby_time = time.time() - start

# reduceByKey approach
start = time.time()
result2 = rdd.mapValues(lambda x: 1).reduceByKey(lambda x, y: x + y)
result2.collect()
reduceby_time = time.time() - start

print(f"groupByKey: {groupby_time:.2f}s")
print(f"reduceByKey: {reduceby_time:.2f}s")
print(f"Improvement: {groupby_time/reduceby_time:.1f}x")
# Typical improvement: 5-10x for large datasets
```

---

## Interview Preparation Summary

### Most Important Concepts to Remember:

**AQE (Adaptive Query Execution):**
- ✅ Runtime optimization (vs static planning)
- ✅ Three main features: join switching, skew handling, partition coalescing
- ✅ Enabled by default in Spark 3.2+
- ✅ Works at stage boundaries using actual statistics

**DPP (Dynamic Partition Pruning):**
- ✅ Eliminates partitions at scan time using join filters
- ✅ Requires partitioned tables and equi-joins
- ✅ Best for star schema data warehouses
- ✅ Can provide 10-100x improvements

**Shuffle & Partitioning:**
- ✅ Shuffle is expensive (network + disk I/O)
- ✅ `spark.sql.shuffle.partitions` controls parallelism
- ✅ Target 100-200MB per partition
- ✅ Data skew is a major performance issue

**Key Performance Techniques:**
- ✅ Use `reduceByKey` instead of `groupByKey`
- ✅ Enable AQE for automatic optimizations
- ✅ Partition tables for DPP benefits
- ✅ Use salting for skewed joins
- ✅ Monitor Spark UI for optimization verification

### Common Interview Questions:

1. **"Explain how AQE improves query performance"**
   - Runtime optimization using actual statistics
   - Three features: join switching, skew handling, coalescing
   - Examples of each optimization

2. **"What's the difference between DPP and static pruning?"**
   - DPP uses join conditions, static uses WHERE clauses
   - DPP happens at runtime, static at compile time
   - DPP enables cross-table optimization

3. **"How do you handle data skew in Spark?"**
   - AQE skew join optimization (automatic)
   - Salting techniques (manual)
   - Custom partitioning
   - Broadcast joins for small tables

4. **"Why is shuffle expensive?"**
   - Network transfer between executors
   - Disk I/O for spill operations
   - Serialization/deserialization overhead
   - Sorting and merging costs

5. **"How do you tune shuffle partitions?"**
   - Based on data size (100-200MB per partition)
   - Based on cluster resources (2-4x cores)
   - Monitor Spark UI for optimization
   - Use AQE for dynamic coalescing

### Quick Reference Commands:

```python
# Enable all optimizations
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")

# Tune shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", 200)

# Broadcast threshold
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760")

# Check partition count
df.rdd.getNumPartitions()

# Analyze table statistics
spark.sql("ANALYZE TABLE sales COMPUTE STATISTICS")

# Enable debug logging
spark.conf.set("spark.sql.adaptive.logLevel", "DEBUG")
```
---

**End of Document**
