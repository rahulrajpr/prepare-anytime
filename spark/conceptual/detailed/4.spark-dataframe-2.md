## 3. DataFrame & Dataset API

### 3.4 Column Operations & Functions

#### 3.4.1 What is the syntax difference when passing multiple columns: .drop("col1", "col2") vs .dropDuplicates(["col1", "col2"])?

**Definition:** Different Spark methods use different parameter styles for multiple columns.

**Syntax Patterns:**

- **Varargs**: `.drop("col1", "col2", "col3")` - individual arguments
- **List**: `.dropDuplicates(["col1", "col2", "col3"])` - single list argument

**Key Points:**

- `drop()`, `select()`, `orderBy()` use varargs
- `dropDuplicates()`, `groupBy()`, `rollup()` use list
- This inconsistency is historical in Spark API

#### 3.4.2 When do you use varargs vs list for passing multiple column names?

**Guidelines:**

- **Varargs**: When method signature expects individual column names
- **List**: When method expects a collection of columns
- **Rule of thumb**: Check API documentation when unsure

#### 3.4.3 What is the difference between count(*), count(1), and count(col)?

**Comparison Table:**

| Expression     | Counts                    | Null Handling  |
| -------------- | ------------------------- | -------------- |
| `count(*)`   | All rows                  | Includes nulls |
| `count(1)`   | All rows                  | Includes nulls |
| `count(col)` | Non-null values in column | Excludes nulls |

**Key Points:**

- `count(*)` and `count(1)` are functionally identical
- `count(col_name)` only counts non-null values
- Performance is similar in Spark 3.x

#### 3.4.4 How do these count variations handle null values differently?

**Null Handling:**

- `count(*)`, `count(1)`: Count every row regardless of nulls
- `count(column_name)`: Only count rows where column is not null

**Interview Tip:** This is a common SQL interview question that applies to Spark as well.

#### 3.4.5 What does monotonically_increasing_id() function generate? Is it guaranteed to be sequential?

**Definition:** Generates unique 64-bit integers that increase across partitions.

**Characteristics:**

- Guaranteed to be unique and increasing
- **Not guaranteed** to be sequential or consecutive
- Contains gaps between values
- Based on partition ID and row index within partition

**Key Points:**

- Format: `(partitionId << 33) + rowIndex`
- Use for unique IDs when exact sequence doesn't matter
- Not suitable for precise row numbering

#### 3.4.6 What are the practical use cases for monotonically_increasing_id()?

**Use Cases:**

- Creating surrogate keys for dimension tables
- Generating unique identifiers for temporary joins
- Adding row identifiers when exact sequence isn't critical

**Limitations:**

- Not for ordered row numbers
- Gaps make it unsuitable for some analytics

#### 3.4.7 What is the difference between row_number(), rank(), and dense_rank() window functions?

**Comparison Table:**

| Function         | Behavior                         | Sequence Gaps | Example Output |
| ---------------- | -------------------------------- | ------------- | -------------- |
| `row_number()` | Unique sequential numbers        | No gaps       | 1,2,3,4,5      |
| `rank()`       | Same rank for ties, skip numbers | Has gaps      | 1,2,2,4,5      |
| `dense_rank()` | Same rank for ties, no skipping  | No gaps       | 1,2,2,3,4      |

**Key Points:**

- All require Window specification with `orderBy()`
- `row_number()` always produces unique numbers
- `rank()` and `dense_rank()` handle ties differently

#### 3.4.8 When would you use lead() and lag() functions?

**Use Cases:**

- **`lag()`**: Compare current row with previous (time series analysis)
- **`lead()`**: Compare current row with next (forecasting, lookahead)

**Examples:**

- Calculating day-over-day changes
- Finding previous/next order for customer
- Computing running differences

#### 3.4.9 What is the first() and last() aggregate function? How do they handle nulls?

**Behavior:**

- **`first()`**: Returns first non-null value in the group
- **`last()`**: Returns last non-null value in the group
- **Null Handling**: Both ignore null values by default

**Key Points:**

- Order-sensitive - depends on data order in group
- Often used with window functions for ordered data
- Consider using `ignoreNulls = true/false` parameter

#### 3.4.10 Explain the difference between collect_list() and collect_set().

**Comparison Table:**

| Function           | Duplicates           | Order              | Return Type |
| ------------------ | -------------------- | ------------------ | ----------- |
| `collect_list()` | Preserves duplicates | Preserves order    | ArrayType   |
| `collect_set()`  | Removes duplicates   | No order guarantee | ArrayType   |

**Use Cases:**

- `collect_list()`: When order matters, all values needed
- `collect_set()`: When unique values needed, order irrelevant

#### 3.4.11 What does explode() function do? Provide an example use case.

**Definition:** Transforms each element of an array or map into a separate row.

**Example:**

```python
# Input DataFrame
# +---------+---------------+
# | name    | hobbies       |
# +---------+---------------+
# | Alice   | [hike, read]  |
# | Bob     | [run, swim]   |
# +---------+---------------+

df.select("name", explode("hobbies").alias("hobby"))

# Output:
# +-------+------+
# | name  | hobby|
# +-------+------+
# | Alice | hike |
# | Alice | read |
# | Bob   | run  |
# | Bob   | swim |
# +-------+------+
```



3.4.12 What is the difference between explode() and explode_outer()?

**Null Handling Difference:**

* **`explode()`** : Drops rows where array/map is null or empty
* **`explode_outer()`** : Preserves rows with null/empty arrays as null values

**Use Case:**

* Use `explode_outer()` when you need to preserve all original rows

#### 3.4.13 What does posexplode() do and how is it different from explode()?

**Difference:**`posexplode()` includes the position index along with the value.

**Output:**

* `explode()`: Single column with array elements
* `posexplode()`: Two columns - position and value

#### 3.4.14 When to use posexplode() vs posexplode_outer()?

**Guidelines:**

* **`posexplode()`** : When you need positions and can drop null/empty arrays
* **`posexplode_outer()`** : When you need positions and must preserve all rows

#### 3.4.15 What is the difference between explode() and posexplode() in terms of output columns?

**Output Comparison:**

* `explode(array_col)` → `value_col`
* `posexplode(array_col)` → `pos_col`, `value_col`

#### 3.4.16 How does inline() differ from inline_outer() when working with arrays of structs?

**Behavior:**

* **`inline()`** : Explodes array of structs, drops null/empty arrays
* **`inline_outer()`** : Explodes array of structs, preserves null/empty as nulls

**Use Case:** When working with nested structured data in arrays.

#### 3.4.17 How do you use array_contains() function?

**Usage:**`array_contains(array_column, value)` returns boolean.

**Example:**

**python**

```
df.filter(array_contains("hobbies", "hike"))
```

#### 3.4.18 What does split() function return and what is its data type?

**Returns:**`ArrayType(StringType())` - splits string into array of substrings.

**Example:**`split("hello,world", ",")` → `["hello", "world"]`

#### 3.4.19 How do you use concat() vs concat_ws() (concat with separator)?

**Difference:**

* **`concat()`** : Joins strings directly: `concat("A", "B")` → `"AB"`
* **`concat_ws()`** : Joins with separator, ignores nulls: `concat_ws("-", "A", null, "B")` → `"A-B"`

#### 3.4.20 What is coalesce() function and how does it differ from coalesce() for repartitioning?

**Critical Distinction:**

* **`coalesce()` (SQL function)** : Returns first non-null argument
* **`coalesce()` (DataFrame method)** : Reduces number of partitions

**Interview Tip:** This naming collision confuses many developers - be clear about context.

#### 3.4.21 What does nvl() or ifnull() do? Are they the same?

**Definition:** Both return first expression if not null, otherwise return second expression.

**Equivalence:**`nvl(expr1, expr2)` ≡ `ifnull(expr1, expr2)` ≡ `coalesce(expr1, expr2)`

#### 3.4.22 Explain when().otherwise() construct with examples.

**Definition:** Spark's conditional expression similar to SQL CASE WHEN.

**Example:**

**python**

```
from pyspark.sql.functions import when

df.withColumn("age_group",
    when(col("age") < 18, "child")
    .when(col("age") < 65, "adult")
    .otherwise("senior"))
```

**Key Points:**

* Chain multiple `.when()` conditions
* `.otherwise()` is mandatory for fallback
* Returns null if no conditions match and no otherwise

#### 3.4.23 What is the difference between withColumn() and select() for adding/transforming columns?

**Comparison Table:**

| Aspect                  | withColumn()                    | select()                |
| ----------------------- | ------------------------------- | ----------------------- |
| **Purpose**       | Add/replace single column       | Select multiple columns |
| **Other Columns** | Keeps all existing columns      | Only specified columns  |
| **Performance**   | Creates new DataFrame each call | Single transformation   |
| **Use Case**      | Incremental column changes      | Bulk column operations  |

#### 3.4.24 Can you use withColumn() multiple times in a chain? What are the performance implications?

**Answer:** Yes, but each `withColumn()` creates a new DataFrame, which can be inefficient.

**Performance Impact:**

* **Inefficient:**
  **python**

```
df.withColumn("col1", ...)
  .withColumn("col2", ...)
  .withColumn("col3", ...)
```

**Efficient:**

**python**

```
df.select("*", 
  expr("...").alias("col1"),
  expr("...").alias("col2"), 
  expr("...").alias("col3"))
```

#### 3.4.25 What does withColumnRenamed() do? Can you rename multiple columns at once?

**Function:** Renames a single column.

**Limitation:** Cannot rename multiple columns directly. Alternatives:

**python**

```
# Single column
df.withColumnRenamed("old_name", "new_name")

# Multiple columns - use select with aliases
df.select([col(c).alias(new_name) for c, new_name in rename_map.items()])
```

#### 3.4.26 What is selectExpr() and when would you use it instead of select()?

**Definition:**`selectExpr()` allows SQL expressions as strings.

**Use Cases:**

* Complex SQL expressions more readable as strings
* Dynamic expression generation
* SQL developer familiarity

**Example:**

**python**

```
df.selectExpr("name", "age * 2 as double_age", "CASE WHEN age > 18 THEN 'adult' ELSE 'child' END")
```

#### 3.4.27 How do you drop multiple columns efficiently?

**Methods:**

**python**

```
# Most efficient - single operation
df.drop("col1", "col2", "col3")

# Using list unpacking
cols_to_drop = ["col1", "col2", "col3"]
df.drop(*cols_to_drop)

# Less efficient - multiple operations
df.drop("col1").drop("col2").drop("col3")
```

#### 3.4.28 What does drop() return if you try to drop a non-existent column?

**Behavior:** Spark ignores non-existent columns by default, no error thrown.

**Configuration:** Can change with `spark.sql.analyzer.failOnInvalidColumn` but defaults to ignore.

### 3.5 String Functions

#### 3.5.1 Explain the regexp_extract() function and its usage for pattern matching.

**Definition:** Extracts the first matching group from a string using regex.

**Syntax:**`regexp_extract(str, pattern, groupIndex)`

**Example:**

**python**

```
regexp_extract("John Doe", "(\\w+) (\\w+)", 1)  # Returns "John"
```

#### 3.5.2 What is the difference between regexp_extract() and regexp_replace()?

**Comparison:**

* **`regexp_extract()`** : Returns matched substring
* **`regexp_replace()`** : Replaces matched pattern with replacement string

#### 3.5.3 How do you use like() and rlike() for pattern matching?

**Pattern Types:**

* **`like()`** : SQL wildcards (`%` = any string, `_` = single char)
* **`rlike()`** : Regular expressions

**Examples:**

**python**

```
df.filter(col("name").like("A%"))    # Names starting with A
df.filter(col("name").rlike("^A.*")) # Same with regex
```

#### 3.5.4 What is the difference between like(), ilike(), and rlike()?

**Comparison Table:**

| Function          | Pattern Type  | Case Sensitivity | Example               |
| ----------------- | ------------- | ---------------- | --------------------- |
| **like()**  | SQL wildcards | Case-sensitive   | `A%`                |
| **ilike()** | SQL wildcards | Case-insensitive | `a%`matches "Alice" |
| **rlike()** | Regex         | Case-sensitive   | `^[A-C]`            |

#### 3.5.5 Which pattern matching function is case-insensitive?

**Answer:**`ilike()` is the only case-insensitive function among these.

#### 3.5.6 What pattern type does rlike() use (SQL wildcards or regex)?

**Answer:**`rlike()` uses regular expressions, not SQL wildcards.

#### 3.5.7 What does substring() function do? What are its parameters?

**Definition:** Extracts substring from string.

**Syntax:**`substring(str, pos, len)`

**Parameters:**

* `str`: Source string
* `pos`: Starting position (1-based)
* `len`: Number of characters to extract

#### 3.5.8 How do you use trim(), ltrim(), and rtrim()?

**Functions:**

* **`trim()`** : Remove whitespace from both ends
* **`ltrim()`** : Remove whitespace from left only
* **`rtrim()`** : Remove whitespace from right only

#### 3.5.9 What is upper(), lower(), initcap() used for?

**String Case Functions:**

* **`upper()`** : Convert to uppercase
* **`lower()`** : Convert to lowercase
* **`initcap()`** : Capitalize first letter of each word

#### 3.5.10 How do you use lpad() and rpad() for padding strings?

**Functions:**

* **`lpad()`** : Pad left side to specified length
* **`rpad()`** : Pad right side to specified length

**Example:**`lpad("hi", 5, "-")` → `"---hi"`

#### 3.5.11 What does length() function return for null values?

**Behavior:** Returns null when input is null.

#### 3.5.12 How do you check if a string contains a substring in Spark?

**Methods:**

**python**

```
col("text").contains("substring")
instr(col("text"), "substring") > 0
col("text").like("%substring%")
```

### 3.6 Date & Time Functions

#### 3.6.1 What are the key date and time functions in Spark?

**Essential Functions:**

* `current_date()`, `current_timestamp()`
* `year()`, `month()`, `day()`, `hour()`, `minute()`, `second()`
* `date_add()`, `date_sub()`
* `datediff()`, `months_between()`

#### 3.6.2 How do you extract year, month, day from a date column?

**Usage:**

**python**

```
df.select(
    year("date_col").alias("year"),
    month("date_col").alias("month"), 
    dayofmonth("date_col").alias("day")
)
```

#### 3.6.3 What does datediff() function calculate?

**Returns:** Number of days between two dates.

**Example:**`datediff("2024-01-15", "2024-01-10")` → `5`

#### 3.6.4 How do you use to_date() and to_timestamp() for type conversion?

**Usage:**

**python**

```
to_date("2024-01-15")                    # String to date
to_timestamp("2024-01-15 10:30:00")     # String to timestamp
to_date(col("string_col"), "yyyy-MM-dd") # With format
```

#### 3.6.5 What is the difference between unix_timestamp() and from_unixtime()?

**Conversion Directions:**

* **`unix_timestamp()`** : Converts timestamp to Unix epoch seconds
* **`from_unixtime()`** : Converts Unix epoch seconds to timestamp

#### 3.6.6 How do you handle different date formats when reading data?

**Approaches:**

1. Specify format in `to_date()`:
   **python**

```
to_date(col("date_str"), "MM/dd/yyyy")
```

Use schema inference with option:

**python**

```
spark.read.option("dateFormat", "MM/dd/yyyy").csv("file.csv")
```

#### 3.6.7 What does date_format() function do?

**Usage:** Formats date/timestamp as string using specified pattern.

**Example:**`date_format("2024-01-15", "MMMM yyyy")` → `"January 2024"`

#### 3.6.8 How do you calculate the difference between two timestamps?

**Methods:**

**python**

```
# Difference in seconds
unix_timestamp("end_ts") - unix_timestamp("start_ts")

# Using built-in functions  
datediff("end_ts", "start_ts")  # Days difference
```

#### 3.6.9 What is add_months() function used for?

**Usage:** Adds calendar months to a date, handling month-end correctly.

**Example:**`add_months("2024-01-31", 1)` → `"2024-02-29"` (leap year)

#### 3.6.10 How do you get the last day of the month using last_day()?

**Usage:**`last_day("2024-02-15")` → `"2024-02-29"`

#### 3.6.11 What does next_day() function do?

**Returns:** The first date after the given date that matches the day-of-week.

**Example:**`next_day("2024-01-15", "Monday")` → First Monday after Jan 15

#### 3.6.12 How do you handle timezone conversions in Spark?

**Functions:**

* `to_utc_timestamp(timestamp, timezone)`
* `from_utc_timestamp(timestamp, timezone)`

**Best Practice:** Store timestamps in UTC, convert to local timezones as needed.

### 3.6.1 Date & Time Intervals in Spark

#### 3.6.1.1 What are the two main interval families in Spark?

**Interval Types:**

* **YEAR-MONTH** : Calendar-based intervals (years, months)
* **DAY-TIME** : Exact duration intervals (days, hours, minutes, seconds, milliseconds)

#### 3.6.1.2 When do you use YEAR-MONTH interval vs DAY-TIME interval?

**Usage Guidelines:**

* **YEAR-MONTH** : For calendar calculations where month length varies
* **DAY-TIME** : For precise duration calculations with fixed time units

#### 3.6.1.3 Why can't you directly cast a day interval to a month interval?

**Reason:** Months have variable lengths (28-31 days), so conversion isn't deterministic.

#### 3.6.1.4 What is the common approximation used when converting between interval types?

**Approximation:** Often uses 30 days per month as rough estimate, but not precise.

#### 3.6.1.5 What is make_interval() function? What makes it unique?

**Definition:** Creates intervals from multiple time units in a single function call.

**Unique Feature:** Can combine years, months, weeks, days, hours, minutes, seconds.

#### 3.6.1.6 What parameters can you specify in make_interval()?

**Parameters:**`make_interval(years, months, weeks, days, hours, minutes, seconds)`

All parameters are optional.

#### 3.6.1.7 When would you use make_interval() over other interval functions?

**Use Case:** When you need complex intervals combining multiple time units.

#### 3.6.1.8 What is make_dt_interval() function? What is its specific purpose?

**Purpose:** Specifically creates DAY-TIME intervals.

**Parameters:**`make_dt_interval(days, hours, minutes, seconds)`

#### 3.6.1.9 What units does make_dt_interval() handle?

**Units:** Days, hours, minutes, seconds only (no months/years).

#### 3.6.1.10 When would you use make_dt_interval() instead of make_interval()?

**Preference:** When working exclusively with day-time intervals for code clarity.

#### 3.6.1.11 What is make_ym_interval() function? What is its specific purpose?

**Purpose:** Specifically creates YEAR-MONTH intervals.

**Parameters:**`make_ym_interval(years, months)`

#### 3.6.1.12 How does make_ym_interval() handle variable month lengths correctly?

**Behavior:** Uses calendar semantics - respects actual month lengths when adding to dates.

#### 3.6.1.13 When would you use make_ym_interval() for calendar-based calculations?

**Use Cases:**

* Adding years/months to dates while respecting calendar
* Age calculations
* Financial periods

#### 3.6.1.14 Compare make_interval() vs make_dt_interval() vs make_ym_interval()

**Comparison Table:**

| Function               | Interval Type | Parameters                                    | Use Case              |
| ---------------------- | ------------- | --------------------------------------------- | --------------------- |
| `make_interval()`    | Mixed         | years, months, weeks, days, hours, mins, secs | General purpose       |
| `make_dt_interval()` | DAY-TIME      | days, hours, minutes, seconds                 | Precise durations     |
| `make_ym_interval()` | YEAR-MONTH    | years, months                                 | Calendar calculations |

### 3.6.2 Date Parsing & Formatting

#### 3.6.2.1 What are the two main approaches to parsing dates in Spark SQL?

**Approaches:**

1. **Implicit parsing** : `DATE('2024-01-15')` or `CAST('2024-01-15' AS DATE)`
2. **Explicit parsing** : `TO_DATE('15/01/2024', 'dd/MM/yyyy')`

#### 3.6.2.2 What date format does default DATE() or CAST AS DATE reliably support?

**Supported Format:** ISO 8601 format (`yyyy-MM-dd`)

#### 3.6.2.3 What happens when you use DATE() on non-ISO format strings?

**Result:** Returns null for unparseable date strings.

#### 3.6.2.4 When must you use TO_DATE(expr, format) instead of DATE()?

**When:** Date strings are in non-ISO formats like:

* `MM/dd/yyyy`
* `dd-MMM-yyyy`
* `yyyy/MM/dd`

#### 3.6.2.5 What format patterns are commonly used with TO_DATE()?

**Common Patterns:**

* `yyyy-MM-dd` (ISO)
* `MM/dd/yyyy` (US)
* `dd-MM-yyyy` (European)
* `yyyyMMdd` (Compact)

#### 3.6.2.6 Are format pattern letters case-sensitive in TO_DATE()? Provide examples.

**Yes, case-sensitive:**

* `'MM'` = month number (01-12)
* `'mm'` = minutes (00-59)
* `'YYYY'` = week-based year
* `'yyyy'` = calendar year

#### 3.6.2.7 What is the difference between 'MM' and 'MMM' in date format patterns?

**Difference:**

* `'MM'`: Month as number (01, 02, ..., 12)
* `'MMM'`: Month as abbreviated name (Jan, Feb, ..., Dec)

#### 3.6.2.8 What is the difference between 'd', 'dd', and 'D' in date format patterns?

**Difference:**

* `'d'`: Day of month (1-31) - minimal digits
* `'dd'`: Day of month (01-31) - two digits
* `'D'`: Day of year (1-366)

#### 3.6.2.9 How does DATE() handle timestamp strings with time components?

**Behavior:** Truncates the time portion, keeps only the date part.

#### 3.6.2.10 What does TO_DATE() return for unparseable strings?

**Returns:** null

#### 3.6.2.11 What are best practices for handling date formats in Spark pipelines?

**Best Practices:**

1. Always specify format for non-ISO dates
2. Validate parsing results with null checks
3. Use consistent formats across pipeline
4. Document date format assumptions
5. Handle multiple formats if source data is inconsistent

#### 3.6.2.12 What is to_char() function used for?

**Usage:** Formats numbers or dates as strings (Oracle compatibility function).

**Example:**`to_char(current_date(), 'YYYY-MM-DD')`

#### 3.6.2.13 How do you use to_timestamp() with custom formats?

**Usage:**`to_timestamp(string_col, 'yyyy-MM-dd HH:mm:ss')`

**Example:**`to_timestamp('2024-01-15 14:30:00', 'yyyy-MM-dd HH:mm:ss')`

#### 3.6.2.14 What is the Java DateTimeFormatter pattern syntax used in Spark?

**Standard:** Spark uses Java DateTimeFormatter patterns, which are similar to but not identical with SimpleDateFormat.

**Common Patterns:**

* `y` = year
* `M` = month
* `d` = day
* `H` = hour (0-23)
* `h` = hour (1-12)
* `m` = minute
* `s` = second
* `a` = AM/PM marker
