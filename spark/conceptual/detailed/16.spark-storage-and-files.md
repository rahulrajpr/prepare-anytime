## **7. File Formats & Storage Systems**

---

### **7.1 Storage Systems**

#### **7.1.1 What is the difference between distributed file storage systems and normal storage systems?**

**Answer:** Distributed systems scale horizontally across multiple machines while normal systems are centralized on single machines.

| Aspect                    | Normal Storage Systems      | Distributed File Storage Systems       |
| ------------------------- | --------------------------- | -------------------------------------- |
| **Architecture**    | Single server/centralized   | Multiple nodes/decentralized           |
| **Scalability**     | Vertical scaling (scale-up) | Horizontal scaling (scale-out)         |
| **Fault Tolerance** | Hardware RAID, backups      | Built-in data replication              |
| **Data Locality**   | Not applicable              | Core principle - compute moves to data |
| **Cost**            | Expensive at scale          | Commodity hardware                     |
| **Examples**        | Local disk, NAS, SAN        | HDFS, S3, GCS, ADLS                    |

**Practical Implementation:**

**python**

```
# Local file system (single machine)
df_local = spark.read.csv("file:///home/user/data.csv")

# Distributed file system (HDFS)
df_hdfs = spark.read.csv("hdfs://namenode:8020/data/data.csv")

# Cloud storage (S3)
df_s3 = spark.read.csv("s3a://my-bucket/data/data.csv")
```

**Interview Tips:**

* Always mention "scale-up vs scale-out" distinction
* Emphasize data locality for Spark performance
* Cloud storage provides decoupled compute and storage

---

#### **7.1.2 What is a Spark data lake?**

**Answer:** A centralized repository on distributed storage that stores all data formats, with Spark as the primary processing engine.

**Key Characteristics:**

* Stores structured, semi-structured, unstructured data
* Schema-on-read rather than schema-on-write
* Built on distributed storage (S3, HDFS, ADLS)
* Spark provides processing capabilities

**Data Lake Architecture:**

**text**

```
Raw Zone (landing) → Cleaned Zone (processed) → Curated Zone (business-ready)
      ↓                    ↓                         ↓
   JSON/CSV             Parquet                 Delta Lake
   Log files            Standardized            Aggregated
```

**Interview Tips:**

* Mention evolution from "data swamps" to "lakehouses"
* Delta Lake/Iceberg/Hudi add reliability to data lakes
* Emphasize Spark's role in ETL and analytics

---

#### **7.1.3 What is HDFS and how does it work with Spark?**

**Answer:**
 HDFS is Hadoop Distributed File System that provides reliable, scalable
 storage. Spark leverages HDFS for data locality and parallel
processing.

**HDFS Components:**

* **NameNode** : Metadata management
* **DataNodes** : Actual data storage
* **Blocks** : Data split into blocks (128MB default)

**Spark Integration:**

**python**

```
# Reading from HDFS
df = spark.read.parquet("hdfs://namenode:8020/data/transactions")

# Writing to HDFS
df.write.parquet("hdfs://namenode:8020/output/results")
```

**Data Locality in Action:**

1. Spark driver queries NameNode for block locations
2. Tasks scheduled on executors near data blocks
3. Parallel reading from multiple DataNodes

**Interview Tips:**

* Emphasize data locality benefits
* Understand block sizes and replication factor
* Know that HDFS is being replaced by cloud storage in modern architectures

---

#### **7.1.4 What are the advantages of cloud storage (S3, ADLS, GCS) for Spark workloads?**

**Answer:** Cloud storage provides infinite scalability, cost-effectiveness, and decoupled architecture.

**Advantages Comparison:**

| Advantage                           | Description              | Impact                           |
| ----------------------------------- | ------------------------ | -------------------------------- |
| **Infinite Scale**            | No storage limits        | Handle petabytes easily          |
| **Cost Effective**            | Pay-per-use              | No upfront hardware costs        |
| **Decoupled Compute/Storage** | Separate scaling         | Independent cluster management   |
| **Durability**                | 99.999999999% (11 nines) | Data protection                  |
| **Managed Service**           | No ops overhead          | Focus on data not infrastructure |

**Practical Implementation:**

**python**

```
# S3 configuration
spark.conf.set("spark.hadoop.fs.s3a.access.key", "your-access-key")
spark.conf.set("spark.hadoop.fs.s3a.secret.key", "your-secret-key")

# Reading from S3
df = spark.read.parquet("s3a://my-bucket/data/")

# Writing to S3 with partitioning
df.write.partitionBy("date").parquet("s3a://my-bucket/output/")
```

**Interview Tips:**

* Highlight "decoupled compute and storage" as key advantage
* Mention eventual consistency considerations
* Discuss cost optimization strategies (storage tiers)

---

### **7.2 File Format Deep Dive - Parquet**

#### **7.2.1 What is columnar storage? How does Parquet implement it?**

**Answer:** Columnar storage stores data by columns rather than rows, enabling better compression and query performance.

**Row vs Column Storage:**

**text**

```
Row Storage (Avro):    Column Storage (Parquet):
[1, "John", 25]        [1, 2, 3, 4, 5]        # id column
[2, "Jane", 30]        ["John", "Jane", ...]   # name column  
[3, "Bob", 35]         [25, 30, 35, ...]       # age column
```

**Parquet Implementation:**

* Data organized in row groups
* Each column stored separately within row groups
* Metadata contains min/max statistics per column chunk

**Interview Tips:**

* Columnar format excels for analytical queries (read few columns)
* Row format better for transactional workloads (read entire rows)

---

#### **7.2.2 What are the advantages of Parquet for analytics workloads?**

**Answer:** Better compression, column pruning, predicate pushdown, and vectorized processing.

**Advantages Table:**

| Advantage                       | Description                | Performance Impact    |
| ------------------------------- | -------------------------- | --------------------- |
| **Column Pruning**        | Read only needed columns   | 2-10x faster reads    |
| **Predicate Pushdown**    | Filter at file level       | Skip irrelevant data  |
| **Better Compression**    | Similar data values        | 75-80% size reduction |
| **Vectorized Processing** | Process columns in batches | CPU efficiency        |

**Code Example:**

**python**

```
# Parquet automatically applies optimizations
df = spark.read.parquet("data/transactions")

# Only 'user_id' and 'amount' columns are read from disk
# Rows with amount < 100 are skipped during read
result = df.select("user_id", "amount").filter(col("amount") > 100)
```

---

#### **7.2.3 How does Parquet handle nested data structures?**

**Answer:** Parquet uses the Dremel encoding with repetition and definition levels to flatten nested structures.

**Nested Data Example:**

**json**

```
{
  "user": {
    "name": "John",
    "address": {
      "street": "123 Main",
      "city": "Boston"
    }
  }
}
```

**Parquet Structure:**

**text**

```
user.name | user.address.street | user.address.city
-------------------------------------------
John      | 123 Main           | Boston
```

**Interview Tips:**

* Understand that nested fields become separate columns
* Complex nesting can impact performance
* Spark can read nested Parquet data as structs/arrays

---

#### **7.2.4 What is a row group in Parquet?**

**Answer:** A row group is a horizontal partitioning of data within a Parquet file, typically containing 10,000-100,000 rows.

**Row Group Structure:**

**text**

```
Parquet File
├── Row Group 1
│   ├── Column Chunk (id)
│   ├── Column Chunk (name)
│   └── Column Chunk (age)
├── Row Group 2
│   ├── Column Chunk (id)
│   └── ...
```

**Benefits:**

* Parallel processing within single file
* Independent compression per row group
* Better predicate pushdown with row group statistics

---

#### **7.2.5 What is a column chunk in Parquet?**

**Answer:** A column chunk contains the data for one column within one row group.

**Column Chunk Contents:**

* Actual data values
* Encoding information
* Compression details
* Statistics (min, max, count)

**Interview Tips:**

* Column chunks are the unit of I/O operations
* Statistics in column chunks enable predicate pushdown

---

#### **7.2.6 How does Parquet encoding and compression work?**

**Answer:** Parquet uses multiple encoding strategies followed by compression algorithms.

**Encoding Strategies:**

* **Dictionary Encoding** : For low-cardinality columns
* **Run-Length Encoding (RLE)** : For repeated values
* **Delta Encoding** : For sorted columns with small variations

**Compression Algorithms:**

**python**

```
# Configuring compression
df.write.option("compression", "snappy").parquet("output/")  # Fast
df.write.option("compression", "gzip").parquet("output/")    # Better ratio
df.write.option("compression", "lzo").parquet("output/")     # Splittable
```

**Interview Tips:**

* Snappy: Good balance of speed and ratio
* GZIP: Better compression but slower
* ZSTD: Modern alternative with good speed/ratio

---

#### **7.2.7 What is predicate pushdown in Parquet and why is it efficient?**

**Answer:** Filter predicates are pushed to the file reading level, skipping irrelevant row groups using statistics.

**Predicate Pushdown Process:**

1. Read column chunk statistics (min/max)
2. Skip row groups that don't satisfy WHERE clause
3. Only read relevant data from disk

**Example:**

**python**

```
# This query only reads row groups where max salary >= 50000
high_earners = df.filter(col("salary") >= 50000)
```

**Efficiency Benefits:**

* Reduced I/O operations
* Less data deserialization
* Better memory utilization

---

#### **7.2.8 What is projection pushdown in Parquet?**

**Answer:** Reading only the required columns rather than entire dataset.

**Projection Pushdown Example:**

**python**

```
# Only 'name' and 'age' columns are read from disk
result = df.select("name", "age")
```

**Interview Tips:**

* Projection pushdown = column pruning
* Combined with predicate pushdown = massive performance gains

---

#### **7.2.9 What are the limitations of Parquet?**

**Answer:** Slow writes, not ideal for row-level operations, small file overhead.

**Limitations Table:**

| Limitation                   | Impact               | Workaround            |
| ---------------------------- | -------------------- | --------------------- |
| **Slow Writes**        | ETL jobs take longer | Use larger row groups |
| **Not Row-Optimized**  | Poor point lookups   | Use indexing layer    |
| **Small File Problem** | Metadata overhead    | Compact small files   |
| **Schema Evolution**   | Complex to manage    | Use Delta Lake        |

---

### **7.3 File Format Deep Dive - Avro**

#### **7.3.1 What is row-based storage? How does Avro use it?**

**Answer:** Row-based storage stores complete records together, making it efficient for reading/writing entire rows.

**Avro Row Storage:**

**text**

```
[Record1: id=1, name="John", age=25]
[Record2: id=2, name="Jane", age=30] 
[Record3: id=3, name="Bob", age=35]
```

**Avro Features:**

* Binary format with JSON schema
* Self-describing (schema embedded)
* Splittable for distributed processing

**Interview Tips:**

* Row-based better for write-heavy workloads
* Good for streaming and transactional data

---

#### **7.3.2 What are the advantages of Avro for streaming and schema evolution?**

**Answer:** Compact binary format, built-in schema evolution, and fast serialization.

**Advantages:**

* **Small Size** : Binary format more compact than JSON
* **Schema Evolution** : Forward/backward compatibility
* **Fast Serialization** : Optimized binary encoding
* **Splittable** : Can be processed in parallel

---

#### **7.3.3 How does Avro handle schema evolution?**

**Answer:** Avro supports backward, forward, and full compatibility through schema resolution.

**Compatibility Types:**

* **Backward** : New schema can read old data
* **Forward** : Old schema can read new data
* **Full** : Both backward and forward

**Schema Evolution Rules:**

* Can add new fields with defaults
* Can remove fields with defaults
* Can change field types with compatibility

---

#### **7.3.4 When would you choose Avro over Parquet?**

**Answer:** Choose Avro for streaming, schema evolution, and write-heavy workloads.

**Avro vs Parquet Use Cases:**

| Scenario          | Recommended Format | Reason                              |
| ----------------- | ------------------ | ----------------------------------- |
| Kafka Streaming   | Avro               | Compact, fast serialization         |
| ETL Pipelines     | Avro               | Schema evolution, write performance |
| Data Analytics    | Parquet            | Columnar pruning, query performance |
| Data Lake Storage | Parquet            | Compression, analytics performance  |

---

#### **7.3.5 How is Avro schema stored and transmitted?**

**Answer:** Schema can be embedded in files or stored separately in schema registry.

**Schema Storage Options:**

**python**

```
# Embedded schema (self-contained)
df.write.format("avro").save("data.avro")

# External schema (Kafka use case)
# Schema stored in Schema Registry, only ID in message
```

**Interview Tips:**

* Kafka + Avro + Schema Registry is common pattern
* Embedded schema increases file size but is self-contained

---

#### **7.3.6 What is the performance trade-off between Avro and Parquet?**

**Answer:** Avro excels at writes and streaming, Parquet excels at analytical reads.

**Performance Comparison:**

| Metric                     | Avro                | Parquet                      |
| -------------------------- | ------------------- | ---------------------------- |
| **Write Speed**      | Faster              | Slower                       |
| **Read Speed**       | Slower (full scans) | Faster (column pruning)      |
| **Storage Size**     | Medium              | Smaller (better compression) |
| **Schema Evolution** | Excellent           | Limited                      |

---

### **7.4 File Format Deep Dive - ORC**

#### **7.4.1 How is ORC similar to and different from Parquet?**

**Answer:** Both are columnar formats but ORC is Hive-optimized while Parquet is Spark-optimized.

**ORC vs Parquet:**

| Aspect                       | ORC                  | Parquet               |
| ---------------------------- | -------------------- | --------------------- |
| **Origin**             | Hive community       | Apache community      |
| **Compression**        | Better for text      | Better for mixed data |
| **ACID Support**       | Native in Hive       | Limited               |
| **Predicate Pushdown** | Yes, with indexes    | Yes, with statistics  |
| **Nested Data**        | Complex type support | Dremel encoding       |

---

#### **7.4.2 What compression techniques does ORC use?**

**Answer:** ORC uses zlib, snappy, LZO, and ZSTD compression with dictionary encoding.

**Compression Options:**

**sql**

```
-- Hive syntax for ORC compression
CREATE TABLE orc_table 
STORED AS ORC
TBLPROPERTIES ("orc.compress"="ZLIB");
```

---

#### **7.4.3 How does ORC handle predicate pushdown?**

**Answer:** ORC uses file-level, stripe-level, and row-level indexes for efficient filtering.

**ORC Indexes:**

* **File Statistics** : Min/max per file
* **Stripe Statistics** : Min/max per stripe
* **Row Index** : Min/max every 10,000 rows

---

#### **7.4.4 What are ORC stripes, row groups, and indexes?**

**Answer:** ORC organizes data in stripes (similar to Parquet row groups) with built-in indexes.

**ORC File Structure:**

**text**

```
ORC File
├── Stripes (250MB default)
│   ├── Index Data (row group statistics)
│   ├── Row Data (columnar storage)
│   └── Stripe Footer
├── File Footer (schema, statistics)
└── Postscript (compression, version)
```

---

#### **7.4.5 When would you choose ORC over Parquet?**

**Answer:** Choose ORC for Hive-heavy environments, ACID requirements, or text-heavy data.

**ORC Preferred When:**

* Primary use with Hive
* Need ACID transactions
* Heavy text data compression
* Existing Hive/ORC infrastructure

---

### **7.5 File Format Deep Dive - Delta Lake**

#### **7.5.1 What is Delta Lake and how is it different from a file format?**

**Answer:** Delta Lake is a storage layer that provides reliability to data lakes, not just a file format.

**Key Differences:**

* **Parquet** : File format for storage
* **Delta Lake** : Management layer on top of Parquet + transaction log

**Delta Lake Components:**

* Parquet files for data storage
* Transaction log for ACID guarantees
* Metadata for schema enforcement

---

#### **7.5.2 What are the ACID transaction guarantees in Delta Lake?**

**Answer:** Atomicity, Consistency, Isolation, Durability for data lake operations.

**ACID Implementation:**

* **Atomic** : All or nothing writes
* **Consistent** : Valid schema maintained
* **Isolated** : Concurrent reads/writes don't conflict
* **Durable** : Committed data persists

---

#### **7.5.3 How does Delta Lake implement time travel?**

**Answer:** Delta maintains version history allowing querying of data at previous points in time.

**Time Travel Syntax:**

**python**

```
# Read data from specific version
df_v1 = spark.read.format("delta").option("versionAsOf", 1).load("/delta/table")

# Read data from specific timestamp
df_timestamp = spark.read.format("delta").option("timestampAsOf", "2024-01-01").load("/delta/table")
```

---

#### **7.5.4 What is the Delta transaction log?**

**Answer:** A log of all transactions that maintains the state of the Delta table.

**Transaction Log Contents:**

* Commit information
* Schema changes
* File additions/removals
* Table statistics

---

#### **7.5.5 How does Delta Lake handle updates and deletes?**

**Answer:** Through marker files and data compaction rather than in-place modifications.

**Update Process:**

1. Mark old files for deletion
2. Write new files with updated data
3. Update transaction log
4. Eventually vacuum old files

---

#### **7.5.6 What is optimize and ZORDER in Delta Lake?**

**Answer:** Optimize compacts small files, ZORDER co-locates related data.

**Optimization Commands:**

**python**

```
# Compact small files
delta_table.optimize().executeCompaction()

# Co-locate data by columns for better pruning
delta_table.optimize().executeZOrderBy("user_id", "date")
```

---

#### **7.5.7 What is vacuum in Delta Lake?**

**Answer:** Removes old files no longer referenced by the Delta table.

**Vacuum Usage:**

**python**

```
# Remove files older than 7 days (default)
delta_table.vacuum()

# Remove files older than 1 hour
delta_table.vacuum(1)
```

**Interview Tips:**

* Vacuum is irreversible - be careful with retention period
* Default retention is 7 days for time travel

---

#### **7.5.8 How does Delta Lake schema enforcement work?**

**Answer:** Delta validates schema on write and prevents incompatible changes.

**Schema Enforcement:**

* Rejects writes with unknown columns
* Validates data types
* Can evolve schema with explicit commands

---

#### **7.5.9 What is schema evolution in Delta Lake?**

**Answer:** Ability to modify table schema while maintaining compatibility.

**Schema Evolution Types:**

**python**

```
# Add new columns (automatic)
spark.sql("ALTER TABLE delta_table ADD COLUMN new_col STRING")

# Change data types (requires explicit enable)
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")
```

---

#### **7.5.10 What are the performance benefits of Delta over Parquet?**

**Answer:** Better file management, data skipping, and query optimization.

**Performance Benefits:**

* **File Management** : Automatic compaction
* **Data Skipping** : Min/max statistics per file
* **ZORDERing** : Better data locality
* **Caching** : Metadata caching for faster queries

---

### **7.6 File Format Deep Dive - Apache Hudi**

#### **7.6.1 What is Apache Hudi and what data management problems does it solve?**

**Answer:** Hudi provides transactional data lake capabilities with upserts and incremental processing.

**Problems Solved:**

* UPSERT operations on data lakes
* Incremental data processing
* GDPR compliance (deletes)
* Change data capture (CDC)

---

#### **7.6.2 What are Copy-on-Write (CoW) and Merge-on-Read (MoR) tables in Hudi?**

**Answer:** Two table types with different trade-offs for upsert performance.

**CoW vs MoR:**

| Aspect                       | Copy-on-Write           | Merge-on-Read               |
| ---------------------------- | ----------------------- | --------------------------- |
| **Upsert Performance** | Slower (rewrites files) | Faster (writes delta logs)  |
| **Read Performance**   | Faster (no merging)     | Slower (merge at read time) |
| **Storage**            | Only base files         | Base files + delta logs     |
| **Use Case**           | Read-heavy workloads    | Write-heavy workloads       |

---

#### **7.6.3 When would you use Hudi over Delta Lake?**

**Answer:** Choose Hudi for specific CDC requirements, MoR patterns, or existing Hudi infrastructure.

**Hudi Preferred When:**

* Need Merge-on-Read for high-frequency updates
* Specific CDC requirements
* Existing Hudi expertise/infrastructure
* Kafka integration for real-time pipelines

---

#### **7.6.4 How does Hudi handle upserts?**

**Answer:** Uses indexing to quickly locate records and either rewrites files (CoW) or writes delta logs (MoR).

**Upsert Process:**

1. Look up record location using index
2. CoW: Rewrite entire files with updates
3. MoR: Write delta log files with changes
4. Update metadata

---

#### **7.6.5 What is Hudi's timeline and commit model?**

**Answer:** Hudi maintains a timeline of all actions on the dataset with commit metadata.

**Timeline Components:**

* Commits (data changes)
* Cleans (file cleanup)
* Compactions (MoR optimization)
* Savepoints (backups)

---

### **7.7 File Format Comparisons**

#### **7.7.1 Compare Parquet vs CSV**

| Aspect                       | Parquet               | CSV                         |
| ---------------------------- | --------------------- | --------------------------- |
| **Storage Efficiency** | 75-80% smaller        | Original size + compression |
| **Read Performance**   | 10-100x faster        | Slow (full scans)           |
| **Write Performance**  | Slower (columnar)     | Faster (sequential)         |
| **Schema Handling**    | Embedded schema       | No schema, inference needed |
| **Use Cases**          | Analytics, data lakes | Data exchange, simple ETL   |

---

#### **7.7.2 Compare Avro vs Parquet vs ORC**

| Aspect                        | Avro          | Parquet      | ORC          |
| ----------------------------- | ------------- | ------------ | ------------ |
| **Analytics Workloads** | Poor          | Excellent    | Excellent    |
| **Streaming Workloads** | Excellent     | Poor         | Poor         |
| **Schema Evolution**    | Excellent     | Good         | Good         |
| **Compression**         | Good          | Excellent    | Excellent    |
| **Ecosystem**           | Kafka, Hadoop | Spark, Arrow | Hive, Presto |

---

#### **7.7.3 Compare Delta Lake vs Apache Hudi vs Apache Iceberg**

| Aspect                      | Delta Lake        | Apache Hudi  | Apache Iceberg              |
| --------------------------- | ----------------- | ------------ | --------------------------- |
| **ACID Transactions** | Excellent         | Excellent    | Excellent                   |
| **Time Travel**       | Built-in          | Built-in     | Built-in                    |
| **Performance**       | Excellent (Spark) | Good         | Excellent (compute engines) |
| **Ecosystem Support** | Spark, Databricks | Spark, Flink | Spark, Flink, Trino         |
| **Schema Evolution**  | Good              | Good         | Excellent                   |

---

#### **7.7.4 When would you use JSON format despite its inefficiency?**

**Answer:** Use JSON for human readability, web APIs, configuration files, and rapid prototyping.

**JSON Use Cases:**

* API responses and web development
* Configuration files
* Rapid prototyping and development
* When human readability is required
* Semi-structured data with flexible schema

---

#### **7.7.5 Trade-offs between text formats and binary formats**

**Text vs Binary Trade-offs:**

| Aspect                     | Text Formats (CSV, JSON) | Binary Formats (Parquet, Avro) |
| -------------------------- | ------------------------ | ------------------------------ |
| **Human Readable**   | Yes                      | No                             |
| **Storage Size**     | Larger                   | Smaller                        |
| **Read Performance** | Slower                   | Faster                         |
| **Schema Support**   | Limited/None             | Strong                         |
| **Tool Support**     | Universal                | Specialized                    |

---

#### **7.7.6 How does compression affect different file formats differently?**

**Answer:** Columnar formats benefit more from compression due to data similarity within columns.

**Compression Impact:**

* **Columnar (Parquet/ORC)** : 70-90% reduction (similar values in columns)
* **Row-based (Avro)** : 50-70% reduction (mixed values in rows)
* **Text (CSV/JSON)** : 60-80% with gzip, but still larger than binary

**Interview Tips:**

* Columnar + compression = best for analytics
* Consider compression speed vs ratio trade-offs
* Snappy for speed, ZSTD for balance, GZIP for maximum compression

---

**Final Interview Tips:**

* Always consider use case when recommending formats
* Parquet for analytics, Avro for streaming, Delta for reliability
* Understand the trade-offs between read vs write performance
* Know when to use managed table formats vs raw file formats

This comprehensive coverage should prepare you well for any file format questions in your Spark interview!
