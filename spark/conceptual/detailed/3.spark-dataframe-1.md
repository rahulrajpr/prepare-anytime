## 3. DataFrame & Dataset API

### 3.1 Basic DataFrame Operations

#### 3.1.1 What is the toDF() method and what is its purpose?

**Definition:** `toDF()` is a method that converts RDDs, lists, or other collections into DataFrames, optionally allowing column naming.

**Usage Examples:**

```python
# From RDD
rdd = sc.parallelize([(1, "Alice"), (2, "Bob")])
df = rdd.toDF(["id", "name"])

# From list of tuples
data = [(1, "Alice"), (2, "Bob")]
df = spark.createDataFrame(data).toDF("id", "name")
```


**Key Points:**

* Converts unstructured data to structured DataFrame
* Enables schema inference or explicit column naming
* Essential for transitioning from RDD to DataFrame API

#### 3.1.2 What does the collect() method do? Explain how it brings data from executor nodes to the driver.

**Definition:**`collect()` is an action that retrieves all data from all partitions and returns it as a Python list to the driver program.

**Data Flow:**

1. Each executor serializes its partition data
2. Data transferred over network to driver node
3. Driver deserializes and aggregates into local list
4. Returns `List[Row]` to driver memory

**Interview Tip:** Emphasize the network transfer and driver memory implications.

#### 3.1.3 What are the risks of using collect() on large datasets?

**Critical Risks:**

* **Driver Out-of-Memory:** Entire dataset must fit in driver's RAM
* **Network Bottleneck:** Massive data transfer across network
* **Single Point Failure:** Driver crash loses entire computation
* **Performance Impact:** Blocks until all data is collected

**Safe Alternatives:**

* `take(n)` for sampling
* `foreach()` for distributed processing
* Write to storage instead of collecting

#### 3.1.4 What does dataframe.schema.simpleString() return?

**Definition:** Returns a simplified string representation of the DataFrame schema in DDL-like format.

**Example Output:**

**python**

```
df.schema.simpleString()
# Returns: "struct<id:int,name:string,age:int>"
```

**Use Case:** Quick schema inspection and documentation.

#### 3.1.5 What does dataframe.rdd.getNumPartitions() return and what is its significance?

**Definition:** Returns the number of partitions in the underlying RDD, indicating data distribution parallelism.

**Significance:**

* **Parallelism:** More partitions = more parallel tasks
* **Memory:** Too many partitions cause overhead
* **Performance:** Optimal partitioning crucial for shuffle operations
* **Tuning:** Guides repartitioning decisions

### 3.1.1 Action Methods - Return Type Comparison

#### 3.1.1.1 What does first() return and what is its return type?

**Definition:**`first()` returns the first row of the DataFrame as a `Row` object.

**Return Type:**`pyspark.sql.types.Row`

**Behavior:** Retrieves first element without guaranteed ordering

#### 3.1.1.2 What does head() return by default? How does head(n) differ?

**Definition:**`head()` returns first row as `Row`, `head(n)` returns first n rows as `List[Row]`

**Return Types:**

* `head()` → `Row`
* `head(5)` → `List[Row]` (5 elements)

#### 3.1.1.3 What does take(n) return and what is its return type?

**Definition:**`take(n)` returns first n rows as a Python list.

**Return Type:**`List[Row]`

**Advantage:** Limited data transfer vs `collect()`

#### 3.1.1.4 What does collect() return and why is it dangerous?

**Definition:**`collect()` returns all rows as `List[Row]`

**Dangers:**

* Driver memory overflow
* Network congestion
* Application failure on large datasets

#### 3.1.1.5 What is the difference between first() and head() in terms of return type?

**Comparison:**

* `first()`: Always returns single `Row` object
* `head()`: Returns `Row` (default) or `List[Row]` (with parameter)

#### 3.1.1.6 Is first() equivalent to head(1)[0]? Explain.

**Answer:** Functionally similar but `first()` is more efficient as it retrieves only one row, while `head(1)` retrieves one row into a list then extracts it.

#### 3.1.1.7 What is the difference between take(3) and head(3) in terms of return type?

**Answer:** No difference - both return `List[Row]` with 3 elements. They are aliases.

#### 3.1.1.8 When would you use take() vs collect()?

**Usage Guidelines:**

* **`take(n)`** : Sampling, debugging, small result sets
* **`collect()`** : Only when result is guaranteed to be small and fits in driver memory
* **Best Practice** : Always prefer `take()` unless you absolutely need all data

### 3.1.2 Sorting Methods - Performance Comparison

#### 3.1.2.1 What is the difference between sort() and orderBy()?

**Definition:**`sort()` and `orderBy()` are aliases - they perform identical global sorting operations.

**Code Equivalence:**

**python**

```
df.sort("column") == df.orderBy("column")
```

#### 3.1.2.2 Are sort() and orderBy() aliases or different methods?

**Answer:** They are complete aliases with identical functionality and performance.

#### 3.1.2.3 What is sortWithinPartitions() and how does it differ from sort()?

**Definition:**`sortWithinPartitions()` sorts data within each partition without global ordering.

**Key Difference:**

* `sort()`: Global total order across all partitions
* `sortWithinPartitions()`: Local order within each partition

#### 3.1.2.4 What is the scope of sorting for sort() vs sortWithinPartitions()?

**Sorting Scope:**

* `sort()`: **Global** - data sorted across entire dataset
* `sortWithinPartitions()`: **Local** - data sorted within each partition only

#### 3.1.2.5 Does sortWithinPartitions() trigger a shuffle? Why or why not?

**Answer:****No shuffle** - it operates on existing partitions without data movement between executors.

#### 3.1.2.6 When would you use sortWithinPartitions() instead of sort()?

**Use Cases for sortWithinPartitions():**

* Pre-sorting before window operations
* Local aggregations within partitions
* When global order isn't required but partition-level order is beneficial
* To avoid shuffle cost of global sort

#### 3.1.2.7 What are the performance implications: sort() vs sortWithinPartitions()?

**Performance Comparison:**

| Aspect                | sort()                 | sortWithinPartitions() |
| --------------------- | ---------------------- | ---------------------- |
| **Shuffle**     | Yes (expensive)        | No (cheap)             |
| **Network I/O** | High                   | None                   |
| **Memory**      | Shuffle buffers needed | Local memory only      |
| **Use Case**    | Global ordering        | Partition optimization |

#### 3.1.2.8 What is the network I/O cost of sort() vs sortWithinPartitions()?

**Network Impact:**

* `sort()`: **High** - requires full data shuffle
* `sortWithinPartitions()`: **Zero** - no data movement between nodes

### 3.2 Schema Management

#### 3.2.1 What are the three approaches to define schemas in Spark DataFrame Reader API?

**Three Schema Approaches:**

1. **Infer Schema** : Automatic detection from data
2. **Explicit Schema** : Programmatic definition
3. **Implicit Schema** : Format-specific defaults

#### 3.2.2 What are the performance implications of using inferSchema vs explicit schema specification?

**Performance Comparison:**

* **inferSchema** :
* **Slow** : Requires data scan to detect types
* **Risky** : May infer incorrect types
* **Convenient** : Good for exploration
* **Explicit Schema** :
* **Fast** : No initial data scan needed
* **Safe** : Type safety guaranteed
* **Production** : Recommended for production

#### 3.2.3 What are the two ways to supply an explicit schema for DataFrame Reader?

**Two Explicit Schema Methods:**

**a) StructType/StructField Approach:**

**python**

```
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True)
])
```

**b) SQL DDL String Notation:**

**python**

```
schema = "id INT, name STRING, age INT"
```

#### 3.2.4 When would you use each schema definition approach?

**Usage Guidelines:**

* **StructType** : Complex schemas, programmatic generation, type safety
* **DDL String** : Simple schemas, SQL familiarity, quick prototyping
* **inferSchema** : Data exploration, unknown schemas

### 3.3 Spark Data Types

#### 3.3.1 What are the primitive data types in Spark?

**Primitive Types:**

* `StringType`: Text data
* `IntegerType`, `LongType`: Whole numbers
* `DoubleType`, `FloatType`: Decimal numbers
* `BooleanType`: True/False values
* `DateType`, `TimestampType`: Date/time values
* `BinaryType`: Raw bytes

#### 3.3.2 What complex data types does Spark support?

**Complex Types:**

* `ArrayType`: Ordered collection of elements
* `MapType`: Key-value pairs
* `StructType`: Nested structured data

#### 3.3.3 How do you define an ArrayType column in a schema?

**ArrayType Definition:**

**python**

```
from pyspark.sql.types import ArrayType, StringType

ArrayType(StringType(), True)  # Array of strings, nullable
ArrayType(IntegerType(), False) # Array of integers, not nullable
```

#### 3.3.4 How do you define a MapType column in a schema?

**MapType Definition:**

**python**

```
from pyspark.sql.types import MapType, StringType, IntegerType

MapType(StringType(), IntegerType(), True)  # Map<String, Int>, nullable
```

#### 3.3.5 How do you define a StructType (nested structure) in a schema?

**StructType Definition:**

**python**

```
from pyspark.sql.types import StructType, StructField

address_schema = StructType([
    StructField("street", StringType(), True),
    StructField("city", StringType(), True),
    StructField("zip", StringType(), True)
])
```

#### 3.3.6 What is the difference between nullable=True and nullable=False in schema definition?

**Nullability:**

* `nullable=True`: Column can contain null values (default)
* `nullable=False`: Column cannot contain nulls (enforced by Spark)

**Performance:** Non-nullable columns can enable optimizations in some cases.

#### 3.3.7 How do you handle null values in different data types?

**Null Handling:**

* All data types support null values when `nullable=True`
* Use `fillna()` to replace nulls
* Use `dropna()` to remove rows with nulls
* Use `isNull()`, `isNotNull()` for filtering

#### 3.3.8 What are DateType and TimestampType? How do they differ?

**Comparison:**

* **DateType** : Calendar date only (year, month, day)
* **TimestampType** : Date + time with timezone support

**Usage:**

**python**

```
DateType()      # 2024-01-15
TimestampType() # 2024-01-15 14:30:45.123
```

#### 3.3.9 What is DecimalType and when should you use it instead of DoubleType?

**DecimalType vs DoubleType:**

* **DecimalType** : Exact decimal arithmetic (financial calculations)
* **DoubleType** : Approximate floating-point (scientific calculations)

**Use DecimalType for:**

* Currency calculations
* Precise decimal operations
* Avoiding floating-point rounding errors

#### 3.3.10 What is BinaryType and what are its use cases?

**BinaryType Definition:** Stores raw byte arrays without interpretation.

**Use Cases:**

* Image files
* Serialized objects
* PDF documents
* Any binary payload
* Machine learning feature vectors
