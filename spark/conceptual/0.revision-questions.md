# Apache Spark Interview Questions - Ultimate Comprehensive Guide (Mega Edition)

## 1. Spark Architecture & Core Concepts

### 1.1 Cluster Architecture
1. What is the Spark cluster architecture? Explain the roles of driver, worker nodes, executors, and cores.
2. What is the difference between an executor, a worker node, and a thread in Spark?
3. How do these components interact during job execution?
4. What is the role of the cluster manager in Spark architecture?

### 1.2 Execution Model
5. What is a DAG (Directed Acyclic Graph) in Spark and how does Spark use it for task scheduling?
6. What is the difference between the DAG Scheduler and the Task Scheduler?
7. What is lazy evaluation in Spark? What are its advantages?
8. Explain the execution hierarchy: spark-submit → applications → jobs → stages → tasks.
9. What happens on the driver node when an action is called on a DataFrame?
10. What happens when a task completes on an executor node? How does the driver track progress?
11. Where does the data go after an action like `collect()` is executed?

### 1.3 Operations & Transformations
12. What is the difference between transformations and actions in Spark? Provide examples.
13. What are narrow dependency transformations? Provide examples.
14. What are wide dependency transformations? Provide examples.
15. Why are narrow transformations more efficient than wide transformations?

## 2. Spark Configuration & Deployment

### 2.1 SparkSession & Context
16. What is the difference between SparkSession and SparkContext?
17. How do DataFrames relate to SparkSession and RDDs to SparkContext?
18. Where is the SparkSession configuration defined?
19. What are the most important Spark configuration parameters?

### 2.2 Cluster Managers
20. What are the different cluster managers available for Spark (YARN, Kubernetes, Mesos, Standalone)?
21. Which cluster manager is preferred for production environments and why?
22. Which cluster managers are used in AWS Glue and Databricks platforms?
23. How is YARN used in Databricks, Google Dataproc, and on-premise setups like Cloudera?

### 2.3 Deployment Modes
24. What are the different deployment modes in Spark (client mode vs cluster mode)?
25. What are the implications of each mode for driver placement and resource allocation?
26. In a high-availability setup for Spark on Kubernetes, how is a driver or executor failure handled differently compared to YARN?

### 2.4 Dynamic Resource Allocation
27. What is dynamic allocation in Spark? How do you configure it?
28. What does `spark.dynamicAllocation.initialExecutors` control?
29. How do AWS Glue and Databricks implement autoscaling on top of Apache Spark?
30. How does dynamic scaling work with Kubernetes?

### 2.5 Cloud Platform Specifics
31. What is a DPU (Data Processing Unit) in AWS Glue?
32. What are AWS Spot Instances and how are they relevant for cost-effective Spark deployments?
33. What are the latest Spark versions available in Databricks, Google Dataproc, and AWS Glue?

## 3. DataFrame & Dataset API

### 3.1 Basic DataFrame Operations
34. What is the `toDF()` method and what is its purpose?
35. What does the `collect()` method do? Explain how it brings data from executor nodes to the driver.
36. What are the risks of using `collect()` on large datasets?
37. What does `dataframe.schema.simpleString()` return?
38. What does `dataframe.rdd.getNumPartitions()` return and what is its significance?

### 3.1.1 Action Methods - Return Type Comparison
39. What does `first()` return and what is its return type?
40. What does `head()` return by default? How does `head(n)` differ?
41. What does `take(n)` return and what is its return type?
42. What does `collect()` return and why is it dangerous?
43. What is the difference between `first()` and `head()` in terms of return type?
44. Is `first()` equivalent to `head(1)[0]`? Explain.
45. What is the difference between `take(3)` and `head(3)` in terms of return type?
46. When would you use `take()` vs `collect()`?

### 3.1.2 Sorting Methods - Performance Comparison
47. What is the difference between `sort()` and `orderBy()`?
48. Are `sort()` and `orderBy()` aliases or different methods?
49. What is `sortWithinPartitions()` and how does it differ from `sort()`?
50. What is the scope of sorting for `sort()` vs `sortWithinPartitions()`?
51. Does `sortWithinPartitions()` trigger a shuffle? Why or why not?
52. When would you use `sortWithinPartitions()` instead of `sort()`?
53. What are the performance implications: `sort()` vs `sortWithinPartitions()`?
54. What is the network I/O cost of `sort()` vs `sortWithinPartitions()`?

### 3.2 Schema Management
55. What are the three approaches to define schemas in Spark DataFrame Reader API?
   - a) Infer schema
   - b) Explicitly specify schema
   - c) Implicit schema from file format
56. What are the performance implications of using `inferSchema` vs explicit schema specification?
57. What are the two ways to supply an explicit schema for DataFrame Reader?
   - a) StructType/StructField approach
   - b) SQL DDL string notation
58. When would you use each schema definition approach?

### 3.3 Spark Data Types
59. What are the primitive data types in Spark (StringType, IntegerType, LongType, DoubleType, BooleanType, etc.)?
60. What complex data types does Spark support (ArrayType, MapType, StructType)?
61. How do you define an ArrayType column in a schema?
62. How do you define a MapType column in a schema?
63. How do you define a StructType (nested structure) in a schema?
64. What is the difference between nullable=True and nullable=False in schema definition?
65. How do you handle null values in different data types?
66. What are DateType and TimestampType? How do they differ?
67. What is DecimalType and when should you use it instead of DoubleType?
68. What is BinaryType and what are its use cases?

### 3.4 Column Operations & Functions
69. What is the syntax difference when passing multiple columns: `.drop("col1", "col2")` vs `.dropDuplicates(["col1", "col2"])`?
70. When do you use varargs vs list for passing multiple column names?
71. What is the difference between `count(*)`, `count(1)`, and `count(col)`?
72. How do these count variations handle null values differently?
73. What does `monotonically_increasing_id()` function generate? Is it guaranteed to be sequential?
74. What are the practical use cases for `monotonically_increasing_id()`?
75. What is the difference between `row_number()`, `rank()`, and `dense_rank()` window functions?
76. When would you use `lead()` and `lag()` functions?
77. What is the `first()` and `last()` aggregate function? How do they handle nulls?
78. Explain the difference between `collect_list()` and `collect_set()`.
79. What does `explode()` function do? Provide an example use case.
80. What is the difference between `explode()` and `explode_outer()`?
81. What does `posexplode()` do and how is it different from `explode()`?
82. When to use `posexplode()` vs `posexplode_outer()`?
83. What is the difference between `explode()` and `posexplode()` in terms of output columns?
84. How does `inline()` differ from `inline_outer()` when working with arrays of structs?
85. How do you use `array_contains()` function?
86. What does `split()` function return and what is its data type?
87. How do you use `concat()` vs `concat_ws()` (concat with separator)?
88. What is `coalesce()` function and how does it differ from `coalesce()` for repartitioning?
89. What does `nvl()` or `ifnull()` do? Are they the same?
90. Explain `when().otherwise()` construct with examples.
91. What is the difference between `withColumn()` and `select()` for adding/transforming columns?
92. Can you use `withColumn()` multiple times in a chain? What are the performance implications?
93. What does `withColumnRenamed()` do? Can you rename multiple columns at once?
94. What is `selectExpr()` and when would you use it instead of `select()`?
95. How do you drop multiple columns efficiently?
96. What does `drop()` return if you try to drop a non-existent column?

### 3.5 String Functions
97. Explain the `regexp_extract()` function and its usage for pattern matching.
98. What is the difference between `regexp_extract()` and `regexp_replace()`?
99. How do you use `like()` and `rlike()` for pattern matching?
100. What is the difference between `like()`, `ilike()`, and `rlike()`?
101. Which pattern matching function is case-insensitive?
102. What pattern type does `rlike()` use (SQL wildcards or regex)?
103. What does `substring()` function do? What are its parameters?
104. How do you use `trim()`, `ltrim()`, and `rtrim()`?
105. What is `upper()`, `lower()`, `initcap()` used for?
106. How do you use `lpad()` and `rpad()` for padding strings?
107. What does `length()` function return for null values?
108. How do you check if a string contains a substring in Spark?

### 3.6 Date & Time Functions
109. What are the key date and time functions in Spark (current_date, current_timestamp, date_add, date_sub)?
110. How do you extract year, month, day from a date column?
111. What does `datediff()` function calculate?
112. How do you use `to_date()` and `to_timestamp()` for type conversion?
113. What is the difference between `unix_timestamp()` and `from_unixtime()`?
114. How do you handle different date formats when reading data?
115. What does `date_format()` function do?
116. How do you calculate the difference between two timestamps?
117. What is `add_months()` function used for?
118. How do you get the last day of the month using `last_day()`?
119. What does `next_day()` function do?
120. How do you handle timezone conversions in Spark?

### 3.6.1 Date & Time Intervals in Spark
121. What are the two main interval families in Spark (YEAR-MONTH and DAY-TIME)?
122. When do you use YEAR-MONTH interval vs DAY-TIME interval?
123. Why can't you directly cast a day interval to a month interval?
124. What is the common approximation used when converting between interval types?
125. What is `make_interval()` function? What makes it unique?
126. What parameters can you specify in `make_interval()`?
127. When would you use `make_interval()` over other interval functions?
128. What is `make_dt_interval()` function? What is its specific purpose?
129. What units does `make_dt_interval()` handle?
130. When would you use `make_dt_interval()` instead of `make_interval()`?
131. What is `make_ym_interval()` function? What is its specific purpose?
132. How does `make_ym_interval()` handle variable month lengths correctly?
133. When would you use `make_ym_interval()` for calendar-based calculations?
134. Compare `make_interval()` vs `make_dt_interval()` vs `make_ym_interval()` - when to use each?

### 3.6.2 Date Parsing & Formatting
135. What are the two main approaches to parsing dates in Spark SQL?
136. What date format does default `DATE()` or `CAST AS DATE` reliably support?
137. What happens when you use `DATE()` on non-ISO format strings?
138. When must you use `TO_DATE(expr, format)` instead of `DATE()`?
139. What format patterns are commonly used with `TO_DATE()`?
140. Are format pattern letters case-sensitive in `TO_DATE()`? Provide examples.
141. What is the difference between 'MM' and 'MMM' in date format patterns?
142. What is the difference between 'd', 'dd', and 'D' in date format patterns?
143. How does `DATE()` handle timestamp strings with time components?
144. What does `TO_DATE()` return for unparseable strings?
145. What are best practices for handling date formats in Spark pipelines?
146. What is `to_char()` function used for?
147. How do you use `to_timestamp()` with custom formats?
148. What is the Java DateTimeFormatter pattern syntax used in Spark?

### 3.7 Aggregate Functions
149. What is the difference between `sum()`, `sumDistinct()`, and `approx_count_distinct()`?
150. When would you use `approx_count_distinct()` instead of `countDistinct()`?
151. What is the difference between `approx_count_distinct()` and `count_min_sketch()`?
152. What does `approx_count_distinct()` measure vs `count_min_sketch()`?
153. Which function gives direct results and which returns serialized binary data?
154. What does `avg()` return for null values?
155. How do you use `min()` and `max()` functions?
156. What is `stddev()` and `variance()` used for?
157. What does `corr()` function calculate (correlation)?
158. How do you use `percentile_approx()` function?
159. What is `grouping()` and `grouping_id()` used for in GROUP BY operations?
160. What does `grouping(col)` return when a column is aggregated?
161. What does `grouping(col)` return when a column is present in the grouping level?
162. What does `grouping_id()` return and how is it calculated?
163. What is a bitmask in the context of `grouping_id()`?
164. When should you use `grouping(col)` vs `grouping_id()`?
165. What is the difference between `GROUP BY` and `GROUP BY WITH ROLLUP`?
166. How does `ROLLUP` create hierarchical aggregations?
167. How does `GROUP BY WITH ROLLUP` handle NULL values differently?

### 3.7.1 Aggregation After GroupBy - Critical Distinction
168. Can you use `select()` after `groupBy()`? Why or why not?
169. What object type does `groupBy()` return?
170. What methods are available on a GroupedData object?
171. What is the difference between DataFrame methods and GroupedData methods?
172. Can you use `agg()` on a regular DataFrame (without groupBy)?
173. Are `agg()` and `select()` interchangeable on a regular DataFrame?
174. What is the only way to perform aggregations after `groupBy()`?
175. Why does `df.groupBy("col").select(sum("value"))` fail?
176. What is the correct syntax for aggregation after groupBy?

### 3.7.2 ROLLUP and CUBE - Advanced Grouping
177. What is the basic purpose of `rollup()` vs `groupBy()`?
178. How many aggregation levels does `rollup()` create?
179. What is the formula for number of result rows in `rollup()`?
180. What is the basic purpose of `cube()` vs `rollup()`?
181. How many combinations does `cube()` create?
182. What is the formula for number of combinations in `cube()` (2^n)?
183. Which is faster: `groupBy()`, `rollup()`, or `cube()`?
184. When would you use `rollup()` over `groupBy()`?
185. When would you use `cube()` over `rollup()`?
186. How do NULL values indicate aggregation levels in `rollup()` and `cube()`?
187. What is the difference between hierarchical relationships in `rollup()` vs `cube()`?
188. What are typical use cases for `rollup()` (financial reports, organizational hierarchies)?
189. What are typical use cases for `cube()` (business intelligence, cross-analysis)?
190. How does `grouping_id()` help identify aggregation levels in `rollup()` and `cube()`?

### 3.7.3 SQL Database Support for ROLLUP and CUBE
191. Which major SQL databases support `ROLLUP`?
192. Which major SQL databases support `CUBE`?
193. Does MySQL fully support `ROLLUP` and `CUBE`?
194. What is the MySQL syntax for `ROLLUP` (WITH ROLLUP)?
195. Does SQLite support `ROLLUP` or `CUBE`?
196. What is the standard SQL syntax for `ROLLUP` and `CUBE`?
197. Do SQL Server, PostgreSQL, and Oracle support both `ROLLUP` and `CUBE`?
198. Does Spark SQL support `ROLLUP` and `CUBE` in both SQL and DataFrame API?
199. Do Trino and Databricks support `ROLLUP` and `CUBE`?
200. What additional grouping features does Spark SQL support (GROUPING SETS)?

### 3.8 Array Functions
201. How do you access array elements using `getItem()` or bracket notation?
202. What does `array()` function do to create arrays from columns?
203. How do you use `array_contains()` to check for element existence?
204. What does `array_distinct()` do?
205. How do you use `array_intersect()`, `array_union()`, `array_except()`?
206. What does `array_join()` do?
207. How do you sort array elements using `array_sort()`?
208. What is `array_max()`, `array_min()`, `size()` used for?
209. How do you use `flatten()` for nested arrays?
210. What does `array_repeat()` function do?
211. How do you use `slice()` to extract a portion of an array?
212. What is `array_position()` used for?
213. How do you remove elements from an array using `array_remove()`?
214. What does `shuffle()` do to array elements?
215. How do you use `zip_with()` for element-wise array operations?

### 3.8.1 Advanced Array Functions & Comparisons
216. What is the difference between `size()` and `cardinality()` functions?
217. Are `size()` and `cardinality()` functionally identical?
218. When would you use `size()` vs `cardinality()` (personal preference)?
219. What is the difference between `reverse()`, `sort_array()`, and `array_sort()`?
220. What parameters does `reverse()` accept? What does it do?
221. What parameters does `sort_array()` accept? How do you control sort order?
222. What parameters does `array_sort()` accept? What makes it unique?
223. Can you use custom sorting logic with `sort_array()`? What about `array_sort()`?
224. When would you use `sort_array()` vs `array_sort()`?
225. What does `aggregate()` function do on arrays?
226. What parameters does `aggregate()` accept (start, merge, finish)?
227. What is the difference between `aggregate()` and `reduce()` on arrays?
228. Are `aggregate()` and `reduce()` functionally identical?
229. Which name is SQL standard: `aggregate()` or `reduce()`?
230. What does `concat()` do for arrays? Can it handle multiple arrays?
231. What is the difference between `element_at()` and `try_element_at()`?
232. What happens when `element_at()` tries to access a non-existent index?
233. What does `try_element_at()` return for non-existent indices?
234. When should you use `try_element_at()` instead of `element_at()`?
235. What does `exists()` function do on arrays?
236. What does `forall()` function do on arrays?
237. What is the difference between `exists()` and `forall()`?
238. Do `exists()` and `forall()` short-circuit? What does this mean?
239. What does `filter()` function do on arrays?
240. What is the difference between `filter()` and `exists()`?
241. How is filtering arrays different from filtering DataFrames?
242. What does `transform()` function do on arrays?
243. What parameters does `transform()` lambda accept (element, index)?
244. What does `arrays_zip()` function do?
245. What does `zip_with()` function do?
246. What is the difference between `arrays_zip()` and `zip_with()`?
247. How many arrays can `arrays_zip()` handle?
248. How many arrays can `zip_with()` handle?
249. What output structure does `arrays_zip()` create?
250. Can you customize the output with `arrays_zip()`?
251. Does `zip_with()` require a lambda function?
252. How do `arrays_zip()` and `zip_with()` handle arrays of different lengths?

### 3.9 Map Functions
253. How do you create a map using `map()` function or `map_from_arrays()`?
254. How do you access map values using `getItem()` or bracket notation?
255. What does `map_keys()` and `map_values()` return?
256. How do you use `map_concat()` to merge maps?
257. What does `map_from_entries()` do?
258. How do you explode maps using `explode()` - what columns does it create?
259. What is `map_filter()` used for?
260. How do you get the size of a map using `size()`?

### 3.9.1 Map Functions Deep Dive & Comparisons
261. What is the difference between `filter()` and `map_filter()`?
262. What input types do `filter()` vs `map_filter()` accept?
263. How many lambda parameters does `map_filter()` accept?
264. What does `map_filter()` return?
265. What does `transform()` do for arrays?
266. What does `transform_keys()` do for maps?
267. What does `transform_values()` do for maps?
268. Compare `transform()` vs `transform_keys()` vs `transform_values()` - when to use each?
269. What lambda parameters does `transform()` accept?
270. What lambda parameters do `transform_keys()` and `transform_values()` accept?
271. Does `transform()` change the size of an array?
272. Does `transform_keys()` or `transform_values()` change the size of a map?
273. What changes when you use `transform_keys()` - keys or values?
274. What changes when you use `transform_values()` - keys or values?
275. Can you use `size()` and `cardinality()` on maps?
276. Does `element_at()` work on maps? How?
277. Does `try_element_at()` work on maps?

### 3.10 Struct Functions
278. How do you access struct fields using dot notation or `getField()`?
279. What does `struct()` function do to create structs from columns?
280. How do you flatten struct columns?
281. Can you use `withColumn()` to modify a field within a struct?
282. How do you select specific fields from a nested struct?

### 3.10.1 Struct Deep Dive & Comparisons
283. What is a Struct in Apache Spark?
284. What is the purpose of using StructType in DataFrames?
285. How do structs enable representation of nested or hierarchical data?
286. What is the difference between `struct()` and `named_struct()`?
287. How do you define field names in `named_struct()`?
288. What field names does `struct()` generate by default?
289. How do you access fields in a struct created with `named_struct()`?
290. What happens to field names when using `struct()` with column names vs literals?
291. When should you use `named_struct()` over `struct()`?
292. When should you use `struct()` over `named_struct()`?
293. What does the schema look like for `named_struct('city','value','state','value')`?
294. What does the schema look like for `struct('value1', 'value2')`?
295. Can you nest structs within structs?
296. How do you access deeply nested struct fields?

### 3.11 Type Conversion & Casting
297. How do you cast columns using `cast()` function?
298. What is the difference between `cast()` and `astype()`?
299. What happens when casting fails (e.g., string "abc" to integer)?
300. How do you handle casting errors gracefully?
301. What does `try_cast()` do in Spark SQL?

### 3.11.1 Numeric Type Casting Functions
302. What is `tinyint()` function and what data type does it cast to?
303. What is `smallint()` function and when would you use it?
304. What is the difference between `int()` and `bigint()` casting?
305. When should you use `tinyint` vs `smallint` vs `int` vs `bigint`?
306. What are the value ranges for tinyint, smallint, int, and bigint?

### 3.11.2 Other Specific Casting Functions
307. What does `binary()` function do?
308. What is `boolean()` casting function used for?
309. How do you use `date()` function for type casting?
310. What does `decimal()` function do?
311. What is the difference between `double()` and `float()` casting?
312. What does `string()` function do for type conversion?
313. How do you use `timestamp()` function?

### 3.11.3 Type Conversion (to_ Functions)
314. What is `to_char()` function? What does it convert from and to?
315. What is `to_varchar()` function used for?
316. What does `to_number()` function do? When do you use it?
317. What is the difference between `to_date()` and `date()` casting?
318. What is the difference between `to_timestamp()` and `timestamp()` casting?
319. What does `to_json()` function do? What data types can it convert?
320. What is `to_binary()` function used for?
321. When would you use `to_` functions vs direct casting with `cast()`?

### 3.11.4 FLOAT vs DOUBLE vs DECIMAL
322. What is the precision difference between FLOAT, DOUBLE, and DECIMAL?
323. How much storage does each numeric type use (FLOAT, DOUBLE, DECIMAL)?
324. What types of arithmetic do FLOAT and DOUBLE use (approximate vs exact)?
325. Do FLOAT and DOUBLE have rounding errors? What about DECIMAL?
326. Which numeric type is fastest for computations?
327. When should you use FLOAT or DOUBLE for data processing?
328. When should you ALWAYS use DECIMAL instead of FLOAT/DOUBLE?
329. Why is DECIMAL the only choice for financial and monetary data?
330. What is the maximum precision supported by DECIMAL in Spark?
331. What are the performance trade-offs between DECIMAL and FLOAT/DOUBLE?
332. Provide examples where using FLOAT/DOUBLE would cause problems in financial calculations.

### 3.12 Null Handling & Data Cleaning
333. What is the difference between `dropna()` and `fillna()`?
334. How do you drop rows with nulls in specific columns using `dropna(subset=[])`?
335. What are the different threshold options in `dropna()`?
336. How do you fill nulls with different values for different columns?
337. What does `na.replace()` do?
338. What is the critical difference between `na.replace()` and `na.fill()`?
339. Is `na.replace()` used for handling missing data or replacing existing values?
340. Can you use `na.replace()` to replace NULL values? Why or why not?
341. How do you use `isNull()` and `isNotNull()` for filtering?
342. What is `nanvl()` used for (NaN value handling)?
343. How do you distinguish between null and NaN in Spark?
344. What is the difference between NULL and NaN in terms of meaning and data types?
345. How do NULL and NaN behave differently in comparisons?
346. What does `dropDuplicates()` do? How do you specify subset of columns?
347. Does `dropDuplicates()` preserve the order of rows?

### 3.12.1 COALESCE, NVL, and NVL2 Functions
348. What does `COALESCE()` function do?
349. How many arguments can `COALESCE()` accept?
350. What does `NVL()` function do? How is it different from `COALESCE()`?
351. How many arguments does `NVL()` accept?
352. What does `NVL2()` function do?
353. What are the three arguments in `NVL2()` and what do they represent?
354. Compare `COALESCE()` vs `NVL()` vs `NVL2()` - when to use each?
355. Is `NVL()` a SQL standard function or Oracle compatibility function?
356. Can you use `COALESCE()` with more than 2 arguments? Provide an example.
357. What does `NVL2(NULL, 'Y', 'N')` return?
358. What does `NVL(NULL, 'X')` return?
359. How would you replicate `NVL2()` behavior using `CASE WHEN`?

### 3.13 Column Expressions & SQL Functions
360. What is the difference between using column names as strings vs Column objects (col(), F.col())?
361. When must you use `col()` or `F.col()` instead of string column names?
362. What does `expr()` function allow you to do?
363. How do you reference columns from different DataFrames after a join?
364. What is the `alias()` method used for?
365. What does `name()` method return for a Column object?

### 3.14 Conditional Logic & Case Statements
366. How do you create complex conditional logic using `when().when().otherwise()`?
367. What happens if you don't provide an `otherwise()` clause?
368. How do you implement SQL CASE WHEN logic in PySpark?
369. Can you nest `when()` conditions? Provide an example?

### 3.15 JSON Functions
370. What is the difference between JSON as a file vs JSON as a column value?
371. Why does Spark convert JSON to Structs (not Maps) by default when parsing?
372. What are the performance implications of Struct vs Map for JSON data?
373. How do you parse JSON strings using `from_json()`?
374. What schema do you need to provide for `from_json()`?
375. How do you convert structs to JSON using `to_json()`?
376. What is the difference between `from_json()` and `to_json()`?
377. What does `get_json_object()` do?
378. How do you use `json_tuple()` to extract multiple fields?
379. What is the difference between `from_json()` and `json_tuple()`?
380. What is the difference between `from_json()` and `get_json_object()` in terms of efficiency?
381. When should you use `from_json()` vs `get_json_object()`?
382. What does `schema_of_json()` function do?
383. What does `json_array_length()` return?
384. What does `json_object_keys()` return?

### 3.16 Advanced Column Operations
385. What does `lit()` function do? When do you use it?
386. How do you create a column with constant values across all rows?
387. What is `input_file_name()` function used for?
388. How do you use `spark_partition_id()` to see data distribution?
389. What does `hash()` function compute?
390. What is `md5()` and `sha1()` used for?
391. How do you use `crc32()` for checksums?
392. What does `base64()` and `unbase64()` do?
393. How do you generate random values using `rand()` and `randn()`?

### 3.17 Set Operations on DataFrames
394. What is the difference between `union()` and `unionAll()`?
395. What is the critical inconsistency between DataFrame API and Spark SQL for union operations?
396. In DataFrame API, do `union()` and `unionAll()` keep or remove duplicates?
397. In Spark SQL, does `UNION` keep or remove duplicates?
398. In Spark SQL, does `UNION ALL` keep or remove duplicates?
399. Why is this inconsistency important to remember?
400. What does `unionByName()` do? How is it different from `union()`?
401. What does `unionByName()` do when schemas differ between DataFrames?
402. What is the `allowMissingColumns` parameter in `unionByName()`?
403. What does `unionByName(allowMissingColumns=True)` enable?
404. When would you use `union()` vs `unionByName()`?
405. What is "strict mode" vs "flexible mode" vs "forgiving mode" for unions?
406. How do you use `intersect()` to find common rows?
407. What is the difference between `intersect()` and `intersectAll()`?
408. Does `intersect()` show unique or all common records?
409. Does `intersectAll()` show unique or all common records?
410. How does `intersectAll()` handle duplicate counts?
411. What does `intersect()` answer vs `intersectAll()`?
412. When would you use `intersect()` vs `intersectAll()`?
413. What does `subtract()` (or `exceptAll()`) do?
414. Do set operations require the same schema in both DataFrames?
415. How do set operations handle duplicates?

### 3.18 Comparison & Logical Operators
416. What is the difference between `=` and `<=>` (null-safe equality)?
417. How does `=` handle NULL comparisons?
418. How does `<=>` handle NULL comparisons?
419. When should you use `<=>` instead of `=`?
420. What is the DataFrame API equivalent of `<=>`?
421. Explain the AND/OR truth table with NULL values.
422. What does `TRUE AND NULL` return?
423. What does `FALSE OR NULL` return?
424. What is short-circuit evaluation in AND/OR operations?

### 3.19 Window Functions
425. What is `rowsBetween` in window functions? Provide examples.
426. What is `rangeBetween` in window functions? How does it differ from `rowsBetween`?
427. Does data shuffling occur during window function operations? Why or why not?
428. What is the difference between `CUME_DIST()` and `PERCENT_RANK()`?
429. What does `CUME_DIST()` calculate?
430. What does `PERCENT_RANK()` calculate?
431. When would you use `CUME_DIST()` vs `PERCENT_RANK()`?
432. What is `asc_nulls_first` vs `asc_nulls_last` in ordering?
433. Where do NULLs appear with `asc_nulls_first`?
434. Where do NULLs appear with `asc_nulls_last`?

### 3.20 DataFrame Gotchas & Common Pitfalls
435. Why does chaining multiple `withColumn()` calls have performance implications?
436. What is the better alternative to multiple `withColumn()` calls?
437. When joining two tables with the same column name (e.g., 'id'), why does `select("*")` work but `select("id")` throws an "ambiguous column" error?
438. How do you resolve column name ambiguity after joins?
439. What happens when you call an action multiple times on the same DataFrame? Is it recomputed?
440. Why should you be careful with `collect()` on large datasets?
441. What is the difference between `df.count()` and `df.select(count("*"))`?
442. Can you modify a DataFrame in place? Why or why not?
443. What happens when you try to access a column that doesn't exist?
444. Why might `df.show()` show different results than the actual data?
445. What is the behavior of `limit()` - does it guarantee which rows are returned?
446. How do column name case sensitivity work in Spark (spark.sql.caseSensitive)?

### 3.21 Performance Tips for DataFrame Operations
447. Why is it better to filter data early in your transformation pipeline?
448. What is the performance difference between `filter()` and `where()` (trick question)?
449. When should you use `repartition()` vs `coalesce()`?
450. What is the performance impact of using UDFs vs built-in functions?
451. Why is `reduceByKey()` preferred over `groupByKey()` in RDD operations?
452. How does column pruning (selecting only needed columns) improve performance?
453. What is predicate pushdown and how does it improve query performance?
454. Why should you avoid using `count()` unnecessarily in your code?

### 3.22 Performance Optimization Best Practices
455. What is the optimal partition size for Spark processing (128MB-256MB)?
456. How do you determine the right number of partitions for your data?
457. What is the formula: optimal_partitions = total_data_size / target_partition_size?
458. Why is it important to avoid small files in distributed processing?
459. What is the small file problem and its performance impact?
460. How do you consolidate small files before processing?
461. What is file compaction and when should you perform it?
462. How does data skewness affect query performance?
463. What are the symptoms of data skew in Spark UI?
464. How do you identify skewed partitions in Spark UI?
465. What is the difference between task duration for skewed vs balanced partitions?
466. What is salting technique for handling data skew?
467. How do you implement salting for skewed join keys?
468. What is broadcast salting and when do you use it?
469. How does adding random salt keys help distribute skewed data?
470. What is isolated salting vs full salting?
471. How many salt keys should you add (multiplicative factor)?
472. What is the performance cost of salting?
473. When should you avoid salting?
474. What is adaptive execution and how does it handle skew automatically?
475. How do you optimize wide transformations (joins, groupBy, repartition)?
476. What is shuffle optimization and why is it critical?
477. How does reducing shuffle improve performance?
478. What is map-side combine and how does it reduce shuffle?
479. What is the difference between `reduceByKey` and `groupByKey`?
480. Why does `reduceByKey` perform better than `groupByKey`?
481. How does `reduceByKey` reduce data transfer?
482. What is combiner logic in map-side operations?
483. When should you use `aggregateByKey` instead of `reduceByKey`?
484. How do you optimize aggregations to minimize shuffle?
485. What is partial aggregation and how does it work?
486. How does `spark.sql.shuffle.partitions` affect aggregation performance?
487. What is the relationship between parallelism and shuffle partitions?
488. How many cores should process each partition ideally (2-4 partitions per core)?
489. What happens if you have too many partitions?
490. What happens if you have too few partitions?
491. How do you balance between parallelism and overhead?
492. What is task serialization overhead?
493. What is task scheduling overhead?
494. How does task scheduling delay affect small tasks?
495. What is the minimum task duration to justify distributed processing?
496. How do you optimize very short tasks (< 100ms)?
497. What is task consolidation and when should you use it?
498. How does broadcast join eliminate shuffle?
499. When should you broadcast the smaller table?
500. What is the maximum size for broadcast (driver memory constraint)?
501. How do you calculate broadcast size vs executor memory?
502. What is the 20% rule for broadcast size?
503. How does bucketing eliminate shuffle in joins?
504. What is pre-shuffling data using bucketing?
505. How do bucketed tables avoid sort-merge join shuffle?
506. What is the cost of creating bucketed tables?
507. When is bucketing worth the upfront cost?
508. How do you verify bucketing is being used in joins?
509. What is partition pruning and how does it reduce data scanning?
510. How does partitioning by column enable pruning?
511. What is the difference between partition pruning and predicate pushdown?
512. How do you optimize for partition pruning?
513. What is dynamic partition pruning (DPP) vs static pruning?
514. How does caching improve performance for iterative algorithms?
515. When should you cache intermediate results?
516. When is caching wasteful (single-use data)?
517. How do you determine what to cache in a multi-stage job?
518. What is the cost of caching (memory usage)?
519. How do you measure cache effectiveness (cache hit ratio)?
520. What is checkpointing and when should you use it vs caching?
521. How does checkpointing break lineage?
522. What is the performance benefit of breaking long lineages?
523. When does long lineage cause performance problems?
524. How do you identify long lineage issues?
525. What is lineage recomputation overhead?
526. How does checkpointing prevent recomputation?
527. What is the cost of checkpointing (disk I/O)?
528. Where should you place checkpoint directory (HDFS, S3)?
529. How do you optimize filter ordering in chained operations?
530. What is the most selective filter and why should it come first?
531. How does filter selectivity affect downstream operations?
532. What is the benefit of filtering before expensive operations (joins, aggregations)?
533. How do you calculate filter selectivity ratio?
534. How does column selection (projection) affect performance?
535. Why should you select only required columns early?
536. What is the I/O savings from column pruning?
537. How does columnar format (Parquet) benefit from column pruning?
538. What is the memory savings from selecting fewer columns?
539. How do you optimize expensive string operations?
540. Why are string operations slower than numeric operations?
541. How does data type affect processing speed?
542. Should you convert strings to numeric types when possible?
543. What is the performance impact of wide rows (many columns)?
544. How do you optimize schemas with hundreds of columns?
545. What is the benefit of nested structures vs flat schemas?
546. How do complex types (arrays, maps, structs) affect performance?
547. When should you denormalize vs normalize data?
548. What is the performance trade-off in denormalization?
549. How does avoiding UDFs improve performance (2-10x faster)?
550. Why are built-in functions faster than UDFs?
551. What optimizations can Catalyst apply to built-in functions?
552. Why can't Catalyst optimize UDFs?
553. What is the serialization overhead in UDFs?
554. When is UDF usage unavoidable?
555. How do you minimize UDF performance impact?
556. What is Pandas UDF (vectorized UDF) and how is it faster?
557. How much faster are Pandas UDFs compared to regular UDFs (3-100x)?
558. When should you use Pandas UDF instead of regular UDF?
559. How do you optimize data serialization (Kryo vs Java)?
560. How much faster is Kryo serialization (2-10x)?
561. When does serialization become a bottleneck?
562. How does serialization affect shuffle performance?
563. How does serialization affect caching efficiency?
564. What is compression and how does it affect performance?
565. What compression codecs are available (snappy, gzip, lzo, lz4)?
566. What is the trade-off between compression ratio and speed?
567. When should you use snappy (balanced, default)?
568. When should you use gzip (maximum compression)?
569. When should you use lz4 (fastest)?
570. How does compression affect I/O vs CPU trade-off?
571. When is compression counterproductive?
572. How do you optimize file formats for your workload?
573. Why is Parquet preferred for analytics (columnar, compressed)?
574. When should you use ORC instead of Parquet?
575. When should you use Avro (schema evolution, streaming)?
576. Why should you avoid CSV and JSON in production?
577. What is the performance difference between text and binary formats (5-20x)?
578. How does file format affect predicate pushdown?
579. How does file format affect compression efficiency?
580. What is the optimal file size for Spark (128MB-1GB)?
581. How do you control output file size?
582. What is `maxRecordsPerFile` and how do you set it?
583. How many files should each partition generate ideally (1 file)?
584. What happens if partitions generate too many small files?
585. How do you consolidate files after writing?
586. What is file coalescing vs file compaction?
587. How do you use `coalesce()` to control output files?
588. What is the difference between `repartition()` and `coalesce()` for file output?
589. When should you repartition before writing?
590. What is the cost of repartition (full shuffle)?
591. How do you optimize write parallelism?
592. What is the relationship between write parallelism and output files?
593. How do you balance write speed vs file count?

### 3.23 Advanced Transformations
594. What is the difference between `map()` and `flatMap()` methods?
595. When would you use `map()` vs `flatMap()`?
596. What is the `cogroup()` operation and how does it differ from join operations?
597. How do you use `mapPartitions()` and when is it more efficient than `map()`?
598. What does `foreachPartition()` do and how is it different from `foreach()`?
599. What is `transform()` method on DataFrames used for?
600. How do you use `pivot()` to reshape data from long to wide format?
601. What does `unpivot()` or `melt()` do (wide to long format)?
602. What is `cube()` operation in GROUP BY?
603. How does `rollup()` differ from `cube()`?
604. What does `groupingSets()` allow you to do?

### 3.24 Function Comparisons & When to Use What
605. `count()` vs `size()` - when to use each?
606. `distinct()` vs `dropDuplicates()` - are they the same?
607. `agg()` vs direct aggregation functions - when to use which approach?
608. `select()` vs `selectExpr()` vs `withColumn()` - comparison and use cases
609. `filter()` vs `where()` - is there any difference?
610. `join()` vs `crossJoin()` - when would you use crossJoin?
611. `union()` vs `unionAll()` vs `unionByName()` - key differences
612. `orderBy()` vs `sort()` - are they the same?
613. `repartition()` vs `coalesce()` - when to use which?
614. `cache()` vs `persist()` - what's the difference?
615. `collect()` vs `take()` vs `head()` - comparison
616. `first()` vs `head()` vs `take(1)` - subtle differences
617. `sample()` vs `sampleBy()` - when to use stratified sampling?
618. `approx_count_distinct()` vs `countDistinct()` - accuracy vs performance trade-off
619. `groupBy()` with `agg()` vs `groupBy()` with direct aggregation
620. Window functions vs GROUP BY - when to use which approach?

### 3.25 Sampling Methods - Comprehensive Comparison
621. What is simple random sampling in Spark?
622. What is stratified random sampling in Spark?
623. What does `sample(withReplacement, fraction, seed)` do?
624. What does `sampleBy(col, fractions, seed)` do?
625. What does `randomSplit(weights, seed)` do?
626. How does `sample()` control sampling - dataset level or group level?
627. How does `sampleBy()` control sampling - dataset level or group level?
628. What does `sampleBy()` return - single or multiple DataFrames?
629. What does `randomSplit()` return - single or multiple DataFrames?
630. Does `sample()` maintain group proportionality?
631. Does `sampleBy()` maintain group proportionality?
632. When would you use `sample()` for quick random subsets?
633. When would you use `sampleBy()` for representative samples by category?
634. When would you use `randomSplit()` for train/validation/test splits?
635. Provide an example of using `sampleBy()` with department-wise sampling.

### 3.26 Repartitioning Methods - Complete Analysis
636. What does `repartition(N)` do conceptually?
637. What does `repartition("col")` do conceptually?
638. What does `repartition(N, "col")` do conceptually?
639. How does `repartition(N)` distribute data - randomly or by hash?
640. How does `repartition("col")` distribute data - randomly or by hash?
641. What is the default number of partitions for `repartition("col")`?
642. How does `repartition(N, "col")` control partition count?
643. Does `repartition(N)` guarantee data locality?
644. Does `repartition("col")` place same column values in same partition?
645. What is the data skew risk for `repartition(N)`?
646. What is the data skew risk for `repartition("col")`?
647. When does `repartition("col")` enable partition pruning?
648. When does `repartition(N, "col")` enable partition pruning?
649. When would you use `repartition(N)` for general load balancing?
650. When would you use `repartition("col")` before filtering on that column?
651. When would you use `repartition(N, "col")` before writing partitioned data?
652. What is the difference between `repartition()` and `repartitionByRange()`?
653. What partitioning method does `repartition()` use?
654. What partitioning method does `repartitionByRange()` use?
655. Does `repartitionByRange()` order values within range boundaries?
656. When would you use `repartitionByRange()` over `repartition()`?

### 3.27 Repartition vs Coalesce - Critical Decision Guide
657. What is the core difference between `repartition()` and `coalesce()`?
658. Does `repartition()` perform a full shuffle?
659. Does `coalesce()` perform a full shuffle?
660. Can `repartition()` increase partition count?
661. Can `coalesce()` increase partition count?
662. Which method guarantees perfect data balance - `repartition()` or `coalesce()`?
663. When is `repartition()` essential (not optional) for preventing data skew?
664. When is `repartition()` essential for memory management and OOM prevention?
665. When is `repartition()` essential for optimizing cluster utilization?
666. When is `repartition()` essential for performance-critical operations?
667. When should you use `coalesce()` after filter operations?
668. When should you use `coalesce()` for writing to storage?
669. What is the execution speed difference between `repartition()` and `coalesce()`?
670. What is the network I/O cost of `repartition()` vs `coalesce()`?
671. What is the data balance quality of `repartition()` vs `coalesce()`?
672. What is the job reliability of `repartition()` vs `coalesce()`?
673. When does `coalesce()` risk data skew or OOM errors?
674. Should you choose `coalesce()` for speed or `repartition()` for reliability?
675. What are critical decision points for using `repartition()`?
676. What are critical decision points for using `coalesce()`?

### 3.28 Caching and Persistence - Deep Dive
677. What is `cache()` method in Spark?
678. What storage level does `cache()` use by default?
679. Is `cache()` lazy or eager evaluation?
680. When does cached data actually get stored - at cache() call or first action?
681. What happens to cached data when memory is full?
682. What is LRU eviction in caching?
683. When should you use `cache()` for multiple actions on same DataFrame?
684. When should you use `cache()` for iterative algorithms?
685. When should you avoid `cache()` for single-use DataFrames?
686. When should you avoid `cache()` in memory-constrained environments?
687. What is the difference between `cache()` and `persist()` in terms of flexibility?
688. Can you customize storage level with `cache()`?
689. Can you customize storage level with `persist()`?
690. What are the available storage levels in Spark?
691. What is MEMORY_ONLY storage level?
692. What is MEMORY_AND_DISK storage level?
693. What is MEMORY_ONLY_SER storage level?
694. What is MEMORY_AND_DISK_SER storage level?
695. What is DISK_ONLY storage level?
696. What is OFF_HEAP storage level?
697. What storage levels support replication (_2 suffix)?
698. How do you unpersist cached data?
699. Is there a separate `uncache()` method?
700. Does `unpersist()` work for both `cache()` and `persist()`?

### 3.29 Checkpoint - Reliability and Fault Tolerance
701. What is `checkpoint()` in Spark?
702. What is the main purpose of checkpointing?
703. Where does `checkpoint()` save data?
704. Does `checkpoint()` break the lineage graph?
705. What is the difference between `checkpoint()` and `localCheckpoint()`?
706. Where does `checkpoint()` save data - reliable storage or local?
707. Where does `localCheckpoint()` save data - reliable storage or local?
708. Does data survive driver/executor failures with `checkpoint()`?
709. Does data survive driver/executor failures with `localCheckpoint()`?
710. When should you use `checkpoint()` for long transformation chains?
711. When should you use `checkpoint()` for iterative algorithms?
712. When should you avoid `checkpoint()` for simple, fast transformations?
713. When should you avoid `checkpoint()` when storage is limited?
714. What is the performance cost of checkpointing?
715. Where should you configure the checkpoint directory?

### 3.30 Pandas Integration
716. What does `toPandas()` do in Spark?
717. Where does data move when you call `toPandas()`?
718. What are the memory implications of `toPandas()`?
719. When should you use `toPandas()` for visualization?
720. When should you use `toPandas()` for small results after aggregation?
721. When should you avoid `toPandas()` for large datasets?
722. When should you avoid `toPandas()` in production pipelines?
723. What is the risk of out-of-memory errors with `toPandas()`?
724. What is `pandas_api()` in Spark?
725. Where does data stay with `pandas_api()` - local or distributed?
726. What is the difference between `toPandas()` and `pandas_api()` in terms of data location?
727. What is the difference between `toPandas()` and `pandas_api()` in terms of processing?
728. What is the maximum data size for `toPandas()` - RAM limited or terabytes?
729. What is the maximum data size for `pandas_api()` - RAM limited or terabytes?
730. Does `toPandas()` have full pandas compatibility?
731. Does `pandas_api()` have full pandas compatibility?
732. When should you use `toPandas()` for Python ML libraries?
733. When should you use `pandas_api()` for big data processing?
734. What is the strategic usage pattern: Process in Spark → Convert to Pandas → Use Python ecosystem?

### 3.31 Temporary Views and SQL Integration
735. What is a temporary view in Spark?
736. How do you create a temporary view?
737. What is the scope of a temporary view - session or application?
738. How long does a temporary view last?
739. What is a global temporary view in Spark?
740. How do you create a global temporary view?
741. What is the scope of a global temporary view - session or application?
742. How do you reference a temporary view in SQL?
743. How do you reference a global temporary view in SQL?
744. What is the SQL syntax for accessing a global temporary view (global_temp.view_name)?
745. When should you use temporary views for single-session work?
746. When should you use global temporary views for cross-session sharing?
747. Can multiple sessions access a temporary view?
748. Can multiple sessions access a global temporary view?

### 3.32 Miscellaneous DataFrame Operations
749. What does `freqItems()` function do in Spark?
750. What is the default threshold for `freqItems()`?
751. What does support = 0.01 mean in `freqItems()`?
752. What values are included when threshold is 1%?
753. What values are excluded when threshold is 1%?
754. When would you use `freqItems()` for pattern mining?

## 4. Spark Collections - Deep Dive

### 4.1 Collections Overview & Fundamentals
755. What are collections in Apache Spark?
756. What are the two main collection types in Spark (ArrayType and MapType)?
757. What is ArrayType? What kind of data does it store?
758. What is MapType? What kind of data does it store?
759. Are Structs considered collections in Spark? Why or why not?
760. What collection functions work on Arrays?
761. What collection functions work on Maps?
762. Do collection functions work on Structs?
763. What is the purpose of using collections in DataFrames?
764. How do collections enable efficient management of semi-structured data?

### 4.2 Map vs Struct - Critical Comparison
765. What is the key difference between StructType and MapType?
766. Are field names in Structs fixed or dynamic?
767. Are keys in Maps fixed or dynamic?
768. When are field names in Structs defined?
769. Can different rows in a Map column have different keys?
770. How do you access a Struct field?
771. How do you access a Map value?
772. When should you use Structs over Maps?
773. When should you use Maps over Structs?
774. Do Struct fields have a fixed order?
775. Is key order guaranteed in Maps?
776. Provide an example use case for Structs.
777. Provide an example use case for Maps.
778. What happens when you know all attributes upfront - Struct or Map?
779. What happens when keys are variable or semi-structured - Struct or Map?
780. Can you have optional fields in Structs?
781. Can you have optional keys in Maps?
782. Compare schema rigidity: Struct vs Map.

### 4.3 Array vs Map - Comparison
783. What is the key difference between ArrayType and MapType?
784. Does ArrayType maintain order?
785. Is order guaranteed in MapType?
786. What is ArrayType best used for?
787. What is MapType best used for?
788. Can array elements be of different types?
789. Can map values be of different types?
790. How do you access array elements by position?
791. How do you access map values by key?

### 4.4 Advanced Collection Operations & Nested Data
792. How do you work with nested data structures in Spark?
793. What are the performance implications of deeply nested schemas?
794. How do you flatten nested structures?
795. When should you denormalize data vs keep it normalized in Spark?
796. How do you handle schema evolution with complex types?
797. Can you have arrays of structs?
798. Can you have maps of arrays?
799. Can you have arrays of maps?
800. How do you query nested arrays of structs?
801. What is the performance impact of deeply nested collections?

### 4.5 Comprehensive Collection Functions Reference
802. Which functions return the number of elements in a collection?
803. What collections support `size()` and `cardinality()`?
804. What is the difference between `reverse()`, `sort_array()`, and `array_sort()` in terms of purpose?
805. Which sorting function allows custom comparator logic?
806. What is the SQL standard name for array reduction: `aggregate()` or `reduce()`?
807. What does `concat()` do and does it support multiple arrays?
808. What is the difference between `element_at()` and `try_element_at()` in error handling?
809. Which function checks if AT LEAST ONE element matches a condition?
810. Which function checks if ALL elements match a condition?
811. Do `exists()` and `forall()` short-circuit? What does this mean for performance?
812. What is the difference between `filter()` (for arrays) and `map_filter()` (for maps)?
813. How many lambda parameters does `map_filter()` require?
814. What does `transform()` work on - Arrays or Maps?
815. What does `transform_keys()` work on - Arrays or Maps?
816. What does `transform_values()` work on - Arrays or Maps?
817. How many arrays does `arrays_zip()` support?
818. How many arrays does `zip_with()` support?
819. What output structure does `arrays_zip()` create?
820. Does `zip_with()` require a lambda function?
821. What happens to shorter arrays when using `arrays_zip()` or `zip_with()`?
822. Which collection functions support both Arrays and Maps?
823. Which functions are functionally identical pairs (have same behavior)?
824. What is the purpose of having both `size()` and `cardinality()` if they're identical?
825. What is the purpose of having both `aggregate()` and `reduce()` if they're identical?

## 5. User-Defined Functions (UDFs)

### 5.1 UDF Registration & Usage
826. How do you register a UDF for use in DataFrame functions?
827. How do you register a UDF for use in SQL expressions?
828. When is a UDF available in the Spark catalog?
829. How do you list all registered functions using `spark.catalog.listFunctions()`?
830. What are the performance implications of UDFs compared to built-in functions?

## 6. Data Sources & I/O Operations

### 6.1 Reading Data - Basics
831. What is the difference between `spark.read.table()` and `spark.read.parquet()`?
832. What does the `read.option('samplingRatio', 'true')` do during schema inference?
833. What is the `option('dateFormat', 'fmt')` used for? What are common date format patterns?
834. How do you handle corrupted or malformed rows when reading CSV files?
835. How do you achieve parallelism when reading from non-partitioned data files?
836. What are the different Spark data sources and sinks available?
837. What is the findspark library and when do you use it?

### 6.1.0 Pandas vs Spark File Reading - Core Differences
838. What is the fundamental difference between Pandas and Spark file reading?
839. What processing model does Pandas use - single-machine or distributed?
840. What processing model does Spark use - single-machine or distributed?
841. Can Pandas read files directly from HTTP/HTTPS URLs?
842. Can Spark read files directly from HTTP/HTTPS URLs?
843. Why doesn't Spark support reading from HTTP/HTTPS URLs directly?
844. What would happen if each Spark executor downloaded from HTTP independently?
845. What problem does independent HTTP downloading cause for data consistency?
846. Does HTTP URL reading violate distributed computing principles? Why?
847. What file systems does Spark support for distributed reading?
848. Can Spark read from local file systems? What is the requirement?
849. Can Spark read from HDFS (Hadoop Distributed File System)?
850. Can Spark read from AWS S3?
851. Can Spark read from Google Cloud Storage?
852. Can Spark read from Azure Blob Storage?
853. What are the three workarounds for reading HTTP URLs in Spark?
854. How do you use the "download then read" workaround for HTTP URLs?
855. How do you use the "Pandas bridge" workaround for HTTP URLs?
856. How do you use the "manual download" workaround for HTTP URLs?
857. Why does Spark require distributed storage where all executors can access the same data?
858. What is the key principle: coordinated access for parallel processing?
859. Do all worker nodes need access to the file in Spark?
860. Does Pandas require worker access to files?
861. What is the core requirement for Spark file reading: any accessible path or distributed storage?

### 6.1.0.2 Pandas vs Spark File Reading - Practical Implementation
862. What is the core requirement for Spark file reading: any accessible path or distributed storage?
863. Why is coordinated access critical for parallel processing in Spark?
864. What happens if Spark allowed HTTP URLs and each executor downloaded independently?
865. Would independent HTTP downloads guarantee data consistency across executors?
866. What distributed computing principle would HTTP URL reading violate?
867. What is the key difference between "accessible path" (Pandas) vs "distributed storage" (Spark)?
868. How do you implement the "download then read" workaround for HTTP URLs in Spark?
869. How do you implement the "Pandas bridge" workaround for HTTP URLs?
870. What is the "manual download + distribute" workaround? When is it useful?
871. How do you verify all Spark executors can access the file path?
872. What are the performance implications of each HTTP URL workaround?
873. When should you use "download then read" vs "Pandas bridge" approach?

### 6.1.1 CSV Reading Options & Gotchas
874. What does `option('header', 'true')` do when reading CSV files?
875. What is `option('inferSchema', 'true')` and what are its performance implications?
876. How do you specify custom delimiters using `option('sep', ',')`?
877. What does `option('quote', '"')` control?
878. How do you handle multi-line records using `option('multiLine', 'true')`?
879. What does `option('escape', '\\')` do?
880. What is `option('nullValue', 'NULL')` used for?
881. How does `option('mode', 'PERMISSIVE')` differ from 'DROPMALFORMED' and 'FAILFAST'?
882. What is `option('columnNameOfCorruptRecord', '_corrupt_record')` used for?
883. How do you handle files with different encodings using `option('encoding', 'UTF-8')`?
884. What does `option('ignoreLeadingWhiteSpace', 'true')` and `option('ignoreTrailingWhiteSpace', 'true')` do?
885. Why might you get different results with `inferSchema=true` on partial data?

### 6.1.2 JSON Reading Options
886. What is `option('multiLine', 'true')` important for when reading JSON?
887. How does JSON schema inference work differently from CSV?
888. What does `option('primitivesAsString', 'true')` do?
889. How do you handle JSON files with inconsistent schemas?

### 6.1.3 Parquet Reading Options
890. Does Parquet require schema inference? Why or why not?
891. What is `option('mergeSchema', 'true')` used for in Parquet?
892. How does Parquet handle predicate pushdown?
893. What are the advantages of columnar storage in Parquet for read performance?

### 6.1.4 ORC & Avro Reading
894. How does ORC compare to Parquet for read performance?
895. What is Avro's advantage for schema evolution?
896. When would you choose ORC over Parquet?

### 6.1.5 JDBC Reading Options
897. How do you read from JDBC sources?
898. What is `option('partitionColumn', 'id')` used for in JDBC reads?
899. How do you specify `lowerBound`, `upperBound`, and `numPartitions` for parallel JDBC reads?
900. What does `option('fetchsize', '1000')` control?
901. What are the performance implications of JDBC reads without proper partitioning?

### 6.2 Writing Data - Basics & Options
902. What is the Sink API in Spark?
903. What does `maxRecordsPerFile` control when writing DataFrames?
904. How do you estimate appropriate values for `maxRecordsPerFile`?
905. What are reasonable file sizes for Spark write operations in production?
906. Why might the number of DataFrame partitions not match the number of output file partitions?
907. Can DataFrame partitions be empty? What impact does this have on output files?
908. What are .crc files in Spark output directories and what is their purpose?

### 6.2.0 DataFrameWriter vs DataFrameWriterV2 - Architectural Evolution

#### 6.2.0.1 Core Architecture
909. What is DataFrameWriter (V1) built on?
910. What is DataFrameWriterV2 (V2) built on?
911. What is the DataSource V1 API?
912. What is the DataSource V2 API?
913. What type of architecture does V1 have - monolithic or pluggable?
914. What type of architecture does V2 have - monolithic or pluggable?
915. Is V1 tightly coupled or loosely coupled with Spark's SQL engine?
916. What was V1 originally designed for - cloud storage or HDFS/relational databases?
917. What was V2 designed for - legacy systems or cloud-native environments?

#### 6.2.0.2 Design Philosophy
918. What is V1's design approach - "one size fits all" or "extensible framework"?
919. What is V2's design approach - "one size fits all" or "extensible framework"?
920. Does V1 have a fixed or customizable write execution pattern?
921. Does V2 have a fixed or customizable write execution pattern?
922. Are batch and streaming treated as separate or unified in V1?
923. Are batch and streaming treated as separate or unified in V2?
924. What type of commit protocols does V1 use - file-system oriented or transaction-aware?
925. What type of commit protocols does V2 use - file-system oriented or transaction-aware?

#### 6.2.0.3 V1 Technical Limitations
926. Does V1 support atomic commits on cloud object stores?
927. Does V1 have transaction boundaries for partial failures?
928. What are the corruption risks in V1 during job failures?
929. Are V1 recovery mechanisms limited or extensive?
930. Is V1's execution model a black box or transparent?
931. Is it easy or difficult to implement custom data sources in V1?
932. Does V1 have limited or extensive push-down capability?
933. Are V1 interface contracts rigid or flexible?
934. Does V1 have fine-grained or coarse-grained overwrite behavior?
935. How well does V1 integrate with catalog systems?
936. Does V1 have limited or extensive schema evolution support?
937. Are V1's data distribution controls basic or advanced?

#### 6.2.0.4 V2 Architectural Solutions
938. Does V2 support pluggable commit protocols?
939. Does V2 support ACID transaction guarantees?
940. Does V2 provide atomic operation guarantees?
941. Does V2 have recovery and rollback capabilities?
942. Does V2 have clean interfaces for custom implementations?
943. Does V2 have an operation push-down framework?
944. Can you customize write optimization in V2?
945. Does V2 provide unified batch and streaming APIs?
946. Does V2 have fine-grained data distribution controls?
947. Does V2 support advanced partitioning strategies?
948. Does V2 have integrated catalog management?
949. Does V2 support native schema evolution?

#### 6.2.0.5 Key Differentiators
950. What execution model does V1 use - fixed pipeline or customizable pipeline?
951. What execution model does V2 use - fixed pipeline or customizable pipeline?
952. Does V1 support planning-time optimizations or only runtime optimizations?
953. Does V2 support both planning-time and runtime optimizations?
954. Is V1 connector development simple or complex?
955. Is V2 connector development simple or complex?
956. Does V1 require deep Spark internals knowledge for connector development?
957. Does V2 have well-defined interfaces for connector development?
958. Was V1 adapted to cloud storage or designed for it?
959. Was V2 adapted to cloud storage or designed for it from inception?
960. Does V1 have basic or native integration with modern table formats (Iceberg, Delta, Hudi)?
961. Does V2 have basic or native integration with modern table formats?

#### 6.2.0.6 Evolution Context
962. What does V1 represent - Spark's origins or Spark's maturity?
963. What does V2 represent - Spark's origins or Spark's maturity?
964. Was V1 born from academic/early internet scale or cloud-native reality?
965. What was V1's primary focus - HDFS/traditional databases or diverse ecosystems?
966. What was V1's processing primacy - batch or streaming?
967. What was V1's deployment model - single data center or global scale?
968. Is V2 designed for cloud-native, hybrid cloud, or single data center?
969. Does V2 unify streaming and batch or treat them separately?
970. Is V2 designed for single deployment or global scale deployment?
971. Does V2 focus on single ecosystem or diverse data ecosystem integration?

#### 6.2.0.7 Practical Implications
972. Is V1 sufficient for basic ETL and analytics?
973. Is V2 necessary for production-grade, reliable pipelines?
974. Should platform developers focus on V1 for innovation or maintenance?
975. Should platform developers focus on V2 for innovation or ecosystem expansion?
976. Should organizations use V1 for new projects or legacy maintenance?
977. Should organizations use V2 as a future-proof foundation?

#### 6.2.0.8 Strategic Direction
978. Is V1 in active development or maintenance mode?
979. Is V2 in active development or maintenance mode?
980. Does V1 receive new feature development?
981. Does V2 receive new feature development?
982. What is V1's status - critical bug fixes only or new features?
983. What is V2's status - maintenance or active development focus?
984. Is V1 on a gradual deprecation path?
985. Is V2 the focus for ecosystem expansion?
986. Is V2 the priority for performance optimization?

#### 6.2.0.9 Summary & Conclusion
987. Does the V1 to V2 transition represent an API version increment or architectural shift?
988. What did V1 address - initial scale challenges or modern reliability requirements?
989. What does V2 address - initial scale or reliability/extensibility/operational requirements?
990. Is V2 designed for cloud-native environments?
991. Does V2 maintain backward compatibility for existing workloads?
992. Is V2 fundamental for Spark to remain relevant in evolving data ecosystems?

### 6.2.0.10 DataFrameWriter V1 vs V2 - Strategic Questions
993. What is the main criticism of V1's "one size fits all" approach?
994. How does V2's "extensible framework" solve V1's limitations?
995. What does "black box execution model" mean in V1? Why is it problematic?
996. What operational challenges arise from V1's coarse-grained overwrite behavior?
997. How does V2's pluggable commit protocol prevent data corruption?
998. What is the benefit of V2's unified batch and streaming APIs?
999. Why is V1 considered sufficient for basic ETL but not production-grade pipelines?
1000. What does "cloud-native by design" mean for V2?
1001. How does V2's native integration with Iceberg/Delta/Hudi differ from V1?
1002. What is the strategic risk of building new pipelines on V1?

### 6.2.0.11 DataFrameWriter V2 - Practical Implementation
1003. How do you determine if your write operation is using V1 or V2?
1004. Can you force Spark to use V2 writer for a specific data source?
1005. What configuration enables V2 writer by default?
1006. How do you implement a custom V2 data source writer?
1007. What interface must you implement for V2 batch writing?
1008. What interface must you implement for V2 streaming writing?
1009. How does V2 writer handle partial write failures differently than V1?
1010. How do you rollback a failed write operation in V2?
1011. What happens when you write to Delta Lake using V1 vs V2?
1012. How do you verify V2 atomic commit succeeded in Spark UI?

### 6.2.1 Write Modes - Comprehensive Analysis
1013. What are the four available write modes in PySpark?
1014. What does 'overwrite' mode do?
1015. What does 'append' mode do?
1016. What does 'ignore' mode do?
1017. What does 'error' or 'errorifexists' mode do?
1018. What is the default write mode in Spark?

#### 6.2.1.1 Write Modes - Behavior Analysis
1019. What happens with 'overwrite' mode when target exists?
1020. What happens with 'overwrite' mode when target doesn't exist?
1021. What happens with 'append' mode when target exists?
1022. What happens with 'append' mode when target doesn't exist?
1023. What happens with 'ignore' mode when target exists?
1024. What happens with 'ignore' mode when target doesn't exist?
1025. What happens with 'errorifexists' mode when target exists?
1026. What happens with 'errorifexists' mode when target doesn't exist?

#### 6.2.1.2 Write Modes - Use Cases
1027. What are common use cases for 'overwrite' mode?
1028. What are common use cases for 'append' mode?
1029. What are common use cases for 'ignore' mode?
1030. What are common use cases for 'errorifexists' mode?
1031. When would you use 'overwrite' for full refreshes?
1032. When would you use 'append' for incremental loads?
1033. When would you use 'ignore' for safe initialization?
1034. When would you use 'errorifexists' for safety default?

#### 6.2.1.3 Write Modes - Risk & Performance
1035. What is the risk level of 'overwrite' mode - high, medium, or low?
1036. What is the risk level of 'append' mode - high, medium, or low?
1037. What is the risk level of 'ignore' mode - high, medium, or low?
1038. What is the risk level of 'errorifexists' mode - high, medium, or low?
1039. Which write mode is safest for data protection?
1040. Which write mode is riskiest for accidental data loss?
1041. What is the performance characteristic of 'ignore' when skipping?
1042. What is the performance characteristic of 'overwrite' for large datasets?
1043. What is the performance characteristic of 'append' mode?

#### 6.2.1.4 Write Modes - Best Practices
1044. What write mode should you use in development for testing?
1045. What write mode should you use in production for incremental loads?
1046. What write mode should you use in production for safety?
1047. What write mode should you use for initialization/first-time setup?
1048. What write mode prevents accidental overwrites?

### 6.2.1.6 Write Modes - Edge Cases
1049. What happens if you use 'overwrite' mode on a partitioned table - all partitions or target partition only?
1050. Can 'append' mode cause duplicate data? How do you prevent it?
1051. What is the difference between 'ignore' mode behavior in files vs tables?
1052. Does 'errorifexists' check metadata or actual data existence?
1053. What happens when 'overwrite' mode fails midway - partial data or nothing?
1054. How does 'append' mode handle schema mismatches?
1055. What happens if you 'append' with different partitioning than existing data?
1056. Does 'ignore' mode validate schema compatibility before skipping?

### 6.2.1.7 Write Modes - Practical Scenarios
1057. How do you implement idempotent writes using 'ignore' mode?
1058. What write mode should you use for time-travel scenarios (Delta Lake)?
1059. Can you combine write modes (e.g., overwrite partitions, append to others)?
1060. What is dynamic partition overwrite mode and how is it different from 'overwrite'?
1061. How do you configure dynamic partition overwrite mode?
1062. When should you use dynamic overwrite instead of static overwrite?
1063. How do you protect production tables from accidental 'overwrite'?
1064. What is the difference between overwriting a table vs overwriting specific partitions?

### 6.3 Partitioning During Writes

#### 6.3.1 partitionBy vs bucketBy vs sortBy - Core Concepts
1065. What is `partitionBy()` - physical or logical separation?
1066. What is `bucketBy()` - physical or logical separation?
1067. What is `sortBy()` - separation or ordering?
1068. What does `partitionBy()` create on storage - folders or files?
1069. What does `bucketBy()` create on storage - folders or files?
1070. What does `sortBy()` create on storage - folders, files, or nothing?
1071. Can you SEE `partitionBy()` separation in file explorer?
1072. Can you SEE `bucketBy()` separation in file explorer?
1073. Can you SEE `sortBy()` separation in file explorer?
1074. What is the structure created by `partitionBy()` - example with country?
1075. What is the structure created by `bucketBy()` - example with user_id?
1076. What is the structure created by `sortBy()` - example with timestamp?

#### 6.3.2 Analogies for Understanding
1077. What is the analogy for `partitionBy()` - library with separate rooms?
1078. What is the analogy for `bucketBy()` - single room with numbered shelves?
1079. What is the analogy for `sortBy()` - books arranged alphabetically?

#### 6.3.3 Availability Matrix - Format Support
1080. Does CSV support `partitionBy()`?
1081. Does CSV support `bucketBy()`?
1082. Does CSV support `sortBy()`?
1083. Does JSON support `partitionBy()`?
1084. Does JSON support `bucketBy()`?
1085. Does JSON support `sortBy()`?
1086. Does Parquet/ORC support `partitionBy()`?
1087. Does Parquet/ORC support `bucketBy()`?
1088. Does Parquet/ORC support `sortBy()`?
1089. Does JDBC support `partitionBy()`?
1090. Does JDBC support `bucketBy()`?
1091. Does JDBC support `sortBy()`?
1092. Does `saveAsTable` support `partitionBy()`?
1093. Does `saveAsTable` support `bucketBy()`?
1094. Does `saveAsTable` support `sortBy()`?
1095. Which write method is the ONLY one that supports `bucketBy()`?
1096. Which write method is the ONLY one that supports `sortBy()`?
1097. Which write method supports all three: `partitionBy()`, `bucketBy()`, and `sortBy()`?

#### 6.3.4 Detailed Comparison
1098. What separation level does `partitionBy()` operate at - directory, file, or row?
1099. What separation level does `bucketBy()` operate at - directory, file, or row?
1100. What separation level does `sortBy()` operate at - directory, file, or row?
1101. Is `partitionBy()` visible in the file system?
1102. Is `bucketBy()` visible in the file system?
1103. Is `sortBy()` visible in the file system?
1104. How do you access data with `partitionBy()` - direct folder navigation?
1105. How do you access data with `bucketBy()` - hash calculation?
1106. How do you access data with `sortBy()` - sequential scanning?
1107. What is the optimal use case for `partitionBy()` - low or high cardinality?
1108. What is the optimal use case for `bucketBy()` - low or high cardinality?
1109. What is the optimal use case for `sortBy()` - specific access pattern?
1110. What performance benefit does `partitionBy()` provide?
1111. What performance benefit does `bucketBy()` provide?
1112. What performance benefit does `sortBy()` provide?
1113. How does `partitionBy()` impact file organization - multiple folders?
1114. How does `bucketBy()` impact file organization - fixed files in one folder?
1115. How does `sortBy()` impact file organization - same files, sorted internally?

#### 6.3.5 When to Use Each Method
1116. When should you use `partitionBy()` - clear categories like country/year?
1117. When should you use `partitionBy()` - frequent filtering by categories?
1118. When should you use `partitionBy()` - data lifecycle management?
1119. What formats work with `partitionBy()` - files or tables or both?
1120. When should you use `bucketBy()` - high-cardinality columns?
1121. When should you use `bucketBy()` - frequently joined tables?
1122. When should you use `bucketBy()` - need even data distribution?
1123. What formats work with `bucketBy()` - files or tables?
1124. When should you use `sortBy()` - range queries (BETWEEN, >, <)?
1125. When should you use `sortBy()` - natural ordering (timestamps)?
1126. When should you use `sortBy()` - better compression?
1127. What formats work with `sortBy()` - files or tables?

#### 6.3.6 Critical Limitations
1128. Can file formats (CSV, JSON, Parquet, ORC) use `bucketBy()`?
1129. Can file formats optimize joins at write time?
1130. Can file formats use `sortBy()` during write?
1131. Must you pre-sort DataFrames before writing to file formats?
1132. Can JDBC writes use `partitionBy()`?
1133. Why doesn't JDBC support `partitionBy()` - who handles partitioning?
1134. Can JDBC writes use `bucketBy()`?
1135. Why doesn't JDBC support `bucketBy()` - who handles indexing?
1136. Can JDBC writes use `sortBy()`?
1137. Why doesn't JDBC support `sortBy()` - who handles query optimization?
1138. Does `saveAsTable` support the full feature set?
1139. Is `saveAsTable` the only method for bucketing and sorting during write?
1140. Does `saveAsTable` require metastore integration?

#### 6.3.7 Best Practices
1141. For maximum performance with `saveAsTable`, what three-layer optimization should you use?
1142. What should you use `partitionBy()` for in the three-layer optimization?
1143. What should you use `bucketBy()` for in the three-layer optimization?
1144. What should you use `sortBy()` for in the three-layer optimization?
1145. What is the ideal cardinality range for `partitionBy()` - 10-1000 distinct values?
1146. What cardinality does `bucketBy()` handle well - millions of distinct values?
1147. Are there cardinality limits for `sortBy()`?
1148. What is the file management risk with `partitionBy()` - too many small files?
1149. What is the file management characteristic of `bucketBy()` - fixed file count?
1150. Does `sortBy()` impact file count?

#### 6.3.8 Summary - Support Matrix
1151. Is `partitionBy()` universal (files + tables)?
1152. Is `bucketBy()` exclusive to `saveAsTable`?
1153. Is `sortBy()` exclusive to `saveAsTable`?
1154. Can you use `bucketBy()` or `sortBy()` with file formats (CSV, JSON, Parquet)?
1155. Can you use any organization methods with JDBC writer?
1156. Can you use `partitionBy()` with JDBC (who handles database partitioning)?

#### 6.3.9 Key Insights
1157. What does `partitionBy()` organize - your storage?
1158. What does `bucketBy()` organize - your data relationships?
1159. What does `sortBy()` organize - your data access patterns?
1160. What is the key principle for choosing between these methods?

### 6.4 Bucketing
1161. What is bucketing in Spark? How do you use `bucketBy()` when writing data?
1162. How does bucketing work: bucket numbers, columns, and hash functions?
1163. What is the purpose of using `sortBy()` in combination with `bucketBy()`?
1164. How does bucketing with sorting optimize sort-merge joins by eliminating shuffle?
1165. Can you use `bucketBy()` with `partitionBy()` together?
1166. What are the limitations of bucketing?
1167. How do you read bucketed tables to take advantage of bucketing?
1168. What happens if you change the number of buckets after writing data?

### 6.4.1 Bucketing - Syntax & Implementation
1169. What is the exact syntax for using all three methods together in saveAsTable?
1170. Can you use bucketBy without sortBy? What happens?
1171. Can you use sortBy without bucketBy? What happens?
1172. What happens if you try to use bucketBy with df.write.parquet()?
1173. What error message do you get when using bucketBy with file formats?
1174. How do you create a bucketed table from an existing non-bucketed table?

### 6.4.2 Bucketing - Verification & Validation
1175. How do you verify bucketing is preserved after reading a bucketed table?
1176. What Spark configuration controls bucket pruning optimization?
1177. How do you check if a table is bucketed using Spark catalog?
1178. How do you verify sortBy ordering is preserved in the data files?
1179. What does DESCRIBE EXTENDED show for bucketed tables?
1180. How do you inspect physical files to verify partitionBy structure?

### 6.4.3 Bucketing - Sizing & Configuration
1181. How many buckets should you create for a table with 1TB data?
1182. What is the formula for calculating optimal bucket count?
1183. What happens if you create too many buckets (e.g., 10,000)?
1184. What happens if you create too few buckets (e.g., 2)?
1185. How does bucket count affect join performance?
1186. Should bucket count be a power of 2? Why or why not?

### 6.4.4 Bucketing - Performance Impact
1187. How does sortBy improve compression ratio in Parquet/ORC?
1188. What is the performance cost of sortBy during write operations?
1189. Does partitionBy increase or decrease write time? Why?
1190. How does bucketBy affect write parallelism?
1191. What is the memory overhead of sortBy during writes?

### 6.4.5 Bucketing - Schema Evolution & Modification
1192. Can you change bucket count after table creation? What's the process?
1193. Can you add new partition columns to an existing partitioned table?
1194. What happens to bucketing when you use ALTER TABLE?
1195. Can you change sortBy columns after table creation?
1196. How do you migrate from non-bucketed to bucketed table?

### 6.4.6 Bucketing - Reading Optimizations
1197. How do you read a bucketed table to leverage bucket-based joins?
1198. What configuration enables bucketed table optimization during reads?
1199. How does Spark know to use bucketing during joins?
1200. What happens if you read a bucketed table without preserving bucketing?
1201. Can you filter on bucket columns to reduce data scan?

### 6.5 Warehouse Tables vs Temp Views - Critical Comparison

#### 6.5.1 Core Characteristics
1202. What is a Spark Warehouse Table (Managed Table)?
1203. What is a Spark Temp View?
1204. Does a Warehouse Table physically store data?
1205. Does a Temp View physically store data?
1206. Where is Warehouse Table data stored (warehouse directory)?
1207. Is a Temp View a logical abstraction or physical storage?
1208. Does a Temp View hold any data itself?

#### 6.5.2 Persistence & Lifetime
1209. Is a Warehouse Table persistent or temporary?
1210. Is a Temp View persistent or temporary?
1211. Does a Warehouse Table survive Spark application restarts?
1212. Does a Temp View survive Spark application restarts?
1213. When does a Warehouse Table disappear - explicit DROP or automatic?
1214. When does a Temp View disappear - explicit DROP or session end?
1215. What is the lifetime of a regular Temp View - session-scoped?
1216. What is the lifetime of a Global Temp View - session or application?
1217. How long does a Warehouse Table persist - until explicitly dropped?
1218. How long does a Temp View persist - until SparkSession ends?

#### 6.5.3 Metadata Management
1219. Is a Warehouse Table cataloged in a metastore?
1220. Is a Temp View cataloged in a metastore?
1221. Where is Warehouse Table schema and location stored?
1222. Is a Temp View definition stored in a metastore?
1223. Who knows about a Temp View definition - metastore or SparkSession?
1224. Can Warehouse Tables be shared across multiple Spark applications?
1225. Can Temp Views be shared across multiple Spark applications?

#### 6.5.4 Impact of DROP Command
1226. What happens when you DROP a Warehouse Table - metadata only or data too?
1227. What happens when you DROP a Temp View - metadata only or data too?
1228. Does DROP TABLE delete underlying data files for Warehouse Tables?
1229. Does DROP VIEW delete underlying data source for Temp Views?
1230. Who "owns" the data in a Warehouse Table - the table itself?
1231. Who "owns" the data in a Temp View - the view or underlying source?

#### 6.5.5 Underlying Data Source
1232. What is the underlying data source for a Warehouse Table - the table itself?
1233. What can be the underlying data source for a Temp View?
1234. Can a Temp View be built on top of an existing table?
1235. Can a Temp View be built on top of a file (CSV, Parquet)?
1236. Can a Temp View be built on top of DataFrame transformations?

#### 6.5.6 Primary Use Cases
1237. What is the primary use case for Warehouse Tables?
1238. What is the primary use case for Temp Views?
1239. Are Warehouse Tables the "single source of truth"?
1240. Are Temp Views suitable for ad-hoc, session-specific manipulation?
1241. Should you use Warehouse Tables for data that needs to be stored long-term?
1242. Should you use Temp Views for exploratory data analysis?

#### 6.5.7 When to Use Which
1243. When should you use a Warehouse Table - persist processing results?
1244. When should you use a Warehouse Table - shared dimension/fact table?
1245. When should you use a Warehouse Table - curated dataset or data mart?
1246. When should you use a Warehouse Table - data lifecycle managed by Spark?
1247. When should you use a Temp View - exploratory data analysis in notebook?
1248. When should you use a Temp View - simplify complex SQL query?
1249. When should you use a Temp View - work with DataFrames and run SQL?
1250. When should you use a Temp View - data temporary for current session?

#### 6.5.8 Key Analogy
1251. What is the analogy for a Warehouse Table - physical house or blueprints?
1252. What is the analogy for a Temp View - physical house or blueprints?
1253. Is a Warehouse Table a permanent structure?
1254. Is a Temp View a temporary description?

### 6.5.9 Warehouse vs Temp Views - Interoperability
1255. Can you create a temp view from a warehouse table? Provide syntax.
1256. Can you create a warehouse table from a temp view? Provide syntax.
1257. What happens when you query a temp view vs warehouse table - performance difference?
1258. Do temp views consume storage space? Where is the "view definition" stored?
1259. What is stored in memory when you create a temp view?

### 6.5.10 Warehouse vs Temp Views - Concurrency & Access Control
1260. Can multiple Spark applications access the same warehouse table simultaneously?
1261. What locking mechanism prevents conflicts when writing to warehouse tables?
1262. Can two sessions create temp views with the same name?
1263. What happens when two applications write to the same warehouse table?
1264. How do you implement read-write concurrency for warehouse tables?

### 6.5.11 Warehouse vs Temp Views - Conversion & Migration
1265. How do you convert a temp view to a permanent warehouse table?
1266. How do you convert a warehouse table to a temp view (why would you)?
1267. What is the syntax to materialize a temp view as a warehouse table?
1268. Can you CTAS (CREATE TABLE AS SELECT) from a temp view?

### 6.5.12 Warehouse vs Temp Views - Lifecycle Management
1269. What happens to temp views when you restart the Spark application (not just session)?
1270. How do you list all temp views in the current session?
1271. How do you list all global temp views across sessions?
1272. Do temp views appear in spark.catalog.listTables()?
1273. How do you drop all temp views programmatically?

### 6.5.13 Warehouse vs Temp Views - Naming & Precedence
1274. Can you have a temp view and warehouse table with the same name?
1275. What is the precedence order: temp view, global temp view, or warehouse table?
1276. How does Spark resolve name conflicts between views and tables?
1277. What happens if you CREATE TEMP VIEW with a name that exists as a table?

## 7. File Formats & Storage Systems

### 7.1 Storage Systems
1278. What is the difference between distributed file storage systems and normal storage systems?
1279. What is a Spark data lake?
1280. What is HDFS and how does it work with Spark?
1281. What are the advantages of cloud storage (S3, ADLS, GCS) for Spark workloads?

### 7.2 File Format Deep Dive - Parquet
1282. What is columnar storage? How does Parquet implement it?
1283. What are the advantages of Parquet for analytics workloads?
1284. How does Parquet handle nested data structures?
1285. What is a row group in Parquet?
1286. What is a column chunk in Parquet?
1287. How does Parquet encoding and compression work?
1288. What is predicate pushdown in Parquet and why is it efficient?
1289. What is projection pushdown in Parquet?
1290. What are the limitations of Parquet?

### 7.3 File Format Deep Dive - Avro
1291. What is row-based storage? How does Avro use it?
1292. What are the advantages of Avro for streaming and schema evolution?
1293. How does Avro handle schema evolution (backward, forward, full compatibility)?
1294. When would you choose Avro over Parquet?
1295. How is Avro schema stored and transmitted?
1296. What is the performance trade-off between Avro and Parquet?

### 7.4 File Format Deep Dive - ORC
1297. How is ORC similar to and different from Parquet?
1298. What compression techniques does ORC use?
1299. How does ORC handle predicate pushdown?
1300. What are ORC stripes, row groups, and indexes?
1301. When would you choose ORC over Parquet?

### 7.5 File Format Deep Dive - Delta Lake
1302. What is Delta Lake and how is it different from a file format?
1303. What are the ACID transaction guarantees in Delta Lake?
1304. How does Delta Lake implement time travel?
1305. What is the Delta transaction log?
1306. How does Delta Lake handle updates and deletes?
1307. What is optimize and ZORDER in Delta Lake?
1308. What is vacuum in Delta Lake?
1309. How does Delta Lake schema enforcement work?
1310. What is schema evolution in Delta Lake?
1311. What are the performance benefits of Delta over Parquet?

### 7.6 File Format Deep Dive - Apache Hudi
1312. What is Apache Hudi and what data management problems does it solve?
1313. What are Copy-on-Write (CoW) and Merge-on-Read (MoR) tables in Hudi?
1314. When would you use Hudi over Delta Lake?
1315. How does Hudi handle upserts?
1316. What is Hudi's timeline and commit model?

### 7.7 File Format Comparisons
1317. Compare Parquet vs CSV in terms of:
   - a) Storage efficiency
   - b) Read performance
   - c) Write performance
   - d) Schema handling
   - e) Use cases
1318. Compare Avro vs Parquet vs ORC for:
   - a) Analytics workloads
   - b) Streaming workloads
   - c) Schema evolution requirements
1319. Compare Delta Lake vs Apache Hudi vs Apache Iceberg for:
   - a) ACID transactions
   - b) Time travel
   - c) Performance
   - d) Ecosystem support
1320. When would you use JSON format despite its inefficiency?
1321. What are the trade-offs between text formats (CSV, JSON) and binary formats (Parquet, Avro, ORC)?
1322. How does compression affect different file formats differently?

## 8. Joins in Spark

### 8.1 Join Types
1323. What is the difference between inner join, outer join, full outer join, and left outer join?
1324. What are the implications of each join type on the result set?

### 8.2 Join Strategies & Types
1325. What is a shuffle sort-merge join (shuffle join)?
1326. What is a broadcast join (broadcast hash join)?
1327. When does Spark choose shuffle sort-merge join vs broadcast join?
1328. What are the trade-offs between these join strategies in terms of memory, network I/O, and performance?
1329. Explain the mechanics, considerations, and advantages of broadcast joins.
1330. What is a shuffle hash join? When is it used?
1331. What is a cartesian join? When does it occur and why should it be avoided?
1332. What is a broadcast nested loop join? When is it used?
1333. Compare all join strategies: Broadcast Hash Join vs Shuffle Hash Join vs Sort-Merge Join vs Cartesian Join vs Broadcast Nested Loop Join.
1334. What conditions must be met for Spark to choose a broadcast join?
1335. What happens if a broadcast join fails due to memory constraints?
1336. How does Spark decide between shuffle hash join and sort-merge join?
1337. What is the difference between an equi-join and a non-equi-join in terms of join strategy selection?
1338. When would Spark use broadcast nested loop join instead of broadcast hash join?

### 8.3 Broadcast Join Deep Dive
1339. How does broadcast join work internally? Explain the three phases: broadcast, hash build, probe.
1340. What data structure is used during the hash build phase of a broadcast join?
1341. How is the smaller table distributed to executor nodes during broadcast?
1342. What is the role of the driver in coordinating broadcast joins?
1343. What does `spark.sql.autoBroadcastJoinThreshold` control? What is its default value (10MB)?
1344. How do you manually force a broadcast join using broadcast hints?
1345. What are the different ways to provide broadcast hints in Spark SQL and DataFrame API?
1346. What happens if you broadcast a table larger than available executor memory?
1347. How do you calculate the in-memory size of a DataFrame for broadcast join decisions?
1348. What is the difference between the on-disk size and in-memory size of data?
1349. Why might a 5MB Parquet file become 50MB in memory?
1350. What compression and encoding affect the size difference between disk and memory?
1351. How does `spark.sql.adaptive.autoBroadcastJoinThreshold` differ from the regular threshold?
1352. Can you broadcast multiple tables in a multi-way join?
1353. What are the memory implications of broadcasting in a multi-way join scenario?
1354. How does broadcast join perform with skewed data on the large table side?
1355. What happens if the broadcast data doesn't fit in executor memory during runtime?
1356. How do you monitor broadcast join performance in Spark UI?
1357. What metrics indicate successful broadcast join execution?
1358. What is broadcast timeout and how do you configure it?
1359. How does broadcast join improve performance compared to sort-merge join?
1360. What network I/O savings does broadcast join provide?
1361. In what scenarios would broadcast join actually be slower than sort-merge join?
1362. How does broadcast join work with partition pruning?
1363. Can broadcast join be used with all join types (inner, left, right, full outer)?
1364. Which join types benefit most from broadcast strategy?

### 8.4 Join Optimization Strategies
1365. How do you optimize Spark joins effectively?
1366. What techniques can be used to improve join performance? (Repartitioning, Broadcasting, Caching, Shuffle tuning, Bucketing)
1367. How do you define what constitutes a "large" DataFrame vs "small" DataFrame for join optimization?
1368. How do you check the size of a DataFrame in a Spark session?
1369. How does `bucketBy()` remove shuffle from sort-merge joins?
1370. What is bucketed sort-merge join and how does it eliminate shuffle?
1371. How do you verify that bucketing is being utilized in a join?
1372. What is the relationship between bucketing, partitioning, and join performance?
1373. When should you pre-partition both DataFrames before joining?
1374. How does repartitioning by join key improve join performance?
1375. What is the optimal number of partitions for join operations?
1376. How do you balance between too few and too many partitions in joins?
1377. What is the role of caching in multi-join queries?
1378. Should you cache before or after filtering when planning joins?
1379. How does filter pushdown before joins improve performance?
1380. What is the impact of selecting only required columns before joining?
1381. How do you optimize joins when both tables are large?
1382. What is salting and how does it help with skewed joins?
1383. How do you implement salting for skewed join keys?
1384. What is the broadcast-replicate strategy for handling skew in joins?
1385. How do you identify which join keys are causing skew?
1386. What statistics should you collect before performing large joins?
1387. How does `ANALYZE TABLE COMPUTE STATISTICS` help with join optimization?
1388. What is the impact of data types on join performance (StringType vs IntegerType for join keys)?
1389. How do null values in join keys affect join performance and strategy selection?
1390. Should you filter out nulls before joining? When and why?

### 8.5 Shuffle Operations in Joins
1391. Explain Map Exchange and Reduce Exchange in shuffle sort-merge joins.
1392. When processing large datasets with multiple joins, what optimization techniques should be considered?
1393. What is shuffle write and shuffle read in the context of joins?
1394. How do you minimize shuffle during join operations?
1395. What is shuffle spill and how does it affect join performance?
1396. How do you identify shuffle-heavy joins in Spark UI?
1397. What metrics indicate excessive shuffling in joins?
1398. What is the relationship between `spark.sql.shuffle.partitions` and join performance?
1399. How does increasing shuffle partitions affect memory consumption during joins?

### 8.6 Adaptive Query Execution (AQE) for Joins
1400. How does AQE improve join performance?
1401. What is dynamic join strategy switching in AQE?
1402. How does AQE convert sort-merge join to broadcast join at runtime?
1403. What triggers AQE to switch join strategies during execution?
1404. What is `spark.sql.adaptive.autoBroadcastJoinThreshold` and how is it different from static threshold?
1405. How does AQE handle skewed joins automatically?
1406. What is skew join optimization in AQE?
1407. How does AQE detect skewed partitions during joins?
1408. What is `spark.sql.adaptive.skewJoin.enabled` configuration?
1409. What is `spark.sql.adaptive.skewJoin.skewedPartitionFactor`?
1410. What is `spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes`?
1411. How does AQE split skewed partitions during joins?
1412. What is the performance impact of AQE's skew join optimization?
1413. How do you verify that AQE optimized your join in Spark UI?
1414. What is dynamic coalescing of shuffle partitions and how does it help joins?
1415. How does AQE reduce the number of reducers after shuffle in joins?
1416. What is `spark.sql.adaptive.coalescePartitions.enabled`?
1417. What is `spark.sql.adaptive.coalescePartitions.minPartitionSize`?
1418. What is `spark.sql.adaptive.advisoryPartitionSizeInBytes`?
1419. Can AQE optimizations be combined (e.g., coalescing + skew handling + join strategy switch)?
1420. What are the prerequisites for AQE to work effectively with joins?
1421. Does AQE work with all join types and strategies?
1422. What is the overhead of enabling AQE for joins?
1423. When might AQE make join performance worse?

### 8.7 Dynamic Partition Pruning (DPP) for Joins
1424. What is Dynamic Partition Pruning (DPP) and how does it optimize joins?
1425. How does DPP differ from static partition pruning?
1426. When is DPP triggered during join execution?
1427. What is the typical scenario where DPP provides significant benefit?
1428. Explain the star schema query optimization using DPP.
1429. How does DPP work in a fact table - dimension table join?
1430. What conditions must be met for DPP to be applied?
1431. What is `spark.sql.optimizer.dynamicPartitionPruning.enabled`?
1432. What is `spark.sql.optimizer.dynamicPartitionPruning.useStats`?
1433. What is `spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio`?
1434. What is `spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly`?
1435. How does DPP interact with broadcast joins?
1436. Can DPP work without broadcast joins? How?
1437. What is the subquery that DPP creates during optimization?
1438. How do you verify DPP is working in the query plan?
1439. What does "DynamicFileSourceFilter" indicate in the physical plan?
1440. How much performance improvement can DPP provide?
1441. What are the limitations of DPP?
1442. Does DPP work with bucketed tables?
1443. Does DPP work with non-partitioned tables?
1444. How does DPP handle multi-level partitioning?
1445. What is the relationship between DPP and predicate pushdown?
1446. Can DPP be combined with AQE optimizations?
1447. How does DPP affect shuffle in joins?
1448. What statistics are needed for DPP to work effectively?
1449. When should you disable DPP?
1450. How does DPP work with complex join conditions?
1451. What is the difference between DPP and bloom filter join optimization?

## 9. Shuffle & Partitioning

### 9.1 Shuffle Fundamentals
1452. What are shuffle operations in Spark?
1453. Explain shuffle-sort operations in the context of GROUP BY operations.
1454. Explain shuffle-sort operations in the context of JOIN operations.
1455. What is `spark.sql.shuffle.partitions`? What does it control?
1456. What are recommended values for `spark.sql.shuffle.partitions` for different workload sizes?
1457. What is the shuffle buffer and what does shuffle buffer size control?

### 9.2 Partition Tuning
1458. What is partition tuning and why is it crucial for optimizing Spark jobs?
1459. How do you find the right balance between parallelism and shuffle overhead?
1460. What is custom partitioning and when would you implement it?
1461. What is the `reduceByKey` operation and why is it more efficient than `groupByKey`?

### 9.3 Data Skewness
1462. What is data skewness in Spark?
1463. What causes uneven distribution of data across partitions?
1464. If shuffle read and write times are significantly uneven, what does this indicate about data distribution?
1465. What are salting techniques for handling skewed datasets?
1466. How does data skewness affect job performance?

### 9.4 Dynamic Partition Pruning
1467. What is Dynamic Partition Pruning (DPP)?
1468. How do you enable Dynamic Partition Pruning?
1469. What scenarios benefit most from Dynamic Partition Pruning?

## 10. Memory Management & Performance

### 10.1 Memory Architecture & Configuration
1470. Explain Spark's Unified Memory Management model.
1471. What is the difference between execution memory and storage memory?
1472. How does unified memory management allow dynamic borrowing between execution and storage?
1473. What is `spark.memory.fraction` and what is its default value (0.6)?
1474. What does `spark.memory.storageFraction` control and what is its default (0.5)?
1475. How much memory is available for execution vs storage by default in Spark?
1476. What is executor memory overhead and what is it used for?
1477. What is `spark.executor.memoryOverhead` and how is it calculated?
1478. What is the formula for total executor memory allocation?
1479. What is `spark.executor.memory` and how do you set it?
1480. What is `spark.driver.memory` and when should you increase it?
1481. What is the difference between on-heap and off-heap memory?
1482. What is `spark.memory.offHeap.enabled` and when should you enable it?
1483. What is `spark.memory.offHeap.size` and how do you configure it?
1484. What are the benefits of using off-heap memory in Spark?
1485. What is the user memory region in Spark's memory model?
1486. What is reserved memory in Spark and what is it used for?
1487. How is executor memory divided: Reserved + User + Unified Memory?
1488. What percentage of memory is reserved in Spark (300MB typically)?
1489. What is the minimum executor memory required by Spark?
1490. How do you calculate optimal executor memory for your workload?
1491. What is the relationship between executor cores and executor memory?
1492. Why shouldn't you allocate too much memory to a single executor?
1493. What is the recommended executor memory size (8-40GB range)?
1494. How does memory management differ between Spark 1.5 and earlier versions?
1495. What was static memory management in older Spark versions?
1496. What problems did unified memory management solve?

### 10.2 Garbage Collection & JVM Tuning
1497. What is the role of garbage collection in Spark's memory management?
1498. How do you tune garbage collection for Spark jobs?
1499. What is the difference between Young Generation and Old Generation in JVM?
1500. What is Full GC and why is it problematic in Spark?
1501. What is Minor GC and how does it affect Spark performance?
1502. What is `spark.executor.extraJavaOptions` used for?
1503. How do you enable GC logging in Spark?
1504. What are the recommended GC settings for Spark applications?
1505. What is G1GC (Garbage First Garbage Collector)?
1506. Why is G1GC recommended for Spark over traditional GC algorithms?
1507. What is CMS (Concurrent Mark Sweep) GC?
1508. How do you configure G1GC for Spark executors?
1509. What is `-XX:+UseG1GC` flag?
1510. What is `-XX:MaxGCPauseMillis` and what value should you set?
1511. What is `-XX:InitiatingHeapOccupancyPercent` for G1GC?
1512. What is the relationship between GC pauses and Spark task execution?
1513. How do you identify GC issues in Spark UI?
1514. What percentage of task time spent in GC is concerning (>10%)?
1515. What is GC overhead and how does it affect throughput?
1516. How does data serialization reduce GC pressure?
1517. What is the impact of caching on GC activity?
1518. How do you reduce object creation to minimize GC?
1519. What is object pooling and when should you use it in Spark?

### 10.3 Data Spilling & Disk I/O
1520. What is data spilling in Spark?
1521. When and why does data spilling occur?
1522. What are the performance implications of data spilling?
1523. What is shuffle spill and how is it different from storage spill?
1524. What triggers spill to disk during shuffle operations?
1525. What triggers spill to disk during caching operations?
1526. How do you identify spilling in Spark UI?
1527. What metrics indicate spilling: "Spill (Memory)", "Spill (Disk)"?
1528. What is `spark.executor.memory` vs `spark.memory.fraction` in context of spilling?
1529. How does increasing executor memory reduce spilling?
1530. How does increasing `spark.sql.shuffle.partitions` affect spilling?
1531. What is the trade-off between partitions and spilling?
1532. What is `spark.shuffle.spill.compress` and should it be enabled?
1533. What is `spark.shuffle.spill.batchSize` and how does it affect performance?
1534. How does data serialization format affect spilling?
1535. Does Kryo serialization reduce spilling compared to Java serialization?
1536. What is the impact of spilling on network I/O?
1537. What is the impact of spilling on disk I/O?
1538. How do you configure local disk directories for spilling?
1539. What is `spark.local.dir` and why should it point to fast disks (SSDs)?
1540. How does disk speed affect spill performance?
1541. What happens if spill directories run out of space?
1542. How do you prevent spilling in memory-intensive operations?
1543. What operations are most likely to cause spilling?
1544. How does caching affect spilling behavior?

### 10.4 Caching & Persistence Strategies
1545. Does caching happen on worker nodes or executors? Explain the relationship.
1546. When should you cache DataFrames?
1547. What storage levels are available for caching in Spark?
1548. What is MEMORY_ONLY storage level?
1549. What is MEMORY_AND_DISK storage level?
1550. What is MEMORY_ONLY_SER (serialized)?
1551. What is MEMORY_AND_DISK_SER?
1552. What is DISK_ONLY storage level?
1553. What is OFF_HEAP storage level?
1554. What storage levels support replication (_2 suffix)?
1555. When should you use MEMORY_ONLY vs MEMORY_AND_DISK?
1556. What are the trade-offs between deserialized vs serialized caching?
1557. When should you use serialized caching?
1558. How much memory does serialized caching save?
1559. What is the CPU overhead of serialized caching?
1560. What is the difference between `cache()` and `persist()`?
1561. Can you specify storage level with `cache()`?
1562. What is the default storage level for `cache()`?
1563. How do you unpersist cached data?
1564. What happens to cached data when executors fail?
1565. How does caching work with replication factor?
1566. How does caching fit into multi-join query optimization strategies?
1567. Should you cache intermediate results in a DAG?
1568. When is caching counterproductive?
1569. How do you monitor cache usage in Spark UI?
1570. What is cache hit ratio and why is it important?
1571. What is LRU (Least Recently Used) eviction in caching?
1572. What happens when cache memory is full?
1573. How do you size cache memory appropriately?
1574. What is `spark.storage.memoryFraction` in older Spark versions?
1575. How does caching interact with shuffle operations?
1576. Should you cache before or after wide transformations?
1577. How does checkpoint differ from cache?
1578. When should you use checkpoint instead of cache?
1579. What is the relationship between persistence and lineage?

### 10.5 Serialization & Performance
1580. Why is serialization important in Spark?
1581. What serialization types are available (Java, Kryo)?
1582. What is SerDe (Serializer/Deserializer) in Spark's context?
1583. What are the performance differences between Java and Kryo serialization?
1584. How much faster is Kryo compared to Java serialization (2-10x)?
1585. How do you enable Kryo serialization?
1586. What is `spark.serializer` configuration?
1587. What is `spark.kryo.registrationRequired`?
1588. What is `spark.kryo.classesToRegister`?
1589. Why should you register classes with Kryo?
1590. What happens if you don't register classes with Kryo?
1591. What is the cost of class registration in Kryo?
1592. How does serialization affect shuffle performance?
1593. How does serialization affect caching efficiency?
1594. How does serialization affect network transfer?
1595. What data types benefit most from Kryo serialization?
1596. When is Java serialization preferred over Kryo?
1597. What is broadcast serialization and how does it work?
1598. How does serialization affect broadcast join performance?
1599. What is task serialization and when does it occur?
1600. What causes "Task not serializable" errors?
1601. How do you fix task serialization issues?
1602. What objects must be serializable in Spark?
1603. How do you make custom classes serializable?

## 11. Catalyst Optimizer & Query Execution

### 11.1 Catalyst Optimizer Deep Dive
1604. What is the Catalyst Optimizer in Spark?
1605. Explain the Catalyst optimization phases: Analysis → Logical Optimization → Physical Planning → Code Generation.
1606. What happens during the Analysis phase of Catalyst?
1607. What is the unresolved logical plan?
1608. What is the resolved logical plan?
1609. How does Catalyst resolve column names and table references?
1610. What is the Catalog in Spark and how does it help Catalyst?
1611. What happens during the Logical Optimization phase?
1612. What rule-based optimizations does Catalyst apply?
1613. What is predicate pushdown in Catalyst?
1614. What is column pruning in Catalyst?
1615. What is constant folding in Catalyst?
1616. What is null propagation in Catalyst?
1617. What is boolean expression simplification?
1618. What is filter combining/merging in Catalyst?
1619. What is projection collapsing in Catalyst?
1620. What happens during the Physical Planning phase?
1621. How does Catalyst generate multiple physical plans?
1622. What is cost-based optimization (CBO) in Catalyst?
1623. How does Catalyst use cost-based optimization to enhance query performance?
1624. What role do statistics play in cost-based query optimization?
1625. What statistics does Spark collect for CBO?
1626. How do you collect table statistics using `ANALYZE TABLE`?
1627. What is the difference between table statistics and column statistics?
1628. What is `ANALYZE TABLE ... COMPUTE STATISTICS`?
1629. What is `ANALYZE TABLE ... COMPUTE STATISTICS FOR COLUMNS`?
1630. What statistics are collected: row count, data size, column histograms?
1631. How do statistics affect join strategy selection?
1632. How do statistics affect join order selection?
1633. What is the cost model used by Catalyst?
1634. How does Catalyst estimate the cost of different physical plans?
1635. What is the cost of a full table scan vs index scan vs broadcast?
1636. What happens during the Code Generation phase (Whole-Stage Code Generation)?
1637. What is Tungsten's role in code generation?
1638. How does whole-stage code generation improve performance?
1639. What is the benefit of generating Java bytecode at runtime?
1640. What operations support whole-stage code generation?
1641. What is the hand-written code generation approach?
1642. How do you view the generated code for a query?
1643. What is `explain(extended=True)` and what does it show?
1644. What is `explain(mode='formatted')` and its output?
1645. What is `explain(mode='cost')` and what does it reveal?
1646. How do you read the physical plan output?
1647. What does the `*` symbol mean in physical plans (whole-stage codegen)?
1648. What optimization rules can you see in the logical plan?

### 11.2 Tungsten Engine Deep Dive
1649. What is the Tungsten Engine?
1650. How does Tungsten optimize execution through code generation and memory management?
1651. What are the three main components of Tungsten?
1652. What is Tungsten's memory management component?
1653. What is Tungsten's cache-aware computation?
1654. What is Tungsten's code generation (whole-stage codegen)?
1655. How does Tungsten reduce CPU overhead?
1656. What is the Unsafe API in Tungsten?
1657. How does Tungsten use off-heap memory?
1658. What is binary data format in Tungsten?
1659. How does Tungsten eliminate virtual function calls?
1660. How does Tungsten reduce object creation overhead?
1661. What is the performance improvement from Tungsten (2-10x)?
1662. What operations benefit most from Tungsten optimizations?
1663. How do you verify Tungsten is being used in your queries?

### 11.3 Predicate Pushdown Deep Dive
1664. What is predicate pushdown?
1665. How does predicate pushdown reduce data processing?
1666. Can Spark push down filters to all types of data sources (internal and external)?
1667. Categorize which data sources support predicate pushdown and which don't.
1668. Does Parquet support predicate pushdown? How?
1669. Does ORC support predicate pushdown? How?
1670. Does Avro support predicate pushdown?
1671. Does CSV support predicate pushdown?
1672. Does JSON support predicate pushdown?
1673. Do JDBC sources support predicate pushdown?
1674. How does JDBC predicate pushdown work?
1675. What filters can be pushed down to JDBC sources?
1676. What is the benefit of pushing filters to the database in JDBC reads?
1677. Does Hive support predicate pushdown?
1678. Does Delta Lake support predicate pushdown?
1679. How does partition pruning relate to predicate pushdown?
1680. What is the difference between predicate pushdown and partition pruning?
1681. How do you verify predicate pushdown is working?
1682. What does the physical plan show for pushed filters?
1683. What is `PushedFilters` in the physical plan?
1684. Why might some filters not be pushed down?
1685. What types of predicates cannot be pushed down?
1686. How does predicate pushdown work with complex expressions?
1687. Can predicates with UDFs be pushed down?
1688. How does projection pushdown complement predicate pushdown?
1689. What is projection pushdown (column pruning at source)?

### 11.4 Adaptive Query Execution (AQE) Deep Dive
1690. What is Adaptive Query Execution (AQE)?
1691. When was AQE introduced in Spark (3.0)?
1692. What is the main difference between static optimization and adaptive optimization?
1693. What optimizations does AQE enable?
1694. How do you enable AQE?
1695. What is `spark.sql.adaptive.enabled` (default true in Spark 3.2+)?
1696. What are the three main features of AQE?

### 11.4.1 AQE: Dynamic Coalescing of Shuffle Partitions
1697. What is dynamically coalescing shuffle partitions?
1698. What problem does partition coalescing solve?
1699. What is `spark.sql.adaptive.coalescePartitions.enabled`?
1700. What is `spark.sql.adaptive.coalescePartitions.minPartitionSize` (default 1MB)?
1701. What is `spark.sql.adaptive.advisoryPartitionSizeInBytes` (default 64MB)?
1702. How does AQE determine the target partition size?
1703. What is `spark.sql.adaptive.coalescePartitions.initialPartitionNum`?
1704. How does AQE coalesce small partitions after shuffle?
1705. What is the algorithm for partition coalescing?
1706. When does partition coalescing happen in the query execution?
1707. What is the benefit of coalescing partitions (reduced tasks, less overhead)?
1708. How much can AQE reduce the number of tasks in shuffle stages?
1709. How do you verify partition coalescing in Spark UI?
1710. What does "AQE coalesced" indicate in the SQL tab?

### 11.4.2 AQE: Dynamic Join Strategy Switching  
1711. What is dynamically switching join strategies?
1712. When does AQE switch from sort-merge join to broadcast join?
1713. What triggers join strategy conversion at runtime?
1714. What is `spark.sql.adaptive.autoBroadcastJoinThreshold` (default 10MB)?
1715. How is adaptive threshold different from static `spark.sql.autoBroadcastJoinThreshold`?
1716. How does AQE measure actual data size after shuffle?
1717. What happens if shuffle output is smaller than expected?
1718. Can AQE convert both sides of a join if they're small enough?
1719. What is the performance benefit of runtime join strategy switching?
1720. How do you verify join strategy switching in Spark UI?
1721. What does "broadcast join after adaptive planning" indicate?

### 11.4.3 AQE: Skew Join Optimization
1722. What is skew join optimization in AQE?
1723. How does AQE detect skewed partitions?
1724. What is `spark.sql.adaptive.skewJoin.enabled` (default true)?
1725. What is `spark.sql.adaptive.skewJoin.skewedPartitionFactor` (default 5)?
1726. What is `spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes` (default 256MB)?
1727. How does the skew detection algorithm work?
1728. What makes a partition "skewed" in AQE's definition?
1729. How does AQE handle skewed partitions?
1730. What is the partition splitting strategy in skew join?
1731. How many sub-partitions does AQE create from skewed partitions?
1732. What is `spark.sql.adaptive.skewJoin.skewedPartitionMaxSplits`?
1733. How does AQE replicate the non-skewed side in skew join?
1734. What is the broadcast-replicate approach for skew handling?
1735. How does skew join optimization improve performance?
1736. What is the overhead of skew join optimization?
1737. How do you verify skew join optimization in Spark UI?
1738. What does "optimized skewed join" indicate in the plan?
1739. Can AQE handle multiple skewed partitions?
1740. Does AQE work with all join types for skew handling?
1741. What join types benefit from skew optimization (inner, left, right)?

### 11.4.4 AQE: Additional Features & Configuration
1742. What is `spark.sql.adaptive.localShuffleReader.enabled`?
1743. How does local shuffle reader optimization work?
1744. What is the benefit of converting shuffle read to local read?
1745. What is `spark.sql.adaptive.optimizeSkewedJoin.enabled`?
1746. What is `spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin`?
1747. How does AQE interact with DPP (Dynamic Partition Pruning)?
1748. Can AQE and DPP be used together?
1749. What is the execution order: DPP then AQE or AQE then DPP?
1750. What are the prerequisites for AQE to work effectively?
1751. Does AQE work with all query types?
1752. What queries benefit most from AQE?
1753. When might AQE make performance worse?
1754. What is the overhead of enabling AQE?
1755. How does AQE affect query planning time?
1756. How do you debug AQE optimizations?
1757. How do you see AQE decisions in Spark UI?
1758. What does the "Adaptive Spark Plan" section show?
1759. How do you compare original plan vs adaptive plan?
1760. What metrics indicate successful AQE optimizations?

## 12. RDDs (Resilient Distributed Datasets)

### 12.1 RDD Fundamentals
1761. What is an RDD (Resilient Distributed Dataset)?
1762. What makes RDDs resilient?
1763. How are RDDs fault-tolerant?
1764. What is the difference between narrow dependency and wide dependency in RDDs?

### 12.2 RDD vs DataFrame
1765. Compare RDDs and DataFrames in terms of:
   - a) Optimization capabilities
   - b) Type safety
   - c) Performance
   - d) Ease of use
1766. When would you prefer using RDDs over DataFrames/Datasets?

### 12.3 API Hierarchy
1767. How do Spark SQL, Dataset API, DataFrame API, Catalyst Optimizer, and RDD relate to each other?
1768. What is Spark Core and how does it relate to higher-level APIs?

## 13. Table Management & Metastore

### 13.1 Table Types
1769. What is the difference between Spark managed tables and unmanaged (external) tables?
1770. When does Spark delete underlying data for managed vs unmanaged tables?
1771. What is the difference between Spark's in-memory database (per session) and Hive metastore (persistent)?

### 13.2 Warehouse Configuration
1772. What does `spark.sql.warehouse.dir` specify?
1773. What is `sparkSession.enableHiveSupport()` and when do you need it?
1774. Why would you enable Hive support for managed tables?

### 13.3 Catalog Operations
1775. What is the Spark Catalog API?
1776. How do you switch databases using `spark.catalog.setCurrentDatabase()`?
1777. How do you list available tables using `spark.catalog.listTables()`?

### 13.4 Tables vs Files
1778. What are the advantages of using Spark SQL tables vs raw Parquet files for external tools (ODBC/JDBC, Tableau, Power BI)?
1779. What are the advantages of using Spark SQL tables vs raw Parquet files for internal Spark API usage?

## 14. Monitoring & Troubleshooting

### 14.1 Spark UI
1780. How do you explore and navigate the Spark UI for performance analysis?
1781. Is Spark UI only available during active Spark sessions?
1782. How can you preserve execution history and logs using log4j?

### 14.2 Metrics & Accumulators
1783. What are accumulators in Spark?
1784. How are accumulators used for distributed counting and metrics collection?
1785. What metrics indicate performance bottlenecks (skew, spilling, GC time)?

### 14.3 Common Errors & Debugging
1786. What is the "out of memory" error and common causes in Spark?
1787. Why do you get "Task not serializable" errors? How do you fix them?
1788. What causes "Stage X has Y failed attempts" and how do you debug it?
1789. What is data skew and what are the symptoms in Spark UI?
1790. How do you identify and fix shuffle spill to disk issues?
1791. What are best practices for naming columns to avoid conflicts?
1792. How do you handle special characters in column names?
1793. What is the impact of data types on performance (e.g., StringType vs IntegerType)?
1794. Why should you avoid using `count()` multiple times on the same DataFrame?
1795. What happens when you mix transformation logic with actions improperly?

## 15. Advanced Topics

### 15.1 Broadcast Variables & Accumulators
1796. What are broadcast variables in Spark?
1797. When should you use broadcast variables?
1798. How do you create and use a broadcast variable?
1799. What are the limitations and size restrictions of broadcast variables?
1800. What are accumulators in Spark?
1801. How are accumulators used for distributed counting and metrics collection?
1802. What is the difference between accumulators and regular variables?
1803. Can you read accumulator values inside transformations? Why or why not?
1804. What happens to accumulator values if a task fails and retries?

### 15.2 Streaming & Real-Time Processing
1805. How does Spark handle time-series data processing?
1806. What is Spark Streaming?
1807. What capabilities does Spark provide for real-time analytics?

### 15.3 Machine Learning
1808. What machine learning libraries are available in Spark (MLlib)?
1809. What are the key components of Spark MLlib?

### 15.4 Graph Processing
1810. What is GraphX?
1811. What graph processing capabilities does GraphX provide?

### 15.5 Performance Optimization Scenarios
1812. When processing a multi-terabyte dataset, what strategies should be considered to optimize data read and write operations?
1813. Should you cache frequently accessed data in memory for large datasets?
1814. How does Spark optimize read/write operations to HDFS compared to Hadoop MapReduce?

## 16. Big Data Fundamentals

### 16.1 Core Concepts
1815. What are the 3 Vs of Big Data (Volume, Velocity, Variety)? Explain each with examples.
1816. What is a data lake architecture?

## 17. Platform-Specific Topics

### 17.1 AWS Glue
1817. How do AWS Glue Dynamic Frames differ from standard Spark DataFrames?
1818. What are the unique features of Glue Dynamic Frames?

### 17.2 Development Tools
1819. What is the role of Zeppelin notebooks in the Spark ecosystem?
1820. How are Zeppelin notebooks different from Databricks notebooks?

## 18. Miscellaneous Important Topics

### 18.1 DataFrame vs SQL - When to Use What
1821. When should you use DataFrame API vs Spark SQL?
1822. Can you mix DataFrame API and SQL in the same application?
1823. How do you register a DataFrame as a temporary view?
1824. What is the difference between `createTempView()` and `createGlobalTempView()`?
1825. How does performance compare between DataFrame API and SQL?
1826. Are there operations easier to express in SQL vs DataFrame API?

### 18.2 Data Sampling & Debugging
1827. How do you use `sample()` for testing on subset of data?
1828. What does `sample(withReplacement, fraction, seed)` mean?
1829. What is stratified sampling using `sampleBy()`?
1830. How do you use `limit()` for quick data inspection?
1831. What does `show(n, truncate)` do? What are the parameters?
1832. How do you use `printSchema()` for debugging?
1833. What does `explain()` show? How do you read the physical plan?
1834. What does `explain(extended=True)` reveal?

### 18.3 Type Safety & Datasets (Scala/Java)
1835. What is the difference between DataFrame and Dataset in Spark?
1836. What are the advantages of type safety in Datasets?
1837. When would you use Dataset over DataFrame?
1838. What is the performance cost of Datasets vs DataFrames?
1839. How does the encoder work in Datasets?

### 18.4 Miscellaneous Important Questions
1840. What is the relationship between Spark SQL engine, Catalyst optimizer, and Tungsten engine?
1841. How do DataFrame API and RDD API differ in their relationship to SparkSession vs SparkContext?
1842. What is the difference between client libraries (PySpark, Spark Scala, Spark Java, SparkR)?
1843. How does PySpark communicate with JVM (Py4J)?
1844. What are the performance implications of using PySpark vs Scala Spark?
1845. When would you drop down to RDD API from DataFrame API?
1846. How do you convert between RDD and DataFrame?
1847. What is the cost of `collect()` in terms of network and memory?
1848. How do you handle timezone-aware timestamp operations?
1849. What is the difference between `current_timestamp()` and `now()`?
1850. How do you generate surrogate keys in distributed systems?
1851. What is `uuid()` function used for?
1852. How do you handle slowly changing dimensions (SCD) in Spark?
1853. What are best practices for handling PII (Personally Identifiable Information) in Spark?
1854. How do you implement data quality checks in Spark pipelines?

## 19. CROSS-TOPIC SYNTHESIS

1855. Compare: partitionBy folders vs bucketed files vs temp views - which provides physical data organization?
1856. When would you use: saveAsTable with bucketBy vs cache() vs checkpoint()?
1857. What's the relationship between write mode 'overwrite' and DROP TABLE?
1858. Can you use Dynamic Partition Overwrite with bucketBy? How?
1859. How does DataFrameWriterV2 improve write mode atomicity compared to V1?
1860. What combination optimizes joins: bucketBy + broadcast hint or partitionBy + cache?
1861. Should you partition before or after bucketing? Does order matter?
1862. How do temp views interact with cached DataFrames?
1863. What happens when you cache a temp view vs cache the underlying DataFrame?
1864. Can you use 'append' mode with bucketed tables? What happens to bucketing?

## 20. REAL-WORLD SCENARIOS

### 20.1 File Organization Scenarios
1865. Scenario: You have 500 small CSV files (10MB each). Should you use partitionBy or bucketBy when consolidating?
1866. Scenario: Daily data arrives in partitions by date. Should you partition by date, bucket by user_id, or both?
1867. Scenario: You need to join tables on high-cardinality user_id daily. What's the optimal write strategy?
1868. Scenario: Historical data is queried by date ranges (last 7 days, last 30 days). How should you organize it?

### 20.2 Join Optimization Scenarios
1869. Scenario: Two tables (100GB and 5TB) joined daily on user_id. What's your optimization strategy?
1870. Scenario: Fact table joined with 10 dimension tables. Should you create bucketed or temp views?
1871. Scenario: Join performance is slow due to data skew. Should you use salting, bucketing, or both?

### 20.3 Data Ingestion Scenarios
1872. Scenario: HTTP URL reading fails in Spark. What's your 3-step troubleshooting approach?
1873. Scenario: Need to ingest 1000 CSV files from HTTP daily. What's the most efficient approach?
1874. Scenario: Source system provides data via REST API. How do you design Spark ingestion?

### 20.4 Write Operations Scenarios
1875. Scenario: Write mode 'overwrite' accidentally deleted production data. How do you prevent this?
1876. Scenario: Append operation created duplicate records. How do you implement deduplication?
1877. Scenario: Need to update specific partitions without touching others. What write strategy?
1878. Scenario: Write operation failed midway. How do you ensure data consistency?

### 20.5 View & Table Management Scenarios
1879. Scenario: Temp view disappeared after session restart. How do you make it persistent?
1880. Scenario: Complex SQL query with 10 CTEs. Should you use temp views or subqueries?
1881. Scenario: Need to share transformed data across multiple notebooks. Temp view or warehouse table?
1882. Scenario: Exploratory analysis on 5TB table. Should you create temp view or sample data?

### 20.6 Storage Optimization Scenarios
1883. Scenario: JDBC write doesn't support partitionBy. How do you achieve similar organization in the database?
1884. Scenario: Parquet files grew from 128MB to 1GB after adding columns. What's the fix?
1885. Scenario: S3 storage costs are high due to small files. How do you consolidate?
1886. Scenario: Need to delete data older than 90 days efficiently. How do you structure storage?

### 20.7 Performance Troubleshooting Scenarios
1887. Scenario: Bucketed join is not using bucket pruning. What are 5 things to check?
1888. Scenario: Write operation is slow with many small partitions. How do you optimize?
1889. Scenario: Reading partitioned table scans all partitions despite filter. Why?
1890. Scenario: Temp view query is slower than direct table query. What's the issue?

### 20.8 Migration & Evolution Scenarios
1891. Scenario: Need to change partitioning scheme from monthly to daily. What's the migration strategy?
1892. Scenario: Migrating from V1 writer to V2 for Delta Lake. What are the steps?
1893. Scenario: Converting CSV files to Parquet with bucketing. What's the optimal workflow?
1894. Scenario: Need to add new columns to bucketed table. Does bucketing break?

## 21. PERFORMANCE VALIDATION

### 21.1 Verification & Monitoring
1895. How do you verify partitionBy is working correctly in Spark UI?
1896. What metrics indicate successful bucketing in Spark UI?
1897. How do you confirm sortBy ordering in written data files?
1898. How do you verify write mode 'ignore' actually skipped the write operation?
1899. What Spark UI metrics show V2 writer is being used?
1900. How do you measure the performance improvement from bucketing?
1901. How do you validate that Dynamic Partition Overwrite worked correctly?
1902. What logs confirm all executors accessed distributed storage successfully?

### 21.2 Performance Testing
1903. How do you benchmark partitionBy vs no partitioning for your workload?
1904. How do you measure the write performance cost of sortBy?
1905. How do you test if bucketing improves join performance for your data?
1906. How do you compare 'overwrite' vs 'append' mode performance?
1907. How do you measure the overhead of creating temp views?
1908. What is the performance difference between V1 and V2 writer for Delta Lake?

### 21.3 Cost Analysis
1909. How do you calculate storage cost savings from partitionBy?
1910. How do you estimate the cost of over-partitioning (too many small files)?
1911. What is the compute cost of bucketing during write vs savings during read?
1912. How do you measure the ROI of migrating to V2 writer?
