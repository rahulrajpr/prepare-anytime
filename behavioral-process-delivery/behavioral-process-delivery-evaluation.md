# AI Round Preparation Guide - Senior Data Engineer (Condensed)

## 1. Requirement Gathering Skills

### Q1: A stakeholder says they need a "dashboard for sales". What do you ask?

**Approach:**
1. **Business Context:** What problem are you solving? Who uses this? What decisions are blocked?
2. **Data Requirements:** Key metrics? Dimensions? Time granularity? Targets?
3. **Technical Constraints:** Real-time vs batch? Data volume? Source systems?
4. **Success Criteria:** How do we measure success? Timeline? Compliance needs?

**Example:** Real-time event analytics for test centers - discovered they needed automated alerts, not just dashboards, which shaped the entire architecture.

---

### Q2: How do you handle changing requirements?

**Strategy:**
1. **Document baseline** - Separate must-haves from nice-to-haves
2. **Build modular** - Use config files, dbt models that adapt to change
3. **Work iteratively** - MVPs with regular stakeholder check-ins
4. **Manage changes** - Track in Jira, assess impact, communicate trade-offs
5. **Over-communicate** - Keep stakeholders informed, transparent about delays

**Key:** Flexibility in design + transparency in communication

---

### Q3: Converting business needs to technical tasks?

**Process:**
1. Understand the 'why' and business terms
2. Map business terms to data/metrics
3. Design architecture (source ‚Üí processing ‚Üí storage ‚Üí consumption)
4. Work backwards from end goal
5. Create Jira stories with clear acceptance criteria
6. Define dependencies and sequence

**Example Structure:**
- AC1: Calculation accuracy (matches manual validation)
- AC2: Performance (completes within SLA)
- AC3: Edge case handling
- AC4: Data quality checks pass

---

## 2. Agile/Scrum/Jira

### Q4: Breaking down pipelines into Jira tasks?

**Framework:**
1. **Identify pipeline stages** (analyze ‚Üí setup ‚Üí develop ‚Üí test ‚Üí deploy ‚Üí document)
2. **Create Epic and Stories** (one epic per major pipeline)
3. **Break stories into tasks** (each completable in 1-3 days)
4. **Add acceptance criteria** (specific, measurable outcomes)
5. **Map dependencies** (use 'blocks'/'is blocked by')
6. **Estimate and prioritize** (story points, critical first)
7. **Use labels for tracking** (migration, high-priority, needs-review)

---

### Q5: Story point estimation?

**Factors to consider:**
- **Complexity:** Logic difficulty, edge cases
- **Effort:** Amount of coding/configuration
- **Uncertainty:** Unknowns, new tools
- **Dependencies:** External team dependencies

**Scale (Fibonacci):**
- 1-2: Simple tasks (config update, basic query)
- 3-5: Moderate complexity (transformation pipeline)
- 8: Complex (new data source integration)
- 13: Very complex (likely needs breakdown)

**Process:** Use planning poker, discuss assumptions, track velocity over time

---

### Q6: Working in sprints as a Data Engineer?

**Sprint Structure:**
- **Planning:** Commit based on velocity, identify blockers early
- **Daily Standups:** What I did, what I'm doing, blockers (concise)
- **During Sprint:** Update Jira daily, pair program when needed
- **Demo:** Show working pipelines, explain business value
- **Retrospective:** Reflect, suggest improvements

**Data Engineering Adaptations:**
- Account for pipeline run times
- Build in validation time (often > coding time)
- Communicate data dependencies early
- Avoid late-Friday prod deployments

---

### Q7: Communicating blockers?

**Effective Communication:**
1. **Identify early** - Don't wait for standup
2. **Immediate notification** - Slack + update Jira status
3. **Provide context:** What's blocked? Why? What have I tried? Who can unblock? Impact?
4. **Suggest solutions** - Don't just report problems
5. **Follow up** - Track until resolved
6. **Escalate when needed** - If blocking sprint goals

**Example:** "Blocked on Task #123. Need production S3 access. Submitted ticket #456, pending security approval (2 days). Risk: May delay sprint goal. Can we expedite?"

---

## 3. Testing Strategy

### Q8: Testing a data pipeline before deployment?

**Multi-Layer Approach:**

1. **Unit Testing:** Pytest for Python, dbt tests for models
2. **dbt Tests:** unique, not_null, relationships, custom business logic
3. **Integration Testing:** Full pipeline with sample data
4. **Data Diff:** Compare old vs new outputs (row counts, aggregates)
5. **Performance Testing:** Production-scale data validation
6. **Staging Environment:** Mirror production, validate before promotion
7. **Failure Scenarios:** Test edge cases, retries, alerts
8. **Smoke Tests:** Quick post-deployment validation
9. **CI/CD Integration:** Automated tests block bad deployments

---

### Q9: Validating huge data volumes?

**Smart Validation Strategies:**

1. **Sampling:** Stratified samples (1%), validate thoroughly
2. **Checksums:** Compare aggregates (total count, sum, distinct IDs, date ranges)
3. **Partitioned Validation:** Validate by partition (faster, easier debugging)
4. **Distributed Checks:** Use Spark for scale (millions of rows easily)
5. **Comparison Queries:** Old vs new (count mismatches)
6. **Incremental Testing:** Start small (1K ‚Üí 100K ‚Üí 1M ‚Üí full)
7. **Historical Comparison:** Compare to known patterns

**Example:** 50M records tested via 500K sample + partition validation + checksums = sub-second validation

---

### Q10: Automated testing in CI/CD?

**Pipeline Structure:**
```
Commit ‚Üí Lint ‚Üí Unit Tests ‚Üí Integration Tests ‚Üí Deploy Staging ‚Üí E2E Tests ‚Üí Deploy Prod ‚Üí Smoke Tests
```

**Key Components:**
- Version control everything (code, configs, IaC)
- Unit tests on every commit (must pass to merge)
- Data quality checks as code (Great Expectations, dbt tests)
- Infrastructure as Code validation (Terraform plan)
- Containerization (Docker for consistency)
- Automated deployment with rollback capability
- Post-deployment monitoring

---

## 4. Productionization/Deployment/Scaling

### Q11: Making pipelines scalable?

**Key Strategies:**

1. **Distributed Processing:** Spark on EMR for parallel processing
2. **Smart Partitioning:** Query only relevant partitions (5 min ‚Üí sub-second)
3. **Incremental Processing:** Process only new/changed data (60% faster, 40% cost reduction)
4. **Optimize File Formats:** Parquet with compression (70% storage reduction)
5. **Right-Size Resources:** Monitor and tune (CPU, memory, auto-scaling)
6. **Materialized Views:** Cache complex transformations
7. **Avoid Data Skew:** Even partition distribution
8. **Decouple Components:** Each stage scales independently

**Metrics to Track:** Throughput (records/hour), cost per GB, query performance

---

### Q12: Ensuring high availability?

**Reliability Measures:**

1. **Redundancy:** Multi-AZ deployments, managed services
2. **Retry Logic:** Automatic retries with exponential backoff
3. **Idempotency:** Safe to retry (use MERGE/UPSERT)
4. **Monitoring:** Real-time alerts (Datadog, CloudWatch)
5. **Graceful Degradation:** Continue with reduced functionality
6. **Health Checks:** Early failure detection
7. **Automated Failover:** Active-passive configurations
8. **Backfill Capability:** Re-process specific date ranges

**Target:** 99.9% uptime with automated recovery

---

### Q13: Handling 2 AM failures proactively?

**Proactive Measures:**

1. **Automated Alerts:** Slack/email with error details and log links
2. **Automatic Retries:** 3 attempts with exponential backoff (handles 80% of issues)
3. **Runbooks:** Step-by-step resolution guides
4. **On-Call Rotation:** For critical pipelines with PagerDuty escalation
5. **Self-Healing:** Retry on rate limits, queue when systems unavailable
6. **Proactive Monitoring:** Alert on leading indicators (2x normal duration, 50% volume drop)
7. **Backfill Support:** Easy re-run for specific dates
8. **Root Cause Analysis:** Fix underlying issues, update runbooks

---

### Q14: Cost optimization?

**Optimization Tactics:**

1. **Right-Size Resources:** Monitor utilization, reduce waste (40% savings)
2. **Spot Instances:** 70-80% cost savings for non-critical batch jobs
3. **Auto-Scaling:** Scale based on workload (30% savings)
4. **Storage Optimization:** Hot data in Redshift, cold in S3 (60% reduction)
5. **Incremental Processing:** dbt incremental models (60% faster, 40% cheaper)
6. **Query Optimization:** Proper indexing, partition pruning
7. **Off-Peak Scheduling:** Run heavy jobs during cheaper hours
8. **Compression:** Parquet with Snappy (70% storage reduction)
9. **Data Retention:** Archive old data to Glacier
10. **Monitor Costs:** AWS Cost Explorer, budget alerts

**Real Impact:** Reduced monthly costs from $12K to $7K (42%)

---

## 5. Behavioral & Leadership

### Q15: Conflicting stakeholder expectations?

**STAR Approach:**
- **Situation:** Marketing wanted real-time (hourly), Finance wanted auditable daily batch
- **Task:** Reconcile conflicting needs without compromising quality or budget
- **Action:** 
  - Facilitated alignment meeting
  - Proposed hybrid: Real-time dashboard (preliminary) + daily reconciled report (source of truth)
  - Set clear expectations, documented differences
  - Phased delivery (Finance first, then Marketing)
- **Result:** Both satisfied, became template for future projects, on-time and budget

---

### Q16: Production incident under pressure?

**STAR Framework:**
- **Situation:** Friday 3 PM - dashboards showing incorrect revenue before Monday board meeting
- **Task:** Fix, validate, restore confidence before Monday
- **Action:**
  1. Stay calm, triage (15 min)
  2. Isolate issue (30 min) - found dbt model error
  3. Root cause (20 min) - missing filter condition
  4. Fix and test (1 hour) - added filter, validated
  5. Deploy to prod (30 min)
  6. Validate in dashboards (20 min)
  7. Communicate throughout
  8. Post-mortem with corrective actions
- **Result:** Resolved by 6 PM, board meeting successful, improved migration process

---

### Q17: End-to-end project ownership?

**Project Structure:**
1. **Requirements Gathering:** Stakeholder interviews, document pain points
2. **Architecture Design:** End-to-end system design with feedback
3. **Project Planning:** Break into sprints, estimate, timeline
4. **Development:** Build core + mentor junior engineers
5. **Testing:** Integration, performance, UAT with stakeholders
6. **Deployment:** Production release with monitoring
7. **Training & Handoff:** User documentation, technical runbooks
8. **Iteration:** Monitor, gather feedback, enhance

**Example Impact:** Real-time analytics platform delivered in 8 weeks, 30% efficiency improvement

---

### Q18: Handling disagreements?

**Approach:**
1. **Listen actively** - Understand their perspective fully
2. **Stay objective** - Focus on problem, not person
3. **Use data** - Present benchmarks, evidence
4. **Find common ground** - Build from shared goals
5. **Be open to being wrong** - Change position if compelling
6. **Escalate if needed** - Involve neutral decision-maker
7. **Stay professional** - No grudges after decision

**Example:** Spark vs Pandas debate ‚Üí Compromised on Pandas for prototyping, Spark for scale when needed

---

### Q19: Prioritizing when everything is urgent?

**Framework (Eisenhower Matrix):**
- **Urgent + Important:** Do immediately (production bugs)
- **Important not Urgent:** Schedule (refactoring)
- **Urgent not Important:** Delegate/defer
- **Neither:** Eliminate

**Process:**
1. Understand true business impact
2. Consult stakeholders on trade-offs
3. Break large tasks into increments
4. Communicate transparently
5. Focus on high-leverage work
6. Time-box tasks
7. Delegate when possible

**Key:** Not everything is equally urgent - focus on impact

---

## 6. System Design & Architecture

### Q20: Design end-to-end batch data pipeline?

**Architecture:**
```
Multiple Sources ‚Üí Ingestion (Airbyte) ‚Üí Data Lake (S3) ‚Üí Transformation (Spark/dbt) ‚Üí Warehouse (Redshift) ‚Üí Consumption (BI Tools)
```

**Key Components:**
1. **Ingestion:** Airbyte for pre-built connectors, handles CDC
2. **Data Lake:** S3 with raw/staging/curated layers, Parquet format
3. **Transformation:** Spark for heavy lifting, dbt for business logic
4. **Warehouse:** Redshift/Snowflake with star schema
5. **Orchestration:** Airflow with sensors

**Design Principles:**
- Scalability: Distributed processing, partitioning, auto-scaling
- Reliability: Idempotency, retries, DLQ, monitoring
- Data Quality: Schema validation, dbt tests, reconciliation

---

### Q21: Near-real-time CDC pipeline?

**Architecture:**
```
Source DB ‚Üí CDC (Debezium) ‚Üí Kafka ‚Üí Stream Processor (Spark) ‚Üí Target Warehouse
```

**Key Decisions:**
1. **CDC Tool:** Debezium (reads transaction logs, minimal source impact)
2. **Message Queue:** Kafka (durable, scalable, replay)
3. **Processing:** Spark Structured Streaming for complex transformations
4. **Target:** Redshift/Snowflake with MERGE operations
5. **Schema Management:** Schema Registry for compatibility

**Challenges & Solutions:**
- Out-of-order events ‚Üí Watermarks
- Exactly-once ‚Üí Kafka transactions + idempotent writes
- Deletes ‚Üí Soft delete flags
- Large initial load ‚Üí Bulk snapshot first, then CDC

---

### Q22: End-to-end data quality checks?

**Layered Approach:**

1. **Ingestion:** Schema validation, format checks
2. **Transformation:** dbt tests (unique, not_null, relationships, custom)
3. **Post-Load:** Row counts, aggregate validation, freshness
4. **Anomaly Detection:** Volume/value range alerts
5. **Cross-System:** Source-to-target reconciliation
6. **Monitoring:** Quality dashboard, SLA tracking

**Failure Handling:** Quarantine bad records, alert but don't block, manual review for critical issues

---

### Q23: Batch vs Streaming decision?

**Choose Batch:**
- Latency not critical (hours/days acceptable)
- Simpler to build/maintain
- Cost efficiency matters
- Data completeness required
- Complex transformations

**Choose Streaming:**
- Low latency critical (seconds/minutes)
- Event-driven use cases
- Continuous data flow
- Enable real-time actions

**Decision Factors:**

| Factor | Batch | Streaming |
|--------|-------|-----------|
| Latency | Hours-days | Seconds-minutes |
| Complexity | Lower | Higher |
| Cost | Lower | Higher |
| Infrastructure | Simpler | More complex |

**Tip:** Start with batch unless clear business need for real-time

---

## 7. Performance Optimization

### Q24: Optimizing Spark jobs?

**Key Techniques:**

1. **Partitioning:** 2-3 partitions per CPU core
2. **Avoid Shuffle:** Broadcast small tables, pre-partition by join key
3. **Strategic Caching:** Cache reused DataFrames only
4. **Handle Skew:** Salt skewed keys, repartition
5. **Right-Size Resources:** Tune executor memory/cores (save 40%)
6. **File Formats:** Always Parquet with compression
7. **Predicate Pushdown:** Filter early
8. **Avoid UDFs:** Use built-in functions (or Pandas UDF)

**Impact Example:** 90 min ‚Üí 30 min (3x faster)

---

### Q25: Optimizing dbt models?

**Optimization Strategies:**

1. **Incremental Models:** Process only new/changed data (60% faster)
2. **Materialization:** View vs Table vs Incremental (choose wisely)
3. **Limit Scans:** Early WHERE clauses, partition tables
4. **Avoid Expensive Ops:** Minimize window functions, DISTINCT
5. **Warehouse Features:** Sort/dist keys (Redshift), clustering (Snowflake)
6. **Parallel Execution:** Leverage dbt's DAG
7. **Pre-Aggregate:** Store rollups for common queries

**Real Impact:** 2-hour pipeline ‚Üí 40 min (3x improvement), 40% cost reduction

---

### Q26: Handling schema drift?

**Prevention & Detection:**

1. **Schema Validation:** Check at ingestion, fail/quarantine on mismatch
2. **Schema Registry:** Centralized, enforce evolution rules
3. **Defensive Models:** Check column existence in dbt
4. **Automated Alerts:** Monitor schema changes
5. **Data Contracts:** Agreements with upstream teams
6. **Graceful Handling:** New columns ‚Üí NULL, removed ‚Üí alert
7. **Versioned Schemas:** Support multiple API versions
8. **Testing:** Include schema tests in CI/CD

**Key:** Detect early, handle gracefully, communicate effectively

---

### Q27: Fault tolerance design?

**Core Principles:**

1. **Idempotency:** MERGE/UPSERT, CREATE OR REPLACE
2. **Retry Logic:** Automatic with exponential backoff
3. **Checkpointing:** Save progress, resume on failure
4. **Dead Letter Queue:** Quarantine failed records, continue pipeline
5. **Graceful Degradation:** Reduced functionality vs complete failure
6. **Data Partitioning:** Process independently, retry failed partitions
7. **Monitoring:** Immediate failure detection
8. **Backfill Support:** Re-process historical data
9. **Avoid Single Points:** Multi-AZ, managed services
10. **State Management:** Track processed data

---

## 8. Data Modeling

### Q28: SCD Type 1 vs Type 2?

**Type 1 (Overwrite):**
- No history preserved
- Use for: Phone, email (current value sufficient)
- Simple, less storage

**Type 2 (Historical Tracking):**
- Full history with effective/end dates
- Use for: Address, pricing, customer segment
- More storage, complex queries

**Implementation (dbt):**
```sql
-- Add effective_date, end_date, is_current columns
-- Close old records, insert new versions
-- Use surrogate keys
```

**Decision Guide:**
- Current value only ‚Üí Type 1
- Need historical context ‚Üí Type 2
- Often use hybrid (Type 1 for most, Type 2 for critical)

---

### Q29: Star vs Snowflake schema?

**Star Schema (Preferred):**
- Central fact table + denormalized dimensions
- ‚úÖ Simple queries, fast performance, BI-friendly
- Use for: Most analytics use cases

**Snowflake Schema:**
- Normalized dimensions (split into sub-tables)
- ‚úÖ Storage savings, easier updates
- ‚ùå Complex queries, slower
- Use for: Very large dimensions where storage critical

**Practical Approach:** Default to star schema (storage is cheap, simplicity valuable)

**Design Principles:**
- Denormalize dimensions
- Use surrogate keys
- Add date dimension
- Keep facts lean (measures only)

---

## 9. Technical Decisions

### Q30: Trino vs Spark?

**Choose Trino:**
- Interactive, ad-hoc queries (sub-second latency)
- Query data in place (S3, HDFS)
- Federated queries (join across sources)
- SQL-first workloads
- Cost-efficient for queries

**Choose Spark:**
- Batch ETL/transformations
- Complex multi-step processing
- Streaming workloads
- Python/Scala code
- Long-running jobs needing fault tolerance

**Summary:** Trino = query engine, Spark = processing engine. Often use both together.

---

### Q31: dbt vs Spark SQL?

**Choose dbt:**
- SQL-first team
- Modular, reusable transformations
- Built-in testing & documentation
- Version control & CI/CD friendly
- Cost-efficient (runs on warehouse)
- Incremental models easy

**Choose Spark SQL:**
- Very large datasets (100GB+)
- Complex Python logic
- Streaming transformations
- Multi-source joins

**Hybrid Approach (Common):**
- Spark: Heavy lifting, large-scale processing
- dbt: Business logic, final modeling
- Example: Spark cleans/lands in S3 ‚Üí dbt transforms in warehouse

**Decision:** If SQL-based and fits in warehouse ‚Üí dbt. If complex processing/Python needed ‚Üí Spark.

---

## 10. Final Tips

### Interview Preparation:

1. **Practice Out Loud:** Record yourself, refine delivery
2. **Use STAR Format:** Situation, Task, Action, Result
3. **Be Concise:** 2-3 minute answers, let them ask for details
4. **Show Senior Thinking:** Consider business, people, process - not just code
5. **Be Honest:** If you don't know, explain how you'd find out
6. **Ask Clarifying Questions:** Shows critical thinking
7. **Stay Calm:** Demonstrate composure under pressure

### Red Flags to Avoid:

- Blaming others for failures
- Rambling without structure
- Shallow technical depth
- No questions at the end
- Arrogance over confidence

### Questions to Ask Interviewer:

1. "What are the biggest data engineering challenges the team faces?"
2. "How does the team balance technical debt with new features?"
3. "What does success look like in the first 6 months?"
4. "How do you approach data governance and quality?"
5. "What's the team culture and collaboration like?"

### Key Differentiators:

- **End-to-end thinking:** Requirements ‚Üí Production ‚Üí Impact
- **Business focus:** Not just building pipelines, driving outcomes
- **Leadership:** Taking ownership, mentoring, improving processes
- **Maturity:** Handling ambiguity, pressure, conflict professionally

---

**Remember:** You're demonstrating Senior-level capabilities - strategic thinking, ownership, collaboration, and technical depth. Show you can deliver business value, not just write code.

Good luck! üöÄ
